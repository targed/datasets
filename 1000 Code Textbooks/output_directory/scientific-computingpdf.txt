Scientiﬁc Computing
Jeffrey R. Chasnov
HKUST
The Hong Kong University of Science and Technology
Department of Mathematics
Clear Water Bay, Kowloon
Hong Kong
Copyright c○2013-2017 by Jeffrey Robert Chasnov
This work is licensed under the Creative Commons Attribution 3.0 Hong Kong License. To view
a copy of this license, visit http://creativecommons.org/licenses/by/3.0/hk/ or send a letter to
Creative Commons, 171 Second Street, Suite 300, San Francisco, California, 94105, USA.
Preface
What follows are my lecture notes for Math 164: Scientiﬁc Computing , taught at
Harvey Mudd College during Spring 2013 while I was on a one semester sabbatical
leave from the Hong Kong University of Science & Technology. These lecture notes
are based on two courses previously taught by me at HKUST: Introduction to Scientiﬁc
Computation and Introduction to Numerical Methods.
Math 164 at Harvey-Mudd is primarily for Math majors and supposes no previous
knowledge of numerical analysis or methods. This course consists of both numerical
methods and computational physics. The numerical methods content includes stan-
dard topics such as IEEE arithmetic, root ﬁnding, linear algebra, interpolation and
least-squares, integration, differentiation, and differential equations. The physics con-
tent includes nonlinear dynamical systems with the pendulum as a model, and com-
putational ﬂuid dynamics with a focus on the steady two-dimensional ﬂow past either
a rectangle or a circle.
Course work is divided into three parts. In the ﬁrst part, numerical methods are
learned and MATLAB is used to solve various computational math problems. The
second and third parts requires students to work on project assignments in dynamical
systems and in computational ﬂuid dynamics. In the ﬁrst project, students are given
the freedom to choose a dynamical system to study numerically and to write a paper
detailing their work. In the second project, students compute the steady ﬂow past
either a rectangle or a circle. Here, students are given the option to focus on either
the efﬁciency of the numerical methods employed or on the physics of the ﬂow ﬁeld
solutions. Written papers should be less than eight pages in length and composed using
the L ATEX typesetting software.
All web surfers are welcome to download these notes at
http://www.math.ust.hk/~machas/scientiﬁc-computing.pdf
and to use these notes freely for teaching and learning. I welcome any comments,
suggestions or corrections sent by email to jeffrey.chasnov@ust.hk.
iii

Contents
I Numerical methods 1
1 IEEE arithmetic 5
1.1 Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.2 IEEE double precision format . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.3 Machine numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.4 Special numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
1.5 Roundoff error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2 Root ﬁnding 11
2.1 Bisection Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.1.1 Estimatep
2=1.41421 . . . using x0=1 and x1=2. . . . . . . . . . 11
2.2 Newton’s Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
2.2.1 Estimatep
2 using x0=1. . . . . . . . . . . . . . . . . . . . . . . . . 12
2.3 Secant Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
2.3.1 Estimatep
2 using x0=1 and x1=2. . . . . . . . . . . . . . . . . . 13
2.4 A fractal from Newton’s Method . . . . . . . . . . . . . . . . . . . . . . . . 13
2.5 Order of convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.5.1 Newton’s Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.5.2 Secant Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
3 Integration 17
3.1 Elementary formulas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
3.1.1 Midpoint rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
3.1.2 Trapezoidal rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
3.1.3 Simpson’s rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
3.2 Composite rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
3.2.1 Trapezoidal rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
3.2.2 Simpson’s rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
3.3 Adaptive integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
4 Differential equations 23
4.1 Initial value problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
4.1.1 Euler method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
4.1.2 Modiﬁed Euler method . . . . . . . . . . . . . . . . . . . . . . . . . 23
4.1.3 Second-order Runge-Kutta methods . . . . . . . . . . . . . . . . . . 24
4.1.4 Higher-order Runge-Kutta methods . . . . . . . . . . . . . . . . . . 25
4.1.5 Adaptive Runge-Kutta Methods . . . . . . . . . . . . . . . . . . . . 26
4.1.6 System of differential equations . . . . . . . . . . . . . . . . . . . . 27
v
CONTENTS
4.2 Boundary value problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
4.2.1 Shooting method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
5 Linear algebra 31
5.1 Gaussian Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
5.2 LU decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
5.3 Partial pivoting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
5.4 MATLAB programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
6 Finite difference approximation 39
6.1 Finite difference formulas . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
6.2 Example: the Laplace equation . . . . . . . . . . . . . . . . . . . . . . . . . 40
7 Iterative methods 43
7.1 Jacobi, Gauss-Seidel and SOR methods . . . . . . . . . . . . . . . . . . . . 43
7.2 Newton’s method for a system of equations . . . . . . . . . . . . . . . . . 45
8 Interpolation 47
8.1 Piecewise linear interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . 47
8.2 Cubic spline interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
8.3 Multidimensional interpolation . . . . . . . . . . . . . . . . . . . . . . . . . 51
9 Least-squares approximation 53
II Dynamical systems and chaos 55
10 The simple pendulum 59
10.1 Governing equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
10.2 Period of motion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
10.2.1 Analytical solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
10.2.2 Numerical solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
11 The damped, driven pendulum 65
11.1 The linear pendulum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
11.1.1 Damped pendulum . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
11.1.2 Driven pendulum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
11.1.3 Damped, driven pendulum . . . . . . . . . . . . . . . . . . . . . . . 66
11.2 The nonlinear pendulum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
12 Concepts and tools 71
12.1 Fixed points and linear stability analysis . . . . . . . . . . . . . . . . . . . 71
12.2 Bifurcations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
12.2.1 Saddle-node bifurcation . . . . . . . . . . . . . . . . . . . . . . . . . 73
12.2.2 Transcritical bifurcation . . . . . . . . . . . . . . . . . . . . . . . . . 74
12.2.3 Pitchfork bifurcations . . . . . . . . . . . . . . . . . . . . . . . . . . 75
12.2.4 Hopf bifurcations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
12.3 Phase portraits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
vi CONTENTS
CONTENTS
12.4 Limit cycles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
12.5 Attractors and basins of attraction . . . . . . . . . . . . . . . . . . . . . . . 80
12.6 Poincaré sections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
12.7 Fractal dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
12.7.1 Classical fractals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
12.7.2 Correlation Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . 86
13 Pendulum dynamics 89
13.1 Phase portrait of the undriven pendulum . . . . . . . . . . . . . . . . . . . 89
13.2 Basin of attraction of the undriven pendulum . . . . . . . . . . . . . . . . 89
13.3 Spontaneous symmetry-breaking bifurcation . . . . . . . . . . . . . . . . . 90
13.4 Period-doubling bifurcations . . . . . . . . . . . . . . . . . . . . . . . . . . 94
13.5 Period doubling in the logistic map . . . . . . . . . . . . . . . . . . . . . . 97
13.6 Computation of the Feigenbaum constant . . . . . . . . . . . . . . . . . . . 101
13.7 Strange attractor of the chaotic pendulum . . . . . . . . . . . . . . . . . . . 103
III Computational ﬂuid dynamics 107
14 The governing equations 111
14.1 Multi-variable calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
14.1.1 Vector algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
14.2 Continuity equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
14.3 Momentum equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
14.3.1 Material derivative . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
14.3.2 Pressure forces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
14.3.3 Viscous forces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
14.3.4 Navier-Stokes equation . . . . . . . . . . . . . . . . . . . . . . . . . 114
14.3.5 Boundary conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
15 Laminar ﬂow 115
15.1 Plane Couette ﬂow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
15.2 Channel ﬂow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
15.3 Pipe ﬂow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
16 Stream function, vorticity equations 117
16.1 Stream function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
16.2 Vorticity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
16.3 Two-dimensional Navier-Stokes equation . . . . . . . . . . . . . . . . . . . 119
17 Flow past an obstacle 123
17.1 Flow past a rectangle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
17.1.1 Finite difference approximation . . . . . . . . . . . . . . . . . . . . 123
17.1.2 Boundary conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
17.2 Flow past a circle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
17.2.1 Log-polar coordinates . . . . . . . . . . . . . . . . . . . . . . . . . . 128
17.2.2 Finite difference approximation . . . . . . . . . . . . . . . . . . . . 130
17.2.3 Boundary conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
CONTENTS vii
CONTENTS
17.2.4 Solution using Newton’s method . . . . . . . . . . . . . . . . . . . 132
17.3 Visualization of the ﬂow ﬁelds . . . . . . . . . . . . . . . . . . . . . . . . . 135
viii CONTENTS
Part I
Numerical methods
1

The ﬁrst part of this course consists of a concise introduction to numerical methods.
We begin by learning how numbers are represented in the computer using the IEEE
standard, and how this can result in round-off errors in numerical computations. We
will then learn some fundamental numerical methods and their associated MATLAB
functions. The numerical methods included are those used for root ﬁnding, integration,
solving differential equations, solving systems of equations, ﬁnite difference methods,
and interpolation.
3
4
Chapter 1
IEEE arithmetic
1.1 Deﬁnitions
The real line is continuous, while computer numbers are discrete. We learn here how
numbers are represented in the computer, and how this can lead to round-off errors.
A number can be represented in the computer as a signed or unsigned integer, or as a
real (ﬂoating point) number. We introduce the following deﬁnitions:
Bit = 0 or 1
Byte = 8 bits
Word = Reals: 4 bytes (single precision)
8 bytes (double precision)
= Integers: 1, 2, 4, or 8 byte signed
1, 2, 4, or 8 byte unsigned
Typically, scientiﬁc computing in MATLAB is in double precision using 8-byte real
numbers. Single precision may be used infrequently in large problems to conserve
memory. Integers may also be used infrequently in special situations. Since double
precision is the default—and what will be used in this class—we will focus here on its
representation.
1.2 IEEE double precision format
Double precision makes use of 8 byte words (64 bits), and usually results in sufﬁciently
accurate computations. The format for a double precision number is
sz}|{

0ez}|{

1
2
3
4
5
6
7
8
9
10
11fz}|{

12 
63
#= ( 1)s2e 10231.f,
where s is the sign bit, e is the biased exponent, and 1.f (using a binary point) is
the signiﬁcand. We see that the 64 bits are distributed so that the sign uses 1-bit,
the exponent uses 11-bits, and the signiﬁcand uses 52-bits. The distribution of bits
between the exponent and the signiﬁcand is to reconcile two conﬂicting desires: that
the numbers should range from very large to very small values, and that the relative
spacing between numbers should be small.
5
1.3. MACHINE NUMBERS
1.3 Machine numbers
We show for illustration how the numbers 1, 2, and 1/2 are represented in double
precision. For the number 1, we write
1.0= ( 1)02(1023 1023)1.0 . (1.1)
From (1.1), we ﬁnd s =0, e=1023, and f =0. Now
1023 =011 1111 1111 (base 2 ),
so that the machine representation of the number 1 is given by
0011 1111 1111 0000 . . . 0000 . (1.2)
One can view the machine representation of numbers in MATLAB using the format hex
command. MATLAB then displays the hex number corresponding to the binary ma-
chine number. Hex maps four bit binary numbers to a single character, where the
binary numbers corresponding to the decimal 0–9 are mapped to the same decimal
numbers, and the binary numbers corresponding to 10–15 are mapped to a–f. The hex
representation of the number 1 given by (1.2) is therefore given by
3ff0 0000 0000 0000.
For the number 2, we have
2.0= ( 1)02(1024 1023)1.0,
with binary representation
0100 0000 0000 0000 . . . 0000,
and hex representation
4000 0000 0000 0000.
For the number 1/2, we have
2.0= ( 1)02(1022 1023)1.0,
with binary representation
0011 1111 1110 0000 . . . 0000,
and hex representation
4fe0 0000 0000 0000.
The numbers 1, 2 and 1/2 can be represented exactly in double precision. But very
large integers, and most real numbers can not. For example, the number 1/3 is inexact,
and so is 1/5, which we consider here. We write
1
5= ( 1)01
8(1+3
5),
= ( 1)021020 1023(1+3
5),
6 CHAPTER 1. IEEE ARITHMETIC
1.3. MACHINE NUMBERS
so that s =0, e=1020 =011 1111 1100 (base 2 ), and f =3/5. The reason 1/5 is inexact
is that 3/5 does not have a ﬁnite representation in binary. To convert 3/5 to binary, we
multiply successively by 2 as follows:
0.6 . . . 0.
1.2 . . . 0.1
0.4 . . . 0.10
0.8 . . . 0.100
1.6 . . . 0.1001
etc.
so that 3/5 exactly in binary is 0. 1001, where the bar denotes an endless repetition.
With only 52 bits to represent f, the fraction 3/5 is inexact and we have
f=1001 1001 . . . 1001 1010,
where we have rounded the repetitive end of the number 1001 to the nearest binary
number 1010. Here the rounding is up because the 53-bit is a 1 followed by not all
zeros. But rounding can also be down if the 53 bit is a 0. Exactly on the boundary
(the 53-bit is a 1 followed by all zeros), rounding is to the nearest even number. In this
special situation that occurs only rarely, if the 52-bit is a 0, then rounding is down, and
if the 52 bit is a 1, then rounding is up.
The machine number 1/5 is therefore represented as
0011 1111 1100 1001 1001 . . . 1001 1010,
or in hex,
3fc999999999999a .
The small error in representing a number such as 1/5 in double precision usually makes
little difference in a computation. One is usually satisﬁed to obtain results accurate to
a few signiﬁcant digits. Nevertheless, computational scientists need to be aware that
most numbers are not represented exactly.
For example, consider subtracting what should be two identical real numbers that
are not identical in the computer. In MATLAB, if one enters on the command line
5*1/5 5*(1/5)
the resulting answer is 0, as one would expect. But if one enters
5^2*1/5^2 5^2*(1/5)^2
the resulting answer is  2.2204e 16, a number slightly different than zero. The rea-
son for this error is that although the number 521/52=25/25 =1 can be represented
exactly, the number 52(1/5)2is inexact and slightly greater than one. Testing for ex-
actly zero in a calculation that depends on the cancelation of real numbers, then, may
not work. More problematic, though, are calculations that subtract two large numbers
with the hope of obtaining a sensible small number. A total loss of precision of the
small number may occur.
CHAPTER 1. IEEE ARITHMETIC 7
1.4. SPECIAL NUMBERS
1.4 Special numbers
Both the largest and smallest exponent are reserved in IEEE arithmetic. When f =0,
the largest exponent, e =111 1111 1111, is used to represent ¥(written in MATLAB
asInf and Inf). When f6=0, the largest exponent is used to represent ‘not a
number’ (written in MATLAB as NaN). IEEE arithmetic also implements what is called
denormal numbers, also called graceful underﬂow. It reserves the smallest exponent,
e=000 0000 0000, to represent numbers for which the representation changes from 1.f
to 0.f.
With the largest exponent reserved, the largest positive double precision number
has s =0, e=111 1111 1110 =2046, and f =1111 1111 . . . 1111 =1 2 52. Called
realmax in MATLAB, we have
realmax = 1.7977e+308 .
With the smallest exponent reserved, the smallest positive double precision number
has s =0, e=000 0000 0001 =1, and f =0000 0000 . . . 0000 =0. Called realmin in
MATLAB, we have
realmin = 2.2251e  308 .
Above realmax , one obtains Inf, and below realmin , one obtains ﬁrst denormal
numbers and then 0.
Another important number is called machine epsilon (called eps in MATLAB). Ma-
chine epsilon is deﬁned as the distance between 1 and the next largest number. If
0d<eps/2, then 1 +d=1 in computer math. Also since
x+y=x(1+y/x),
if 0y/x<eps/2, then x+y=xin computer math.
Now, the number 1 in double precision IEEE format is written as
1=201.000 . . . 0,
with 52 0’s following the binary point. The number just larger than 1 has a 1 in the
52nd position after the decimal point. Therefore,
eps=2 522.2204 e 016.
What is the distance between 1 and the number just smaller than 1? Here, the
number just smaller than one can be written as
2 11.111 . . . 1 =2 1(1+ (1 2 52)) = 1 2 53
Therefore, this distance is 2 53=eps/2.
Here, we observe that the spacing between numbers is uniform between powers of
2, but changes by a factor of two with each additional power of two. For example, the
spacing of numbers between 1 and 2 is 2 52, between 2 and 4 is 2 51, between 4 and
8 is 2 50, etc. An exception occurs for denormal numbers, where the spacing becomes
uniform all the way down to zero. Denormal numbers implement a graceful underﬂow,
and are not to be used for normal computation.
8 CHAPTER 1. IEEE ARITHMETIC
1.5. ROUNDOFF ERROR
1.5 Roundoff error
Consider solving the quadratic equation
x2+2bx 1=0,
where bis a parameter. The quadratic formula yields the two solutions
x= bp
b2+1.
Now, consider the solution with b>0 and x>0 (the x+solution) given by
x= b+p
b2+1. (1.3)
Asb!¥,
x= b+p
b2+1
= b+bp
1+1/b2
=b(p
1+1/b2 1)
b
1+1
2b2 1
=1
2b.
Now in double precision, realmin2.210 308and in professional software one
would like xto be accurate to this value before it goes to 0 via denormal numbers.
Therefore, xshould be computed accurately to b1/(2realmin )210307. What
happens if we compute (1.3) directly? Then x=0 when b2+1=b2, or 1 +1/b2=1.
Therefore, x=0 when 1/ b2<eps/2, or b>p
2/eps108.
The way that a professional mathematical software designer solves this problem is
to compute the solution for xwhen b>0 as
x=1
b+p
b2+1.
In this form, when b2+1=b2, then x=1/2b, which is the correct asymptotic form.
CHAPTER 1. IEEE ARITHMETIC 9
1.5. ROUNDOFF ERROR
10 CHAPTER 1. IEEE ARITHMETIC
Chapter 2
Root ﬁnding
The problem is to solve f(x) =0 for xwhen an explicit analytical solution is impos-
sible. All methods compute a sequence x0,x1,x2, . . . that converges to the root x=r
satisfying f(r) =0.
2.1 Bisection Method
The bisection method is conceptually the simplest method and almost always works.
However, it is also the slowest method, and most of the time should be avoided.
We ﬁrst choose x0and x1such that x0<r<x1. We say that x0and x1bracket
the root. With f(r) = 0, we want f(x0)and f(x1)to be of opposite sign, so that
f(x0)f(x1)<0. We then assign x2to be the midpoint of x0and x1, that is x2=
(x1+x0)/2, or
x2=x1 x1 x0
2.
The sign of f(x2)is then determined, and the value of x3is chosen as either the mid-
point of x2and x0or as the midpoint of x2and x1, depending on whether x2and x0
bracket the root, or x2and x1bracket the root. The root, therefore, stays bracketed at
all times. The algorithm proceeds in this fashion and is typically stopped when the
absolute value of the increment to the last best approximation to the root (above, given
byjx1 x0j/2) is smaller than some required precision.
2.1.1 Estimatep
2=1.41421 . . . using x0=1and x1=2.
Nowp
2 is the zero of the function f(x) = x2 2. We iterate with x0=1 and x1=2.
We have
x2=2 2 1
2=3
2=1.5.
Now, f(x2) =9/4 2=1/4>0 so that x2and x0bracket the root. Therefore,
x3=3
2 3
2 1
2=5
4=1.25.
Now, f(x3) =25/16 2= 7/16<0 so that x3and x2bracket the root. Therefore,
x4=5
4 5
4 3
2
2=11
8=1.375,
and so on.
11
2.2. NEWTON’S METHOD
2.2 Newton’s Method
This is the fastest method, but requires analytical computation of the derivative of f(x).
If the derivative is known, then this method should be used, although convergence to
the desired root is not guaranteed.
We can derive Newton’s Method from a Taylor series expansion. We again want to
construct a sequence x0,x1,x2, . . . that converges to the root x=r. Consider the xn+1
member of this sequence, and Taylor series expand f(xn+1)about the point xn. We
have
f(xn+1) = f(xn) + ( xn+1 xn)f0(xn) +. . . .
To determine xn+1, we drop the higher-order terms in the Taylor series, and assume
f(xn+1) =0. Solving for xn+1, we have
xn+1=xn f(xn)
f0(xn).
Starting Newton’s Method requires a guess for x0, to be chosen as close as possible to
the root x=r.
2.2.1 Estimatep
2using x0=1.
Again, we solve f(x) = 0, where f(x) = x2 2. To implement Newton’s Method, we
use f0(x) =2x. Therefore, Newton’s Method is the iteration
xn+1=xn x2
n 2
2xn.
With our initial guess x0=1, we have
x1=1  1
2=3
2=1.5,
x2=3
2 9
4 2
3=17
12=1.416667,
x3=17
12 172
122 2
17
6=577
408=1.41426.
2.3 Secant Method
The Secant Method is second fastest to Newton’s Method, and is most often used when
it is not possible to take an analytical derivative of the function f(x). We write in place
off0(xn),
f0(xn)f(xn) f(xn 1)
xn xn 1.
Starting the Secant Method requires a guess for both x0and x1. These values need not
bracket the root, but convergence is not guaranteed.
12 CHAPTER 2. ROOT FINDING
2.4. A FRACTAL FROM NEWTON’S METHOD
2.3.1 Estimatep
2using x0=1and x1=2.
Again, we solve f(x) =0, where f(x) =x2 2. The Secant Method iterates
xn+1=xn (x2
n 2)(xn xn 1)
x2n x2
n 1
=xn x2
n 2
xn+xn 1.
With x0=1 and x1=2, we have
x2=2 4 2
3=4
3=1.33333,
x3=4
3 16
9 2
4
3+2=21
15=1.4,
x4=21
15 
21
152
 2
21
15+4
3=174
123=1.41463.
2.4 A fractal from Newton’s Method
Consider the complex roots of the equation f(z) =0, where
f(z) =z3 1.
These roots are the three cubic roots of unity. With
ei2pn=1, n=0, 1, 2, . . .
the three unique cubic roots of unity are given by
1, ei2p/3,ei4p/3.
With
eiq=cosq+isinq,
and cos (2p/3) = 1/2, sin (2p/3) =p
3/2, the three cubic roots of unity are
r1=1, r2= 1
2+p
3
2i,r3= 1
2 p
3
2i.
The interesting idea here is to determine in the complex plane which initial values of
z0converge using Newton’s method to which of the three cube roots of unity.
Newton’s method generalized to the complex plane is
zn+1=zn z3
n 1
3z2n.
If the iteration converges to r1, we color z0red; r2, blue; r3, green. The result will be
shown in lecture.
CHAPTER 2. ROOT FINDING 13
2.5. ORDER OF CONVERGENCE
2.5 Order of convergence
Letrbe the root and xnbe the nth approximation to the root. Deﬁne the error as
en=r xn.
If for large nwe have the approximate relationship
jen+1j=kjenjp,
with ka positive constant and p1, then we say the root-ﬁnding numerical method is
of order p. Larger values of pcorrespond to faster convergence to the root. The order
of convergence of bisection is one: the error is reduced by approximately a factor of 2
with each iteration so that
jen+1j=1
2jenj.
We now ﬁnd the order of convergence for Newton’s Method and for the Secant Method.
2.5.1 Newton’s Method
We start with Newton’s Method
xn+1=xn f(xn)
f0(xn),
where the root rsatisﬁes f(r) =0. Subtracting both sides from r, we have
r xn+1=r xn+f(xn)
f0(xn),
or
en+1=en+f(xn)
f0(xn). (2.1)
We use Taylor series to expand the functions f(xn)and f0(xn)about the root r, using
f(r) =0. We have
f(xn) = f(r) + ( xn r)f0(r) +1
2(xn r)2f00(r) +. . . ,
= enf0(r) +1
2e2
nf00(r) +. . . ;
f0(xn) = f0(r) + ( xn r)f00(r) +1
2(xn r)2f000(r) +. . . ,
=f0(r) enf00(r) +1
2e2
nf000(r) +. . . .(2.2)
To make further progress, we will make use of the following standard Taylor series:
1
1 e=1+e+e2+. . . , (2.3)
14 CHAPTER 2. ROOT FINDING
2.5. ORDER OF CONVERGENCE
which converges for jej<1. Substituting (2.2) into (2.1), and using (2.3) yields
en+1=en+f(xn)
f0(xn)
=en+ enf0(r) +1
2e2
nf00(r) +. . .
f0(r) enf00(r) +1
2e2nf000(r) +. . .
=en+ en+1
2e2
nf00(r)
f0(r)+. . .
1 enf00(r)
f0(r)+. . .
=en+
 en+1
2e2
nf00(r)
f0(r)+. . .
1+enf00(r)
f0(r)+. . .
=en+
 en+e2
n1
2f00(r)
f0(r) f00(r)
f0(r)
+. . .
= 1
2f00(r)
f0(r)e2
n+. . .
Therefore, we have shown that
jen+1j=kjenj2
asn!¥, with
k=1
2f00(r)
f0(r),
provided f0(r)6=0. Newton’s method is thus of order 2 at simple roots.
2.5.2 Secant Method
Determining the order of the Secant Method proceeds in a similar fashion. We start
with
xn+1=xn (xn xn 1)f(xn)
f(xn) f(xn 1).
We subtract both sides from rand make use of
xn xn 1= (r xn 1) (r xn)
=en 1 en,
and the Taylor series
f(xn) = enf0(r) +1
2e2
nf00(r) +. . . ,
f(xn 1) = en 1f0(r) +1
2e2
n 1f00(r) +. . . ,
so that
f(xn) f(xn 1) = ( en 1 en)f0(r) +1
2(e2
n e2
n 1)f00(r) +. . .
= (en 1 en)
f0(r) 1
2(en 1+en)f00(r) +. . .
.
CHAPTER 2. ROOT FINDING 15
2.5. ORDER OF CONVERGENCE
We therefore have
en+1=en+ enf0(r) +1
2e2
nf00(r) +. . .
f0(r) 1
2(en 1+en)f00(r) +. . .
=en en1 1
2enf00(r)
f0(r)+. . .
1 1
2(en 1+en)f00(r)
f0(r)+. . .
=en en
1 1
2enf00(r)
f0(r)+. . .
1+1
2(en 1+en)f00(r)
f0(r)+. . .
= 1
2f00(r)
f0(r)en 1en+. . . ,
or to leading order
jen+1j=1
2f00(r)
f0(r)jen 1jjenj. (2.4)
The order of convergence is not yet obvious from this equation, but should be less than
quadratic because jen 1j>jenj. To determine the scaling law we look for a solution of
the form
jen+1j=kjenjp.
From this ansatz, we also have
jenj=kjen 1jp,
and therefore
jen+1j=kp+1jen 1jp2.
Substitution into (2.4) results in
kp+1jen 1jp2=k
2f00(r)
f0(r)jen 1jp+1.
Equating the coefﬁcient and the power of en 1results in
kp=1
2f00(r)
f0(r),
and
p2=p+1.
The order of convergence of the Secant Method, given by p, is therefore the positive
root of the quadratic equation p2 p 1=0, or
p=1+p
5
21.618,
which coincidentally is a famous irrational number that is called The Golden Ratio,
and goes by the symbol F. We see that the Secant Method has an order of convergence
lying between the Bisection Method and Newton’s Method.
16 CHAPTER 2. ROOT FINDING
Chapter 3
Integration
We want to construct numerical algorithms that can perform deﬁnite integrals of
the form
I=Zb
af(x)dx. (3.1)
Calculating these deﬁnite integrals numerically is called numerical integration ,numerical
quadrature , or more simply quadrature .
3.1 Elementary formulas
We ﬁrst consider integration from 0 to h, with hsmall, to serve as the building blocks
for integration over larger domains. We here deﬁne Ihas the following integral:
Ih=Zh
0f(x)dx. (3.2)
To perform this integral, we consider a Taylor series expansion of f(x)about the value
x=h/2:
f(x) = f(h/2) + ( x h/2)f0(h/2) +(x h/2)2
2f00(h/2)
+(x h/2)3
6f000(h/2) +(x h/2)4
24f0000(h/2) +. . .
3.1.1 Midpoint rule
The midpoint rule makes use of only the ﬁrst term in the Taylor series expansion. Here,
we will determine the error in this approximation. Integrating,
Ih=h f(h/2) +Zh
0
(x h/2)f0(h/2) +(x h/2)2
2f00(h/2)
+(x h/2)3
6f000(h/2) +(x h/2)4
24f0000(h/2) +. . .
dx.
Changing variables by letting y=x h/2 and dy=dx, and simplifying the integral
depending on whether the integrand is even or odd, we have
Ih=h f(h/2)
+Zh/2
 h/2
y f0(h/2) +y2
2f00(h/2) +y3
6f000(h/2) +y4
24f0000(h/2) +. . .
dy
=h f(h/2) +Zh/2
0
y2f00(h/2) +y4
12f0000(h/2) +. . .
dy.
17
3.1. ELEMENTARY FORMULAS
The integrals that we need here are
Zh
2
0y2dy=h3
24,Zh
2
0y4dy=h5
160.
Therefore,
Ih=h f(h/2) +h3
24f00(h/2) +h5
1920f0000(h/2) +. . . . (3.3)
3.1.2 Trapezoidal rule
From the Taylor series expansion of f(x)about x=h/2, we have
f(0) = f(h/2) h
2f0(h/2) +h2
8f00(h/2) h3
48f000(h/2) +h4
384f0000(h/2) +. . . ,
and
f(h) = f(h/2) +h
2f0(h/2) +h2
8f00(h/2) +h3
48f000(h/2) +h4
384f0000(h/2) +. . . .
Adding and multiplying by h/2 we obtain
h
2 
f(0) +f(h)=h f(h/2) +h3
8f00(h/2) +h5
384f0000(h/2) +. . . .
We now substitute for the ﬁrst term on the right-hand-side using the midpoint rule
formula:
h
2 
f(0) +f(h)=
Ih h3
24f00(h/2) h5
1920f0000(h/2)
+h3
8f00(h/2) +h5
384f0000(h/2) +. . . ,
and solving for Ih, we ﬁnd
Ih=h
2 
f(0) +f(h) h3
12f00(h/2) h5
480f0000(h/2) +. . . . (3.4)
3.1.3 Simpson’s rule
To obtain Simpson’s rule, we combine the midpoint and trapezoidal rule to eliminate
the error term proportional to h3. Multiplying (3.3) by two and adding to (3.4), we
obtain
3Ih=h
2f(h/2) +1
2(f(0) +f(h))
+h52
1920 1
480
f0000(h/2) +. . . ,
or
Ih=h
6 
f(0) +4f(h/2) +f(h) h5
2880f0000(h/2) +. . . .
Usually, Simpson’s rule is written by considering the three consecutive points 0, hand
2h. Substituting h!2h, we obtain the standard result
I2h=h
3 
f(0) +4f(h) +f(2h) h5
90f0000(h) +. . . . (3.5)
18 CHAPTER 3. INTEGRATION
3.2. COMPOSITE RULES
3.2 Composite rules
We now use our elementary formulas obtained for (3.2) to perform the integral given
by (3.1).
3.2.1 Trapezoidal rule
We suppose that the function f(x)is known at the n+1 points labeled as x0,x1, . . . , xn,
with the endpoints given by x0=aand xn=b. Deﬁne
fi=f(xi),hi=xi+1 xi.
Then the integral of (3.1) may be decomposed as
Zb
af(x)dx=n 1
å
i=0Zxi+1
xif(x)dx
=n 1
å
i=0Zhi
0f(xi+s)ds,
where the last equality arises from the change-of-variables s=x xi. Applying the
trapezoidal rule to the integral, we have
Zb
af(x)dx=1
2n 1
å
i=0hi(fi+fi+1). (3.6)
If the points are not evenly spaced, say because the data are experimental values, then
thehimay differ for each value of iand (3.6) is to be used directly.
However, if the points are evenly spaced, say because f(x)can be computed, we
have hi=h, independent of i. We can then deﬁne
xi=a+ih,i=0, 1, . . . , n;
and since the end point bsatisﬁes b=a+nh, we have
h=b a
n.
The composite trapezoidal rule for evenly space points then becomes
Zb
af(x)dx=h
2n 1
å
i=0(fi+fi+1)
=h
2(f0+2f1++2fn 1+fn). (3.7)
The ﬁrst and last terms have a multiple of one; all other terms have a multiple of two;
and the entire sum is multiplied by h/2.
CHAPTER 3. INTEGRATION 19
3.3. ADAPTIVE INTEGRATION
a d c e b
Figure 3.1: Adaptive Simpson quadrature: Level 1.
3.2.2 Simpson’s rule
We here consider the composite Simpson’s rule for evenly space points. We apply
Simpson’s rule over intervals of 2 h, starting from aand ending at b:
Zb
af(x)dx=h
3(f0+4f1+f2)+h
3(f2+4f3+f4)+. . .
+h
3(fn 2+4fn 1+fn).
Note that nmust be even for this scheme to work. Combining terms, we have
Zb
af(x)dx=h
3(f0+4f1+2f2+4f3+2f4++4fn 1+fn).
The ﬁrst and last terms have a multiple of one; the even indexed terms have a multiple
of 2; the odd indexed terms have a multiple of 4; and the entire sum is multiplied by
h/3.
3.3 Adaptive integration
The useful MATLAB function quad.m performs numerical integration using adaptive
Simpson quadrature. The idea is to let the computation itself decide on the grid size
required to achieve a certain level of accuracy. Moreover, the grid size need not be the
same over the entire region of integration.
We begin the adaptive integration at what is called Level 1. The uniformly spaced
points at which the function f(x)is to be evaluated are shown in Fig. 3.1. The distance
between the points aand bis taken to be 2 h, so that
h=b a
2.
Integration using Simpson’s rule (3.5) with grid size hyields for the integral I,
I=h
3 
f(a) +4f(c) +f(b) h5
90f0000(x),
where xis some value satisfying axb.
20 CHAPTER 3. INTEGRATION
3.3. ADAPTIVE INTEGRATION
Integration using Simpson’s rule twice with grid size h/2 yields
I=h
6 
f(a) +4f(d) +2f(c) +4f(e) +f(b) (h/2)5
90f0000(xl) (h/2)5
90f0000(xr),
with xlandxrsome values satisfying axlcand cxrb.
We now deﬁne the two approximations to the integral by
S1=h
3 
f(a) +4f(c) +f(b)
,
S2=h
6 
f(a) +4f(d) +2f(c) +4f(e) +f(b)
,
and the two associated errors by
E1= h5
90f0000(x),
E2= h5
2590 
f0000(xl) +f0000(xr)
.
We now ask whether the value of S2for the integral is accurate enough, or must we
further reﬁne the calculation and go to Level 2? To answer this question, we make the
simplifying approximation that all of the fourth-order derivatives of f(x)in the error
terms are equal; that is,
f0000(x) = f0000(xl) = f0000(xr) =C.
Then
E1= h5
90C,
E2= h5
2490C=1
16E1.
Now since the integral is equal to the approximation plus its associated error,
S1+E1=S2+E2,
and since
E1=16E2,
we can derive an estimate for the error term E2:
E2=1
15(S2 S1).
Therefore, given some speciﬁc value of the tolerance tol, if
1
15(S2 S1)<tol,
then we can accept S2asI. If the error estimate is larger in magnitude than tol, then
we proceed to Level 2.
CHAPTER 3. INTEGRATION 21
3.3. ADAPTIVE INTEGRATION
The computation at Level 2 further divides the integration interval from atobinto
the two integration intervals atocand ctob, and proceeds with the above procedure
independently on both halves. Integration can be stopped on either half provided
the tolerance is less than tol/2 (since the sum of both errors must be less than tol).
Otherwise, either half can proceed to Level 3, and so on.
As a side note, the two values of Igiven above (for integration with step size hand
h/2) can be combined to give a more accurate value for I given by
I=16S2 S1
15+O(h7),
where the error terms of O (h5)approximately cancel. This free lunch, so to speak, is
called Richardson’s extrapolation.
22 CHAPTER 3. INTEGRATION
Chapter 4
Differential equations
We now discuss the numerical solution of ordinary differential equations. We will
include the initial value problem and the boundary value problem.
4.1 Initial value problem
We begin with the simple Euler method, then discuss the more sophisticated Runge-
Kutta methods, and conclude with the Runge-Kutta-Fehlberg method, as implemented
in the MATLAB function ode45.m . Our differential equations are for x=x(t), where
the time tis the independent variable.
4.1.1 Euler method
The Euler method is the most straightforward method to integrate a differential equa-
tion. Consider the ﬁrst-order differential equation
˙x=f(t,x), (4.1)
with the initial condition x(0) = x0. Deﬁne tn=nDtand xn=x(tn). A Taylor series
expansion of xn+1results in
xn+1=x(tn+Dt)
=x(tn) +Dt˙x(tn) +O(Dt2)
=x(tn) +Dt f(tn,xn) +O(Dt2).
The Euler Method is therefore written as
xn+1=x(tn) +Dt f(tn,xn).
We say that the Euler method steps forward in time using a time-step Dt, starting from
the initial value x0=x(0). The local error of the Euler Method is O (Dt2). The global
error, however, incurred when integrating to a time T, is a factor of 1/ Dtlarger and is
given by O (Dt). It is therefore customary to call the Euler Method a ﬁrst-order method .
4.1.2 Modiﬁed Euler method
This method is a so-called predictor-corrector method. It is also the ﬁrst of what we
will see are Runge-Kutta methods. As before, we want to solve (4.1). The idea is to
average the value of ˙xat the beginning and end of the time step. That is, we would like
to modify the Euler method and write
xn+1=xn+1
2Dt 
f(tn,xn) +f(tn+Dt,xn+1)
.
23
4.1. INITIAL VALUE PROBLEM
The obvious problem with this formula is that the unknown value xn+1appears on the
right-hand-side. We can, however, estimate this value, in what is called the predictor
step. For the predictor step, we use the Euler method to ﬁnd
xp
n+1=xn+Dt f(tn,xn).
The corrector step then becomes
xn+1=xn+1
2Dt 
f(tn,xn) +f(tn+Dt,xp
n+1)
.
The Modiﬁed Euler Method can be rewritten in the following form that we will later
identify as a Runge-Kutta method:
k1=Dt f(tn,xn),
k2=Dt f(tn+Dt,xn+k1),
xn+1=xn+1
2(k1+k2).(4.2)
4.1.3 Second-order Runge-Kutta methods
We now derive the complete family of second-order Runge-Kutta methods. Higher-
order methods can be similarly derived, but require substantially more algebra.
We again consider the differential equation given by (4.1). A general second-order
Runge-Kutta method may be written in the form
k1=Dt f(tn,xn),
k2=Dt f(tn+aDt,xn+bk1),
xn+1=xn+ak1+bk2,(4.3)
with a,b,aand bconstants that deﬁne the particular second-order Runge-Kutta method.
These constants are to be constrained by setting the local error of the second-order
Runge-Kutta method to be O (Dt3). Intuitively, we might guess that two of the con-
straints will be a+b=1 and a=b.
We compute the Taylor series of xn+1directly, and from the Runge-Kutta method,
and require them to be the same to order Dt2. First, we compute the Taylor series of
xn+1. We have
xn+1=x(tn+Dt)
=x(tn) +Dt˙x(tn) +1
2(Dt)2¨x(tn) +O(Dt3).
Now,
˙x(tn) = f(tn,xn).
The second derivative is more complicated and requires partial derivatives. We have
¨x(tn) =d
dtf(t,x(t))
t=tn
=ft(tn,xn) +˙x(tn)fx(tn,xn)
=ft(tn,xn) +f(tn,xn)fx(tn,xn).
24 CHAPTER 4. DIFFERENTIAL EQUATIONS
4.1. INITIAL VALUE PROBLEM
Therefore,
xn+1=xn+Dt f(tn,xn)
+1
2(Dt)2 
ft(tn,xn) +f(tn,xn)fx(tn,xn)+O(Dt3). (4.4)
Second, we compute xn+1from the Runge-Kutta method given by (4.3). Combining
(4.3) into a single expression, we have
xn+1=xn+aDt f(tn,xn)
+bDt f 
tn+aDt,xn+bDt f(tn,xn)+O(Dt3).
We Taylor series expand using
f 
tn+aDt,xn+bDt f(tn,xn)
=f(tn,xn) +aDt ft(tn,xn) +bDt f(tn,xn)fx(tn,xn) +O(Dt2).
The Runge-Kutta formula is therefore
xn+1=xn+ (a+b)Dt f(tn,xn)
+ (Dt)2 
ab ft(tn,xn) +bb f(tn,xn)fx(tn,xn)+O(Dt3). (4.5)
Comparing (4.4) and (4.5), we ﬁnd
a+b=1,ab=1/2, bb=1/2.
Since there are only three equations for four parameters, there exists a family of second-
order Runge-Kutta methods.
The modiﬁed Euler method given by (4.2) corresponds to a=b=1 and a=
b=1/2. Another second-order Runge-Kutta method, called the midpoint method,
corresponds to a=b=1/2, a=0 and b=1. This method is written as
k1=Dt f(tn,xn),
k2=Dt f
tn+1
2Dt,xn+1
2k1
,
xn+1=xn+k2.
4.1.4 Higher-order Runge-Kutta methods
The general second-order Runge-Kutta method was given by (4.3). The general form of
the third-order method is given by
k1=Dt f(tn,xn),
k2=Dt f(tn+aDt,xn+bk1),
k3=Dt f(tn+gDt,xn+dk1+ek2),
xn+1=xn+ak1+bk2+ck3.
CHAPTER 4. DIFFERENTIAL EQUATIONS 25
4.1. INITIAL VALUE PROBLEM
The following constraints on the constants can be guessed: a=b,g=d+e, and
a+b+c=1. Remaining constraints need to be derived.
The fourth-order method has a k1,k2,k3and k4. The ﬁfth-order method requires at
least to k6. The table below gives the minimum order of the method and the number of
stages required.
order 2 3 4 5 6 7 8
minimum # stages 2 3 4 6 7 9 11
Because of the jump in the number of stages required between the fourth- and ﬁfth-
order methods, the fourth-order Runge-Kutta method has some appeal. The general
fourth-order method with four stages has 13 constants and 11 constraints. A particu-
larly simple fourth-order method that has been widely used in the past by physicists is
given by
k1=Dt f(tn,xn),
k2=Dt f
tn+1
2Dt,xn+1
2k1
,
k3=Dt f
tn+1
2Dt,xn+1
2k2
,
k4=Dt f(tn+Dt,xn+k3),
xn+1=xn+1
6(k1+2k2+2k3+k4).
4.1.5 Adaptive Runge-Kutta Methods
As in adaptive integration, it is useful to devise an ode integrator that automatically
ﬁnds the appropriate Dt. The Dormand-Prince Method, which is implemented in
MATLAB’s ode45.m , ﬁnds the appropriate step size by comparing the results of a
ﬁfth-order and fourth-order method. It requires six function evaluations per time step,
and constructs both a ﬁfth-order and a fourth-order method from the same function
evaluations.
Suppose the ﬁfth-order method ﬁnds xn+1with local error O (Dt6), and the fourth-
order method ﬁnds x0
n+1with local error O (Dt5). Let #be the desired error tolerance of
the method, and let ebe the actual error. We can estimate efrom the difference between
the ﬁfth- and fourth-order methods; that is,
e=jxn+1 x0
n+1j.
Now eis of O (Dt5), where Dtis the step size taken. Let Dtbe the estimated step size
required to get the desired error #. Then we have
e/#= (Dt)5/(Dt)5,
or solving for Dt,
Dt=Dt#
e1/5
.
26 CHAPTER 4. DIFFERENTIAL EQUATIONS
4.1. INITIAL VALUE PROBLEM
On the one hand, if the actual error is less that the desired error, e<#, then we accept
xn+1and do the next time step using the larger value of Dt. On the other hand, if the
actual error is greater than the desired error, e>#, then we reject the integration step
and redo the time step using the smaller value of Dt. In practice, one usually increases
the time step slightly less and decreases the time step slightly more to prevent the
wastefulness of too many failed time steps.
4.1.6 System of differential equations
Our numerical methods can be easily adapted to solve higher-order differential equa-
tions, or equivalently, a system of differential equations. First, we show how a second-
order differential equation can be reduced to two ﬁrst-order equations. Consider
¨x=f(t,x,˙x).
This second-order equation can be rewritten as two ﬁrst-order equations by deﬁning
u=˙x. We then have the system
˙x=u,˙u=f(t,x,u).
This trick also works for higher-order equations. For example, the third-order equation
...x=f(t,x,˙x,¨x),
can be written as
˙x=u,˙u=v,˙v=f(t,x,u,v).
We can generalize the Runge-Kutta method to solve a system of differential equa-
tions. As an example, consider the following system of two odes,
˙x=f(t,x,y),˙y=g(t,x,y),
with the initial conditions x(0) =x0and y(0) =y0. The generalization of the commonly
CHAPTER 4. DIFFERENTIAL EQUATIONS 27
4.2. BOUNDARY VALUE PROBLEMS
used fourth-order Runge-Kutta method would be
k1=Dt f(tn,xn,yn),
l1=Dtg(tn,xn,yn),
k2=Dt f
tn+1
2Dt,xn+1
2k1,yn+1
2l1
,
l2=Dtg
tn+1
2Dt,xn+1
2k1,yn+1
2l1
,
k3=Dt f
tn+1
2Dt,xn+1
2k2,yn+1
2l2
,
l3=Dtg
tn+1
2Dt,xn+1
2k2,yn+1
2l2
,
k4=Dt f(tn+Dt,xn+k3,yn+l3),
l4=Dtg(tn+Dt,xn+k3,yn+l3),
xn+1=xn+1
6(k1+2k2+2k3+k4),
yn+1=yn+1
6(l1+2l2+2l3+l4).
4.2 Boundary value problems
4.2.1 Shooting method
We consider the general ode of the form
d2y
dx2=f(x,y,dy/dx),
with two-point boundary conditions y(0) = Aand y(1) = B. We will ﬁrst formulate
the ode as an initial value problem. We have
dy
dx=z,dz
dx=f(x,y,z).
The initial condition y(0) = Ais known, but the second initial condition z(0) = bis
unknown. Our goal is to determine bsuch that y(1) =B.
In fact, this is a root-ﬁnding problem for an appropriately deﬁned function. We
deﬁne the function F=F(b)such that
F(b) =y(1) B,
28 CHAPTER 4. DIFFERENTIAL EQUATIONS
4.2. BOUNDARY VALUE PROBLEMS
where y(1)is the numerical value obtained from integrating the coupled ﬁrst-order
differential equations with y(0) = Aand z(0) = b. Our root-ﬁnding routine will want
to solve F(b) =0. (The method is called shooting because the slope of the solution curve
fory=y(x)atx=0 is given by b, and the solution hits the value y(1)atx=1. This
looks like pointing a gun and trying to shoot the target, which is B.)
To determine the value of bthat solves F(b) =0, we iterate using the secant method,
given by
bn+1=bn F(bn)bn bn 1
F(bn) F(bn 1).
We need to start with two initial guesses for b, solving the ode for the two corre-
sponding values of y(1). Then the secant method will give us the next value of bto try,
and we iterate until jy(1) Bj<tol, where tol is some speciﬁed tolerance for the error.
CHAPTER 4. DIFFERENTIAL EQUATIONS 29
4.2. BOUNDARY VALUE PROBLEMS
30 CHAPTER 4. DIFFERENTIAL EQUATIONS
Chapter 5
Linear algebra
Consider the system of nlinear equations and nunknowns, given by
a11x1+a12x2++a1nxn=b1,
a21x1+a22x2++a2nxn=b2,
......
an1x1+an2x2++annxn=bn.
We can write this system as the matrix equation
Ax=b, (5.1)
with
A=0
BBB@a11 a12 a1n
a21 a22 a2n
............
an1an2 ann1
CCCA,x=0
BBB@x1
x2
...
xn1
CCCA,b=0
BBB@b1
b2
...
bn1
CCCA.
This chapter details the numerical solution of (5.1).
5.1 Gaussian Elimination
The standard numerical algorithm to solve a system of linear equations is called Gaus-
sian Elimination. We can illustrate this algorithm by example.
Consider the system of equations
 3x1+2x2 x3= 1,
6x1 6x2+7x3= 7,
3x1 4x2+4x3= 6.
To perform Gaussian elimination, we form an Augmented Matrix by combining the
matrix A with the column vector b:
0
@ 3 2 1 1
6 6 7 7
3 4 4 61
A.
Row reduction is then performed on this matrix. Allowed operations are (1) multiply
any row by a constant, (2) add multiple of one row to another row, (3) interchange the
order of any rows. The goal is to convert the original matrix into an upper-triangular
matrix.
31
5.2. LU DECOMPOSITION
We start with the ﬁrst row of the matrix and work our way down as follows. First
we multiply the ﬁrst row by 2 and add it to the second row, and add the ﬁrst row to
the third row: 0
@ 3 2 1 1
0 2 5 9
0 2 3 71
A.
We then go to the second row. We multiply this row by  1 and add it to the third row:
0
@ 3 2 1 1
0 2 5 9
0 0 2 21
A.
The resulting equations can be determined from the matrix and are given by
 3x1+2x2 x3= 1
 2x2+5x3= 9
 2x3=2.
These equations can be solved by backward substitution, starting from the last equation
and working backwards. We have
 2x3=2!x3= 1
 2x2= 9 5x3= 4!x2=2,
 3x1= 1 2x2+x3= 6!x1=2.
Therefore,0
@x1
x2
x31
A=0
@2
2
 11
A.
5.2 LU decomposition
The process of Gaussian Elimination also results in the factoring of the matrix A to
A=LU,
where L is a lower triangular matrix and U is an upper triangular matrix. Using the
same matrix A as in the last section, we show how this factorization is realized. We
have 0
@ 3 2 1
6 6 7
3 4 41
A!0
@ 3 2 1
0 2 5
0 2 31
A=M1A,
where
M1A=0
@1 0 0
2 1 0
1 0 11
A0
@ 3 2 1
6 6 7
3 4 41
A=0
@ 3 2 1
0 2 5
0 2 31
A.
32 CHAPTER 5. LINEAR ALGEBRA
5.2. LU DECOMPOSITION
Note that the matrix M 1performs row elimination on the ﬁrst column. Two times the
ﬁrst row is added to the second row and one times the ﬁrst row is added to the third
row. The entries of the column of M 1come from 2 = (6/ 3)and 1 = (3/ 3)as
required for row elimination. The number  3 is called the pivot.
The next step is
0
@ 3 2 1
0 2 5
0 2 31
A!0
@ 3 2 1
0 2 5
0 0 21
A=M2(M1A),
where
M2(M1A) =0
@1 0 0
0 1 0
0 1 11
A0
@ 3 2 1
0 2 5
0 2 31
A=0
@ 3 2 1
0 2 5
0 0 21
A.
Here, M 2multiplies the second row by  1= ( 2/ 2)and adds it to the third row.
The pivot is 2.
We now have
M2M1A=U
or
A=M 1
1M 1
2U.
The inverse matrices are easy to ﬁnd. The matrix M 1multiples the ﬁrst row by 2 and
adds it to the second row, and multiplies the ﬁrst row by 1 and adds it to the third row.
To invert these operations, we need to multiply the ﬁrst row by  2 and add it to the
second row, and multiply the ﬁrst row by  1 and add it to the third row. To check,
with
M1M 1
1=I,
we have 0
@1 0 0
2 1 0
1 0 11
A0
@1 0 0
 2 1 0
 1 0 11
A=0
@1 0 0
0 1 0
0 0 11
A.
Similarly,
M 1
2=0
@1 0 0
0 1 0
0 1 11
A
Therefore,
L=M 1
1M 1
2
is given by
L=0
@1 0 0
 2 1 0
 1 0 11
A0
@1 0 0
0 1 0
0 1 11
A=0
@1 0 0
 2 1 0
 1 1 11
A,
which is lower triangular. The off-diagonal elements of M 1
1and M 1
2are simply com-
bined to form L. Our LU decomposition is therefore
0
@ 3 2 1
6 6 7
3 4 41
A=0
@1 0 0
 2 1 0
 1 1 11
A0
@ 3 2 1
0 2 5
0 0 21
A.
CHAPTER 5. LINEAR ALGEBRA 33
5.2. LU DECOMPOSITION
Another nice feature of the LU decomposition is that it can be done by overwriting A,
therefore saving memory if the matrix A is very large.
The LU decomposition is useful when one needs to solve A x=bforxwhen A is
ﬁxed and there are many different b’s. First one determines L and U using Gaussian
elimination. Then one writes
(LU)x=L(Ux) =b.
We let
y=Ux,
and ﬁrst solve
Ly=b
foryby forward substitution. We then solve
Ux=y
forxby backward substitution. If we count operations, we can show that solving
(LU)x=bis substantially faster once L and U are in hand than solving A x=b
directly by Gaussian elimination.
We now illustrate the solution of LU x=busing our previous example, where
L=0
@1 0 0
 2 1 0
 1 1 11
A, U =0
@ 3 2 1
0 2 5
0 0 21
A,b=0
@ 1
 7
 61
A.
With y=Ux, we ﬁrst solve L y=b, that is
0
@1 0 0
 2 1 0
 1 1 11
A0
@y1
y2
y31
A=0
@ 1
 7
 61
A.
Using forward substitution
y1= 1,
y2= 7+2y1= 9,
y3= 6+y1 y2=2.
We now solve U x=y, that is
0
@ 3 2 1
0 2 5
0 0 21
A0
@x1
x2
x31
A=0
@ 1
 9
21
A.
Using backward substitution,
 2x3=2!x3= 1,
 2x2= 9 5x3= 4!x2=2,
 3x1= 1 2x2+x3= 6!x1=2,
and we have once again determined
0
@x1
x2
x31
A=0
@2
2
 11
A.
34 CHAPTER 5. LINEAR ALGEBRA
5.3. PARTIAL PIVOTING
5.3 Partial pivoting
When performing Gaussian elimination, the diagonal element that one uses during the
elimination procedure is called the pivot. To obtain the correct multiple, one uses the
pivot as the divisor to the elements below the pivot. Gaussian elimination in this form
will fail if the pivot is zero. In this situation, a row interchange must be performed.
Even if the pivot is not identically zero, a small value can result in big round-off
errors. For very large matrices, one can easily lose all accuracy in the solution. To avoid
these round-off errors arising from small pivots, row interchanges are made, and this
technique is called partial pivoting (partial pivoting is in contrast to complete pivoting,
where both rows and columns are interchanged). We will illustrate by example the LU
decomposition using partial pivoting.
Consider
A=0
@ 2 2 1
6 6 7
3 8 41
A.
We interchange rows to place the largest element (in absolute value) in the pivot, or a11,
position. That is,
A!0
@6 6 7
 2 2 1
3 8 41
A=P12A,
where
P12=0
@0 1 0
1 0 0
0 0 11
A
is a permutation matrix that when multiplied on the left interchanges the ﬁrst and
second rows of a matrix. Note that P 1
12=P12. The elimination step is then
P12A!0
@6 6 7
0 0 4/3
0 5 1/21
A=M1P12A,
where
M1=0
@1 0 0
1/3 1 0
 1/2 0 11
A.
The ﬁnal step requires one more row interchange:
M1P12A!0
@6 6 7
0 5 1/2
0 0 4/31
A=P23M1P12A=U.
Since the permutation matrices given by P are their own inverses, we can write our
result as
(P23M1P23)P23P12A=U.
CHAPTER 5. LINEAR ALGEBRA 35
5.4. MATLAB PROGRAMMING
Multiplication of M on the left by P interchanges rows while multiplication on the right
by P interchanges columns. That is,
P230
@1 0 0
1/3 1 0
 1/2 0 11
AP23=0
@1 0 0
 1/2 0 1
1/3 1 01
AP23=0
@1 0 0
 1/2 1 0
1/3 0 11
A.
The net result on M 1is an interchange of the nondiagonal elements 1/3 and  1/2.
We can then multiply by the inverse of (P23M1P23)to obtain
P23P12A= (P23M1P23) 1U,
which we write as
PA=LU.
Instead of L, MATLAB will write this as
A= (P 1L)U.
For convenience, we will just denote (P 1L)by L, but by L here we mean a permutated
lower triangular matrix.
5.4 MATLAB programming
In MATLAB, to solve A x=bforxusing Gaussian elimination, one types
x=A\b;
wherensolves for xusing the most efﬁcient algorithm available, depending on the
form of A. If A is a general nnmatrix, then ﬁrst the LU decomposition of A is found
using partial pivoting, and then xis determined from permuted forward and backward
substitution. If A is upper or lower triangular, then forward or backward substitution
(or their permuted version) is used directly.
If there are many different right-hand-sides, one would ﬁrst directly ﬁnd the LU
decomposition of A using a function call, and then solve using n. That is, one would
iterate for different b’s the following expressions:
[L U]=lu(A);
y=L\b;
x=U\y;
where the second and third lines can be shortened to
x=U\(L\b);
where the parenthesis are required. In lecture, I will demonstrate these solutions in
MATLAB using the matrix A = [ 3, 2, 1; 6, 6, 7; 3, 4, 4] and the
right-hand-side b = [ 1; 7; 6], which is the example that we did by hand.
36 CHAPTER 5. LINEAR ALGEBRA
5.4. MATLAB PROGRAMMING
Although we do not detail the algorithm here, MATLAB can also solve the linear
algebra eigenvalue problem. Here, the mathematical problem to solve is given by
Ax=lx,
where A is a square matrix, lare the eigenvalues, and the associated x’s are the eigen-
vectors. The MATLAB subroutine that solves this problem is eig.m. To only ﬁnd the
eigenvalues of A, one types
lambda = eig(A); .
To ﬁnd both the eigenvalues and eigenvectors, one types
[v,lambda] = eig(A); .
More information can be found from the MATLAB help pages. One of the nice features
about programming in MATLAB is that no great sin is commited if one uses a built-
in function without spending the time required to fully understand the underlying
algorithm.
CHAPTER 5. LINEAR ALGEBRA 37
5.4. MATLAB PROGRAMMING
38 CHAPTER 5. LINEAR ALGEBRA
Chapter 6
Finite difference approximation
We introduce here numerical differentiation, also called ﬁnite difference approxi-
mation. This technique is commonly used to discretize and solve partial differential
equations.
6.1 Finite difference formulas
Consider the Taylor series approximation for y(x+h)and y(x h), given by
y(x+h) =y(x) +hy0(x) +1
2h2y00(x) +1
6h3y000(x) +1
24h4y0000(x) +. . . ,
y(x h) =y(x) hy0(x) +1
2h2y00(x) 1
6h3y000(x) +1
24h4y0000(x) +. . . .
The standard deﬁnitions of the derivatives give the ﬁrst-order approximations
y0(x) =y(x+h) y(x)
h+O(h),
y0(x) =y(x) y(x h)
h+O(h).
The more widely-used second-order approximation is called the central-difference ap-
proximation and is given by
y0(x) =y(x+h) y(x h)
2h+O(h2).
The ﬁnite difference approximation to the second derivative can be found from consid-
ering
y(x+h) +y(x h) =2y(x) +h2y00(x) +1
12h4y0000(x) +. . . ,
from which we ﬁnd
y00(x) =y(x+h) 2y(x) +y(x h)
h2+O(h2).
Often a second-order method is required for xon the boundaries of the domain. For
a boundary point on the left, a second-order forward difference method requires the
additional Taylor series
y(x+2h) =y(x) +2hy0(x) +2h2y00(x) +4
3h3y000(x) +. . . .
We combine the Taylor series for y(x+h)and y(x+2h)to eliminate the term propor-
tional to h2:
y(x+2h) 4y(x+h) = 3y(x) 2hy0(x) +O(h3).
39
6.2. EXAMPLE: THE LAPLACE EQUATION
Therefore,
y0(x) = 3y(x) +4y(x+h) y(x+2h)
2h+O(h2).
For a boundary point on the right, we send h!  hto ﬁnd
y0(x) =3y(x) 4y(x h) +y(x 2h)
2h+O(h2).
6.2 Example: the Laplace equation
As an example of the ﬁnite difference technique, let us consider how to discretize the
two dimensional Laplace equation
¶2
¶x2+¶2
¶y2
F=0
on the rectangular domain [0, 1][0, 1]. We assume here that the values of Fare known
on the boundaries of the domain. We form a two-dimensional grid with Nintervals in
each direction together with end points that lie on the boundaries by writing
xi=ih, i=0, 1, . . . , N,
yj=jh, j=0, 1, . . . , N,
where h=1/N. We will also denote the value of F(xi,yj)byFi,j. The ﬁnite difference
approximation for the second derivatives at the interior point (xi,yj)then results in an
equation that we write in the form
4Fi,j Fi+1,j Fi 1,j Fi,j+1 Fi,j 1=0, (6.1)
valid for i=1, 2, . . . , N 1 and j=1, 2, . . . , N 1. The boundary values F0,j,FN,j,Fi,0,
andFi,Nare assumed to be given.
One can observe that (6.1) represents a system of (N 1)2linear equations for
(N 1)2unknowns. We can write this as a matrix equation if we decide how to order
the unknowns Fi,jinto a vector. The standard ordering is called natural ordering, and
proceeds from the bottom of the grid along rows moving towards the top. This orders
the unknowns as
F= [F1,1,F2,1, . . . ,F(N 1),1, . . . ,F1,(N 1),F2(,N 1), . . . ,F(N 1),(N 1)]T.
To illustrate the construction of the matrix equation, we consider the case N=3,
with two interior points in each direction. The four resulting linear equations with the
boundary terms written on the right-hand-side are
4F1,1 F2,1 F1,2=F0,1+F1,0,
4F2,1 F1,1 F2,2=F3,1+F2,0,
4F1,2 F2,2 F1,1=F0,2+F1,3,
4F2,2 F1,2 F2,1=F3,2+F2,3;
40 CHAPTER 6. FINITE DIFFERENCE APPROXIMATION
6.2. EXAMPLE: THE LAPLACE EQUATION
and the corresponding matrix equation is
0
BB@4 1 1 0
 1 4 0 1
 1 0 4 1
0 1 1 41
CCA0
BB@F1,1
F2,1
F1,2
F2,21
CCA=0
BB@F0,1+F1,0
F3,1+F2,0
F0,2+F1,3
F3,2+F2,31
CCA.
The pattern here may not be obvious, but the Laplacian matrix decomposes into 2-by-2
block matrices.
With some thought, a general result can be obtained for an Nx-by-Nygrid of internal
points (with Nx+1 and Ny+1 intervals in the x- and y-directions, respectively). The
Laplacian matrix then decomposes into Nx-by-Nxblock matrices. The diagonal contains
Nyof these Nx-by-Nxblock matrices, each of which are tridiagonal with a 4 on the
diagonal and a 1 on the off-diagonals. Immediately above and below these block
matrices on the diagonal are Ny 1 block matrices also of size Nx-by-Nx, each of which
are diagonal with  1 along the diagonal.
MATLAB code for the Laplacian matrix can be found on the web in the function
sp_laplace.m . This code was written by Bordner and Saied in 1995, and I have
written a more modern and faster version of this code in sp_laplace_new.m .
An alternative solution method, which we will later make use of in §17.2.4, includes
the boundary values in the solution vector. If one of the boundary conditions is at
position nin this vector, then row nof the left-hand-side matrix will have just a one
on the diagonal with all other elements equal to zero (i.e, a row of the identity matrix),
and the corresponding element in the right-hand-side vector will have the value at
the boundary. Here, with the total number of grid points (including the boundary
points) in the x- and ydirections given by Nxand Ny, the left-hand-side matrix is then
generated using A=sp_laplace_new(N_x,N_y) , and all the rows corresponding to
the boundary values are replaced with the corresponding rows of the identity matrix.
This formulation may be slightly easier to code, and also easier to incorporate other
more general boundary conditions.
CHAPTER 6. FINITE DIFFERENCE APPROXIMATION 41
6.2. EXAMPLE: THE LAPLACE EQUATION
42 CHAPTER 6. FINITE DIFFERENCE APPROXIMATION
Chapter 7
Iterative methods
7.1 Jacobi, Gauss-Seidel and SOR methods
Iterative methods are often used for solving a system of nonlinear equations. Even
for linear systems, iterative methods have some advantages. They may require less
memory and may be computationally faster. They are also easier to code. Here, without
detailing the theoretical numerical analysis, we will simply explain the related iterative
methods that go by the names of the Jacobi method, the Gauss-Seidel method, and
the Successive Over Relaxation method (or SOR). We illustrate these three methods by
showing how to solve the two-dimensional Poisson equation, an equation that we will
later need to solve to determine the ﬂow ﬁeld past an obstacle.
The Poisson equation is given by
 r2F=f,
where
r2=¶2
¶x2+¶2
¶y2
is the usual two-dimensional Laplacian. This could be a linear equation with fin-
dependent of F, or a nonlinear equation where fmay be some nonlinear function of
F.
After discretizing the Poisson equation using the centered ﬁnite difference approxi-
mation for the second derivatives, we obtain on a grid with uniform grid spacing h,
4Fi,j Fi+1,j Fi 1,j Fi,j+1 Fi,j 1=h2fi,j. (7.1)
The Jacobi method simply solves the discretized equation (7.1) for Fi,jiteratively. With
superscripts indicating iteration steps, we have
F(n+1)
i,j=1
4
F(n)
i+1,j+F(n)
i 1,j+F(n)
i,j+1+F(n)
i,j 1+h2f(n)
i,j
. (7.2)
In the old FORTRAN-77 scientiﬁc programming language, implementing the Jacobi method
required the saving of Fin two different arrays, one corresponding to the n-th iteration, and one
corresponding to the (n+1)-st iteration. When the iteration was done with a single array, the
method was called Gauss-Seidel. In the standard Gauss-Seidel method, the array was updated
row-by-row and had the form
F(n+1)
i,j=1
4
F(n)
i+1,j+F(n+1)
i 1,j+F(n)
i,j+1+F(n+1)
i,j 1+h2f(n)
i,j
. (7.3)
The Gauss-Seidel method had the double advantage of requiring less memory and converging
more rapidly than the Jacobi method.
A variant of Gauss-Seidel that can also be found in textbooks is called red-black Gauss-Seidel.
In this algorithm, the grid is viewed as a checkerboard with red and black squares. An updating
43
7.1. JACOBI, GAUSS-SEIDEL AND SOR METHODS
ofFi,jis done in two passes: in the ﬁrst pass, Fi,jis updated only on the red squares; in the
second pass, only on the black squares. Because of the structure of the Laplacian ﬁnite difference
operator, when solving r2F=0 the updated values of Fon the red squares depend only on the
values of Fon the black squares, and the updated values on the black squares depend only on
the red squares.
Translating FORTRAN-77 directly into MATLAB, the Gauss-Seidel method in the standard
form could be implemented on a uniform grid with N 1 and M 1 internal grid points in the
x- and y-directions, respectively, as
for index2=2:M
for index1=2:N
phi(index1,index2)=0.25 *(phi(index1+1,index2) ...
+phi(index1 1,index2)+phi(index1,index2+1) ...
+phi(index1,index2  1)+h^2 *f(index1,index2);
end
end
Nowadays, however, programming languages such as MATLAB operate on vectors, which are op-
timized for modern computers, and the explicit loops that used to be the workhorse of FORTRAN-
77 are now vectorized.
So to properly implement the Jacobi method, say, in MATLAB, one can predeﬁne the vectors
index1 andindex2 that contain the index numbers of the ﬁrst and second indices of the matrix
variable phi. These deﬁnitions would be
index1=2:N; index2=2:M;
where index1 andindex2 reference the internal grid points of the domain. The variable phi
is assumed to be known on the boundaries of the domain corresponding to the indices (1,1) ,
(N+1,1) ,(1,M+1) , and (N+1,M+1) . The Jacobi method in MATLAB can then be coded in one
line as
phi(index1,index2)=0.25 *(phi(index1+1,index2)+phi(index1  1,index2) ...
+phi(index1,index2+1)+phi(index1,index2  1)+h^2 *f(index1,index2); .
The red-black Gauss-Seidel method could be implemented in a somewhat similar fashion. One
must now predeﬁne the vectors even1 ,odd1 ,even2 , and odd2 , with deﬁnitions
even1=2:2:N; even2=2:2:M;
odd1=3:2:N; odd2=3:2:M; .
The red-black Gauss-Seidel method then requires the following four coding lines to implement:
phi(even1,even2)=0.25 *(phi(even1+1,even2)+phi(even1  1,even2) ...
+phi(even1,even2+1)+phi(even1,even2  1)+h^2 *f(even1,even2);
phi(odd1,odd2)=0.25 *(phi(odd1+1,odd2)+phi(odd1  1,odd2) ...
+phi(odd1,odd2+1)+phi(odd1,odd2  1)+h^2 *f(odd1,odd2);
phi(even1,odd2)=0.25 *(phi(even1+1,odd2)+phi(even1  1,odd2) ...
+phi(even1,odd2+1)+phi(even1,odd2  1)+h^2 *f(even1,odd2);
phi(odd1,even2)=0.25 *(phi(odd1+1,even2)+phi(odd1  1,even2) ...
+phi(odd1,even2+1)+phi(odd1,even2  1)+h^2 *f(odd1,even2); .
Each iteration of the red-black Gauss-Seidel method will run slower than that of the Jacobi
method, and the red-black Gauss-Seidel method will only be useful if the slower iterations are
compensated by a faster convergence.
44 CHAPTER 7. ITERATIVE METHODS
7.2. NEWTON’S METHOD FOR A SYSTEM OF EQUATIONS
In practice, however, the Jacobi method or the red-black Gauss Seidel method is replaced
by the corresponding Successive Over Relaxation method (SOR method). We will illustrate this
method using the Jacobi method, though the better approach is to use red-black Gauss-Seidel.
The Jacobi method is ﬁrst rewritten by adding and subtracting the diagonal term F(n)
i,j:
F(n+1)
i,j=F(n)
i,j+1
4
F(n)
i+1,j+F(n)
i 1,j+F(n)
i,j+1+F(n)
i,j 1 4F(n)
i,j+h2f(n)
i,j
.
In this form, we see that the Jacobi method updates the value of Fi,jat each iteration. We can
either magnify or diminish this update by introducing a relaxation parameter l. We have
F(n+1)
i,j=F(n)
i,j+l
4
F(n)
i+1,j+F(n)
i 1,j+F(n)
i,j+1+F(n)
i,j 1 4F(n)
i,j+h2f(n)
i,j
,
which can be written more efﬁciently as
F(n+1)
i,j= (1 l)F(n)
i,j+l
4
F(n)
i+1,j+F(n)
i 1,j+F(n)
i,j+1+F(n)
i,j 1+h2f(n)
i,j
.
When used with Gauss-Seidel, a value of lin the range 1 <l<2 can often be used to accelerate
convergence of the iteration. When l>1, the modiﬁer over in Successive Over Relaxation is
apt. When the right-hand-side of the Poisson equation is a nonlinear function of F, however, the
l=1 Gauss-Seidel method may fail to converge. In this case, it may be reasonable to choose a
value of lless than one, and perhaps a better name for the method would be Successive Under
Relaxation. When under relaxing, the convergence of the iteration will of course be slowed. But
this is the cost that must sometimes be paid for stability.
7.2 Newton’s method for a system of nonlinear equations
A system of nonlinear equations can be solved using a version of the iterative Newton’s Method
for root ﬁnding. Although in practice, Newton’s method is often applied to a large nonlinear
system, we will illustrate the method here for the simple system of two equations and two
unknowns.
Suppose that we want to solve
f(x,y) =0, g(x,y) =0,
for the unknowns xand y. We want to construct two simultaneous sequences x0,x1,x2, . . . and
y0,y1,y2, . . . that converge to the roots. To construct these sequences, we Taylor series expand
f(xn+1,yn+1)and g(xn+1,yn+1)about the point (xn,yn). Using the partial derivatives fx=
¶f/¶x,fy=¶f/¶y, etc., the two-dimensional Taylor series expansions, displaying only the linear
terms, are given by
f(xn+1,yn+1) = f(xn,yn) + ( xn+1 xn)fx(xn,yn)
+ (yn+1 yn)fy(xn,yn) +. . .
g(xn+1,yn+1) =g(xn,yn) + ( xn+1 xn)gx(xn,yn)
+ (yn+1 yn)gy(xn,yn) +. . . .
To obtain Newton’s method, we take f(xn+1,yn+1) = 0,g(xn+1,yn+1) = 0, and drop higher-
order terms above linear. Although one can then ﬁnd a system of linear equations for xn+1and
yn+1, it is more convenient to deﬁne the variables
Dxn=xn+1 xn,Dyn=yn+1 yn.
CHAPTER 7. ITERATIVE METHODS 45
7.2. NEWTON’S METHOD FOR A SYSTEM OF EQUATIONS
The iteration equations will then be given by
xn+1=xn+Dxn,yn+1=yn+Dyn;
and the linear equations to be solved for DxnandDynare given by
fxfy
gxgyDxn
Dyn
= f
 g
,
where f,g,fx,fy,gx, and gyare all evaluated at the point (xn,yn). The two-dimensional case is
easily generalized to ndimensions. The matrix of partial derivatives is called the Jacobian Matrix.
We illustrate Newton’s Method by ﬁnding numerically the steady state solution of the Lorenz
equations, given by
s(y x) =0,
rx y xz=0,
xy bz=0,
where x,y, and zare the unknown variables and s,r, and bare the known parameters. Here, we
have a three-dimensional homogeneous system f=0,g=0, and h=0, with
f(x,y,z) =s(y x),
g(x,y,z) =rx y xz,
h(x,y,z) =xy bz.
The partial derivatives can be computed to be
fx= s, fy=s, fz=0,
gx=r z, gy= 1, gz= x,
hx=y, hy=x, hz= b.
The iteration equation is therefore
0
@ s s 0
r zn 1 xn
yn xn b1
A0
@Dxn
Dyn
Dzn1
A= 0
@s(yn xn)
rxn yn xnzn
xnyn bzn1
A,
with
xn+1=xn+Dxn,
yn+1=yn+Dyn,
zn+1=zn+Dzn.
The sample MATLAB program that solves this system is in the m-ﬁle newton_system.m .
46 CHAPTER 7. ITERATIVE METHODS
Chapter 8
Interpolation
Consider the following problem: Given the values of a known function y=f(x)at a sequence
of ordered points x0,x1, . . . , xn, ﬁnd f(x)for arbitrary x. When x0xxn, the problem is called
interpolation. When x<x0orx>xn, the problem is called extrapolation.
With yi=f(xi), the problem of interpolation is basically one of drawing a smooth curve
through the known points (x0,y0),(x1,y1), . . . ,(xn,yn). This is not the same problem as drawing
a smooth curve that approximates a set of data points that have experimental error. This latter
problem is called least-squares approximation, which is considered in the next chapter.
It is possible to interpolate the n+1 known points by a unique polynomial of degree n.
When n=1, the polynomial is a linear function; when n=2, the polynomial is a quadratic
function. Although low order polynomials are sometimes used when the number of points are
few, higher-order polynomial interpolates tend to be unstable, and are not of much practical use.
Here, we will consider the more useful piece-wise polynomial interpolation. The two most
used are piecewise linear interpolation, and cubic spline interpolation. The ﬁrst makes use of
linear polynomials, and the second cubic polynomials.
8.1 Piecewise linear interpolation
Here, we use linear polynomials. This is the default interpolation typically used when plotting
data.
Suppose the interpolating function is y=g(x), and as previously, there are n+1 points to
interpolate. We construct the function g(x)out of nlocal linear polynomials. We write
g(x) =gi(x), for xixxi+1,
where
gi(x) =ai(x xi) +bi,
and i=0, 1, . . . , n 1.
We now require y=gi(x)to pass through the endpoints (xi,yi)and(xi+1,yi+1). We have
yi=bi,
yi+1=ai(xi+1 xi) +bi.
Therefore, the coefﬁcients of gi(x)are determined to be
ai=yi+1 yi
xi+1 xi,bi=yi.
Although piecewise linear interpolation is widely used, particularly in plotting routines, it suffers
from a discontinuity in the derivative at each point. This results in a function which may not look
smooth if the points are too widely spaced. We next consider a more challenging algorithm that
uses cubic polynomials.
47
8.2. CUBIC SPLINE INTERPOLATION
8.2 Cubic spline interpolation
The n+1 points to be interpolated are again
(x0,y0),(x1,y1), . . .(xn,yn).
Here, we use npiecewise cubic polynomials for interpolation,
gi(x) =ai(x xi)3+bi(x xi)2+ci(x xi) +di,i=0, 1, . . . , n 1,
with the global interpolation function written as
g(x) =gi(x), for xixxi+1.
To achieve a smooth interpolation we impose that g(x)and its ﬁrst and second derivatives are
continuous. The requirement that g(x)is continuous (and goes through all n+1 points) results
in the two constraints
gi(xi) =yi,i=0 to n 1, (8.1)
gi(xi+1) =yi+1,i=0 to n 1. (8.2)
The requirement that g0(x)is continuous results in
g0
i(xi+1) =g0
i+1(xi+1),i=0 to n 2. (8.3)
And the requirement that g00(x)is continuous results in
g00
i(xi+1) =g00
i+1(xi+1),i=0 to n 2. (8.4)
There are ncubic polynomials gi(x)and each cubic polynomial has four free coefﬁcients; there
are therefore a total of 4 nunknown coefﬁcients. The number of constraining equations from (8.1)-
(8.4) is 2 n+2(n 1) = 4n 2. With 4 n 2 constraints and 4 nunknowns, two more conditions
are required for a unique solution. These are usually chosen to be extra conditions on the ﬁrst
g0(x)and last gn 1(x)polynomials. We will discuss these extra conditions later.
We now proceed to determine equations for the unknown coefﬁcients of the cubic polynomi-
als. The polynomials and their ﬁrst two derivatives are given by
gi(x) =ai(x xi)3+bi(x xi)2+ci(x xi) +di, (8.5)
g0
i(x) =3ai(x xi)2+2bi(x xi) +ci, (8.6)
g00
i(x) =6ai(x xi) +2bi. (8.7)
We will consider the four conditions (8.1)-(8.4) in turn. From (8.1) and (8.5), we have
di=yi,i=0 to n 1, (8.8)
which directly solves for all of the d-coefﬁcients.
To satisfy (8.2), we ﬁrst deﬁne
hi=xi+1 xi,
and
fi=yi+1 yi.
Now, from (8.2) and (8.5), using (8.8), we obtain the nequations
aih3
i+bih2
i+cihi=fi,i=0 to n 1. (8.9)
48 CHAPTER 8. INTERPOLATION
8.2. CUBIC SPLINE INTERPOLATION
From (8.3) and (8.6) we obtain the n 1 equations
3aih2
i+2bihi+ci=ci+1,i=0 to n 2. (8.10)
From (8.4) and (8.7) we obtain the n 1 equations
3aihi+bi=bi+1i=0 to n 2. (8.11)
It will be useful to include a deﬁnition of the coefﬁcient bn, which is now missing. (The index of
the cubic polynomial coefﬁcients only go up to n 1.) We simply extend (8.11) up to i=n 1
and so write
3an 1hn 1+bn 1=bn, (8.12)
which can be viewed as a deﬁnition of bn.
We now proceed to eliminate the sets of a- and c-coefﬁcients (with the d-coefﬁcients already
eliminated in (8.8)) to ﬁnd a system of linear equations for the b-coefﬁcients. From (8.11) and
(8.12), we can solve for the n a-coefﬁcients to ﬁnd
ai=1
3hi(bi+1 bi),i=0 to n 1. (8.13)
From (8.9), we can solve for the n c-coefﬁcients as follows:
ci=1
hi
fi aih3
i bih2
i
=1
hi
fi 1
3hi(bi+1 bi)h3
i bih2
i
=fi
hi 1
3hi(bi+1+2bi),i=0 to n 1. (8.14)
We can now ﬁnd an equation for the b-coefﬁcients by substituting (8.13) and (8.14) into (8.10):
31
3hi(bi+1 bi)
h2
i+2bihi+fi
hi 1
3hi(bi+1+2bi)
=fi+1
hi+1 1
3hi+1(bi+2+2bi+1)
,
which simpliﬁes to
1
3hibi+2
3(hi+hi+1)bi+1+1
3hi+1bi+2=fi+1
hi+1 fi
hi, (8.15)
an equation that is valid for i=0 to n 2. Therefore, (8.15) represent n 1 equations for the
n+1 unknown b-coefﬁcients. Accordingly, we write the matrix equation for the b-coefﬁcients,
leaving the ﬁrst and last row absent, as
0
BBBBB@. . . . . . . . . . . . missing . . . . . .
1
3h02
3(h0+h1)1
3h1. . . 0 0 0
.....................
0 0 0 . . .1
3hn 22
3(hn 2+hn 1)1
3hn 1
. . . . . . . . . . . . missing . . . . . .1
CCCCCA0
BBBBB@b0
b1
...
bn 1
bn1
CCCCCA
=0
BBBBBB@missing
f1
h1 f0
h0...
fn 1
hn 1 fn 2
hn 2
missing1
CCCCCCA.
CHAPTER 8. INTERPOLATION 49
8.2. CUBIC SPLINE INTERPOLATION
Once the missing ﬁrst and last equations are speciﬁed, the matrix equation for the b-coefﬁcients
can be solved by Gaussian elimination. And once the b-coefﬁcients are determined, the a- and
c-coefﬁcients can also be determined from (8.13) and (8.14), with the d-coefﬁcients already known
from (8.8). The piecewise cubic polynomials, then, are known and g(x)can be used for interpo-
lation to any value xsatisfying x0xxn.
The missing ﬁrst and last equations can be speciﬁed in several ways, and here we show the
two ways that are allowed by the MATLAB function spline.m. The ﬁrst way should be used when
the derivative g0(x)is known at the endpoints x0and xn; that is, suppose we know the values of
aandbsuch that
g0
0(x0) =a,g0
n 1(xn) =b.
From the known value of a, and using (8.6) and (8.14), we have
a=c0
=f0
h0 1
3h0(b1+2b0).
Therefore, the missing ﬁrst equation is determined to be
2
3h0b0+1
3h0b1=f0
h0 a. (8.16)
From the known value of b, and using (8.6), (8.13), and (8.14), we have
b=3an 1h2
n 1+2bn 1hn 1+cn 1
=31
3hn 1(bn bn 1)
h2
n 1+2bn 1hn 1+fn 1
hn 1 1
3hn 1(bn+2bn 1)
,
which simpliﬁes to
1
3hn 1bn 1+2
3hn 1bn=b fn 1
hn 1, (8.17)
to be used as the missing last equation.
The second way of specifying the missing ﬁrst and last equations is called the not-a-knot
condition, which assumes that
g0(x) =g1(x),gn 2(x) =gn 1(x).
Considering the ﬁrst of these equations, from (8.5) we have
a0(x x0)3+b0(x x0)2+c0(x x0) +d0
=a1(x x1)3+b1(x x1)2+c1(x x1) +d1.
Now two cubic polynomials can be proven to be identical if at some value of x, the polynomials
and their ﬁrst three derivatives are identical. Our conditions of continuity at x=x1already
require that at this value of xthese two polynomials and their ﬁrst two derivatives are identical.
The polynomials themselves will be identical, then, if their third derivatives are also identical at
x=x1, or if
a0=a1.
From (8.13), we have
1
3h0(b1 b0) =1
3h1(b2 b1),
or after simpliﬁcation
h1b0 (h0+h1)b1+h0b2=0, (8.18)
50 CHAPTER 8. INTERPOLATION
8.3. MULTIDIMENSIONAL INTERPOLATION
which provides us our missing ﬁrst equation. A similar argument at x=xn 1also provides us
with our last equation,
hn 1bn 2 (hn 2+hn 1)bn 1+hn 2bn=0. (8.19)
The MATLAB subroutines spline.m and ppval.m can be used for cubic spline interpolation
(see also interp1.m). I will illustrate these routines in class and post sample code on the course
web site.
8.3 Multidimensional interpolation
Suppose we are interpolating the value of a function of two variables,
z=f(x,y).
The known values are given by
zij=f(xi,yj),
with i=0, 1, . . . , nand j=0, 1, . . . , n. Note that the (x,y)points at which f(x,y)are known lie
on a grid in the x yplane.
Letz=g(x,y)be the interpolating function, satisfying zij=g(xi,yj). A two-dimensional
interpolation to ﬁnd the value of gat the point (x,y)may be done by ﬁrst performing n+1
one-dimensional interpolations in yto ﬁnd the value of gat the n+1 points (x0,y),(x1,y), . . . ,
(xn,y), followed by a single one-dimensional interpolation in xto ﬁnd the value of gat(x,y).
In other words, two-dimensional interpolation on a grid of dimension (n+1)(n+1)is
done by ﬁrst performing n+1 one-dimensional interpolations to the value yfollowed by a single
one-dimensional interpolation to the value x. Two-dimensional interpolation can be generalized
to higher dimensions. The MATLAB functions to perform two- and three-dimensional interpola-
tion are interp2.m and interp3.m.
CHAPTER 8. INTERPOLATION 51
8.3. MULTIDIMENSIONAL INTERPOLATION
52 CHAPTER 8. INTERPOLATION
Chapter 9
Least-squares approximation
The method of least-squares is commonly used to ﬁt a parameterized curve to experimental
data. In general, the ﬁtting curve is not expected to pass through the data points, making this
problem substantially different from the one of interpolation.
We consider here only the most common situation: the ﬁtting of a straight line through data
with the same experimental error for all the data points. We assume that the data to be ﬁtted are
given by (xi,yi), with i=1 to n.
We write for the ﬁtting curve
y(x) =ax+b.
The distance rifrom the data point (xi,yi)and the ﬁtting curve is given by
ri=yi y(xi)
=yi (axi+b).
A least-squares ﬁt minimizes the sum of the squares of the ri’s. This minimum can be shown to
result in the most probable values of aandb.
We deﬁne
r=n
å
i=1r2
i
=n
å
i=1 
yi (axi+b)2.
To minimize rwith respect to aandb, we solve
¶r
¶a=0,¶r
¶b=0.
Taking the partial derivatives, we have
¶r
¶a=n
å
i=12( xi) 
yi (axi+b)=0,
¶r
¶b=n
å
i=12( 1) 
yi (axi+b)=0.
These equations form a system of two linear equations in the two unknowns aandb, which is
evident when rewritten in the form
an
å
i=1x2
i+bn
å
i=1xi=n
å
i=1xiyi,
an
å
i=1xi+bn=n
å
i=1yi.
These equations can be solved numerically, and MATLAB provides a built-in subroutine called
polyﬁt.m. With the data in the vectors x and y, the MATLAB call
p = polyﬁt(x,y,1);
returns the values p(1) =aand p(2) =b, which can then be used to draw the ﬁtting line.
53
54 CHAPTER 9. LEAST-SQUARES APPROXIMATION
Part II
Dynamical systems and chaos
55

The second part of this course will include a discussion of dynamical systems theory and
chaos. Our main vehicle for this discussion will be the motion of the one-dimensional driven,
damped pendulum.
57
58
Chapter 10
The simple pendulum
We ﬁrst consider the simple pendulum shown in Fig. 10.1. A mass is attached to a massless
rigid rod, and is constrained to move along an arc of a circle centered at the pivot point. Suppose
lis the ﬁxed length of the connecting rod, and qis the angle it makes with the vertical axis. We
will derive the governing equations for the motion of the mass, and an equation which can be
solved to determine the period of oscillation.
10.1 Governing equations
The governing equations for the pendulum are derived from Newton’s equation, F=ma. Be-
cause the pendulum is constained to move along an arc, we can write Newton’s equation directly
for the displacement sof the pendulum along the arc with origin at the bottom and positive
direction to the right.
The relevant force on the pendulum is the component of the gravitational force along the arc,
and from Fig. 10.1 is seen to be
Fg= mgsinq, (10.1)
where the negative sign signiﬁes a force acting along the negative sdirection when 0 <q<p,
and the positive sdirection when p<q<0.
Newton’s equation for the simple pendulum moving along the arc is therefore
m¨s= mgsinq.
Now, the relationship between the arc length sand the angle qis given by s=lq, and therefore
¨s=l¨q. The simple pendulum equation can then be written in terms of the angle qas
¨q+w2sinq=0, (10.2)
with
w=p
g/l. (10.3)
The standard small angle approximation sin qqresults in the well-known equation for the
simple harmonic oscillator,
¨q+w2q=0. (10.4)
We have derived the equations of motion by supposing that the pendulum is constrained
to move along the arc of a circle. Such a constraint is valid provided the pendulum mass is
connected to a massless rigid rod. If the rod is replaced by a massless spring, say, then oscillations
in the length of the spring can occur. Deriving the equations for this more general physical
situation requires considering the vector form of Newton’s equation.
With the origin of the coordinate system located at the base of the pendulum, we deﬁne the
positive x-direction to be pointing down, and the positive ydirection to be pointing to the right.
The mass is located at the position vector x= (x,y). Both gravity and the tension force Tacts on
the mass, and the governing equations are given by
m¨x=mg Tcosq,
m¨y= Tsinq.
59
10.1. GOVERNING EQUATIONS
Figure 10.1: Forces acting on the pendulum.
It is informative to construct the equivalent governing equations in polar coordinates. To do so,
we form the complex coordinate z=x+iy, and make use of the polar form for z, given by
z=reiq.
The governing equations then become
d2
dt2
reiq
=g T
meiq. (10.5)
The second derivative can be computed using the product and chain rules, and one ﬁnds
d2
dt2
reiq
= ¨r r˙q2+i 
r¨q+2˙r˙q
eiq.
Dividing both sides of (10.5) by eiq, we obtain
 ¨r r˙q2+i 
r¨q+2˙r˙q=ge iq T
m. (10.6)
The two governing equations in polar coordinates, then, are determined by equating the real
parts and the imaginary parts of (10.6), and we ﬁnd
¨r r˙q2=gcosq T
m, (10.7)
r¨q+2˙r˙q= gsinq. (10.8)
If the connector is a rigid rod, as we initially assumed, then r=l,˙r=0, and ¨r=0. The ﬁrst
equation (10.7) is then an equation for the tension Tin the rod, and the second equation (10.8)
reduces to our previously derived (10.2). If the connector is a spring, then Hooke’s law may be
applied. Suppose the spring constant is kand the unstretched spring has length l. Then
T=k(r l)
in (10.7), and a pair of simultaneous second-order equations govern the motion. The stable
equilibrium with the mass at rest at the bottom satisﬁes q=0 and r=l+mg/k; i.e., the spring
is stretched to counterbalance the hanging mass.
60 CHAPTER 10. THE SIMPLE PENDULUM
10.2. PERIOD OF MOTION
10.2 Period of motion
In general, the period of the simple pendulum depends on the amplitude of its motion. For small
amplitude oscillations, the simple pendulum equation (10.2) reduces to the simple harmonic os-
cillator equation (10.4), and the period becomes independent of amplitude. The general solution
of the simple harmonic oscillator equation can be written as
q(t) =Acos(wt+j).
Initial conditions on qand ˙qdetermine Aandj, and by redeﬁning the origin of time, one can
always choose j=0 and A>0. With this choice, A=qm, the maximum amplitude of the
pendulum, and the analytical solution of (10.4) is
q(t) =qmcoswt,
where wis called the angular frequency of oscillation. The period of motion is related to the
angular frequency by
T=2p/w,
and is independent of the amplitude qm.
If sin qqis no longer a valid approximation, then we need to solve the simple pendulum
equation (10.2). We ﬁrst derive a closed form analytical expression, and then explain how to
compute a numerical solution.
10.2.1 Analytical solution
A standard procedure for solving unfamiliar differential equations is to try and determine some
combination of variables that is independent of time. Here, we can multiply (10.2) by ˙qto obtain
˙q¨q+w2˙qsinq=0. (10.9)
Now,
˙q¨q=d
dt1
2˙q2
,˙qsinq=d
dt( cosq),
so that (10.9) can be written as
d
dt1
2˙q2 w2cosq
=0. (10.10)
The combination of variables in the parenthesis is therefore independent of time and is called an
integral of motion. It is also said to be conserved, and (10.10) is called a conservation law. In
physics, this integral of motion (multiplied by ml2) is identiﬁed with the energy of the oscillator.
The value of this integral of motion at t=0 is given by w2cosqm, so the derived conserva-
tion law takes the form
1
2˙q2 w2cosq= w2cosqm, (10.11)
which is a separable ﬁrst-order differential equation.
We can compute the period of oscillation Tas four times the time it takes for the pendulum
to go from its initial height to the bottom. During this quarter cycle of oscillation, dq/dt<0 so
that from (10.11),
dq
dt= p
2wp
cosq cosqm.
After separating and integrating over a quarter period, we have
ZT/4
0dt= p
2
2wZ0
qmdqp
cosq cosqm,
CHAPTER 10. THE SIMPLE PENDULUM 61
10.2. PERIOD OF MOTION
or
T=2p
2
wZqm
0dqp
cosq cosqm. (10.12)
We can transform this equation for the period into a more standard form using a trigonometric
identity and a substitution. The trig-identity is the well-known half-angle formula for sin, given
by
sin2(q/2) =1
2(1 cosq).
Using this identity, we write
cosq=1 2 sin2(q/2), cos qm=1 2 sin2(qm/2), (10.13)
and substituting (10.13) into (10.12) results in
T=2
wZqm
0dqq
sin2(qm/2) sin2(q/2). (10.14)
We now deﬁne the constant
a=sin(qm/2), (10.15)
and perform the substitution
sinf=1
asin(q/2). (10.16)
Developing this substitution, we have
cosfdf=1
2acos(q/2)dq. (10.17)
Now,
cos(q/2) =q
1 sin2(q/2)
=q
1 a2sin2f,
so that (10.17) can be solved for dq:
dq=2acosfq
1 a2sin2fdf. (10.18)
Using (10.16), the domain of integration q2[0,qm]transforms into f2[0,p/2]. Therefore,
substituting (10.16) and (10.18) into (10.14) results in the standard equation for the period given
by
T=4
wZp/2
0dfq
1 a2sin2f, (10.19)
with agiven by (10.15). The integral T=T(a)is called the complete elliptic integral of the ﬁrst
kind.
For small amplitudes of oscillation, it is possible to determine the leading-order dependence
of the period on qm. Now ais given by (10.15), so that a Taylor series to leading order yields
a2=1
4q2
m+O(q4
m).
62 CHAPTER 10. THE SIMPLE PENDULUM
10.2. PERIOD OF MOTION
Similarly, the Taylor series expansion of the integrand of (10.19) is given by
1q
1 a2sin2f=1+1
2a2sin2f+O(a4)
=1+1
8q2
msin2f+O(q4
m).
Therefore, from (10.19),
T=4
wZp/2
0df
1+1
8q2
msin2f
+O(q4
m).
UsingZp/2
0dfsin2f=p
4,
we have
T=2p
w
1+q2m
16
+O(q4
m). (10.20)
10.2.2 Numerical solution
We discuss here two methods for computing the period of the pendulum T=T(qm)as a function
of the maximum amplitude. The period can be found in units of w 1; that is, we compute wT.
The ﬁrst method makes use of our analytical work and performs a numerical integration of
(10.19). Algorithms for numerical integration are discussed in Chapter 3. In particular, use can
be made of adaptive Simpson’s quadrature, implemented in the MATLAB function quad.m .
The second method, which is just as reasonable, solves the differential equation (10.2) directly.
Nondimensionalizing using t=wt, equation (10.2) becomes
d2q
dt2+sinq=0. (10.21)
To solve (10.21), we write this second-order equation as the system of two ﬁrst-order equations
dq
dt=u,
du
dt= sinq,
with initial conditions q(0) =qmand u(0) = 0. We then determine the (dimensionless) time
required for the pendulum to move to the position q=0: this time will be equal to one-fourth of
the period of motion.
Algorithms for integration of ordinary differential equations are discussed in Chapter 4. In
particular, use can be made of a Runge-Kutta (4,5) formula, the Dormand-Prince pair, that is
implemented in the MATLAB function ode45.m .
Perhaps the simplest way to compute the period is to make use of the Event Location Property
of the MATLAB ode solver. Through the odeset option, it is possible to instruct ode45.m to
end the time integration when the event q=0 occurs, and to return the time at which this event
takes place.
A graph of the dimensionless period wTversus the amplitude qmis shown in Fig. 10.2. For
comparison, the low-order analytical result of (10.20) is shown as the dashed line.
CHAPTER 10. THE SIMPLE PENDULUM 63
10.2. PERIOD OF MOTION
0 0.5 1 1.5 2 2.5 3681012141618
θmωT
Figure 10.2: The dimensionless period of the simple pendulum wT versus amplitude qm. The
solid line is the numerical result and the dashed line is a low-order approximation.
64 CHAPTER 10. THE SIMPLE PENDULUM
Chapter 11
The damped, driven pendulum
The simple pendulum is the mathematical idealization of a frictionless pendulum. We now
consider the effects of friction as well as an externally imposed periodic force. The frictional force
is modeled as
Ff= gl˙q,
where the frictional force is opposite in sign to the velocity, and thus opposes motion. The
positive parameter gis called the coefﬁcient of friction. The external periodic force is modeled as
Fe=FcosWt,
where Fis the force’s amplitude and Wis the force’s angular frequency. If we also include the
gravitational force given by (10.1), Newton’s equation can then be written as
¨q+l˙q+w2sinq=fcosWt, (11.1)
where l=g/m,f=F/ml, and wis deﬁned in (10.3). An analytical solution of (11.1) is
possible only for small oscillations. Indeed, the damped, driven pendulum can be chaotic when
oscillations are large.
11.1 The linear pendulum
11.1.1 Damped pendulum
Here, we exclude the external force, and consider the damped pendulum using the small am-
plitude approximation sin qq. The governing equation becomes the linear, second-order,
homogeneous differential equation given
¨q+l˙q+w2q=0, (11.2)
which is usually discussed in detail in a ﬁrst course on differential equations.
The characteristic equation of (11.2) is obtained by the ansatz q(t) =exp(at), which yields
a2+la+w2=0, (11.3)
with solution
a= 1
2l1
2p
l2 4w2. (11.4)
For convenience, we deﬁne b=l/2 so that (11.4) becomes
a= bq
b2 w2. (11.5)
The discriminant of (11.5) is b2 w2, and its sign determines the nature of the damped oscilla-
tions.
The underdamped pendulum satisﬁes b<w, and we write
a= biw,
65
11.1. THE LINEAR PENDULUM
where w=p
w2 b2and i=p 1. In this case, the general solution of (11.2) is a damped
oscillation given by
q(t) =e bt(Acoswt+Bsinwt).
The overdamped pendulum satisﬁes b>w, and the general solution is an exponential decay
and is given by
q(t) =c1ea+t+c2ea t,
where both a+anda are negative.
The critically damped pendulum corresponds to the special case when b=w, and with
a+=a =a<0, the general solution is given by
q(t) =(c1+c2t)eat.
11.1.2 Driven pendulum
Here, we neglect friction but include the external periodic force. The small amplitude approxi-
mation results in the governing equation
¨q+w2q=fcosWt. (11.6)
An interesting solution occurs exactly at resonance, when the external forcing frequency Wex-
actly matches the frequency wof the unforced oscillator. Here, the inhomogeneous term of
the differential equation is a solution of the homogeneous equation. With the initial conditions
q(0) =q0and ˙q(0) =0, the solution at resonance can be determined to be
q(t) =q0coswt+f
2wtsinwt,
which is a sum of a homogeneous solution (with coefﬁcients determined to satisfy the initial con-
ditions) plus the particular solution. The particular solution is an oscillation with an amplitude
that increases linearly with time. Eventually, the small amplitude approximation used to derive
(11.6) will become invalid.
An interesting computation solves the pendulum equation at resonance—replacing w2qin
(11.6) by w2sinq—with the pendulum initially at rest at the bottom ( q0=0). What happens to
the amplitude of the oscillation after its initial linear increase?
11.1.3 Damped, driven pendulum
Here, we consider both friction and an external periodic force. The small amplitude approxima-
tion of (11.1) is given by
¨q+l˙q+w2q=fcosWt. (11.7)
The general solution to (11.7) is determined by adding a particular solution to the general solution
of the homogeneous equation. Because of friction, the homogeneous solutions decay to zero
leaving at long times only the non-decaying particular solution. To ﬁnd this particular solution,
we note that the complex ode given by
¨z+l˙z+w2z=f eiWt, (11.8)
with z=x+iy, represents two real odes given by
¨x+l˙x+w2x=fcosWt,¨y+l˙y+w2y=fsinWt,
where the ﬁrst equation is the same as (11.7). We can therefore solve the complex ode (11.8) for
z(t), and then take as our solution q(t) =Re(z).
66 CHAPTER 11. THE DAMPED, DRIVEN PENDULUM
11.1. THE LINEAR PENDULUM
With the ansatz zp=AeiWt, we have from (11.8)
 W2A+ilWA+w2A=f,
or solving for A,
A=f
(w2 W2) +ilW. (11.9)
The complex coefﬁcient Adetermines both the amplitude and the phase of the oscillation. We
ﬁrst rewrite Aby multiplying the numerator and denominator by the complex conjugate of the
denominator:
A=f (w2 W2) ilW
(w2 W2)2+l2W2.
Now, using the polar form of a complex number, we have
(w2 W2) ilW=q
(w2 W2)2+l2W2eif,
where tan f=lW/(W2 w2). Therefore, Acan be rewritten as
A=f eif
p
(w2 W2)2+l2W2.
With the particular solution given by q(t) =Re(Aeiwt), we have
q(t) = 
fp
(w2 W2)2+l2W2!
Re
ei(Wt+f)
(11.10)
= 
fp
(w2 W2)2+l2W2!
cos(Wt+f). (11.11)
The amplitude of the pendulum’s oscillation at long times is therefore given by
fp
(w2 W2)2+l2W2,
and the phase shift of the oscillation relative to the external periodic force is given by f.
For example, if the external forcing frequency is tuned to match the frequency of the unforced
oscillator, that is, W=w, then one obtains directly from (11.9) that A=f/(ilw), so that the
asymptotic solution for q(t)is given by
q(t) =f
lwsinwt. (11.12)
The oscillator is observed to be p/2 out of phase with the external force, or in other words, the
velocity of the oscillator, not the position, is in phase with the force.
The solution given by (11.12) shows that large amplitude oscillations can result by either
increasing f, or decreasing lorw. As the amplitude of oscillation becomes large, the small
amplitude approximation sin qqmay become inaccurate and the true pendulum solution may
diverge from (11.12).
CHAPTER 11. THE DAMPED, DRIVEN PENDULUM 67
11.2. THE NONLINEAR PENDULUM
11.2 The nonlinear pendulum
As we already eluded, the fully nonlinear damped, driven pendulum can become chaotic. To
study (11.1) numerically, or for that matter any other equation, the number of free parameters
should be reduced to a minimum. This usually means that the governing equations should
be nondimensionalized, and the dimensional parameters should be grouped into a minimum
number of dimensionless parameters. How many dimensionless parameters will there be? The
answer to this question is called the Buckingham PTheorem.
The Buckingham PTheorem :If an equation involves n dimensional parameters that are speciﬁed
in terms of k independent units, then the equation can be nondimensionalized to one involving n  k
dimensionless parameters.
Now, the damped, driven pendulum equation (11.1) contains four dimensional parameters, l,
f,w, and W, and has a single independent unit, namely time. Therefore, this equation can
be nondimensionalized to an equation with only three dimensionless parameters. Namely, we
nondimensionalize time using one of the dimensional parameters. Here, we choose w, with units
of inverse time, and write
t=wt,
where tis now the dimensionless time. The damped, driven pendulum equation (11.1) therefore
nondimensionalizes to
d2q
dt2+l
wdq
dt+sinq=f
w2
cosW
w
t
, (11.13)
and the remaining three dimensionless groupings of parameters are evidently
l
w,f
w2,W
w.
We may give these three dimensionless groupings new names. Rather than introduce even more
named parameters into the problem, I will now call the dimensionless time t, and reuse some of
the other parameter names, with the understanding that the damped, driven pendulum equation
that we will now study numerically is dimensionless. We will therefore study the equation
¨q+1
q˙q+sinq=fcoswt, (11.14)
with the now dimensionless parameters named q,fandw.
Equation (11.14) is called a non-autonomous equation. For a differential equation to be called
autonomous, the independent variable tmust not appear explicitly. It is possible to write this
second-order non-autonomous differential equation as a system of three ﬁrst-order autonomous
equations by introducing the dependent variable y=wt. We therefore have
˙q=u,
˙u= 1
qu sinq+fcosy, (11.15)
˙y=w.
The necessary conditions for an autonomous system of differential equations to admit chaotic
solutions are (1) the system has at least three independent dynamical variables, and; (2) the
system contains at least one nonlinear coupling. Here, we see that the damped, driven pendulum
68 CHAPTER 11. THE DAMPED, DRIVEN PENDULUM
11.2. THE NONLINEAR PENDULUM
equation satisﬁes these conditions, where the three independent dynamical variables are q,uand
y, and there are two nonlinear couplings, sin qand cos y, where we already know that the ﬁrst
nonlinear coupling is required for chaotic solutions.
But what exactly is chaos? What we are considering here is called deterministic chaos, that
is chaotic solutions to deterministic equations such as a non-stochastic differential equation. Al-
though there is no deﬁnitive deﬁnition of chaos, perhaps its most important characteristic is a
solution’s sensitivity to initial conditions. A small change in initial conditions can lead to a large
deviation in a solution’s behavior. A solution’s sensitivity to initial conditions has been called
the Butterﬂy Effect, where the image of a butterﬂy appeared in the title of a talk that one of the
founders of the ﬁeld, Edward Lorenz, gave in 1972: “Does the ﬂap of a butterﬂy’s wings in Brazil
set off a tornado in Texas?”
We can easily observe that the small amplitude approximation of (11.14) can not admit chaotic
solutions. Suppose we consider two solutions q1(t)andq2(t)to the approximate equations, these
two solutions differing only in their initial conditions. We therefore have
¨q1+1
q˙q1+q1=fcoswt,
¨q2+1
q˙q2+q2=fcoswt.
If we deﬁne d=q2 q1, then the equation satisﬁed by d=d(t)is given by
¨d+1
q˙d+d=0,
which is the undriven, damped pendulum equation. Therefore, d(t)!0 for large times, and the
solution for q2andq1eventually converge, despite different initial conditions. In other words,
there is no sensitivity to initial conditions in the solution. Only for large amplitudes where the
approximation sin qqbecomes invalid, are chaotic solutions possible.
We will next learn some of the concepts and tools required for a numerical exploration of
chaos in dynamical systems.
CHAPTER 11. THE DAMPED, DRIVEN PENDULUM 69
11.2. THE NONLINEAR PENDULUM
70 CHAPTER 11. THE DAMPED, DRIVEN PENDULUM
Chapter 12
Concepts and tools
We introduce here the concepts and tools that are useful in studying nonlinear dynamical
systems.
12.1 Fixed points and linear stability analysis
Consider the one-dimensional differential equation for x=x(t)given by
˙x=f(x). (12.1)
We say that xis a ﬁxed point, or equilibrium point, of (12.1) if f(x) =0, so that at a ﬁxed point,
˙x=0. The name ﬁxed point is apt since the solution to (12.1) with initial condition x(0) = xis
ﬁxed at x(t) =xfor all time t.
A ﬁxed point, however, can be stable or unstable. A ﬁxed point is said to be stable if a small
perturbation decays in time; it is said to be unstable if a small perturbation grows in time.
We can determine stability by a linear analysis. Let x=x+e(t), where erepresents a small
perturbation of the solution from the ﬁxed point x. Because xis a constant, ˙x=˙e; and because
xis a ﬁxed point, f(x) =0. Taylor series expanding about e=0, we have
˙e=f(x+e)
=f(x) +ef0(x) +. . .
=ef0(x) +. . . .
The omitted terms in the Taylor series expansion are proportional to e2, and can be made
negligible—at least over a short time interval—by taking e(0)small. The differential equation
to be considered, ˙e=f0(x)e, is therefore linear, and has the solution
e(t) =e(0)ef0(x)t.
The perturbation of the ﬁxed point solution x(t) = xthus decays or grows exponentially de-
pending on the sign of f0(x). The stability condition on xis therefore
xisa stable ﬁxed point if f0(x)<0,
an unstable ﬁxed point if f0(x)>0.
For the special case f0(x) = 0, we say the ﬁxed point is marginally stable. We will see that
bifurcations usually occur at parameter values where ﬁxed points become marginally stable.
Difference equations, or maps, may be similarly analyzed. Consider the one-dimensional
map given by
xn+1=f(xn). (12.2)
We say that xn=xis a ﬁxed point of the map if x=f(x). The stability of this ﬁxed point can
then be determined by writing xn=x+enso that (12.2) becomes
x+en+1=f(x+en)
=f(x) +enf0(x) +. . .
=x+enf0(x) +. . . .
71
12.1. FIXED POINTS AND LINEAR STABILITY ANALYSIS
Therefore, to leading-order in e,en+1
en=f0(x),
and the stability condition on xfor a one-dimensional map is
xisa stable ﬁxed point if jf0(x)j<1,
an unstable ﬁxed point if jf0(x)j>1.
Here, marginal stability occurs when jf0(x)j=1, and bifurcations usually occur at parameter
values where the ﬁxed point becomes marginally stable. If f0(x) = 0, then the ﬁxed point
is called superstable. Perturbations to a superstable ﬁxed point decay especially fast, making
numerical calculations at superstable ﬁxed points more rapidly convergent.
The tools of ﬁxed point and linear stability analysis are also applicable to higher-order sys-
tems of equations. Consider the two-dimensional system of differential equations given by
˙x=f(x,y),˙y=g(x,y). (12.3)
The point (x,y)is said to be a ﬁxed point of (12.3) if f(x,y) = 0 and g(x,y) = 0. Again,
the local stability of a ﬁxed point can be determined by a linear analysis. We let x(t) =x+e(t)
and y(t) = y+d(t), where eanddare small independent perturbations from the ﬁxed point.
Making use of the two dimensional Taylor series of f(x,y)and g(x,y)about the ﬁxed point, or
equivalently about (e,d) = ( 0, 0), we have
˙e=f(x+e,y+d)
=f+e¶f
¶x+d¶f
¶y+. . .
=e¶f
¶x+d¶f
¶y+. . . ,
˙d=g(x+e,y+d)
=g+e¶g
¶x+d¶g
¶y+. . .
=e¶g
¶x+d¶g
¶y+. . . ,
where in the Taylor series f,gand the similarly marked partial derivatives all denote functions
evaluated at the ﬁxed point. Neglecting higher-order terms in the Taylor series, we thus have a
system of odes for the perturbation, given in matrix form as
d
dte
d
=¶f/¶x¶f/¶y
¶g/¶x¶g/¶ye
d
. (12.4)
The two-by-two matrix in (12.4) is called the Jacobian matrix at the ﬁxed point. An eigenvalue
analysis of the Jacobian matrix will typically yield two eigenvalues l1andl2. These eigenvalues
may be real and distinct, complex conjugate pairs, or repeated. The ﬁxed point is stable (all
perturbations decay exponentially) if both eigenvalues have negative real parts. The ﬁxed point
is unstable (some perturbations grow exponentially) if at least one eigenvalue has a positive real
part. Fixed points can be further classiﬁed as stable or unstable nodes, unstable saddle points,
stable or unstable spiral points, or stable or unstable improper nodes.
72 CHAPTER 12. CONCEPTS AND TOOLS
12.2. BIFURCATIONS
→←→
r<0→ →
r=0 r>0dx/dt
x(a)
(b)x*
r
Figure 12.1: Saddlenode bifurcation. (a) ˙x versus x; (b) bifurcation diagram.
12.2 Bifurcations
For nonlinear systems, small changes in the parameters of the system can result in qualitative
changes in the dynamics. These qualitative changes are called bifurcations. Here we consider
four classic bifurcations of one-dimensional nonlinear differential equations: the saddle-node bi-
furcation, the transcritical bifurcation, and the supercritical and subcritical pitchfork bifurcations.
The differential equation that we will consider is in general written as
˙x=fr(x),
where the subscript rrepresents a parameter that results in a bifurcation when varied across zero.
The simplest differential equations that exhibit these bifurcations are called the normal forms , and
correspond to a local analysis (i.e., Taylor series expansion) of more general differential equations
around the ﬁxed point, together with a possible rescaling of x.
12.2.1 Saddle-node bifurcation
The saddle-node bifurcation results in ﬁxed points being created or destroyed. The normal form
for a saddle-node bifurcation is given by
˙x=r+x2.
The ﬁxed points are x=p r. Clearly, two real ﬁxed points exist when r<0 and no real
ﬁxed points exist when r>0. The stability of the ﬁxed points when r<0 are determined by the
CHAPTER 12. CONCEPTS AND TOOLS 73
12.2. BIFURCATIONS
← → ←
r<0dx/dt
x(a)
← ←
r=0← ← →
r>0
x*
r(b)
Figure 12.2: Transcritical bifurcation. (a) ˙x versus x; (b) bifurcation diagram.
derivative of f(x) =r+x2, given by f0(x) =2x. Therefore, the negative ﬁxed point is stable and
the positive ﬁxed point is unstable.
We can illustrate this bifurcation. First, in Fig. 12.1(a), we plot ˙xversus xfor the three
parameter values corresponding to r<0,r=0 and r>0. The values at which ˙x=0 correspond
to the ﬁxed points, and arrows are drawn indicating how the solution x(t)evolves (to the right
if˙x>0 and to the left if ˙x<0). The stable ﬁxed point is indicated by a ﬁlled circle and the
unstable ﬁxed point by an open circle. Note that when r=0, solutions converge to the origin
from the left, but diverge from the origin on the right.
Second, we plot the standard bifurcation diagram in Fig. 12.1(b), where the ﬁxed point xis
plotted versus the bifurcation parameter r. As is the custom, the stable ﬁxed point is denoted by
a solid line and the unstable ﬁxed point by a dashed line. Note that the two ﬁxed points collide
and annihilate at r=0, and there are no ﬁxed points for r>0.
12.2.2 Transcritical bifurcation
A transcritical bifurcation occurs when there is an exchange of stabilities between two ﬁxed
points. The normal form for a transcritical bifurcation is given by
˙x=rx x2.
The ﬁxed points are x=0 and x=r. The derivative of the right-hand-side is f0(x) =r 2x, so
that f0(0) =rand f0(r) = r. Therefore, for r<0,x=0 is stable and x=ris unstable, while
forr>0,x=ris stable and x=0 is unstable. The two ﬁxed points thus exchange stability as
rpasses through zero. The transcritical bifurcation is illustrated in Fig. 12.2.
74 CHAPTER 12. CONCEPTS AND TOOLS
12.2. BIFURCATIONS
→ ←
r<0dx/dt
x(a)
→ ←
r=0← → ← →
r>0
x*
r(b)
Figure 12.3: Supercritical pitchfork bifurcation. (a) ˙x versus x; (b) bifurcation diagram.
12.2.3 Pitchfork bifurcations
The pitchfork bifurcations occur when one ﬁxed point becomes three at the bifurcation point.
Pitchfork bifurcations are usually associated with the physical phenomena called symmetry
breaking. Pitchfork bifurcations come in two types. In the supercritical pitchfork bifurcation,
the stability of the original ﬁxed point changes from stable to unstable and a new pair of stable
ﬁxed points are created above (super-) the bifurcation point. In the subcritical bifurcation, the
stability of the original ﬁxed point again changes from stable to unstable but a new pair of now
unstable ﬁxed points are created below (sub-) the bifurcation point.
Supercritical pitchfork bifurcation
The normal form for the supercritical pitchfork bifurcation is given by
˙x=rx x3. (12.5)
Note that the linear term results in exponential growth when r>0 and the nonlinear term
stabilizes this growth. The ﬁxed points are x=0 and x=pr, the latter ﬁxed points existing
only when r>0. The derivative of fisf0(x) = r 3x2so that f0(0) = rand f0(pr) = 2r.
Therefore, the ﬁxed point x=0 is stable for r<0 and unstable for r>0 while the ﬁxed points
x=prexist and are stable for r>0. Notice that the ﬁxed point x=0 becomes unstable as
rcrosses zero and two new stable ﬁxed points x=prare born. The supercritical pitchfork
bifurcation is illustrated in Fig. 12.3.
CHAPTER 12. CONCEPTS AND TOOLS 75
12.2. BIFURCATIONS
x*
r
Figure 12.4: Subcritical pitchfork bifurcation diagram.
The pitchfork bifurcation illustrates the physics of symmetry breaking. The differential equa-
tion (12.5) is invariant under the transformation x!  x. Fixed point solutions of this equation
that obey this same symmetry are called symmetric; ﬁxed points that do not are called asym-
metric. Here, x=0 is the symmetric ﬁxed point and x=prare asymmetric. Asymmetric
ﬁxed points always occur in pairs, and mirror each others stability characteristics. Only the initial
conditions determine which asymmetric ﬁxed point is asymptotically attained.
Subcritical pitchfork bifurcation
In the subcritical case, the cubic term is destabilizing. The normal form (to order x3) is
˙x=rx+x3.
The ﬁxed points are x=0 and x=p r, the latter ﬁxed points existing only when r0.
The derivative of the right-hand-side is f0(x) = r+3x2so that f0(0) = rand f0(p r) = 2r.
Therefore, the ﬁxed point x=0 is stable for r<0 and unstable for r>0 while the ﬁxed points
x=p rexist and are unstable for r<0. There are no stable ﬁxed points when r>0.
The absence of stable ﬁxed points for r>0 indicates that the neglect of terms of higher-order
inxthan x3in the normal form may be unwarranted. Keeping to the intrinsic symmetry of the
equations (only odd powers of xso that the equation is invariant when x!  x) we can add a
stabilizing nonlinear term proportional to x5. The extended normal form (to order x5) is
˙x=rx+x3 x5,
and is somewhat more difﬁcult to analyze. The ﬁxed points are solutions of
x(r+x2 x4) =0.
The ﬁxed point x=0 exists for all r, and four additional ﬁxed points can be found from the
solutions of the quadratic equation in x2:
x=r
1
2(1p
1+4r).
These ﬁxed points exist only if xis real. Clearly, for the inner square-root to be real, r  1/4.
Also observe that 1  p
1+4rbecomes negative for r>0. We thus have three intervals in rto
76 CHAPTER 12. CONCEPTS AND TOOLS
12.2. BIFURCATIONS
consider, and these regions and their ﬁxed points are
r< 1
4:x=0(one ﬁxed point );
 1
4<r<0 : x=0, x=r
1
2(1p
1+4r) (ﬁve ﬁxed points );
r>0 : x=0, x=r
1
2(1+p
1+4r) (three ﬁxed points ).
Stability is determined from f0(x) = r+3x2 5x4. Now, f0(0) = rsox=0 is stable for r<0
and unstable for r>0. The calculation for the other four roots can be simpliﬁed by noting that
xsatisﬁes r+x2 x4=0, or x4=r+x2. Therefore,
f0(x) =r+3x2
 5x4

=r+3x2
 5(r+x2
)
= 4r 2x2

= 2(2r+x2
).
With x2=1
2(1p
1+4r), we have
f0(x) = 2
2r+1
2(1p
1+4r)
= 
(1+4r)p
1+4r
= p
1+4rp
1+4r1
.
Clearly, the plus root is always stable since f0(x)<0. The minus root exists only for  1
4<r<0
and is unstable since f0(x)>0. We summarize the stability of the various ﬁxed points:
r< 1
4:x=0(stable );
 1
4<r<0 : x=0,(stable )
x=r
1
2(1+p
1+4r) (stable );
x=r
1
2(1 p
1+4r) (unstable );
r>0 : x=0(unstable )
x=r
1
2(1+p
1+4r) ( stable ).
The bifurcation diagram is shown in Fig. 12.4. Notice that there in addition to a subcritical
pitchfork bifurcation at the origin, there are two symmetric saddlenode bifurcations that occur
when r= 1/4.
We can imagine what happens to the solution to the ode as rincreases from negative values,
supposing there is some noise in the system so that x(t)ﬂuctuates around a stable ﬁxed point.
Forr< 1/4, the solution x(t)ﬂuctuates around the stable ﬁxed point x=0. As rincreases
into the range 1/4<r<0, the solution will remain close to the stable ﬁxed point x=0.
However, a so-called catastrophe occurs as soon as r>0. The x=0 ﬁxed point is lost and the
CHAPTER 12. CONCEPTS AND TOOLS 77
12.3. PHASE PORTRAITS
solution will jump up (or down) to the only remaining ﬁxed point. A similar catastrophe can
happen as rdecreases from positive values. In this case, the jump occurs as soon as r< 1/4.
Since the behavior of x(t)is different depending on whether we increase or decrease r, we say
that the system exhibits hysteresis. The existence of a subcritical pitchfork bifurcation can be
very dangerous in engineering applications since a small change in the physical parameters of a
problem can result in a large change in the equilibrium state. Physically, this can result in the
collapse of a structure.
12.2.4 Hopf bifurcations
A new type of bifurcation can occur in two dimensions. Suppose there is some control parameter
m. Furthermore, suppose that for m<0, a two-dimensional system approaches a ﬁxed point
by exponentially-damped oscillations. We know that the Jacobian matrix at the ﬁxed point with
m<0 will have complex conjugate eigenvalues with negative real parts. Now suppose that when
m>0 the real parts of the eigenvalues become positive so that the ﬁxed point becomes unstable.
This change in stability of the ﬁxed point is called a Hopf bifurcation . The Hopf bifurcations also
come in two types: a supercritical Hopf bifurcation and a subcritical Hopf bifurcation. For the
supercritical Hopf bifurcation, as mincreases slightly above zero, the resulting oscillation around
the now unstable ﬁxed point is quickly stabilized at small amplitude, and one obtains a limit
cycle. For the subcritical Hopf bifurcation, as mincreases slightly above zero, the limit cycle
immediately jumps to large amplitude.
12.3 Phase portraits
The phase space of a dynamical system consists of the independent dynamical variables. For
example, the phase space of the damped, driven pendulum equations given by (11.15) is three
dimensional and consists of q,uandf. The unforced equations have only a two-dimensional
phase space, consisting of qand u.
An important feature of paths in phase space is that they can never cross except at a ﬁxed
point, which may be stable if all paths go into the point, or unstable if some paths go out. This
is a consequence of the uniqueness of the solution to a differential equation. Every point on the
phase-space diagram represents a possible initial condition for the equations, and must have a
unique trajectory associated with that initial condition.
The simple pendulum is a conservative system, exhibiting a conservation law for energy,
and this implies a conservation of phase space area (or volume). The damped pendulum is
nonconservative, however, and this implies a shrinking of phase space area (or volume).
Indeed, it is possible to derive a general condition to determine whether phase space volume
is conserved or shrinks. We consider here, for convenience, equations in a three-dimensional
phase space given by
˙x=F(x,y,z),˙y=G(x,y,z),˙z=H(x,y,z).
We can consider a small volume of phase space G=G(t), given by
G(t) =DxDyDz, (12.6)
where
Dx=x1(t) x0(t),Dy=y1(t) y0(t),Dz=z1(t) z0(t).
The initial phase-space volume at time trepresents a box with one corner at the point (x0(t),y0(t),z0(t))
and the opposing corner at (x1(t),y1(t),z1(t)). This initial phase-space box then evolves over
time.
78 CHAPTER 12. CONCEPTS AND TOOLS
12.4. LIMIT CYCLES
To determine how an edge emanating from (x0,y0,z0)with length Dxevolves, we write using
a ﬁrst-order Taylor series expansion in Dt
x0(t+Dt) =x0(t) +DtF(x0,y0,z0),
x1(t+Dt) =x1(t) +DtF(x1,y0,z0).
Therefore,
Dx(t+Dt) =Dx(t) +Dt 
F(x1,y0,z0) F(x0,y0,z0)
. (12.7)
Subtracting Dx(t)from both sides and dividing by Dt, we have as Dt!0,
d
dt(Dx) = 
F(x1,y0,z0) F(x0,y0,z0)
.
WithDxsmall, we therefore obtain to ﬁrst order in Dx,
d
dt(Dx) =Dx¶F
¶x,
where the partial derivative is evaluated at the point (x0(t),y0(t),z0(t)).
Similarly, we also have
d
dt(Dy) =Dy¶G
¶y,d
dt(Dz) =Dz¶H
¶z.
Since
dG
dt=d(Dx)
dtDyDz+Dxd(Dy)
dtDz+DxDyd(Dz)
dt,
we have
dG
dt=¶F
¶x+¶G
¶y+¶H
¶z
G, (12.8)
valid for the evolution of an inﬁnitesimal box.
We can conclude from (12.8) that phase-space volume is conserved if the divergence on the
right-hand-side vanishes, or that phase-space volume contracts exponentially if the divergence is
negative.
For example, we can consider the equations for the damped, driven pendulum given by
(11.15). The divergence in this case is derived from
¶
¶q(u) +¶
¶u
 1
qu sinq+fcosy
+¶
¶y(w) = 1
q;
and provided q>0 (i.e., the pendulum is damped), phase-space volume contracts. For the
undamped pendulum (where q!¥), phase-space volume is conserved.
12.4 Limit cycles
The asymptotic state of a nonlinear system may be a limit cycle instead of a ﬁxed point. A stable
limit cycle is a periodic solution on which all nearby solutions converge. Limit cycles can also
be unstable. Determining the existence of a limit cycle from a nonlinear system of equations
through analytical means is usually impossible, but limit cycles are easily found numerically.
The damped, driven pendulum equations has no ﬁxed points, but does have limit cycles.
CHAPTER 12. CONCEPTS AND TOOLS 79
12.5. ATTRACTORS AND BASINS OF ATTRACTION
12.5 Attractors and basins of attraction
An attractor is a stable conﬁguration in phase space. For example, an attractor can be a stable
ﬁxed point or a limit cycle. The damped, non-driven pendulum converges to a stable ﬁxed point
corresponding to the pendulum at rest at the bottom. The damped, driven pendulum for certain
values of the parameters converges to a limit cycle. We will see that the chaotic pendulum also
has an attractor of a different sort, called a strange attractor.
The domain of attraction of a stable ﬁxed point, or of a limit cycle, is called the attractor’s
basin of attraction. The basin of attraction consists of all initial conditions for which solutions
asymptotically converge to the attractor. For nonlinear systems, basins of attraction can have
complicated geometries.
12.6 Poincaré sections
The Poincaré section is a method to reduce by one or more dimensions the phase space diagrams
of higher dimensional systems. Most commonly, a three-dimensional phase space can be reduced
to two-dimensions, for which a plot may be easier to interpret. For systems with a periodic
driving force such as the damped, driven pendulum system given by (11.15), the two-dimensional
phase space (q,u)can be viewed stroboscopically, with the strobe period taken to be the period of
forcing, that is 2 p/w. For the damped, driven pendulum, the third dynamical variable ysatisﬁes
y=y0+wt, and the plotting of the phase-space variables (q,u)at the times t0,t0+2p/w,
t0+4p/w, etc., is equivalent to plotting the values of (q,u)every time the phase-space variable
yin incremented by 2 p.
For systems without periodic forcing, a plane can be placed in the three-dimensional phase
space, and a point can be plotted on this plane every time the orbit passes through it. For
example, if the dynamical variables are x,yand z, a plane might be placed at z=0 and the
values of (x,y)plotted every time zpasses through zero. Usually, one also speciﬁes the direction
of passage, so that a point is plotted only when ˙z>0, say.
12.7 Fractal dimensions
The attractor of the chaotic pendulum is called strange because it occupies a fractional dimension
of phase space. To understand what is meant by fractional dimensions, we ﬁrst review some
classical fractals.
12.7.1 Classical fractals
Cantor set
Perhaps the most famous classical fractal is the Cantor set. To construct the Cantor set, we start
with the line segment S0= [0, 1]. We then remove the middle one-third of this line segment, and
denote the remaining set as S1= [0, 1/3 ][[2/3, 1 ]. Then we remove the middle one-third of both
remaining line segments, and denote this set as S2= [0, 1/9 ][[2/9, 1/3 ][[2/3, 7/9 ][[8/9, 1 ].
Repeating this process ad inﬁnitum, the Cantor set is deﬁned as
S=limn!¥Sn. (12.9)
An illustration showing how to construct the Cantor set is shown in Fig. 12.5.
80 CHAPTER 12. CONCEPTS AND TOOLS
12.7. FRACTAL DIMENSIONS
Figure 12.5: Construction of the Cantor set.
The Cantor set has some unusual properties. First, we compute its length. Let lndenote the
length of the set Sn. Clearly, l0=1. Since S1is constructed by removing the middle third of S0,
we have l1=2/3. To construct S2, we again remove the middle third of the two line segments of
S1, reducing the length of S1by another factor of 2/3, so that l2= (2/3)2, and so on. We obtain
ln= (2/3)n, and the lim n!¥ln=0. Therefore, the Cantor set has zero length.
The Cantor set, then, does not consist of line segments. Yet, the Cantor set is certainly not an
empty set. We can see that at stage 1, there are two interior endpoints 1/3 and 2/3, and these
will never be removed; at stage 2, there are the additional interior endpoints 1/9, 2/9, 7/9 and
8/9; and at stage 3, we add eight more interior endpoints. We see, then, that at stage kwe add 2k
more interior endpoints. An inﬁnite but countable number of endpoints are therefore included
in the Cantor set.
But the Cantor set consists of much more than just these endpoints, and we will in fact show
that the Cantor set is an uncountable set. Recall from analysis, that an inﬁnite set of points can
be either countable or uncountable. A countable set of points is a set that can be put in a one-
to-one correspondence with the set of natural numbers. As is well known, the inﬁnite set of all
rational numbers is countable whereas the inﬁnite set of real numbers is uncountable. By listing
the endpoints of the Cantor set above, each stage adding 2kmore endpoints, we have shown that
the set of all endpoints is a countable set.
In order to prove that the Cantor set is uncountable, it is helpful to make use of the base 3
representation of numbers. Recall that any number N, given by
N=. . . aB3+bB2+cB+dB0+eB 1+fB 2+gB 3+. . . ,
can be written in base Bas
N=. . . abcd.efg . . . (base B),
CHAPTER 12. CONCEPTS AND TOOLS 81
12.7. FRACTAL DIMENSIONS
where the period separating the integer part of the number from the fractional part is in general
called a radix point. For base 10, of course, the radix point is called a decimal point, and for base
2, a binary point.
Now, consider the Cantor set. In the ﬁrst stage, all numbers lying between 1/3 and 2/3 are
removed. The remaining numbers, then, must be of the form 0.0 . . . (base 3) or 0.2 . . . (base 3),
since (almost) all the numbers having the form 0.1 . . . (base 3) have been removed. The single
exception is the endpoint 1/3 =0.1 (base 3), but this number can also be written as 1/3 =0.02
(base 3), where the bar over a number or numbers signiﬁes an inﬁnite repetition. In the second
stage, all numbers lying between 1/9 and 2/9, say, are removed, and these numbers are of the
form 0.01 . . . (base 3). We can see, then, that the Cantor set can be deﬁned as the set of all
numbers on the unit interval that contain no 1’s in their base 3 representation.
Using base 3 notation, it is easy to ﬁnd a number that is not an endpoint, yet is in the Cantor
set. For example, the number 1/4 is not an endpoint. We can convert 1/4 to base 3 by the
following calculation:
1
43=0.75,
0.753=2.25,
0.253=0.75,
0.753=2.25,
and so on, from which we ﬁnd that 1/4 =0.02 (base 3). The number 1/4, therefore, has no 1’s in
its base 3 expansion and so is an element of the Cantor set.
We can now prove that the Cantor set is an uncountable set. The proof is similar to that used
to prove that the real numbers are uncountable. If we suppose that the Cantor set is countable,
then we can list all of its elements using a base 3 expansion; that is,
x1=0.x11x12x13. . .(base 3 ),
x2=0.x21x22x23. . .(base 3 ),
x3=0.x31x32x33. . .(base 3 ),
and so on, where all the xij’s must be either a 0 or a 2 (i.e., no 1’s are allowed). It is now simple
to name a number ynot on this list. We have
y=0.y11y22y33. . . ,
where y116=x11,y226=x22,y336=x33, etc.. That is, if x11=0, then y11=2; if x11=2, then
y11=0, and so on. By constructing a number not on the list, we have obtained a reductio ad
absurdum, and we can conclude that the Cantor set is uncountable.
Note that the endpoints of the Cantor set are just those numbers written in base 3 that end
with either all 0’s or all 2’s, and these indeed form only a very small subset of the entire Cantor
set.
From the base 3 representation, we can now see that the Cantor set has the following interest-
ing characteristic. On the one hand, any small interval around a point in the Cantor set contains
another point in the Cantor set. On the other hand, there is an interval between any two points in
the Cantor set that is not in the Cantor set. For example, take the point 1/4 =0.02 in the Cantor
set, and the interval [1/4 3 4, 1/4+3 4]. In base 3, the interval is given by [0.0201 02, 0.0210 02].
There are, of course, an inﬁnite number of points in the Cantor set in this interval, one of them
being 0.0202 to the left of 1/4, and 0.02022 to the right of 1/4. If the interval was established
using 3 n, for any n, we can still ﬁnd other points in the interval that are in the Cantor set.
Also, between any two points in the Cantor set, there is an interval that was removed during the
82 CHAPTER 12. CONCEPTS AND TOOLS
12.7. FRACTAL DIMENSIONS
construction of the Cantor set. So the Cantor set consists of neither line segments nor a set of
discrete points.
The Cantor set is also self-similar, meaning that it contains smaller copies of itself at all scales.
The Cantor set is called a fractal, and is said to have a fractal dimension. Integer dimensions
are more familiar to us: we say that a line has dimension 1; an area, dimension 2; and a volume,
dimension 3.
For self-similar sets, we can place the deﬁnition of dimension on a more mathematical basis.
A line segment, a square, and a cube can also be considered self-similar. Suppose that a self-
similar set Sis composed of mcopies of itself scaled down by a factor of r. As examples, the
line segment [0, 1]is composed of two copies of itself scaled down by a factor of two; namely,
the segments [0, 1/2 ]and[1/2, 1 ]. A square is composed of four copies of itself scaled down by
a factor of two. And a cube is composed of eight copies of itself scaled down by a factor of two.
We write
m=rD,
where Dis called the similarity dimension of the set. With r=2, the line segment has m=2,
the square has m=4, and the cube has m=8, yielding the usual dimensions D=1, 2 and 3,
respectively.
Now the Cantor set is composed of two copies of itself scaled down by a factor of three.
Therefore,
2=3D,
and the similarity dimension of the Cantor set is given by D=log 2/ log 30.6309, which has
dimension smaller than that of a line, but larger than that of a point.
Sierpinski triangle
Figure 12.6: Construction of the Sierpinski triangle.
Other classical fractals can be constructed similarly to the Cantor set. The Sierpinski triangle
starts with an equilateral triangle, S0, with unit sides. We then draw lines connecting the three
midpoints of the three sides. The just formed equilateral middle triangle of side length 1/2 is
CHAPTER 12. CONCEPTS AND TOOLS 83
12.7. FRACTAL DIMENSIONS
then removed from S0to form the set S1. This process is repeated on the remaining equilateral
triangles, as illustrated in Fig. 12.6. The Sierpinski triangle Sis deﬁned as the limit of this
repetition; that is, S=lim n!¥Sn.
The Sierpinski triangle is composed of three copies of itself scaled down by a factor of two,
so that
3=2D,
and the similarity dimension is D=log 3/ log 21.5850, which has dimension between a line
and an area.
Similar to the Cantor set, the Sierpinksi triangle, though existing in a two-dimensional space,
has zero area. If we let Aibe the area of the set Si, then since area is reduced by a factor of 3/4
with each iteration, we have
An=3
4n
A0,
so that A=lim n!¥An=0.
Sierpinski carpet
Figure 12.7: Construction of the Sierpinski Carpet
The Sierpinski carpet starts with a square, S0, with unit sides. Here, we remove a center
square of side length 1/3; the remaining set is called S1. This process is then repeated as illus-
trated in Fig. 12.7.
Here, the Sierpinski carpet is composed of eight copies of itself scaled down by a factor of
three, so that
8=3D, (12.10)
and the similarity dimension is D=log 8/ log 31.8928, somewhat larger than the Sierpinski
triangle. One can say, then, that the Sierpinski carpet is more space ﬁlling than the Sierpinski
triangle, though it still has zero area.
Koch curve
Like the Cantor set, we start with the unit interval, but now we replace the middle one-third by
two line segments of length 1/3, as illustrated in 12.8, to form the set S1. This process is then
repeated on the four line segments of length 1/3 to form S2, and so on, as illustrated in Fig. 12.8.
84 CHAPTER 12. CONCEPTS AND TOOLS
12.7. FRACTAL DIMENSIONS
Figure 12.8: Construction of the Koch curve.
Here, the Koch curve is composed of four copies of itself scaled down by a factor of three, so
that
4=3D, (12.11)
and the similarity dimension is D=log 4/ log 31.2619. The Koch curve therefore has a
dimension lying between a line and an area. Indeed, with each iteration, the length of the Koch
curve increases by a factor of 4/3 so that its length is inﬁnite.
Koch snow ﬂake
Figure 12.9: Construction of the Koch snow ﬂake.
Here, we start with an equilateral triangle. Similar to the Koch curve, we replace the middle
one-third of each side by two line segments, as illustrated in 12.9. The boundary of the Koch
snow ﬂake has the same fractal dimension as the Koch curve, and is of inﬁnite length. The area
bounded by the boundary is obviously ﬁnite, however, and can be shown to be 8/5 of the area of
the original triangle. Interestingly, here an inﬁnite perimeter encloses a ﬁnite area.
CHAPTER 12. CONCEPTS AND TOOLS 85
12.7. FRACTAL DIMENSIONS
12.7.2 Correlation Dimension
Modern studies of dynamical systems have discovered sets named strange attractors. These sets
share some of the characteristics of classical fractals: they are not space ﬁlling yet have structure
at arbitrarily small scale. However, they are not perfectly self-similar in the sense of the classical
fractals, having arisen from the chaotic evolution of some dynamical system. Here, we attempt to
generalize the deﬁnition of dimension so that it is applicable to strange attractors and relatively
easy to compute. The deﬁnition and numerical algorithm described here was ﬁrst proposed in a
paper by Grassberger and Procaccia (1993).
Deﬁnitions
Consider a set SofNpoints. Denote the points in this set as xi, with i=1, 2, . . . , Nand denote the
distance between two points xiand xjasrij. Note that there are N(N 1)/2 distinct distances.
We deﬁne the correlation integral C(r)to be
C(r) = lim
N!¥2
N(N 1)fnumber of distinct distances rijless than rg.
IfC(r)µrDover a wide range of rwhen Nis sufﬁciently large, then Dis to be called the
correlation dimension of the set S. Note that with this normalization, C(r) = 1 for rlarger than
the largest possible distance between two points on the set S.
The correlation dimension agrees with the similarity dimension for an exactly self-similar set.
As a speciﬁc example, consider the Cantor set. Suppose Npoints that lie on the Cantor set are
chosen at random to be in S. Since every point on the set is within a distance r=1 of every other
point, one has C(1) = 1. Now, approximately one-half of the points in Slie between 0 and 1/3,
and the other half lie between 2/3 and 1. Therefore, only 1/2 of the possible distinct distances
rijwill be less than 1/3 (as N!¥), so that C(1/3) = 1/2. Continuing in this fashion, we ﬁnd
in general that C(1/3s) =1/2s. With r=1/3s, we have
s= logr
log 3,
so that
C(r) =2logr/ log 3
=exp(log 2 log r/ log 3 )
=rlog 2/ log 3,
valid for small values of r. We have thus found a correlation dimension, D=log 2/ log 3, in
agreement with the previously determined similarity dimension.
Numerical computation
Given a ﬁnite set SofNpoints, we want to formulate a fast numerical algorithm to compute C(r)
over a sufﬁciently wide range of rto accurately determine the correlation dimension Dfrom a
log-log plot of C(r)versus r. A point xiin the set Smay be a real number, or may be a coordinate
pair. If the points come from a Poincaré section, then the fractal dimension of the Poincaré section
will be one less than the actual fractal dimension of the attractor in the full phase space.
Deﬁne the distance rijbetween two points xiand xjinSto be the standard Euclidean distance.
To obtain an accurate value of D, one needs to throw away an initial transient before collecting
points for the set S.
86 CHAPTER 12. CONCEPTS AND TOOLS
12.7. FRACTAL DIMENSIONS
Since we are interested in a graph of log Cversus log r, we compute C(r)at points rthat are
evenly spaced in log r. For example, we can compute C(r)at the points rs=2s, where stakes on
integer values.
First, one counts the number of distinct distance rijthat lie in the interval rs 1rij<rs.
Denote this count by M(s). An approximation to the correlation integral C(rs)using the Ndata
points is then obtained from
C(rs) =2
N(N 1)s
å
s0= ¥M(s0).
One can make use of a built-in MATLAB function to speed up the computation. The function
call
[F,E] = log2(x);
returns the ﬂoating point number F and the integer E such that x = F *2^E, where 0.5jFj<1
and E is an integer. To count each rijin the appropriate bin M(s), one can compute all the values
ofrijand place them into the Matlab array r. Then one uses the vector form of log2.m to
compute the corresponding values of s:
[~,s] = log2(r); .
One can then increment the count in a Matlab array Mthat corresponds to the particular integer
values of s. The Matlab function cumsum.m can then be used to compute the Matlab array C.
Finally, a least-squares analysis of the data is required to compute the fractal dimension D.
By directly viewing the log-log plot, one can choose which adjacent points to ﬁt a straight line
through, and then use the method of least-squares to compute the slope. The MATLAB function
polyfit.m can determine the best ﬁt line and the corresponding slope. Ideally, one would also
compute a statistical error associated with this slope, and such an error should go to zero as the
number of points computed approaches inﬁnity.
CHAPTER 12. CONCEPTS AND TOOLS 87
12.7. FRACTAL DIMENSIONS
88 CHAPTER 12. CONCEPTS AND TOOLS
Chapter 13
Pendulum dynamics
13.1 Phase portrait of the undriven pendulum
The undriven pendulum has only a two dimensional space, forming a phase plane where it is
easy to visualize the phase portraits. The phase portrait of the small amplitude simple pendulum
is particularly easy to draw. The dimensionless form of the simple pendulum equation is
¨q+q=0,
and the solutions for q=q(t)and u=˙q(t)are given by
q(t) =qmcos(t+j),u(t) = qmsin(t+j).
The phase portrait in the q-uphase plane consists of concentric circles, with clockwise motion
along these circles, as seen in the small diameter circles of Fig. 13.1. As the amplitude increases,
the approximation sin qqloses validity, and the relevant dimensionless equation becomes
¨q+sinq=0.
Beyond the small diameter circles of Fig. 13.1, one observes that the connected lines become
elongated in uas the amplitude increases, implying the pendulum is slowed down when the
amplitude becomes large (i.e., the period of the pendulum is lengthened). Eventually, a ﬁnal
closed curve is drawn, called the separatrix, that separates the pendulum’s motion from periodic
to rotary, the latter motion corresponding to a pendulum that swings over the top in a circular
motion. Exactly on the separatrix trajectory, the pendulum comes to rest at the angle p, or 180,
which is an unstable ﬁxed point of the pendulum.
The simple pendulum is a conservative system, exhibiting a conservation law for energy, and
this implies a conservation of phase space area (or volume). If one evolves over time a given
initial area of the phase space of Fig. 13.1, the area of this phase space will be conserved. When
the pendulum is damped, however, the area of this phase space will shrink to zero.
13.2 Basin of attraction of the undriven pendulum
We consider the damped, undriven pendulum with governing equation
¨q+1
q˙q+sinq=0,
and the stable ﬁxed point given by (q,˙q) = ( 0, 0). How can we determine the basin of attraction
of this ﬁxed point? Of course, with ˙q=0, only the values  p<q<pwill lie in the basin of
attraction. But we also need to compute the basin of attraction for nonzero initial values of ˙q.
As is often the case, to devise a numerical algorithm it is best to appeal directly to the physics.
We want to ﬁnd the borderline of initial conditions between either the attractive ﬁxed points (0, 0)
and(2p, 0), or the attractive ﬁxed points (0, 0)and( 2p, 0). The initial conditions just on the
border result in the pendulum reaching either the unstable ﬁxed points (p, 0)or( p, 0). A small
89
13.3. SPONTANEOUS SYMMETRY-BREAKING BIFURCATION
−3−2−1 0123−3−2−10123
θu
Figure 13.1: Phase portrait of the simple pendulum.
perturbation from (p, 0)will result in one of the attractive points (0, 0)or(2p, 0); from ( p, 0),
one of the attractive points (0, 0)or( 2p, 0).
The algorithm then initializes the calculation at either the unstable ﬁxed point (p, 0)or
( p, 0), with a small perturbation in either the position or velocity that results in the ﬁnal attract-
ing point being (0, 0). We can call these values ﬁnal conditions because the differential equations
are then integrated backward in time to determine all possible initial conditions that can result
in these ﬁnal conditions. You can convince yourself that the four ﬁnal conditions that should be
used are the two pairs (p, e),(p e, 0), and ( p,e),( p+e, 0), with every small. The ﬁrst of
each pair corresponds to the immediately previous motion being rotary, and the second of each
pair corresponds to the immediately previous motion being oscillatory.
A graph of the basins of attraction of (0, 0)forq=4, corresponding to an underdamped
pendulum (critical damping is q=1/2) is shown in Fig. 13.2. The attracting ﬁxed point is
marked by an ‘x’, and the region inside the two curved lines is the basin of attraction.
13.3 Spontaneous symmetry-breaking bifurcation
An interesting supercritical pitchfork bifurcation occurs in the pendulum equations, where at the
bifurcation point one stable limit cycle splits into two. The single limit cycle displays a symmetry
that is no longer respected individually by the two new limit cycles. This type of pitchfork
bifurcation is a manifestation of what is called spontaneous symmetry breaking.
90 CHAPTER 13. PENDULUM DYNAMICS
13.3. SPONTANEOUS SYMMETRY-BREAKING BIFURCATION
−20−15−10 −5 05101520−8−6−4−202468
θ˙θ
Figure 13.2: Basin of attraction of (q,˙q) = ( 0, 0)for the unforced, underdamped pendulum
with q =4. The cross marks the attracting ﬁxed point.
The pendulum equation (11.14), written again here, is given by
¨q+1
q˙q+sinq=fcoswt. (13.1)
Using sin ( q) = sinqand cos (wt p) = coswt, the pendulum equation can be seen to be
invariant under the transformation
q! q,t!t p/w, (13.2)
with the physical interpretation that the equations of motion make no distinction between the
right and left sides of the vertical, a consequence of the symmetry of both the pendulum and the
external force.
Consider again the asymptotic small amplitude solution given by (11.10), which in dimen-
sionless variables is
q(t) =fp
(1 w2)2+ (w/q)2cos(wt+f),
with
tanf=w/q
w2 1.
We can observe that this solution is also invariant under (13.2): the small amplitude solution
obeys q(t) = q(t p/w). We say that this solution is symmetric, meaning it obeys the same
symmetry as the governing equations; that is, the motion of the small amplitude pendulum is
symmetrical about the vertical.
In general, if q=q1(t)is a solution of (13.1), then so is q=q2(t), where q2(t) = q1(t p/w).
We can prove this mathematically. Assume q=q1(t)satisﬁes (13.1). We then show that q=q2
CHAPTER 13. PENDULUM DYNAMICS 91
13.3. SPONTANEOUS SYMMETRY-BREAKING BIFURCATION
−4−2 024−2.5−2−1.5−1−0.500.511.522.5
θ˙θ
−4−2 024−2.5−2−1.5−1−0.500.511.522.5
θ
Figure 13.3: Phase-space projection of the pendulum solution before and after spontaneous
symmetry breaking. The ‘x’ represents the pendulum at rest at the bottom. The ‘o’s denote the
positions (q,˙q)and( q, ˙q)at the times t =0and t =p/w, respectively. In both plots,
f=1.5,w=2/3. (a) q =1.24 and the solution is observed to be symmetric; (b) q =1.3and
one pair of asymmetric solutions is observed.
also satisﬁes (13.1) by the following calculation:
¨q2(t) +1
q˙q2(t) +sin(q2(t)) = ¨q1(t p/w) 1
q˙q1(t p/w) sin(q1(t p/w))
= fcos(wt p)
=fcoswt.
If the two solutions q1(t)andq2(t)are equal, then we say that this solution is symmetric.
Here, we are considering asymptotic solutions that are independent of the initial conditions, since
the initial conditions themselves can also break the symmetry. However, if q1(t)andq2(t)are
not equal, we say that these solutions are asymmetric, and that spontaneous symmetry breaking
has occurred. Evidently, spontaneous symmetry breaking is a decidedly nonlinear phenomena.
After symmetry breaking occurs, the asymmetric solutions must occur in pairs, and the bifurca-
tion point looks like a pitchfork bifurcation, and can be super- or sub-critical. Any subsequent
bifurcation that occurs to one of the asymmetric solutions must also be mirrored by the other.
Spontaneous symmetry breaking occurs in the pendulum dynamics at the dimensionless
parameter values f=1.5,w=2/3, and approximately q=1.246. For qjust less than 1.246, the
single stable asymptotic solution for q=q(t)is symmetric, and for qjust greater than 1.246, there
exists a pair of stable asymptotic solutions that are asymmetric.
By projecting the phase-space trajectory onto the q ˙qplane, in Fig. 13.3 we display both
the symmetric solution when q=1.24 and the two asymmetric solutions when q=1.30. To
explicitly observe the symmetry of the solutions, we mark the value of (q,˙q)at the time t=
n2p/w, with na positive integer (equivalent to the time t=0), and the value of ( q, ˙q)
at the time t=n2p/w+p/w(equivalent to the time t=p/w). For a symmetric solution,
these two points mark the same point on the trajectory, and for asymmetric solutions they mark
92 CHAPTER 13. PENDULUM DYNAMICS
13.3. SPONTANEOUS SYMMETRY-BREAKING BIFURCATION
1.2 1.22 1.24 1.26 1.28 1.3−1−0.8−0.6−0.4−0.200.20.40.60.81
q/angbracketleftθ/angbracketright
Figure 13.4: Bifurcation diagram exhibiting spontaneous symmetry breaking. Here, f =1.5,
w=2/3, and q is the control parameter. We plot hqiversus q.
−4 −3 −2 −1 0 1 2 3 4−2.5−2−1.5−1−0.500.511.522.5
θ˙θ
Figure 13.5: Phase-space projection of the pendulum solution after spontaneous symmetry
breaking, as in Fig. 13.3b. The unstable symmetric limit cycle is the dashed-line curve.
CHAPTER 13. PENDULUM DYNAMICS 93
13.4. PERIOD-DOUBLING BIFURCATIONS
points on different trajectories. Notice that after the occurrence of symmetry breaking, one of the
asymptotic solutions undergoes an oscillation centered to the right of the vertical, and the other,
centered to the left.
We can graph a bifurcation diagram associated with the symmetry breaking of the solutions.
We ﬁx f=1.5 and w=2/3 and vary qacross the bifurcation point q=1.246. We need to
distinguish the symmetric from the asymmetric limit cycles, and one method is to compute the
average value of q(t)over one period of oscillation; that is,
hqi=1
TZT
0qdt,
where T=2p/w. For the symmetric solution, hqi=0, whereashqitakes on both positive and
negative values after symmetry breaking occurs. In Fig. 13.4, we plot the value of hqiversus q.
At a value of approximately q=1.246, spontaneous symmetry breaking occurs and the stable
symmetric limit cycle splits into two asymmetric limit cycles, in what is evidently a supercritical
pitchfork bifurcation.
The symmetric limit cycle still exists after the bifurcation point, and though unstable, can
also be computed. To compute the unstable cycle, one could determine the values of qand ˙qat
t=0 that lie on this cycle, and then integrate over one period (over which the instability doesn’t
have sufﬁcient time to develop). The problem of determining the correct initial conditions can be
cast as a problem in multidimensional root-ﬁnding. The key idea is that a symmetric limit cycle
satisﬁes q(t) = q(t p/w). We therefore determine the solution vector of initial conditions
(q(0),˙q(0))that satisﬁes the two equations
q(0) +q(p/w) =0,
˙q(0) +˙q(p/w) =0,
where q(p/w)and ˙q(p/w)are determined by integrating from t=0 the differential equations
using ode45.m with the initial conditions (q(0),˙q(0)). Root-ﬁnding can be done using either
a two-dimensional version of the secant rule or, more simply, the built-in MATLAB function
fsolve.m . Convergence to the roots is robust, and an initial guess for the roots can be taken,
for instance, as (q(0),˙q(0)) = ( 0, 0). A plot of the two stable asymmetric limit cycles, and the
unstable symmetric limit cycle when f=1.5,w=2/3 and q=1.3 is shown in Fig. 13.5.
13.4 Period-doubling bifurcations
Asqincreases further, another series of bifurcations occur, called period doubling bifurcations.
These too are supercritical pitchfork bifurcations. These bifurcations happen simultaneously to
the solutions with positive and negative values of I, and we need only consider one of these
branches here. Before the ﬁrst bifurcation occurs, the pendulum has period 2 p/w—the same
period as the external force—and the phase-space trajectory forms a closed loop when the equa-
tions are integrated over a single period. After the ﬁrst period-doubling bifurcation, which occurs
approximately at q=1.348, the period of the pendulum becomes twice the period of the external
force. In Fig. 13.6, we plot a phase-space projection onto the q ˙qplane before and after the ﬁrst
period-doubling bifurcation. Before the bifurcation, we say the pendulum has period one, and
after the bifurcation we say the pendulum has period two. Note that for the pendulum of period
two, the phase-space trajectory loops around twice before closing, each loop corresponding to
one period of the external force. To further illustrate the period-two oscillation, in Fig. 13.7 we
plot the time series of q(t)versus tover exactly two periods of the external force. Evidently, the
pendulum motion is now periodic with period twice the period of the external force (the period
of the external force is approximately 9.4).
94 CHAPTER 13. PENDULUM DYNAMICS
13.4. PERIOD-DOUBLING BIFURCATIONS
−4 −2 0 2−2−1.5−1−0.500.511.522.5
θ˙θ
−4 −2 0 2−2−1.5−1−0.500.511.522.5
θ
Figure 13.6: Phase-space projection of the pendulum solution before and after the ﬁrst period-
doubling bifurcation. In both plots, f =1.5,w=2/3. (a) q =1.34 and the oscillation has
period one; (b) q =1.36 and the oscillation has period two.
0510152025303540−4−3−2−1012
tθ
Figure 13.7: Time series of the pendulum oscillation with period two. Here, f =1.5,w=2/3,
and q =1.36.
CHAPTER 13. PENDULUM DYNAMICS 95
13.4. PERIOD-DOUBLING BIFURCATIONS
−4 −2 0 2−2−1.5−1−0.500.511.522.5
θ˙θ
−4 −2 0 2−2−1.5−1−0.500.511.522.5
θ
Figure 13.8: Phase-space projection of the pendulum solution before and after the third period-
doubling bifurcation. In both plots, f =1.5,w=2/3. (a) q =1.3740 and the oscillation has
period four; (b) q =1.3755 and the oscillation has period eight.
The period-doubling bifurcations continue with increasing q. The second doubling from
period two to period four occurs at approximately q=1.370 and the third doubling from period
four to period eight occurs at approximately q=1.375. The q ˙qphase-space projections for
period four and period eight are shown in Fig. 13.8.
A bifurcation diagram can be plotted that illustrates these period-doubling bifurcations. We
use a Poincaré section to plot the value of qat the times corresponding to 2 pn/w, with nan
integer. The control parameter for the bifurcation is q, and in Fig. 13.9, we plot the bifurcation
diagram for 1.34 <q<1.38. In general, the angle qshould be mapped into the interval  p<
q<p, but for these parameter values there is no rotary motion. Period-doubling bifurcations are
observed, and eventually the pendulum becomes aperiodic. Additional windows of periodicity
in the aperiodic regions of qare also apparent.
The aperiodic behavior of the pendulum observed in Fig. 13.9 corresponds to a chaotic pen-
dulum. If only the pendulum exhibited the period-doubling route to chaos, then the results
shown in Fig. 13.9, though interesting, would be of lesser importance. But in fact many other
nonlinear systems also exhibit this route to chaos, and there are some universal features of Fig.
13.9, ﬁrst discovered by Feigenbaum in 1975. One of these features is now called the Feigenbaum
constant, d.
To compute the Feigenbaum constant, one ﬁrst deﬁnes qnto be the value of qat which the
pendulum motion bifurcates from period 2n 1to period 2n. We have already mentioned that
q11.348, q21.370, and q31.375. We now deﬁne
dn=qn+1 qn
qn+2 qn+1, (13.3)
and the Feigenbaum constant as
d=limn!¥dn. (13.4)
96 CHAPTER 13. PENDULUM DYNAMICS
13.5. PERIOD DOUBLING IN THE LOGISTIC MAP
1.341.345 1.351.355 1.361.365 1.371.375 1.38−3.2−3−2.8−2.6−2.4−2.2−2−1.8−1.6
qθ
Figure 13.9: Bifurcation diagram for period-doubling in the pendulum. A Poincaré section plots
qat the times t =2pn/w, with n an integer. Here, f =1.5,w=2/3, and 1.34<q<1.38
Note that for the pendulum, we can already compute
d1=1.370 1.348
1.375 1.370=4.400.
The meaning of dcan be better elucidated by writing
qn+2 qn+1=qn+1 qn
dn;
and by continuing to iterate this equation, we obtain
qn+2 qn+1=q2 q1
d1d2dn.
With all the dn’s approximately equal to d, we then have the scaling
qn+2 qn+1µd n.
Adlarger than one would then insure that the bifurcations occur increasingly closer together, so
that an inﬁnite period (and chaos) is eventually attained. The value of dcan be computed to high
accuracy and has been found to be
d=4.669201609102990 . . . ,
and our value of d1=4.4 is a rough approximation.
13.5 Period doubling in the logistic map
Feigenbaum originally discovered the period-doubling route to chaos by studying a simple one-
dimensional map. A one-dimensional map with a single control parameter mcan be written
as
xn+1=fm(xn), (13.5)
CHAPTER 13. PENDULUM DYNAMICS 97
13.5. PERIOD DOUBLING IN THE LOGISTIC MAP
where fm(x)is some speciﬁed function. A one-dimensional map is iterated, starting with some
initial value x0, to obtain the sequence x1,x2,x3, . . . . If the sequence converges to x, then xis a
stable ﬁxed point of the map.
The speciﬁc one-dimensional map we will study here is the logistic map, with
fm(x) =mx(1 x). (13.6)
The logistic map is perhaps the simplest nonlinear equation that exhibits the period-doubling
route to chaos. To constrain the values of xnto lie between zero and unity, we assume that
0<m<4 and that 0 <x0<1.
A period-1 cycle for the logistic map corresponds to a stable ﬁxed point. Stable ﬁxed points
are solutions of the equation x=fm(x), or
x=mx(1 x).
The two solutions are given by x=0 and x=1 1/m. The ﬁrst ﬁxed point x=0 must be
stable for 0 <m<1, being the only ﬁxed point lying between zero and one that exists in this
range. To determine the stability of the second ﬁxed point, we make use of the linear stability
analysis discussed in §12.1.
For a one-dimensional map, xis a stable ﬁxed point of (13.5) if jf0m(x)j<1. For the logistic
map given by (13.6), f0m(0) =mso that x=0 is stable for 0 <m<1 as we have already surmised,
and for the second ﬁxed point
f0
m(1 1/m) =2 m.
Therefore, we ﬁnd that x=1 1/mis stable for 1 <m<3.
What happens when mbecomes larger than three? We will now show that the ﬁrst period-
doubling bifurcation occurs at m=3. Because of the simplicity of the logistic map, we can
determine explicitly the period-2 cycle. Consider the following composite map:
gm(x) = fm 
fm(x)
. (13.7)
Fixed points of this map will consist of both period-1 cycles and period-2 cycles of the original
map (13.6). If x=xis a ﬁxed point of the composite map (13.7), then xsatisﬁes the equation
x=gm(x).
A period-2 cycle of the map fm(x)necessarily corresponds to two distinct ﬁxed points of the
composite map (13.7). We denote these two ﬁxed points by x0and x1, which satisfy
x1=fm(x0),x0=fm(x1).
We will call x0,x1the orbit of the period-2 cycle, with the later generalization of calling x0,x1,
. . . , x2n 1the orbit of the period-2ncycle.
The period-2 cycle can now be determined analytically by solving
x=fm 
fm(x)
=fm 
mx(1 x)
=m 
mx(1 x) 
1 mx(1 x)
,
which is a quartic equation for x. Two solutions corresponding to period-1 cycles are known:
x=0 and x=1 1/m. Factoring out these two solutions, the second by using long division,
results in the quadratic equation given by
m2x2 m(m+1)x+ (m+1) =0.
98 CHAPTER 13. PENDULUM DYNAMICS
13.5. PERIOD DOUBLING IN THE LOGISTIC MAP
The period-2 cycle, then, corresponds to the two roots of this quadratic equation; that is,
x0=1
2m
(m+1) +q
(m+1)(m 3)
,x1=1
2m
(m+1) q
(m+1)(m 3)
. (13.8)
These roots are valid solutions for m3. Exactly at m=3, the period-2 cycle satisﬁes x0=x1=
2/3, which coincides with the value of the period-1 cycle x=1 1/m=2/3. At m=3, then, we
expect the ﬁxed point of the composite map corresponding to a period-1 cycle to go unstable via
a supercritical pitchfork bifurcation to a pair of stable ﬁxed points, corresponding to a period-2
cycle.
When does this period-2 cycle become unstable? At m=3 and x=2/3, we have
f0
m(x) =m(1 2x)
= 1,
so that the period-1 cycle becomes unstable when the derivative of the map function attains the
value of 1, and it is reasonable to expect that the period-2 cycle also becomes unstable when
g0
m(x0) = 1, g0
m(x1) = 1. (13.9)
Now,
g0
m(x0) = f0
m 
fm(x0)
f0
m(x0)
=f0
m(x1)f0
m(x0),
and
g0
m(x1) = f0
m 
fm(x1)
f0
m(x1)
=f0
m(x0)f0
m(x1).
The two equations of (13.9) are therefore identical, and the period-2 cycle will go unstable at the
value of msatisfying
f0
m(x0)f0
m(x1) = 1,
where x0and x1are given by (13.8).
The equation for m, then, is given by
m2(1 2x0)(1 2x1) = 1,
or
m2 
1 2(x0+x1) +4x0x1+1=0. (13.10)
Now, from (13.8),
x0+x1=m+1
m,x0x1=m+1
m2. (13.11)
Substitution of (13.11) into (13.10) results, after simpliﬁcation, in the quadratic equation
m2 2m 5=0,
with the only positive solution given by
m=1+p
6
3.449490.
We therefore expect the period-2 cycle to bifurcate to a period-4 cycle at m3.449490.
CHAPTER 13. PENDULUM DYNAMICS 99
13.5. PERIOD DOUBLING IN THE LOGISTIC MAP
nmn
1 3
2 3.449490 . . .
3 3.544090 . . .
4 3.564407 . . .
5 3.568759 . . .
6 3.569692 . . .
7 3.569891 . . .
8 3.569934 . . .
Table 13.1: The ﬁrst eight values of mnat which period-doubling bifurcations occur.
Figure 13.10: Bifurcation diagram for period-doubling in the logistic map.
100 CHAPTER 13. PENDULUM DYNAMICS
13.6. COMPUTATION OF THE FEIGENBAUM CONSTANT
If we deﬁne mnto be the value of mat which the period-2n 1cycle bifurcates to a period-2n
cycle, then we have determined analytically that m1=3 and m2=1+p
6. We list in Table 13.1,
the ﬁrst eight approximate values of mn, computed numerically.
We can compute a bifurcation diagram for the logistic map. For 2 <m<4, we plot the
iterates from the map, discarding initial transients. The bifurcation diagram is shown in Fig.
13.10. Notice the uncanny similarity between the bifurcation diagram for the logistic map, and
the one we have previously computed for the damped, driven pendulum equation, Fig. 13.9.
Also note that the computation of Fig. 13.10, being on the order of seconds, is substantially faster
than that of Fig. 13.9, which took about an hour, because a one-dimensional map is much faster
to compute than the Poincaré section of a pair of coupled ﬁrst-order differential equations.
13.6 Computation of the Feigenbaum constant
Period doubling in the logistic map enables an accurate computation of the Feigenbaum constant
d, deﬁned as
d=limn!¥dn, (13.12)
where
dn=mn+1 mn
mn+2 mn+1. (13.13)
Table 13.1 lists the known ﬁrst eight values of mnat the bifurcation points. These values, and
those at even larger values of n, are in fact very difﬁcult to compute with high precision because
of the slow convergence of the iterates at the bifurcation points. Rather, we will instead compute
the values of mat what are called superstable cycles. This now well-known method for computing
dwas ﬁrst described by Keith Briggs (1989).
Recall that for the general one-dimensional map
xn+1=fm(xn),
a perturbation ento a ﬁxed point xdecays as
en+1=f0
m(x)en.
At a so-called superstable ﬁxed point, however, f0m(x) = 0 and the perturbation decays very
much faster as
en+1=1
2f00
m(x)e2
n.
What are the superstable ﬁxed points of the logistic map? Now, the values xithat are in the
orbit of a period-2ncycle are ﬁxed points of the composite map
xn+1=gm(xn), (13.14)
where gm=fmfm fm, where the composition is repeated 2ntimes. The orbit of a super-
stable cycle, then, consists of superstable ﬁxed points of (13.14). If the period-2ncycle has orbit
x0,x1, . . . , x2n 1, we have fm(x0) =x1,fm(x1) =x2, . . . , fm(x2n 1) =x0, and by the chain rule,
g0
m(xi) = f0
m(x0)f0
m(x1)f0
m(x2n 1),
for all xiin the orbit of the period-2ncycle. With
f0
m(x) =m(1 2x),
we have g0m(x) = 0 for x=1/2. Therefore, if x0=1/2, say, is in the orbit of a period-2ncycle,
then this cycle is superstable.
CHAPTER 13. PENDULUM DYNAMICS 101
13.6. COMPUTATION OF THE FEIGENBAUM CONSTANT
At the bifurcation point creating a period-2ncycle, the cycle has marginal stability and
g0m(x0) = 1. As mincreases, g0m(x0)decreases, and eventually the period-2ncycle loses stabil-
ity when g0m(x0) = 1. At some intermediate value of m, then, there exists a value of x0with
g0m(x0) =0, and here we may assign x0=1/2. Therefore, every period-2ncycle contains a value
ofmfor which x0=1/2 is in the orbit of the cycle.
Accordingly, we deﬁne mnto be the value of mat which x0=1/2 is in the orbit of the
period-2ncycle. We can modify the deﬁnition of the Feigenbaum constant (13.13) to be
dn=mn+1 mn
mn+2 mn+1. (13.15)
Though the values of dncomputed from (13.13) and (13.15) will differ slightly, the values as
n!¥should be the same.
We can easily determine the ﬁrst two values of mn. For the period-1 cycle, we have
1
2=m01
2
1 1
2
,
orm0=2, as conﬁrmed from Fig. 13.10. To determine m1, we make use of the period-2 cycle
given by (13.7), and Fig. (13.10), which shows that the smaller root passes through x0=1/2.
Therefore,
1
2=1
2m1
(m1+1) q
(m1+1)(m1 3)
;
and solving for m1, we obtain the quadratic equation
m2
1 2m1 4=0,
with solution m1=1+p
53.2361. Further values of mnwill be computed numerically.
To determine mn, we need to solve the equation G(m) =0, where
G(m) =gm(1/2) 1
2, (13.16)
and where as before, gm(x)is the composition of fm(x)repeated 2ntimes. The roots of (13.16) are
given by m0,m1, . . .mn, so that the desired root is the largest one.
We shall use Newton’s method, §2.2, to solve (13.16). To implement Newton’s method, we
need to compute both G(m)and G0(m). Deﬁne N=2n. Then using the logistic map
xn+1=mxn(1 xn), (13.17)
and iterating with x0=1/2, we obtain x1,x2, . . . , xN. This orbit is superstable if xN=1/2.
Therefore, we have
G(m) =xN 1/2.
Moreover,
G0(m) =x0
N,
where the derivative is with respect to m. From (13.17), we have
x0
n+1=xn(1 xn) +mx0
n(1 xn) mxnx0
n
=xn(1 xn) +mx0
n(1 2xn).
Since we always choose x0=1/2, independent of m, we have as a starting value x0
0=0. Therefore,
to compute both xNand x0
N, we iterate 2ntimes the coupled map equations
xn+1=mxn(1 xn),
x0
n+1=xn(1 xn) +mx0
n(1 2xn),
102 CHAPTER 13. PENDULUM DYNAMICS
13.7. STRANGE ATTRACTOR OF THE CHAOTIC PENDULUM
n m n dn
0 2 4.7089430135405
1 1 +p
5 4.6807709980107
2 3.4985616993277 4.6629596111141
3 3.5546408627688 4.6684039259164
4 3.5666673798563 4.6689537409802
5 3.5692435316371 4.6691571813703
6 3.5697952937499 4.6691910014915
7 3.5699134654223 4.6691994819801
8 3.5699387742333 4.6692010884670
9 3.5699441946081 4.6692015881423
10 3.5699453554865 4.6692023902759
11 3.5699456041111 4.6691974782669
12 3.5699456573588 4.6693329633696
13 3.5699456687629
14 3.5699456712052
Table 13.2: The ﬁrst fourteen values of mn, and estimates of the Feigenbaum delta.
with initial values x0=1/2 and x0
0=0. Newton’s method then solves for mnby iterating
m(i+1)=m(i) xN 1/2
x0
N,
until convergence of m(i)tomn. In double precision, we have been able to achieve a precision of
about 14 digits, which we ﬁnd can be obtained in fewer than 5 iterations of Newton’s method.
For Newton’s method to converge to mn, we need a good guess for m(0). We can use the
previous best estimate for the Feigenbaum delta to predict mn. From (13.15), we ﬁnd
m(0)=mn 1+mn 1 mn 2
dn 2.
Although we can not compute dn 2without knowing mn, we can nevertheless use the estimate
dn 2dn 3. The computation, then, starts with n=2, and we can begin by taking d 1=4.4, so
that, for example,
m(0)=3.2361 +3.2361 2
4.4
=3.5170.
Using this algorithm, we have produced Table 13.2 for mn, with corresponding calculations
ofdn. As the values of mnconverge, the corresponding values of dnbegin to lose precision. It
would appear that our best estimate from the table is d4.66920, compared to the known value
ofd=4.669201609102990 . . . , computed by a different algorithm capable of achieving higher
precision.
13.7 Strange attractor of the chaotic pendulum
After the period-doubling cascade, the pendulum motion becomes chaotic. We choose parameter
values q=4,f=1.5, and w=2/3 in the chaotic regime, and after discarding an initial transient
CHAPTER 13. PENDULUM DYNAMICS 103
13.7. STRANGE ATTRACTOR OF THE CHAOTIC PENDULUM
of 256 forcing periods, compute a Poincaré section of the phase-space trajectory in the q-˙qplane,
sampling points every forcing period. The values of the periodic variable qare mapped onto the
interval p<q<p. The full Poincaré section consisting of 50,000 points is shown in the top
drawing of Fig. 13.11, and a blowup of the points within the drawn rectangle (from a sample of
200,000 points over the entire attractor) is shown in the bottom drawing.
-3 -2 -1 0 1 2 3-1-0.500.511.522.53
1.11.21.31.41.51.61.71.81.922.11.61.71.81.922.12.22.32.42.52.6
Figure 13.11: A Poincaré section of the chaotic pendulum. The values of the parameters are
q=4, f=1.5, and w=2/3. The top ﬁgure shows the entire attractor and the bottom ﬁgure
is a blow-up of the region inside the drawn rectangle.
From Fig. 13.11, it appears that the Poincaré section has structure on all scales, which is
reminiscent of the classical fractals discussed in §12.7. The set of points shown in Fig. 13.11 is
called a strange attractor, and will be seen to have a fractional correlation dimension.
Using the algorithm for computing a correlation dimension discussed in §12.7, we draw a
104 CHAPTER 13. PENDULUM DYNAMICS
13.7. STRANGE ATTRACTOR OF THE CHAOTIC PENDULUM
10−610−410−210010−810−610−410−2100
rC
Figure 13.12: The correlation integral C (r)versus r for the strange attractor shown in Fig.
13.11, using a 13,000 point sample. The least-squares line on the log-log plot yields a correlation
dimension of the Poincaré section of approximately D =1.25.
log-log plot of the correlation integral C(r)versus r, shown in Fig. 13.12. A least-squares ﬁt of
a straight line to the middle region of the plot yields a correlation dimension for the Poincaré
section of approximately D=1.25.
CHAPTER 13. PENDULUM DYNAMICS 105
13.7. STRANGE ATTRACTOR OF THE CHAOTIC PENDULUM
106 CHAPTER 13. PENDULUM DYNAMICS
Part III
Computational ﬂuid dynamics
107

The third part of this course considers a problem in computational ﬂuid dynamics (cfd).
Namely, we consider the steady two-dimensional ﬂow past a rectangle or a circle.
109
110
Chapter 14
Derivation of the governing
equations
We derive here the governing equations for the velocity u=u(x,t)of a ﬂowing ﬂuid.
14.1 Multi-variable calculus
Fluid ﬂows typically take place in three-dimensional space, and the governing equations will
contain derivatives in all three directions. The mathematics learned in a multi-variable calculus
course will therefore be useful. Here, I summarize some of this mathematics.
14.1.1 Vector algebra
Examples of vectors will be the position vector xand the velocity vector u. We will use the
Cartesian coordinate system to write vectors in terms of their components as
x= (x,y,z),u= (u,v,w),
or sometimes as
x= (x1,x2,x3),u= (u1,u2,u3).
Another notation makes use of the cartesian unit vectors, ˆ x,ˆ y, and ˆ z:
x=xˆ x+yˆ y+zˆ z,u=uˆ x+vˆ y+wˆ z.
The velocity uis called a vector ﬁeld because it is a vector that is a function of the position vector
x.
The dot product between two vectors uand vis given by
uv=u1v1+u2v2+u3v3
=uivi,
where in the last expression we use the Einstein summation convention: when an index occurs
twice in a single term, it is summed over. From hereon, the Einstein summation convention will
be assumed unless explicitly stated otherwise.
The cross product between two vectors is given by a determinant:
uv=ˆ x ˆ y ˆ z
u1u2u3
v1v2v3
= (u2v3 u3v2,u3v1 u1v3,u1v2 u2v1).
The cross-product of two vectors is a vector, and the components of the cross product can be
written more succinctly using the Levi-Civita tensor, deﬁned as
eijk=8
><
>:1 if (i,j,k)is an even permutation of (1, 2, 3 ),
 1 if (i,j,k)is an odd permutation of (1, 2, 3 ),
0 if any index is repeated.
111
14.2. CONTINUITY EQUATION
Using the Levi-Civita tensor, the i-th component of the cross product can be written as
(uv)i=eijkujvk.
Another useful tensor is the Kronecker delta, deﬁned as
dij=(
0 if i6=j,
1 if i=j.
Note that vidij=vjand that dii=3. A useful identity between the Levi-Civita tensor and the
Kronecker delta is given by
eijkeimn=djmdkn djndkm.
Gauss’s theorem (or the divergence theorem) and Stokes’ theorem are usually introduced in
a course on multi-variable calculus. We will state these theorems here.
First, Gauss’s theorem. Let Vbe a three-dimensional volume bounded by a smooth surface
S, and let Fbe a vector ﬁeld in V. Then Gauss’s theorem states that
Z
SFˆndS=Z
VrFdV, (14.1)
where ˆnis the outward facing unit normal vector to the bounding surface S.
Second, Stokes’ theorem. Let Sbe a smooth surface bounded by a simple closed curve Cwith
positive orientation. Then Stokes’ theorem states that
I
CFdr=Z
S(rF)ˆndS. (14.2)
14.2 Continuity equation
We consider a control volume Vof ﬂuid bounded by a smooth surface S. The continuity equation
expresses the conservation of mass. The time-derivative of the total mass of the ﬂuid contained
in the volume Vis equal to the (negative) of the total mass of ﬂuid that ﬂows out of the boundary
ofV; that is;
d
dtZ
Vr(x,t)dV= Z
Sr(x,t)u(x,t)ˆndS. (14.3)
The integral on the right-hand-side represents the ﬂux of mass through the boundary Sand
has units of mass per unit time. We now apply the divergence theorem to the integral on the
right-hand-side:Z
Sr(x,t)u(x,t)ˆndS=Z
Vr(ru)dV. (14.4)
Combining the left- and right-hand-sides, we have
Z
V¶r
¶t+r(ru)
dV=0. (14.5)
Because the control volume is arbitrary, the integrand must vanish identically, and we thus obtain
the continuity equation
¶r
¶t+r(ru) =0. (14.6)
We will only be considering here incompressible ﬂuids, where we may assume that r(x,t)is
a constant, independent of both space and time. The continuity equation (14.6) then becomes an
equation for conservation of ﬂuid volume, and is given by
ru=0. (14.7)
This equation is called the incompressibility condition.
112 CHAPTER 14. THE GOVERNING EQUATIONS
14.3. MOMENTUM EQUATION
14.3 Momentum equation
14.3.1 Material derivative
The Navier-Stokes equation is derived from applying Newton’s law F=mato a ﬂuid ﬂow. We
ﬁrst consider the acceleration of a ﬂuid element. The velocity of the ﬂuid at a ﬁxed position xis
given by u(x,t), but the ﬂuid element is not at a ﬁxed position but follows the ﬂuid in motion.
Now a general application of the chain rule yields
d
dtu(x,t) =¶u
¶t+¶u
¶xj¶xj
¶t.
If the position xis ﬁxed, then ¶xj/¶t=0. But if x=x(t)represents the position of the ﬂuid
element, then ¶xj/¶t=uj. The latter assumption is called the material derivative and is written
as
Du
Dt=¶u
¶t+uj¶u
¶xj
=¶u
¶t+ (ur)u,
and represents the acceleration of a ﬂuid element as it ﬂows with the ﬂuid. Instead of the mass
of the ﬂuid element, we consider the mass per unit volume, and the right-hand-side of F=ma
becomes
r¶u
¶t+ (ur)u
.
We now need ﬁnd the forces per unit volume acting on the ﬂowing ﬂuid element. We consider
both pressure forces and viscous forces.
14.3.2 Pressure forces
We consider the normal pressure forces acting on two opposing faces of a control volume of ﬂuid.
With Athe area of the rectangular face at ﬁxed y, and dythe depth, the volume of the box is
Ady, and the net pressure force per unit volume acting on the control volume in the y-direction
is given by
fp=pA (p+dp)A
Ady
= dp
dy.
Similar considerations for the xand zdirections yield the pressure force vector per unit volume
to be
fp= ¶p
¶x,¶p
¶y,¶p
¶z
= rp.
14.3.3 Viscous forces
The viscosity of a ﬂuid measures its internal resistance to ﬂow. Consider a ﬂuid conﬁned between
two very large plates of surface area A, separated by a small distance dy. Suppose the bottom
CHAPTER 14. THE GOVERNING EQUATIONS 113
14.3. MOMENTUM EQUATION
plate is stationary and the top plate move with velocity duin the x- direction. The applied force
per unit area required to keep the top surface in motion is empirically given by
F
A=mdu
dy,
where mis called the dynamic viscosity. Of course there is also an opposite force required to keep
the bottom surface stationary. The difference between these two forces is the net viscous force on
the ﬂuid element. Taking the difference, the resulting net force per unit area will be proportional
to the second derivative of the velocity. Now the viscous forces act in all directions and on all the
faces of the control volume. Without going into further technical details, we present the general
form (for a so-called Newtonian ﬂuid) of the viscous force vector per unit volume:
fv=m¶2u
¶x2+¶2u
¶y2+¶2u
¶z2,¶2v
¶x2+¶2v
¶y2+¶2v
¶z2,¶2w
¶x2+¶2w
¶y2+¶2w
¶z2
=mr2u.
14.3.4 Navier-Stokes equation
Putting together all the terms, the Navier-Stokes equation is written as
r¶u
¶t+ (ur)u
= rp+mr2u.
Now, instead of the dynamic viscosity m, one usually deﬁnes the kinematic viscosity n=m/r.
The governing equations of ﬂuid mechanics for a so-called incompressible Newtonian ﬂuid, then,
are given by both the continuity equation and the Navier-Stokes equation; that is,
ru=0, (14.8)
¶u
¶t+ (ur)u= 1
rrp+nr2u. (14.9)
14.3.5 Boundary conditions
Boundary conditions must be prescribed when ﬂows contact solid surfaces. We will assume rigid,
impermeable surfaces. If ˆ nis the normal unit vector to the surface, and if there is no motion of
the surface in the direction of its normal vector, then the condition of impermeability yields
uˆ n=0.
We will also assume the no-slip condition: a viscous ﬂuid should have zero velocity relative to a
solid surface. In other words, a stationary or moving solid surface drags along the ﬂuid touching
it with the same velocity. The no-slip condition can be expressed mathematically as
uˆ n=Vˆ n,
where uis the velocity of the ﬂuid, Vis the velocity of the surface, and ˆ nis the normal vector to
the surface.
Boundary conditions may also be prescribed far from any wall or obstacle. The free-stream
boundary condition states that u=Uat inﬁnity, where Uis called the free-stream velocity.
114 CHAPTER 14. THE GOVERNING EQUATIONS
Chapter 15
Laminar ﬂow
Smoothly ﬂowing ﬂuids, with the ﬂuid ﬂowing in undisrupted layers, are called laminar
ﬂows. There are several iconic laminar ﬂows, whose velocity ﬁelds are readily found by solving
the continuity and Navier-Stokes equations.
15.1 Plane Couette ﬂow
Plane Couette ﬂow consists of a ﬂuid ﬂowing between two inﬁnite plates separated by a distance
d. The lower plate is stationary, and the upper plate is moving to the right with velocity U. The
pressure pis constant and the ﬂuid is incompressible.
We look for a steady solution for the velocity ﬁeld of the form
u(x,y,z) = 
u(y), 0, 0
.
The incompressibility condition is automatically satisﬁed, and the ﬁrst-component of the Navier-
Stokes equation reduces to
n¶2u
¶y2=0.
Applying the boundary conditions u(0) = 0 and u(d) = Uon the lower and upper plates, the
laminar ﬂow solution is given by
u(y) =Uy
d.
15.2 Channel ﬂow
Channel ﬂow, or Poiseuille ﬂow, also consists of a ﬂuid ﬂowing between two inﬁnite plates
separated by a distance d, but with both plates stationary. Here, there is a constant pressure
gradient along the x-direction in which the ﬂuid ﬂows. Again, we look for a steady solution for
the velocity ﬁeld of the form
u(x,y,z) = 
u(y), 0, 0
,
and with
p=p(x)
and
dp
dx= G, (15.1)
with Ga positive constant. The ﬁrst-component of the Navier-Stokes equation becomes
 1
rdp
dx+nd2u
dy2=0. (15.2)
Using (15.1) in (15.2) leads to
d2u
dy2= G
nr,
115
15.3. PIPE FLOW
which can be solved using the no-slip boundary conditions u(0) =u(d) =0. We ﬁnd
u(y) =Gd2
2nry
d
1 y
d
.
The maximum velocity of the ﬂuid occurs at the midline, y=d/2, and is given by
umax=Gd2
8nr.
15.3 Pipe ﬂow
Pipe ﬂow consists of ﬂow through a pipe of circular cross-section radius R, with a constant
pressure gradient along the pipe length. With the pressure gradient along the x-direction, we
look for a steady solution of the velocity ﬁeld of the the form
u= 
u(y,z), 0, 0
.
With the constant pressure gradient deﬁned as in (15.1), the Navier-Stokes equation reduces to
¶2u
¶y2+¶2u
¶z2= G
nr. (15.3)
The use of polar coordinates in the y-zplane can aid in solving (15.3). With
u=u(r),
we have
¶2
¶y2+¶2
¶z2=1
rd
dr
rd
dr
,
so that (15.3) becomes the differential equation
d
dr
rdu
dr
= Gr
nr,
with no-slip boundary condition u(R) =0. The ﬁrst integration from 0 to ryields
rdu
dr= Gr2
2nr;
and after division by r, the second integration from rtoRyields
u(r) =GR2
4nr
1 r
R2
.
The maximum velocity occurs at the pipe midline, r=0, and is given by
umax=GR2
4nr.
116 CHAPTER 15. LAMINAR FLOW
Chapter 16
Stream function, vorticity
equations
16.1 Stream function
A streamline at time tis deﬁned as the curve whose tangent is everywhere parallel to the velocity
vector. With dxalong the tangent, we have
udx=0;
and with u= (u,v,w)and dx= (dx,dy,dz), the cross product yields the three equations
vdz=wdy,udz=wdx,udy=vdx, (16.1)
or equivalently,
dx
u=dy
v=dz
w.
Streamlines have the following two properties. They cannot intersect except at a point of
zero velocity, and as streamlines converge the ﬂuid speed increases. The latter is a consequence
of the incompressibility of the ﬂuid: as the same ﬂow rate of ﬂuid passes through a smaller
cross-sectional area, the ﬂuid velocity must increase.
Related to streamlines is the stream function. We specialize here to a two dimensional ﬂow,
with
u= 
u(x,y),v(x,y), 0
.
The incompressibility condition becomes
¶u
¶x+¶v
¶y=0,
which can be satisﬁed by deﬁning the scalar stream function y=y(x,y)by
u(x,y) =¶y
¶y,v(x,y) = ¶y
¶x.
Now, the differential of the stream function y(x,y)satisﬁes
dy=¶y
¶xdx+¶y
¶ydy
= vdx+udy,
which from (16.1) is equal to zero along streamlines. Thus the contour curves of constant y
represent the streamlines of the ﬂow ﬁeld, and can provide a good visualization of a ﬂuid ﬂow
in two dimensions.
117
16.2. VORTICITY
16.2 Vorticity
The vector vorticity ﬁeld is deﬁned from the vector velocity ﬁeld by
w=ru. (16.2)
The vorticity is a measure of the local rotation of the ﬂuid as can be seen from an application of
Stokes’ theorem:
Z
SwˆndS=Z
S(ru)ˆndS
=I
Cudr.
Flows without vorticity are called irrotational, or potential ﬂow, and vorticity is sometimes called
swirl.
The governing equation for vorticity may be found by taking the curl of the Navier-Stokes
equation; that is,
r¶u
¶t+ (ur)u
=r
 1
rrp+nr2u
.
Computing term-by-term, we have
r¶u
¶t
=¶
¶t(ru)
=¶w
¶t.
And because the curl of a gradient is zero,
r
 1
rrp
=0.
Also,
rn
nr2uo
=nr2(ru)
=nr2w.
The remaining term to compute is the curl of the convection term in the Navier-Stokes equation.
We ﬁrst consider the following equality (where the subscript isigniﬁes the i-th component of the
vector):
fu(ru)gi=eijkujeklm¶um
¶xl
=ekijeklmuj¶um
¶xl
= (dildjm dimdjl)uj¶um
¶xl
=um¶um
¶xi ul¶ui
¶xl
=1
2¶
¶xiumum ul¶ui
¶xl.
118 CHAPTER 16. STREAM FUNCTION, VORTICITY EQUATIONS
16.3. TWO-DIMENSIONAL NAVIER-STOKES EQUATION
Therefore, in vector form,
u(ru) =1
2r(u2) (ur)u.
This identity allows us to write
(ur)u=1
2r(u2) u(ru).
Taking the curl of both sides and making use of the curl of a gradient equals zero and ru=w,
results in
rf(ur)ug= r(uw)
=r(wu).
Combining all the above terms, we have thus obtained the vorticity equation
¶w
¶t+r(wu) =nr2w. (16.3)
An alternative form of the vorticity equation rewrites the convection term to explicitly include
the substantive derivative. We have
fr(wu)gi=eijk¶
¶xjeklmwlum
=ekijeklm¶
¶xj(wlum)
= (dildjm dimdjl)¶
¶xj(wlum)
=¶
¶xm(wium) ¶
¶xl(wlui)
=um¶wi
¶xm wl¶ui
¶xl,
where to obtain the last equality we have used both ¶um/¶xm=0 and ¶wl/¶xl=0. Therefore,
in vector form,
r(wu) = ( ur)w (wr)u.
The vorticity equation can then be rewritten as
¶w
¶t+ (ur)w= (wr)u+nr2w. (16.4)
Compared to the Navier-Stokes equation, there is an extra term, called the vortex stretching term,
on the right-hand-side of (16.4).
16.3 Two-dimensional Navier-Stokes equation
We have already seen that in two dimensions, the incompressibility condition is automatically
satisﬁed by deﬁning the stream function y(x,t). Also in two dimensions, the vorticity can be
CHAPTER 16. STREAM FUNCTION, VORTICITY EQUATIONS 119
16.3. TWO-DIMENSIONAL NAVIER-STOKES EQUATION
reduced to a scalar ﬁeld. With u= 
u(x,y),v(x,y)
, we have
w=ru
=ˆ x ˆ y ˆ z
¶/¶x ¶/¶y¶/¶z
u(x,y)v(x,y) 0
=ˆ z¶v
¶x ¶u
¶y
=w(x,y)ˆ z,
where we have now deﬁned the scalar ﬁeld wto be the third component of the vector vorticity
ﬁeld. Making use of the stream function, we then have
w=¶v
¶x ¶u
¶y
= ¶2y
¶x2 ¶2y
¶y2.
Therefore, in vector form, we have
r2y= w, (16.5)
where
r2=¶2
¶x2+¶2
¶y2
is the two-dimensional Laplacian.
Now, with w=w(x,y)ˆ z, the third component of the vorticity equation (16.4) becomes
¶w
¶t+ (ur)w=nr2w,
where the vortex stretching term can be seen to vanish. We can also write
ur=u¶
¶x+v¶
¶y
=¶y
¶y¶
¶x ¶y
¶x¶
¶y.
The vorticity equation in two dimensions then becomes
¶w
¶t+¶y
¶y¶w
¶x ¶y
¶x¶w
¶y
=nr2w.
For a stationary ﬂow, this equation becomes the Poisson equation,
r2w=1
n¶y
¶y¶w
¶x ¶y
¶x¶w
¶y
. (16.6)
We have thus obtained for a stationary ﬂow two coupled Poisson equations for y(x,y)andw(x,y)
given by (16.5) and (16.6).
The pressure ﬁeld p(x,y)decouples from these two Poisson equations, but can be determined
if desired. We take the divergence of the Navier-Stokes equation, (14.9), and application of the
incompressibility condition, (14.8), yields
r¶u
¶t=0,rr2u=0,
120 CHAPTER 16. STREAM FUNCTION, VORTICITY EQUATIONS
16.3. TWO-DIMENSIONAL NAVIER-STOKES EQUATION
resulting in
r2p= rr (ur)u
. (16.7)
We would like to eliminate the velocity vector ﬁeld in (16.7) in favor of the stream function. We
compute
r(ur)u=¶
¶xi 
uj¶ui
¶xj!
=¶uj
¶xi¶ui
¶xj+uj¶2ui
¶xi¶xj.
The second term on the right-hand-side vanishes because of the incompressibility condition. We
therefore have
r(ur)u=¶uj
¶xi¶ui
¶xj
=¶u
¶x2
+¶v
¶x¶u
¶y
+¶u
¶y¶v
¶x
+¶v
¶y2
=¶2y
¶x¶y2
 2¶2y
¶x2¶2y
¶y2+¶2y
¶x¶y2
= 2"
¶2y
¶x2¶2y
¶y2 ¶2y
¶x¶y2#
.
Using (16.7), the pressure ﬁeld therefore satisﬁes the Poisson equation
r2p=2r"
¶2y
¶x2¶2y
¶y2 ¶2y
¶x¶y2#
.
CHAPTER 16. STREAM FUNCTION, VORTICITY EQUATIONS 121
16.3. TWO-DIMENSIONAL NAVIER-STOKES EQUATION
122 CHAPTER 16. STREAM FUNCTION, VORTICITY EQUATIONS
Chapter 17
Steady, two-dimensional ﬂow
past an obstacle
We now consider a classic problem in computational ﬂuid dynamics: the steady, two-dimensional
ﬂow past an obstacle. Here, we consider ﬂows past a rectangle and a circle.
First, we consider ﬂow past a rectangle. The simple Cartisian coordinate system is most
suitable for this problem, and the boundaries of the internal rectangle can be aligned with the
computational grid. Second, we consider ﬂow past a circle. Here, the polar coordinate system
is most suitable, and this introduces some additional analytical complications to the problem
formulation. Nevertheless, we will see that the computation of ﬂow past a circle may in fact be
simpler than ﬂow past a rectangle. Although ﬂow past a rectangle contains two dimensionless
parameters, ﬂow past a circle contains only one. Furthermore, ﬂow past a circle may be solved
within a rectangular domain having no internal boundaries.
17.1 Flow past a rectangle
The free stream velocity is given by u=Uˆ xand the rectangular obstacle is assumed to have
width Wand height H. Now, the stream function has units of velocity times length, and the
vorticity has units of velocity divided by length. The steady, two-dimensional Poisson equations
for the stream function and the vorticity, given by (16.5) and (16.6), may be nondimensionalized
using the velocity Uand the length W. The resulting dimensionless equations can be written as
 r2y=w, (17.1)
 r2w=Re¶y
¶x¶w
¶y ¶y
¶y¶w
¶x
, (17.2)
where the dimensionless parameter Re is called the Reynolds number. An additional dimension-
less parameter arises from the aspect ratio of the rectangular obstacle, and is denoted by a. These
two dimensionless parameters are deﬁned by
Re=UW
n,a=W
H. (17.3)
A solution for the scalar stream function and scalar vorticity ﬁeld will be sought for different
values of the Reynolds number Re at a ﬁxed aspect ratio a.
17.1.1 Finite difference approximation
We construct a rectangular grid for a numerical solution. We will make use of square grid cells,
and write
xi=ih, i=0, 1, . . . , Nx; (17.4a)
yj=jh, j=0, 1, . . . , Ny, (17.4b)
123
17.1. FLOW PAST A RECTANGLE
where Nxand Nyare the number of grid cells spanning the x- and y-directions, and his the side
length of a grid cell.
To obtain an accurate solution, we require the boundaries of the obstacle to lie exactly on the
boundaries of the grid cell. The width of the obstacle in our dimensionless formulation is unity,
and we place the front of the obstacle at x=mhand the back of the obstacle at x= (m+I)h,
where we must have
hI=1.
With Ispeciﬁed, the grid spacing is determined by h=1/I.
We will only look for steady solutions for the ﬂow ﬁeld that are symmetric about the midline
of the obstacle. Assuming symmetry, we need only solve for the ﬂow ﬁeld in the upper half of
the domain. We place the center line of the obstacle at y=0 and the top of the rectangle at hJ.
The dimensionless half-height of the obstacle is given by 1/2 a, so that
hJ=1
2a.
Forcing the rectangle to lie on the grid lines constrains the choice of aspect ratio and the values
ofIand Jsuch that
a=I
2J.
Reasonable values of ato consider are a=. . . , 1/4, 1/2, 1, 2, 4, . . . , etc, and Iand Jcan be adjusted
accordingly.
The physics of the problem is speciﬁed through the two dimensionless parameters Re and a.
The numerics of the problem is speciﬁed by the parameters Nx,Ny,h, and the placement of the
rectangle in the computational domain. We look for convergence of the numerical solution as
h!0,Nx,Ny!¥and the rectangle is placed far from the boundaries of the computationally
domain.
Discretizing the governing equations, we now write
yi,j=y(xi,yj),wi,j=w(xi,yj).
To solve the coupled Poisson equations given by (17.1) and (17.2), we make use of the SOR
method, previously described in §7.1. The notation we use here is for the Jacobi method, but
faster convergence is likely to be achieved using red-back Gauss-Seidel. The Poisson equation for
the stream function, given by (17.1), becomes
yn+1
i,j= (1 ry)yn
i,j+ry
4
yn
i+1,j+yn
i 1,j+yn
i,j+1+yn
i,j 1+h2wn
i,j
. (17.5)
The Poisson equation for the vorticity, given by (17.2), requires use of the centered ﬁnite difference
approximation for the derivatives that appear on the right-hand-side. For x=xi,y=yi, these
approximations are given by
¶y
¶x1
2h
yi+1,j yi 1,j
,¶y
¶y1
2h
yi,j+1 yi,j 1
,
¶w
¶x1
2h
wi+1,j wi 1,j
,¶w
¶y1
2h
wi,j+1 wi,j 1
.
We then write for (17.2),
wn+1
i,j= (1 rw)wn
i,j+rw
4
wn
i+1,j+wn
i 1,j+wn
i,j+1+wn
i,j 1+Re
4fn
i,j
, (17.6)
where
fn
ij=
yn
i+1,j yn
i 1,j
wn
i,j+1 wn
i,j 1
 
yn
i,j+1 yn
i,j 1
wn
i+1,j wn
i 1,j
. (17.7)
124 CHAPTER 17. FLOW PAST AN OBSTACLE
17.1. FLOW PAST A RECTANGLE
Now, the right-hand-side of (17.6) contains a nonlinear term given by (17.7). This nonlinearity
can result in the iterations becoming unstable.
The iterations can be stabilized as follows. First, the relaxation parameters, ryand rw, should
be less than or equal to unity, and unstable iterations can often be made stable by decreasing
rw. One needs to numerically experiment to obtain the best trade-off between computationally
stability and speed. Second, to determine the solution with Reynolds number Re, the iteration
should be initialized using the steady solution for a slightly smaller Reynolds number. Initial
conditions for the ﬁrst solution with Re slightly larger than zero should be chosen so that this
ﬁrst iteration is stable.
The path of convergence can be tracked during the iterations. We deﬁne
#n+1
y=max
i,jyn+1
i,j yn
i,j,
#n+1
w=max
i,jwn+1
i,j wn
i,j.
The iterations are to be stopped when the values of #n+1
yand#n+1ware less than some pre-deﬁned
error tolerance, say 10 8.
17.1.2 Boundary conditions
Boundary conditions on yi,jandwi,jmust be prescribed at i=0 (inﬂow), i=Nx(outﬂow),
j=0 (midline), and j=Ny(top of computational domain). Also, boundary conditions must be
prescribed on the surface of the obstacle; that is, on the front surface: i=m, 0jJ; the back
surface: i=m+I, 0jJ; and the top surface: mim+I,j=J. Inside of the obstacle,
m<i<m+I, 0<j<J, no solution is sought.
For the inﬂow and top-of-domain boundary conditions, we may assume that the ﬂow ﬁeld
satisﬁes dimensionless free-stream conditions; that is,
u=1, v=0.
The vorticity may be taken to be zero, and the stream function satisﬁes
¶y
¶y=1,¶y
¶x=0.
Integrating the ﬁrst of these equations, we obtain
y=y+f(x);
and from the second equation we obtain f(x) =c, where cis a constant. Without loss of general-
ity, we may choose c=0. Therefore, for the inﬂow and top-of-domain boundary conditions, we
have
y=y,w=0. (17.8)
At the top of the domain, notice that y=Nyhis a constant.
For the outﬂow boundary conditions, we have two possible choices. We could assume free-
stream conditions if we place the outﬂow boundary sufﬁciently far away from the obstacle. How-
ever, one would expect that the disturbance to the ﬂow ﬁeld downstream of the obstacle might
be substantially greater than that upstream of the obstacle. Perhaps better outﬂow boundary
conditions may be zero normal derivatives of the ﬂow ﬁeld; that is
¶y
¶x=0,¶w
¶x=0.
CHAPTER 17. FLOW PAST AN OBSTACLE 125
17.1. FLOW PAST A RECTANGLE
For the midline boundary conditions, we will assume a symmetric ﬂow ﬁeld so that the ﬂow
pattern will look the same when rotated about the x-axis. The symmetry conditions are therefore
given by
u(x, y) =u(x,y),v(x, y) = v(x,y).
The vorticity exhibits the symmetry given by
w(x, y) =¶v(x, y)
¶x ¶u(x, y)
¶( y)
= ¶v(x,y)
¶x+¶u(x,y)
¶(y)
= w(x,y).
On the midline ( y=0), then, w(x, 0) = 0. Also, v(x, 0) = 0. With v= ¶y/¶x, we must
have y(x, 0)independent of x, ory(x, 0)equal to a constant. Matching the midline boundary
condition to our chosen inﬂow condition determines y(x, 0) =0.
Our boundary conditions are discretized using (17.4). The outﬂow condition of zero normal
derivative could be approximated either to ﬁrst- or second-order. Since it is an approximate
boundary condition, ﬁrst-order is probably sufﬁcient and we use yNx,j=yNx 1,jandwNx,j=
wNx 1,j.
Putting all these results together, the boundary conditions on the borders of the computa-
tional domain are given by
yi,0=0, wi,0=0, midline;
y0,j=jh, w0,j=0, inﬂow;
yNx,j=yNx 1,j, wNx,j=wNx 1,j, outﬂow;
yi,Ny=Nyh, wi,Ny=0, top-of-domain.
Boundary conditions on the obstacle can be derived from the no-penetration and no-slip con-
ditions. From the no-penetration condition, u=0 on the sides and v=0 on the top. Therefore,
on the sides, ¶y/¶y=0, and since the side boundaries are parallel to the y-axis, ymust be
constant. On the top, ¶y/¶x=0, and since the top is parallel to the x-axis, ymust be constant.
Matching the constant to the value of yon the midline, we obtain y=0 along the boundary of
the obstacle.
From the no-slip condition, v=0 on the sides and u=0 on the top. Therefore, ¶y/¶x=0
on the sides and ¶y/¶y=0 on the top. To interpret the no-slip boundary conditions in terms of
boundary conditions on the vorticity, we make use of (17.1); that is,
w= ¶2y
¶x2+¶2y
¶y2
. (17.9)
First consider the sides of the obstacle. Since yis independent of ywe have ¶2y/¶y2=0,
and (17.9) becomes
w= ¶2y
¶x2. (17.10)
We now Taylor series expand y(xm h,yj)andy(xm 2h,yj)about (xm,yj), corresponding to
the front face of the rectangular obstacle. We have to order h3:
ym 1,j=ym,j h¶y
¶x
m,j+1
2h2¶2y
¶x2
m,j 1
6h3¶3y
¶x3
m,j+O(h4),
ym 2,j=ym,j 2h¶y
¶x
m,j+2h2¶2y
¶x2
m,j 4
3h3¶3y
¶x3
m,j+O(h4).
126 CHAPTER 17. FLOW PAST AN OBSTACLE
17.1. FLOW PAST A RECTANGLE
The ﬁrst terms in the two Taylor series expansions are zero because of the no-penetration con-
dition, and the second terms are zero because of the no-slip condition. The third terms acan be
rewritten using (17.10), and we obtain
ym 1,j= 1
2h2wm,j 1
6h3¶3y
¶x3
m,j+O(h4),
ym 2,j= 2h2wm,j 4
3h3¶3y
¶x3
m,j+O(h4).
We multiply the ﬁrst equation by  8 and add it to the second equation to eliminate the h3term.
We obtain
 8ym 1,j+ym 2,j=2h2wm,j+O(h4).
Solving for the vorticity, we have a second-order accurate boundary condition given by
wm,j=ym 2,j 8ym 1,j
2h2.
Similar considerations applied to the back face of the rectangular obstacle yields
wm+I,j=ym+I+2,j 8ym+I+1,j
2h2.
On the top of the obstacle, y=Jhis ﬁxed, and since yis independent of x, we have ¶2y/¶x2=
0. Therefore,
w= ¶2y
¶y2. (17.11)
We now Taylor series expand y(xi,yJ+1)andy(xi,yJ+2)about (xi,yJ). To order h3,
yi,J+1=yi,J+h¶y
¶y
i,J+1
2h2¶2y
¶y2
i,J+1
6h3¶3y
¶y3
i,J+O(h4),
yi,J+2=yi,J+2h¶y
¶y
i,J+2h2¶2y
¶y2
i,J+4
3h3¶3y
¶y3
i,J+O(h4).
Again, the ﬁrst and second terms in the Taylor series expansion are zero, and making use of
(17.11), we obtain
yi,J+1= 1
2h2wi,J+1
6h3¶3y
¶y3
i,J+O(h4),
yi,J+2= 2h2wi,J+4
3h3¶3y
¶y3
i,J+O(h4).
Again, we multiply the ﬁrst equation by  8 and add it to the second equation to obtain
 8yi,J+1+yi,J+2=2h2wi,J+O(h4).
Solving for the vorticity on the top surface, we have to second-order accuracy
wi,J=yi,J+2 8yi,J+1
2h2.
CHAPTER 17. FLOW PAST AN OBSTACLE 127
17.2. FLOW PAST A CIRCLE
We summarize the boundary conditions on the obstacle:
front face: ym,j=0, wm,j=ym 2,j 8ym 1,j
2h2, 0jJ;
back face: ym+I,j=0, wm+I,j=ym+I+2,j 8ym+I+1,j
2h2, 0jJ;
top face: yi,J=0, wi,J=yi,J+2 8yi,J+1
2h2, mim+I.
17.2 Flow past a circle
We now consider ﬂow past a circular obstacle of radius R, with free-stream velocity u=Uˆ x.
Here, we nondimensionalize the governing equations using Uand R. We will deﬁne the Reynolds
number—the only dimensionless parameter of this problem—by
Re=2UR
n. (17.12)
The extra factor of 2 bases the deﬁnition of the Reynolds number on the diameter of the circle
rather than the radius, which allows a better comparison to computations of ﬂow past a square
(a=1), where the Reynolds number was based on the side length.
The dimensionless governing equations in vector form, can be written as
r2y= w (17.13a)
r2w=Re
2urw, (17.13b)
where the extra factor of one-half arises from nondimensionalizing the equation using the radius
of the obstacle R, but deﬁning the Reynolds number in terms of the diameter 2 R.
17.2.1 Log-polar coordinates
Although the free-stream velocity is best expressed in Cartesian coordinates, the boundaries of
the circular obstacle are more simply expressed in polar coordinates, with origin at the center of
the circle. Polar coordinates are deﬁned in the usual way by
x=rcosq,y=rsinq,
with the cartesian unit vectors given in terms of the polar unit vectors by
ˆ x=cosqˆ r sinqˆq,ˆy=sinqˆ r+cosqˆq.
The polar unit vectors are functions of position, and their derivatives are given by
¶ˆ r
¶r=0,¶ˆ r
¶q=ˆq,¶ˆq
¶r=0,¶ˆq
¶q= ˆ r.
The del differential operator in polar coordinates is given by
r=ˆ r¶
¶r+ˆq1
r¶
¶q,
and the two-dimensional Laplacian is given by
r2=1
r2
r¶
¶r
r¶
¶r
+¶2
¶q2
. (17.14)
128 CHAPTER 17. FLOW PAST AN OBSTACLE
17.2. FLOW PAST A CIRCLE
The velocity ﬁeld is written in polar coordinates as
u=urˆ r+uqˆq.
The free-stream velocity in polar coordinates is found to be
u=Uˆ x
=U 
cosqˆ r sinqˆq
, (17.15)
from which can be read off the components in polar coordinates. The continuity equation ru=
0 in polar coordinates is given by
1
r¶
¶r(rur)+1
r¶uq
¶q=0,
so that the stream function can be deﬁned by
rur=¶y
¶q,uq= ¶y
¶r. (17.16)
The vorticity, here in cylindrical coordinates, is given by
w=ru
=ˆ z1
r¶
¶r(ruq) 1
r¶ur
¶q
,
so that the z-component of the vorticity for a two-dimensional ﬂow is given by
w=1
r¶
¶r(ruq) 1
r¶ur
¶q. (17.17)
Furthermore,
ur= 
urˆ r+uqˆq
ˆ r¶
¶r+ˆq1
r¶
¶q
=ur¶
¶r+uq
r¶
¶q
=1
r¶y
¶q¶
¶r 1
r¶y
¶r¶
¶q. (17.18)
The governing equations given by (17.13), then, with the Laplacian given by (17.14), and the
convection term given by (17.18), are
r2y= w, (17.19)
r2w=Re
21
r¶y
¶q¶w
¶r 1
r¶y
¶r¶w
¶q
. (17.20)
The recurring factor r¶/¶rin the polar coordinate Laplacian, (17.14), is awkward to discretize
and we look for a change of variables r=r(x), where
r¶
¶r=¶
¶x.
Now,
¶
¶x=dr
dx¶
¶r,
CHAPTER 17. FLOW PAST AN OBSTACLE 129
17.2. FLOW PAST A CIRCLE
so that we require
dr
dx=r. (17.21)
This simple differential equation can be solved if we take as our boundary condition x=0 when
r=1, corresponding to points lying on the boundary of the circular obstacle. The solution of
(17.21) is therefore given by
r=ex.
The Laplacian in the so-called log polar coordinates then becomes
r2=1
r2
r¶
¶r
r¶
¶r
+¶2
¶q2
=e 2x¶2
¶x2+¶2
¶q2
.
Also, transforming the right-hand-side of (17.20), we have
1
r¶y
¶q¶w
¶r 1
r¶y
¶r¶w
¶q=1
r2
r¶w
¶r¶y
¶q r¶y
¶r¶w
¶q
=e 2x¶y
¶q¶w
¶x ¶y
¶x¶w
¶q
.
The governing equations for y=y(x,q)andw=w(x,q)in log-polar coordinates can therefore
be written as
 ¶2
¶x2+¶2
¶q2
y=e2xw, (17.22a)
 ¶2
¶x2+¶2
¶q2
w=Re
2¶y
¶x¶w
¶q ¶y
¶q¶w
¶x
. (17.22b)
17.2.2 Finite difference approximation
A ﬁnite difference approximation to the governing equations proceeds on a grid in (x,q)space.
The grid is deﬁned for 0 xxmand 0qp, so that the computational domain forms a
rectangle without holes. The sides of the rectangle correspond to the boundary of the circular
obstacle ( x=0), the free stream ( x=xm), the midline behind the obstacle ( q=0), and the
midline in front of the obstacle ( q=p) .
We discretize the equations using square grid cells, and write
xi=ih, i=0, 1, . . . , n; (17.23a)
qj=jh, j=0, 1, . . . , m, (17.23b)
where nand mare the number of grid cells spanning the x- and q-directions, and his the side
length of a grid cell. Because 0 qp, the grid spacing must satisfy
h=p
m,
and the maximum value of xis given by
xmax=np
m.
The radius of the computational domain is therefore given by exmax, which is to be compared to
the obstacle radius of unity. The choice n=mwould yield exmax23, and the choice n=2m
130 CHAPTER 17. FLOW PAST AN OBSTACLE
17.2. FLOW PAST A CIRCLE
would yield exmax535. To perform an accurate computation, it is likely that both the value of
m(and n) and the value of xmaxwill need to increase with Reynolds number.
Again we make use of the SOR method, using the notation for the Jacobi method, although
faster convergence is likely to be achieved using red-black Gauss-Seidel. The ﬁnite difference
approximation to (17.22) thus becomes
yn+1
i,j= (1 ry)yn
i,j+ry
4
yn
i+1,j+yn
i 1,j+yn
i,j+1+yn
i,j 1+h2e2xiwn
i,j
, (17.24)
and
wn+1
i,j= (1 rw)wn
i,j+rw
4
wn
i+1,j+wn
i 1,j+wn
i,j+1+wn
i,j 1+Re
8fn
i,j
, (17.25)
where
fn
ij=
yn
i+1,j yn
i 1,j
wn
i,j+1 wn
i,j 1
 
yn
i,j+1 yn
i,j 1
wn
i+1,j wn
i 1,j
. (17.26)
17.2.3 Boundary conditions
Boundary conditions must be prescribed on the sides of the rectangular computational domain.
The boundary conditions on the two sides corresponding to the midline of the physical domain,
q=0 and q=p, satisfy y=0 and w=0. The boundary condition on the side corresponding to
the circular obstacle, x=0, is again determined from the no-penentration and no-slip conditions,
and are given by y=0 and ¶y/¶x=0. And the free-stream boundary condition may be applied
atx=xmax.
We ﬁrst consider the free-stream boundary condition. The dimensionless free-stream velocity
ﬁeld in polar coordinates can be found from (17.15),
u=cosqˆ r sinqˆq.
The stream function, therefore, satisﬁes the free-stream conditions
¶y
¶q=rcosq,¶y
¶r=sinq,
and by inspection, the solution that also satisﬁes y=0 when q=0,pis given by
y(r,q) =rsinq.
In log-polar coordinates, we therefore have the free-stream boundary condition
y(xmax,q) =exmaxsinq.
One has two options for the vorticity in the free stream. One could take the vorticity in the free
stream to be zero, so that
w(xmax,q) =0.
A second, more gentle option is to take the derivative of the vorticity to be zero, so that
¶w
¶x(xmax,q) =0.
This second option seems to have somewhat better stability properties for the ﬂow ﬁeld far
downstream of the obstacle. Ideally, the computed values of interest should be independent of
which of these boundary conditions is chosen, and ﬁnding ﬂow-ﬁeld solutions using both of
these boundary conditions provides a good measure of accuracy.
CHAPTER 17. FLOW PAST AN OBSTACLE 131
17.2. FLOW PAST A CIRCLE
The remaining missing boundary condition is for the vorticity on the obstacle. Again, we
need to convert the two boundary conditions on the stream function, y=0 and ¶y/¶x=0 to a
boundary condition on yandw. From (17.22), we have
w= e 2x¶2y
¶x2+¶2y
¶q2
,
and since on the circle y=0, independent of q, and x=0, we have
w= ¶2y
¶x2. (17.27)
A Taylor series expansion one and two grid points away from the circular obstacle yields
y1,j=y0,j+h¶y
¶x(0,j)+1
2h2¶2y
¶x2(0,j)+1
6h3¶3y
¶x3(0,j)+O(h4),
y2,j=y0,j+2h¶y
¶x(0,j)+2h2¶2y
¶x2(0,j)+4
3h3¶3y
¶x3(0,j)+O(h4).
Now, both y=0 and ¶y/¶x=0 at the grid point (0,j). Using the equation for the vorticity on
the circle, (17.27), results in
y1,j= 1
2h2w0,j+1
6h3¶3y
¶x3(0,j)+O(h4),
y2,j= 2h2w0,j+4
3h3¶3y
¶x3(0,j)+O(h4).
We multiply the ﬁrst equation by  8 and add it to the second equation to eliminate the h3term.
We obtain
 8y1,j+y2,j=2h2w0,j+O(h4).
Solving for the vorticity, we obtain our boundary condition accurate to second order:
w0,j=y2,j 8y1,j
2h2.
The boundary conditions are summarized below:
x=0, 0qp:y0,j=0,w0,j=y2,j 8y1,j
2h2;
x=xmax, 0qp:yn,j=exmaxsinjh,wn,j=0 orwn,j=wn 1,j;
0xxmax,q=0 :yi,0=0,wi,0=0;
0xxmax,q=p:yi,m=0,wi,m=0.(17.28)
17.2.4 Solution using Newton’s method
We consider here a much more efﬁcient method to ﬁnd the steady ﬂuid ﬂow solution. Unfortu-
nately, this method is also more difﬁcult to program. Recall from §7.2 that Newton’s method can
be used to solve a system of nonlinear equations. Newton’s method as a root-ﬁnding routine has
the strong advantage of being very fast when it converges, but the disadvantage of not always
converging. Here, the problem of convergence can be overcome by solving for larger Re using as
an initial guess the solution for slightly smaller Re, with slightly to be deﬁned by trial and error.
132 CHAPTER 17. FLOW PAST AN OBSTACLE
17.2. FLOW PAST A CIRCLE
Recall that Newton’s method can solve a system of nonlinear equations of the form
F(y,w) =0, G(y,w) =0. (17.29)
Newton’s method is implemented by writing
y(k+1)=y(k)+Dy,w(k+1)=w(k)+Dw, (17.30)
and the iteration scheme is derived by linearizing (17.29) in DyandDwto obtain
JDy
Dw
= F
G
, (17.31)
where Jis the Jacobian matrix of the functions Fand G. All functions are evaluated at y(k)and
w(k).
Here, we should view yandwas a large number of unknowns and Fand Ga correspond-
ingly large number of equations, where the total number of equations must necessarily equal the
total number of unknowns.
If we rewrite our governing equations into the form given by (17.29), we have
 ¶2
¶x2+¶2
¶q2
y e2xw=0, (17.32a)
 ¶2
¶x2+¶2
¶q2
w Re
2¶y
¶x¶w
¶q ¶y
¶q¶w
¶x
=0. (17.32b)
With nand mgrid cells in the x- and q-directions, the partial differential equations of (17.32)
represent 2 (n 1)(m 1)coupled nonlinear equations for yi,jandwi,jon the internal grid points.
We will also include the boundary values in the solution vector that will add an additional two
unknowns and two equations for each boundary point, bringing the total number of equations
(and unknowns) to 2 (n+1)(m+1).
The form of the Jacobian matrix may be determined by linearizing (17.32) in DyandDw.
Using (17.30), we have
 ¶2
¶x2+¶2
¶q2
(y(k)+Dy) e2x(w(k)+Dw) =0,
and
 ¶2
¶x2+¶2
¶q2
(w(k)+Dw)
 Re
2 
¶(y(k)+Dy)
¶x¶(w(k)+Dw)
¶q ¶(y(k)+Dy)
¶q¶(w(k)+Dw)
¶x!
=0.
Liinearization in DyandDwthen results in
 ¶2
¶x2+¶2
¶q2
Dy e2xDw= 
 ¶2
¶x2+¶2
¶q2
y(k) e2xw(k)
, (17.33)
and
 ¶2
¶x2+¶2
¶q2
Dw Re
2 
¶w(k)
¶q¶Dy
¶x ¶w(k)
¶x¶Dy
¶q+¶y(k)
¶x¶Dw
¶q ¶y(k)
¶q¶Dw
¶x!
= "
 ¶2
¶x2+¶2
¶q2
w(k) Re
2 
¶y(k)
¶x¶w(k)
¶q ¶y(k)
¶q¶w(k)
¶x!#
, (17.34)
CHAPTER 17. FLOW PAST AN OBSTACLE 133
17.2. FLOW PAST A CIRCLE
where the ﬁrst equation was already linear in the Dvariables, but the second equation was
originally quadratic, and the quadratic terms have now been dropped. The equations given by
(17.33) and (17.34) can be observed to be in the form of the Newton’s iteration equations given
by (17.31).
Numerically, both DyandDwwill be vectors formed by a natural ordering of the grid points,
as detailed in §6.2. These two vectors will then be stacked into a single vector as shown in (17.31).
To write the Jacobian matrix, we employ the shorthand notation ¶2
x=¶2/¶x2,yx=¶y/¶x, and
so on. The Jacobian matrix can then be written symbolically as
J =0
@ 
¶2
x+¶2
q
 e2xI
0 
¶2
x+¶2
q1
A Re
20 0
wq¶x wx¶qyx¶q yq¶x
, (17.35)
where I is the identity matrix and the derivatives of yandwin the second matrix are all evaluated
at the kth iteration.
The Jacobian matrix as written is valid for the grid points interior to the boundary, where
each row of J corresponds to an equation for either yorwat a speciﬁc interior grid point. The
Laplacian-like operator is represented by a Laplacian matrix, and the derivative operators are
represented by derivative matrices. The terms e2x,¶w/¶q, and the other derivative terms are to
be evaluated at the grid point corresponding to the row in which they are found.
To incorporate boundary conditions, we extend the vectors DyandDwto also include the
points on the boundary as they occur in the natural ordering of the grid points. To the Jaco-
bian matrix and the right-hand-side of (17.31) are then added the appropriate equations for the
boundary conditions in the rows corresponding to the boundary points. By explicitly includ-
ing the boundary points in the solution vector, the second-order accurate Laplacian matrix and
derivative matrices present in J can handle the grid points lying directly next to the boundaries
without special treatment.
The relevant boundary conditions to be implemented are the boundary conditions on Dy
andDw. The boundary conditions on the ﬁelds yandwthemselves have already been given
by (17.28). The grid points with ﬁxed boundary conditions on yandwthat do not change with
iterations will have a one on the diagonal in the Jacobian matrix corresponding to that grid point,
and a zero on the right-hand-side. In other words, yandwwill not change on iteration of
Newton’s method, and their initial values need to be chosen to satisfy the appropriate boundary
conditions.
The two boundary conditions which change on iteration, namely
w0,j=y2,j 8y1,j
2h2,wn,j=wn 1,j,
must be implemented in the Newton’s method iteration as
Dw0,j=Dy2,j 8Dy1,j
2h2,Dwn,j=Dwn 1,j,
and these equations occur in the rows corresponding to the grid points (0,j)and(n,j), with j=0
tom. Again, the initial conditions for the iteration must satisfy the correct boundary conditions.
The MATLAB implementation of (17.31) using (17.35), requires both the construction of the
(n+1)(m+1)(n+1)(m+1)matrix that includes both the Jacobian matrix and the boundary
conditions, and the construction of the corresponding right-hand-side of the equation. For the
Laplacian matrix, one can make use of the function sp_laplace_new.m ; and one also needs to
construct the derivative matrices ¶xand¶q. Both of these matrices are banded, with a band of
positive ones above the main diagonal and a band of negative ones below the main diagonal. For
¶x, the bands are directly above and below the diagonal. For ¶q, the bands are a distance n+1
134 CHAPTER 17. FLOW PAST AN OBSTACLE
17.3. VISUALIZATION OF THE FLOW FIELDS
away from the diagonal, corresponding to n+1 grid points in each row. Both the Laplacian and
derivative matrices are to be constructed for (n+1)(m+1)grid points and placed into a 2 2
matrix using the MATLAB function kron.m , which generates a block matrix by implementing
the so-called Kronecker product. Rows corresponding to the boundary points are then to be
replaced by the equations for the boundary conditions.
The MATLAB code needs to be written efﬁciently, using sparse matrices. A proﬁling of this
code should show that most of the computational time is spent solving (17.31) (with boundary
conditions added) for DyandDwusing the MATLAB backslash operator. With 4GB RAM and a
notebook computer bought circa 2013, and with the resolution 512 256 and using the Re=150
result as the initial ﬁeld, I can solve for Re=200 in seven iterations to an accuracy of 10 12. The
total run time was about 48 sec with about 42 sec spent on the single line containing J\b.
17.3 Visualization of the ﬂow ﬁelds
Obtaining correct contour plots of the stream function and vorticity can be a challenge and in this
section I will provide some guidance. The basic MATLAB functions required are meshgrid.m ,
pol2cart.m ,contour.m , and clabel.m . More fancy functions such as contourf.m and
imagesc may also be used, though I will not discuss these here.
Suppose the values of the stream function are known on a grid in two dimensional Cartesian
coordinates. A contour plot draws curves following speciﬁed (or default) constant values of the
stream function in the x-yplane. Viewing the curves on which the stream function is constant
gives a clear visualization of the ﬂuid ﬂow.
To make the best use of the function contour.m , one speciﬁes the x-ygrid on which the
values of the stream function are known. The stream function variable psi, say, is given as an
n-by-mmatrix. We will examine a simple example to understand how to organize the data.
Let us assume that the stream function is known at all the values of xand yon the two-
dimensional grid speciﬁed by x=[0,1,2] andy=[0,1] . To properly label the axis of the contour
plot, we use the function meshgrid , and write [X,Y]=meshgrid(x,y) . The values assigned
toXandYare the following 2-by-3 matrices:
X=0 1 2
0 1 2
,Y=0 0 0
1 1 1
.
The variable psi must have the same dimensions as the variables XandY. Suppose psi is given
by
psi=a b c
d e f
.
Then the data must be organized so that psi =aat(x,y) = ( 0, 0), psi =bat(x,y) = ( 1, 0),
psi=dat(x,y) = ( 0, 1), etc. Notice that the values of psi across a row ( psi(i,j) ,j=1:3 )
correspond to different xlocations, and the values of psi down a column (( psi(i,j) ,i=1:2 ))
correspond to different ylocations. Although this is visually intuitive since xcorresponds to
horizontal variation and yto vertical variation, it is algebraically counterintuitive: the ﬁrst index
ofpsi corresponds to the yvariation and the second index corresponds to the xvariation. If one
uses the notation psi (x,y)during computation, then to plot one needs to take the transpose of
the matrix psi.
Now the computation of the ﬂow ﬁeld around a circle is done using log-polar coordinates
(x,q). To construct a contour plot, the solution ﬁelds need to be transformed to cartesian co-
ordinates. The MATLAB function pol2cart.m provides a simple solution. One deﬁnes the
variables theta andxithat deﬁnes the mesh in log-polar coordinates, and then ﬁrst trans-
forms to standard polar coordinates with r=exp(xi) . The polar coordinate grid is then con-
structed from [THETA, R]=meshgrid(theta,r) , and the cartesian grid is constructed from
CHAPTER 17. FLOW PAST AN OBSTACLE 135
17.3. VISUALIZATION OF THE FLOW FIELDS
xy
−3−2−1012345678−3−2−10123
Figure 17.1: Contour plots of the stream function (y >0) and vorticity (y <0) for Re =50.
Negative contours are in red, the zero contour in black, and positive contours in blue. The
contour levels for the stream function are -0.05, -0.04, -0.02, 0, 0.05, 0.2, 0.4, 0.6, 0.8, 1.1 and
those for the vorticity are -0.2, -0.05, 0, 0.25, 0.5, 0.75, 1, 1.5, 2.
[X, Y]=pol2cart(THETA,R) . The ﬁelds can then be plotted directly using the cartesian grid,
even though this grid is not uniform. That is, a simple contour plot can be made with the com-
mand contour(X,Y,psi) . More sophisticated calls to contour.m specify the precise contour
lines to be plotted, and their labelling using clabel.m .
A nice way to plot both the stream function and the vorticity ﬁelds on a single graph is to
plot the stream function contours for y>0 and the vorticity contours for y<0, making use of
the symmetry of the ﬁelds around the x-axis. In way of illustration, a plot of the stream function
and vorticity contours for Re=50 is shown in Fig. 17.1.
136 CHAPTER 17. FLOW PAST AN OBSTACLE
