
The Linux Programmer’s
Toolbox
Prentice Hall 
Open Source Software Development Series
Arnold Robbins, Series Editor
“Real world code from real world applications”
Open Source technology has revolutionized the computing world. Many large-scale projects are 
in production use worldwide, such as Apache, MySQL, and Postgres, with programmers writing applications in a variety of languages including Perl, Python, and PHP . These technologies are in use on many different systems, ranging from proprietary systems, to Linux systems, to traditional UNIX systems, to mainframes.
The Prentice Hall Open Source Software Development Series  is designed to bring you the 
best of these Open Source technologies. Not only will you learn how to use them for your projects, but you will learn from them. By seeing real code from real applications, you will learn 
the best practices of Open Source developers the world over.
Titles currently in the series include:
Linux
® Debugging and Performance Tuning: Tips and T echniques
Steve Best0131492470, Paper, ©2006
Understanding AJAX: Using JavaScript to Create Rich Internet Applications
Joshua Eichorn0132216353, Paper, ©2007
Embedded Linux Primer
Christopher Hallinan0131679848, Paper, ©2007
SELinux by Example
Frank Mayer, David Caplan, Karl MacMillan0131963694, Paper, ©2007
UNIX to Linux® Porting
Alfredo Mendoza, Chakarat Skawratananond, Artis Walker0131871099, Paper, ©2006 
Linux Programming by Example: The Fundamentals
Arnold Robbins0131429647, Paper, ©2004
The Linux
® Kernel Primer: A Top-Down Approach  for x86 and PowerPC Architectures
Claudia Salzberg, Gordon Fischer, Steven Smolski0131181637, Paper, ©2006
OSD_Series_7x9_25.indd    1 6/27/06    5:44:08 PM
The Linux Programmer’s
Toolbox
John Fusco
Upper Saddle River, NJ • Boston • Indianapolis • San Francisco
New York • Toronto • Montreal • London • Munich • Paris • Madrid
Cape Town • Sydney • Tokyo • Singapore • Mexico City

Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trade-
marks. Where those designations appear in this book, and the publisher was aware of a trademark claim, thedesignations have been printed with initial capital letters or in all capitals.
The author and publisher have taken care in the preparation of this book, but make no expressed or implied war-
ranty of any kind and assume no responsibility for errors or omissions. No liability is assumed for incidental or con-
sequential damages in connection with or arising out of the use of the information or programs contained herein.
The publisher offers excellent discounts on this book when ordered in quantity for bulk purchases or special
sales, which may include electronic versions and/or custom covers and content particular to your business,training goals, marketing focus, and branding interests. For more information, please contact:
U.S. Corporate and Government Sales(800) 382-3419corpsales@pearsontechgroup.com
For sales outside the United States, please contact:
International Salesinternational@pearsoned.com
Visit us on the Web: www.prenhallprofessional.com
Library of Congress Cataloging-in-Publication Data
Fusco, John.
The Linux programmer’s toolbox / John Fusco.
p. cm.
Includes bibliographical references and index.
ISBN 0-13-219857-6 (pbk. : alk. paper)
1.  Linux. 2.  Operating systems (Computers)  I. Title. 
QA76.76.O63F875 2007005.4'32—dc22
2006039343
Copyright © 2007 Pearson Education, Inc.
All rights reserved. Printed in the United States of America. This publication is protected by copyright, and per-
mission must be obtained from the publisher prior to any prohibited reproduction, storage in a retrieval sys-tem, or transmission in any form or by any means, electronic, mechanical, photocopying, recording, or likewise.For information regarding permissions, write to:
Pearson Education, Inc.Rights and Contracts DepartmentOne Lake StreetUpper Saddle River, NJ 07458Fax: (201) 236-3290
ISBN 0-13-219857-6Text printed in the United States on recycled paper at Courier in Stoughton, Massachusetts.First printing, March 2007
To my wife, Lisa, and my children, Andrew, Alex, and Samantha.
This page intentionally left blank 
Foreword . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xvii
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . xix
Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxiiiAbout the Author . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxv
Chapter 1 Downloading and Installing Open Source Tools . . . . . . . . . . . . . . . . . 1
1.1  Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2  What Is Open Source? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21.3  What Does Open Source Mean to You? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.3.1  Finding Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.3.2  Distribution Formats . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.4  An Introduction to Archive Files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.4.1  Identifying Archive Files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.4.2  Querying an Archive File . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71.4.3  Extracting Files from an Archive File . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
1.5  Know Your Package Manager . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
1.5.1  Choosing Source or Binary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
1.5.2  Working with Packages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
1.6  Some Words about Security and Packages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
1.6.1  The Need for Authentication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
1.6.2  Basic Package Authentication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191.6.3  Package Authentication with Digital Signatures . . . . . . . . . . . . . . . . . . . . . . . 211.6.4  GPG Signatures with RPM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221.6.5  When You Can’t Authenticate a Package . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
viiContents
1.7  Inspecting Package Contents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
1.7.1  How to Inspect Packages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
1.7.2  A Closer Look at RPM Packages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301.7.3  A Closer Look at Debian Packages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
1.8  Keeping Packages up to Date . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
1.8.1  Apt: Advanced Package Tool . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
1.8.2  Yum: Yellowdog Updater Modified . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351.8.3  Synaptic: The GUI Front End for APT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361.8.4  up2date: The Red Hat Package Updater . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
1.9  Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
1.9.1  Tools Used in This Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
1.9.2  Online References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
Chapter 2 Building from Source . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
2.1  Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
2.2  Build Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
2.2.1  Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
2.2.2  Understanding make . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 442.2.3  How Programs Are Linked . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 672.2.4  Understanding Libraries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
2.3  The Build Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
2.3.1  The GNU Build Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
2.3.2  The configure Stage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 742.3.3  The Build Stage: make . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 772.3.4  The Install Stage: make install . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
2.4  Understanding Errors and Warnings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
2.4.1  Common Makefile Mistakes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
2.4.2  Errors during the configure Stage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 822.4.3  Errors during the Build Stage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 832.4.4  Understanding Compiler Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 862.4.5  Understanding Compiler Warnings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 882.4.6  Understanding Linker Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
2.5  Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
2.5.1  Tools Used in This Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
2.5.2  Online References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101viii Contents
Chapter 3 Finding Help . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
3.1  Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
3.2  Online Help Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
3.2.1  The man Page . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
3.2.2  man Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1053.2.3  Searching the man Pages: apropos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1073.2.4  Getting the Right man Page: whatis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1103.2.5  Things to Look for in the man Page . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1113.2.6  Some Recommended man Pages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1123.2.7  GNU info . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1153.2.8  Viewing info Pages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1153.2.9  Searching info Pages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1183.2.10  Recommended info Pages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1193.2.11  Desktop Help Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
3.3  Other Places to Look . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
3.3.1  /usr/share/doc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
3.3.2  Cross Referencing and Indexing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1213.3.3  Package Queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
3.4  Documentation Formats . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
3.4.1  TeX/LaTeX/DVI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
3.4.2  Texinfo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1253.4.3  DocBook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1263.4.4  HTML . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1273.4.5  PostScript . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1293.4.6  Portable Document Format (PDF) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1303.4.7  troff . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
3.5  Internet Sources of Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
3.5.1  www.gnu.org . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
3.5.2  SourceForge.net . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1323.5.3  The Linux Documentation Project . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1333.5.4  Usenet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1343.5.5  Mailing Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1343.5.6  Other Forums . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
3.6  Finding Information about the Linux Kernel . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
3.6.1  The Kernel Build . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
3.6.2  Kernel Modules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137Contents ix
3.6.3  Miscellaneous Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
3.7  Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
3.7.1  Tools Used in This Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
3.7.2  Online Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
Chapter 4 Editing and Maintaining Source Files . . . . . . . . . . . . . . . . . . . . . . . 141
4.1  Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
4.2  The Text Editor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
4.2.1  The Default Editor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
4.2.2  What to Look for in a Text Editor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1444.2.3  The Big T wo: vi and Emacs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1464.2.4  Vim: vi Improved . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1464.2.5  Emacs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1704.2.6  Attack of the Clones . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1794.2.7  Some GUI Text Editors at a Glance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1824.2.8  Memory Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1874.2.9  Editor Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
4.3  Revision Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
4.3.1  Revision Control Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
4.3.2  Defining Revision Control Terms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1914.3.3  Supporting Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1934.3.4  Introducing diff and patch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1934.3.5  Reviewing and Merging Changes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
4.4  Source Code Beautifiers and Browsers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
4.4.1  The Indent Code Beautifier . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
4.4.2  Astyle Artistic Style . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2064.4.3  Analyzing Code with cflow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2074.4.4  Analyzing Code with ctags . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2104.4.5  Browsing Code with cscope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2114.4.6  Browsing and Documenting Code with Doxygen . . . . . . . . . . . . . . . . . . . . 2124.4.7  Using the Compiler to Analyze Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
4.5  Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
4.5.1  Tools Used in This Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
4.5.2  References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2174.5.3  Online Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218x Contents
Chapter 5 What Every Developer Should Know about the Kernel . . . . . . . . . 221
5.1  Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
5.2  User Mode versus Kernel Mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222
5.2.1  System Calls . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
5.2.2  Moving Data between User Space and Kernel Space . . . . . . . . . . . . . . . . . . 226
5.3  The Process Scheduler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226
5.3.1  A Scheduling Primer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
5.3.2  Blocking, Preemption, and Yielding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2285.3.3  Scheduling Priority and Fairness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2295.3.4  Priorities and Nice Value . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2345.3.5  Real-Time Priorities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2355.3.6  Creating Real-Time Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2385.3.7  Process States . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2395.3.8  How Time Is Measured . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
5.4  Understanding Devices and Device Drivers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
5.4.1  Device Driver Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
5.4.2  A Word about Kernel Modules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2595.4.3  Device Nodes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2605.4.4  Devices and I/O . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272
5.5  The I/O Scheduler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
5.5.1  The Linus Elevator (aka noop) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
5.5.2  Deadline I/O Scheduler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2845.5.3  Anticipatory I/O Scheduler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2845.5.4  Complete Fair Queuing I/O Scheduler . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2855.5.5  Selecting an I/O Scheduler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
5.6  Memory Management in User Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
5.6.1  Virtual Memory Explained . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
5.6.2  Running out of Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
5.7  Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315
5.7.1  Tools Used in This Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315
5.7.2  APIs Discussed in This Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3165.7.3  Online References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3165.7.4  References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316Contents xi
Chapter 6 Understanding Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
6.1  Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
6.2  Where Processes Come From . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
6.2.1  fork and vfork . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318
6.2.2  Copy on Write . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3196.2.3  clone . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 320
6.3  The exec Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 320
6.3.1  Executable Scripts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
6.3.2  Executable Object Files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3246.3.3  Miscellaneous Binaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324
6.4  Process Synchronization with wait . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
6.5  The Process Footprint . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329
6.5.1  File Descriptors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331
6.5.2  Stack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3386.5.3  Resident and Locked Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339
6.6  Setting Process Limits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340
6.7  Processes and procfs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3436.8  Tools for Managing Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 346
6.8.1  Displaying Process Information with ps . . . . . . . . . . . . . . . . . . . . . . . . . . . . 346
6.8.2  Advanced Process Information Using Formats . . . . . . . . . . . . . . . . . . . . . . . 3496.8.3  Finding Processes by Name with ps and pgrep . . . . . . . . . . . . . . . . . . . . . . . 3526.8.4  Watching Process Memory Usage with pmap . . . . . . . . . . . . . . . . . . . . . . . . 3536.8.5  Sending Signals to Processes by Name . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354
6.9  Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355
6.9.1  System Calls and APIs Used in This Chapter . . . . . . . . . . . . . . . . . . . . . . . . 356
6.9.2  Tools Used in This Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3566.9.3  Online Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356
Chapter 7 Communication between Processes . . . . . . . . . . . . . . . . . . . . . . . . . 357
7.1  Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357
7.2  IPC Using Plain Files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358
7.2.1  File Locking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362
7.2.2  Drawbacks of Using Files for IPC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363xii Contents
7.3  Shared Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363
7.3.1  Shared Memory with the POSIX API . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364
7.3.2  Shared Memory with the System V API . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
7.4  Signals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370
7.4.1  Sending Signals to a Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371
7.4.2  Handling a Signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3717.4.3  The Signal Mask and Signal Handling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3737.4.4  Real-Time Signals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3767.4.5  Advanced Signals with sigqueue and sigaction . . . . . . . . . . . . . . . . . . . . . . . 378
7.5  Pipes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381
7.6  Sockets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 2
7.6.1  Creating Sockets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383
7.6.3  Client/Server Example Using Local Sockets . . . . . . . . . . . . . . . . . . . . . . . . . 3877.6.4  Client Sever Using Network Sockets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 392
7.7  Message Queues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393
7.7.1  The System V Message Queue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393
7.7.2  The POSIX Message Queue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3977.7.3  D ifference between POSIX Message Queues and System V Message Queues . . . 402
7.8  Semaphores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402
7.8.1  Semaphores with the POSIX API . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407
7.8.2  Semaphores with the System V API . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 410
7.9  Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 412
7.9.1  System Calls and APIs Used in This Chapter . . . . . . . . . . . . . . . . . . . . . . . . 412
7.9.2  References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4147.9.3  Online Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 414
Chapter 8 Debugging IPC with Shell Commands . . . . . . . . . . . . . . . . . . . . . . 415
8.1  Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415
8.2  Tools for Working with Open Files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415
8.2.1  lsof  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 416
8.2.2  fuser . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4178.2.3  ls   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4188.2.4  file . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4188.2.5  stat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419Contents xiii
8.3  Dumping Data from a File . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 420
8.3.1  The strings Command . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 422
8.3.2  The xxd Command . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4238.3.3  The hexdump Command . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4248.3.4  The od Command . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425
8.4  Shell Tools for System V IPC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 426
8.4.1  System V Shared Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 426
8.4.2  System V Message Queues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4298.4.3  System V Semaphores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 430
8.5  Tools for Working with POSIX IPC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431
8.5.1  POSIX Shared Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431
8.5.2  POSIX Message Queues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4328.5.3  POSIX Semaphores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433
8.6  Tools for Working with Signals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 434
8.7  Tools for Working with Pipes and Sockets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 437
8.7.1  Pipes and FIFOs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 437
8.7.2  Sockets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 438
8.8  Using Inodes to Identify Files and IPC Objects . . . . . . . . . . . . . . . . . . . . . . . . . . 440
8.9  Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 442
8.9.1  Tools Used in This Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 442
8.9.2  Online Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 443
Chapter 9 Performance Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445
9.1  Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445
9.2  System Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445
9.2.1  Memory Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 446
9.2.2  CPU Utilization and Bus Contention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4569.2.3  Devices and Interrupts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4599.2.4  Tools for Finding System Performance Issues . . . . . . . . . . . . . . . . . . . . . . . . 467
9.3  Application Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 475
9.3.1  The First Step with the time Command . . . . . . . . . . . . . . . . . . . . . . . . . . . . 475
9.3.2  Understanding Your Processor Architecture with x86info . . . . . . . . . . . . . . 4769.3.3  Using Valgrind to Examine Instruction Efficiency . . . . . . . . . . . . . . . . . . . . 4809.3.4  Introducing ltrace . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4849.3.5  Using strace to Monitor Program Performance . . . . . . . . . . . . . . . . . . . . . . 4859.3.6  T raditional Performance T uning Tools: gcov and gprof . . . . . . . . . . . . . . . . . 4879.3.7  Introducing OProfile . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 494xiv Contents
9.4  Multiprocessor Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 501
9.4.1  Types of SMP Hardware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 501
9.4.2  Programming on an SMP Machine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 506
9.5  Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 509
9.5.1  Performance Issues in This Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 510
9.5.2  Terms Introduced in This Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5109.5.3  Tools Used in This Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5109.5.4  Online Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5119.5.5  References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 511
Chapter 10 Debugging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 513
10.1  Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 513
10.2  The Most Basic Debugging Tool: printf . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 514
10.2.1  Problems with Using printf . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 514
10.2.2  Using printf Effectively . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51910.2.3  Some Final Words on printf Debugging . . . . . . . . . . . . . . . . . . . . . . . . . . . 528
10.3  Getting Comfortable with the GNU Debugger: gdb . . . . . . . . . . . . . . . . . . . . . 529
10.3.1  Running Your Code with gdb . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 530
10.3.2  Stopping and Restarting Execution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53110.3.3  Inspecting and Manipulating Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54110.3.4  Attaching to a Running Process with gdb . . . . . . . . . . . . . . . . . . . . . . . . . 55310.3.5  Debugging Core Files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55310.3.6  Debugging Multithreaded Programs with gdb . . . . . . . . . . . . . . . . . . . . . . 55710.3.7  Debugging Optimized Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 558
10.4  Debugging Shared Objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 561
10.4.1  When and Why to Use Shared Objects . . . . . . . . . . . . . . . . . . . . . . . . . . . 562
10.4.2  Creating Shared Objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56310.4.3  Locating Shared Objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56410.4.4  Overriding the Default Shared Object Locations . . . . . . . . . . . . . . . . . . . . 56410.4.5  Security Issues with Shared Objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56510.4.6  Tools for Working with Shared Objects . . . . . . . . . . . . . . . . . . . . . . . . . . . 565
10.5  Looking for Memory Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 569
10.5.1  Double Free . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 569
10.5.2  Memory Leaks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57010.5.3  Buffer Overflows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57010.5.4  glibc Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57210.5.5  Using Valgrind to Debug Memory Issues . . . . . . . . . . . . . . . . . . . . . . . . . . 57610.5.6  Looking for Overflows with Electric Fence . . . . . . . . . . . . . . . . . . . . . . . . 581Contents xv
10.6  Unconventional Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 583
10.6.1  Creating Your Own Black Box . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 584
10.6.2  Getting Backtraces at Runtime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58710.6.3  Forcing Core Dumps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58910.6.4  Using Signals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59010.6.5  Using procfs for Debugging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 591
10.7  Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 594
10.7.1  Tools Used in This Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 594
10.7.2  Online Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59510.7.3  References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 595
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  597xvi Contents
xviiForeword
OK, so you’ve mastered the basics of Linux. You can run ls, grep, find, and sort, and
as a C or C++ programmer, you know how to use the Linux system calls. You knowthat there’s much more to life than “point and click” and that Linux will give it toyou. You’re just not sure yet how. So you ask yourself, “What’s next?”
This book gives you the answer. John’s knowledge is broad, and he shows the no-
longer-novice Linux user how to climb up the next part of the learning curve towardmastery.
From command-line tools for debugging and performance analysis to the range
of files in /proc, John shows you how to use all of them to make your day-to-daylife with Linux easier and more productive.
Besides a lot of “what” (what tools, what options, what files), there’s a lot of “why”
here. John shows you why things work the way they do. In turn, this lets you under-stand why the “what” is effective and internalize the Zen of Linux (and Unix!).
There’s a ton of great stuff in this book. I hope you learn a lot. I know I did, and
that’s saying something.
Enjoy,Arnold Robbins
Series Editor
This page intentionally left blank 
xixPreface
Linux has no shortage of tools. Many are inherited from Unix, with cryptic two-
letter names that conjure up images of developers trying to preserve space on apunch card. Happily, those days are long gone, but the legacy remains.
Many of those old tools are still quite useful. Most are highly specialized. Each
may do only one thing but does it very well. Highly specialized tools often havemany options that can make them intimidating to use. Consider the first time youused 
grep and learned what a regular expression was. Perhaps you haven’t mastered
regular expression syntax yet (don’t worry; no one else has, either). That’s notimportant, because you don’t need to be a master of regular expressions to put
grep to good use.
If there’s one thing that I hope you learn from this book, it’s that there are many
tools out there that you can use without having to master them. You don’t need toinvest an enormous amount of time reading manuals before you can be productive.I hope you will discover new tools that you may not have been familiar with. Someof the tools this book looks at are quite old and some are new. All of them are use-ful. As you learn more about each tool, you will find more uses for it.
I use the term toolloosely in this book. To me, creating tools is as important as
using tools, so I have included various APIs that are not usually covered in muchdetail in other books. In addition, this book provides some background on theinternal workings of the Linux kernel that are necessary to understand what sometools are trying to tell you. I present a unique perspective on the kernel: the user’spoint of view. You will find enough information to allow you to understand theground rules that the kernel sets for every process, and I promise you will not haveto read a single line of kernel source code.
What you will not find in this book is reconstituted man pages or other docu-
mentation stitched into the text. The GNU and Linux developers have done a great
job of documenting their work, but that documentation can be hard to find for the
inexperienced user. Rather than reprint documentation that will be out of date bythe time you read this, I show you some ingenious ways to find the most up-to-datedocumentation.
GNU/Linux documentation is abundant, but it’s not always easy to read. You
can read a 10,000-word document for a tool and still not have a clue what the tooldoes or how to use it. This is where I have tried to fill in the missing pieces. I havetried to explain not just how to use each tool, but also why you would want to useit. Wherever possible, I have provided simple, brief examples that you can type andmodify yourself to enhance your understanding of the tools and Linux itself.
What all the tools in this book have in common is that they are available at no
cost. Most come with standard Linux distributions, and for those that may not, Ihave included URLs so that you can download them yourself.
As much as possible, I tried to keep the material interesting and fun. 
Who Should Read This Book
This book is written for intermediate to advanced Linux programmers who wish tobecome more productive and gain a better understanding of the Linux program-ming environment. If you’re an experienced Windows programmer who feels like afish out of water in the Linux environment, then this book is for you, too.
Non-programmers should also find this book useful because many of the tools and
topics I cover have applications beyond programming. If you are a system adminis-trator, or just a Linux enthusiast, then there’s something for you in this book, too.
The Purpose of This Book
I wrote this book as a follow-up to an article I wrote for the Linux Journal entitled
“Ten Commands Every Linux Developer Should Know.” The inspiration for thisarticle came from my own experience as a Linux programmer. In my daily work Imake it a point to invest some of my time in learning something new, even if itmeans a temporary lull in progress on my project. Invariably this strategy has paidoff. I have always been amazed at how many times I learned about a tool or fea-ture that I concluded would not be useful, only to find a use for it shortly after-ward. This has always been a powerful motivation for me to keep learning. I hopethat by reading this book, you will follow my example and enhance your skills ona regular basis.xx Preface
It’s also just plain fun to learn about this stuff. If you are like me, you enjoy work-
ing with Linux. Motivating yourself to learn more has never been a problem.Because Linux is open source, you have the opportunity to understand all of itsinner workings, which is not possible with closed source environments likeWindows. In this book I present several freely available resources available to helpyou learn more.
How to Read This Book
The chapters are presented such that each chapter can stand on its own. Later chap-ters require some background knowledge that is presented in the earlier chapters.Wherever possible, I have cross-referenced the material to help you find the neces-sary background information.
I believe the best way to learn is by example, so I have tried to provide simple exam-
ples wherever possible. I encourage the reader to try the examples and experiment.
How This Book Is Organized
Chapter 1, Downloading and Installing Open Source Tools, covers the mechanismsused to distribute open source code. I discuss the various package formats used bydifferent distributions and the advantages and disadvantages of each. I present sev-eral tools used to maintain packages and how to use them.
Chapter 2, Building from Source, covers the basics of building an open source
project. I present some of the tools used to build software and alternatives that areemerging. There are several tips and tricks in this chapter that you can use to mas-ter your use of 
make . I also show you how to configure projects that are distributed
with GNU’s autoconf tools so that you can customize them to meet your needs.
Finally, I cover the stages of the build that are often misunderstood by many pro-grammers. I look at some of the errors and warnings you are likely to encounter andhow to interpret them.
Chapter 3, Finding Help, looks at the various documentation formats tucked
away in your Linux distribution that you may not know about. I look at the toolsused to read these formats and discuss effective ways to use them.
Chapter 4, Editing and Maintaining Source Files, discusses the various text edi-
tors available for programmers as well as the advantages and disadvantages of each.I present a set of features that every programmer should look for in an editor andmeasure each editor against these. This chapter also covers the basics of revisioncontrol, which is vital for software project management.Preface xxi
Chapter 5, What Every Developer Should Know about the Kernel, looks at the
kernel from a user’s perspective. In this chapter you will find the necessary back-ground information required to understand the workings of a Linux system. I intro-duce several tools that allow you to see how your code interacts with the kernel.
Chapter 6, Understanding Processes, focuses on processes, their characteristics,
and how to manage them. I cover a good deal of background required to introducethe tools in this chapter and understand why they are useful. In addition, this chap-ter introduces several programming APIs that you can use to create your own tools.
Chapter 7, Communication Between Processes, introduces the concepts behind
inter-process communication (IPC). This chapter contains mostly background infor-mation required for Chapter 8. Along with each IPC mechanism, I introduce theAPIs required to use it along with a working example.
Chapter 8, Debugging IPC with Shell Commands, presents several tools available
to debug applications that use IPC. It builds on the information from Chapter 7 tohelp you interpret the output of these tools, which can be difficult to understand.
Chapter 9, Performance T uning, introduces tools to measure the performance of
your system as well as the performance of individual applications. I present severalexamples to illustrate how programming can impact performance. I also discusssome of the performance issues that are unique to multi-core processors.
Chapter  10, Debugging, presents several tools and techniques that you can use
to debug applications. I look at some open source memory debugging tools includ-ing Valgrind and Electric Fence. I also take an in-depth look at the capabilities of
gdb, and how to use it effectively.xxii Preface
xxiiiAcknowledgments
I would like to thank my wife, Lisa, without whom this book would not have been
possible. Too often, she had to be a single mom while I worked in seclusion. Withouther support, I would never have been able to take advantage of this opportunity.Thanks also to my children—Andrew, Alex, and Samantha—who had to spend toomuch time without their dad during the course of this work.
My thanks also go to Arnold Robbins, who provided wonderful advice and over-
sight. His experience and authoritative knowledge were invaluable to me during thecourse of this work. Thanks for making this an enjoyable learning experience for me.
Thanks also to Debra Williams Cauley for her patience and diligence putting up
with my missed deadlines and schedule slips. This first-time author is grateful toyou for keeping everything on track.
Finally, I would like to thank Mark Taub for recruiting me and giving me this
wonderful opportunity.
This page intentionally left blank 
About the Author
John Fusco is a software developer for GE Healthcare, based in Waukesha,
Wisconsin, specializing in Linux applications and device drivers. John has workedon Unix software for more than ten years and has been developing applications forLinux since kernel version 2.0. John has written articles for Embedded Systems
Programming and Linux Journal. This is his first book.
xxv
This page intentionally left blank 
1.1 Introduction
In this chapter, I discuss the different formats for distributing free software, how to
manipulate them, and where to find them. I examine archive files and package filesin detail, as well as the most common tools commands used to manipulate them.
It can be dangerous to accept software from strangers. I cover various security
issues that you should be aware of and things you can do to protect yourself. I intro-duce the concept of authentication and trust, and discuss how it applies to security.For those times when authentication is not possible, I show you how to inspectpackages and archives.
Finally, I introduce some tools for managing packages on package-based distri-
butions and how to get the most out of them.
1
1Downloading and Installing
Open Source Tools
1.2 What Is Open Source?
The term open source is a marketing term for free software, created by the Open
Source Initiative (OSI).1This organization was founded to promote the principles
of free software that had its roots in the GNU Project, founded by RichardStallman. One goal of OSI is to counter some of the negative stereotypes about freesoftware and promote the free sharing of source code. 
At first, many businesses were afraid of using open source software. No doubt the
marketing departments of some large software companies had something to do withit. Conventional wisdom says, “You get what you pay for.” Some feared that thelicenses (like the GNU Public License) would act like a virus so that by creatingprojects using free software, they, too, would have to make their source code public.
Fortunately, most of those fears have subsided. Many large businesses are freely
using and promoting open source code in their own projects. Some have even basedentire products on open source software. The genie is out of the bottle.
1.3 What Does Open Source Mean to You?
To most people, open source software simply means a lot of high-quality softwareavailable at no cost. Unfortunately, a lot of not-so-high-quality software is availableas well, but that’s part of the process. Good project ideas flourish and improve,while bad ones wither and die. Picking open source software is a bit like pickingfruit: It takes some experience to know when it’s ripe.
A natural selection process is going on at many levels. At the source code level,
features and code are selected (based on patches) so that only the best code gets in.As a consumer, you select the projects to download, which drives the vitality of aproject. No one wants to develop code for a project that no one is using. Fewerdownloads attract fewer developers. More downloads mean more developers, whichin turn means more code to choose among and, thus, better code. Sometimes select-ing a project to try is a gamble, but the only things at stake are your time and effort.It’s inevitable that you will make some regrettable choices once in a while, but takeheart: It’s all part of the process.
For some people, not knowing what you are getting is part of the fun. It’s like
opening a birthday gift. For others, it’s a nuisance and a waste of time. If you’re2 Chapter 1 • Downloading and Installing Open Source Tools
1. www.opensource.org
looking for the convenience of shrink-wrapped software that just installs and runs,
there are open source projects for you—just not as many. Fortunately, there aremany resources on the Internet to help you make good choices.
1.3.1 Finding Tools
The first place you should look before you start trolling the Internet is your distri-bution CDs. Assuming that you installed Linux from a set of CDs or a DVD, youprobably have a lot of tools that were not installed. Most distributions ship withmuch more software on the CDs than is installed in a default installation. Typically,you are given a choice when you install the OS as to what kind of system you wantto create. This results in an arbitrary set of packages being installed to your system,based on someone’s idea of what a “workstation” or a “server” is. 
You can always add to the set of installed software manually by locating the raw
packages on the installation CDs. The drawback here is that the packages usuallyare not arranged in any particular order, so you have to know what you are lookingfor. Some distributions have graphical interfaces that arrange the packages into cat-egories to help you pick which software to install.
If you don’t know what you are looking for, the Internet should be your next des-
tination. Several Web sites serve as clearinghouses for open source software. Onesuch site is www.freshmeat.net. Here, you will find software arranged by categoriesso that it’s easy to find what you’re looking for. While writing this book, for exam-ple, I searched Freshmeat for the term word processors and found 71 projects avail-
able. Imagine having to choose among 71 different word processors!
Freshmeat allows you to filter your results to help you narrow down your choices.
My results included various operating systems besides Linux and projects in variousstages of development. So I chose to limit my search to projects that have Linuxsupport, that are mature, and that use an OSI-approved Open Source license.(Freshmeat results include commercial software as well.) This reduced the numberto 12 projects—a much more manageable number. A closer look revealed that sev-eral of these projects were not what I was looking for, given the broad interpreta-tion of the term word processor. After trying a few more filters, I was able to uncover
a few well-known, high-quality projects, such as AbiWord, and a few I never heardof before. There were some notable absences, such as OpenOffice, which I am usingto write this book. It turns out that the reason I didn’t find OpenOffice was becauseit was filed under “Office/Business :: Office Suites,” not “word processors.” Themoral of the story is that if you don’t find what you are looking for, keep looking.1.3 What Does Open Source Mean to You? 3
1.3.2 Distribution Formats
Now that you’ve found some software that you are interested in, you probably have
some more choices to make. Mature projects usually offer ready-to-install packagesin one or more package formats. Less mature projects often offer only source code orbinary files in an archive file. Often, this is a good indicator of what you are gettinginto. Downloading a software package file can be like buying a new car: You don’tneed to know how it works; you just turn the key, and it starts. By contrast, down-loading an archive of source or binaries can be like buying a used car: It helps if youknow something about cars; otherwise, you won’t know what you’re getting into.
Usually, when a project provides an installable package, it’s a sign that the proj-
ect has matured. It is also a sign that the release cycle of the project is stable. If theproject were delivering new releases every week, it probably wouldn’t bother mak-ing packages. With a software package file and a little luck, you might be able toinstall it and run. But as with a new car, it is possible to get a lemon once in a while. 
The alternative to a package file is an archive file, which for Linux-based projects
is usually a compressed 
tarfile. An archive file is a collection of files packed into a
single file using an archiving tool such as the tarcommand. Usually, the files are
compressed with the gzip program to save space; often, they are referred to as tar
filesor tarballs.
Tar files are the preferred format for distributing source code for projects. They
are easy to create and use, and every programmer is familiar with the tarprogram.
Less often, you will find tar files that have binary executables in them. This is aquick-and-dirty alternative to packaging and should be avoided unless you knowwhat you are doing. In general, tar files are for people who have some knowledge ofprogramming and system administration.
1.4 An Introduction to Archive Files
At some point in the process of downloading and installing open source software,you are going to encounter an archive file of one sort or another. An archive file is
any file that contains a collection of other files. If you are a Windows user, you areno doubt familiar with the predominant Windows archiver, PKZip. Linux archiveutilities function similarly except that unlike PKZip, they do not include compres-sion. Instead, Linux archive tools concentrate on archiving and leave the compres-sion to another tool (typically, 
gzip or bzip2 ). That’s the UNIX philosophy.4 Chapter 1 • Downloading and Installing Open Source Tools
Naturally, because this is Linux, you have more than one choice of archivers, but as
an open source consumer, you have to take what you’re given. So even though youare most likely to encounter 
tarfiles exclusively, it’s good at least to know that
other tools are available.
An archive utility has some special requirements beyond just preserving filenames
and data. In addition to a file’s pathname and data, the archive has to preserve eachfile’s metadata. Metadata includes the file’s owner, group, and other attributes (such
as read/write/execute permissions). The archiver records all this information suchthat a file can be deleted from the file system and restored later from the archivewith no loss of information. If you archive an executable file and then delete it fromyour file system, that file should still be executable when you restore it. In Windows,the filename would indicate whether the file is executable via the extension (such as
.exe ). Linux uses the file’s metadata to indicate whether it is executable, so this data
must be stored by the archiver to be preserved.
The most common archive tools used in Linux are listed in Table 1-1. By far the
most popular archive format is tar. The name tarcomes from a contraction of tape
archive, which is a legacy from its days as a tape backup utility. These days, taris
most commonly used as a general-purpose tool to archive groups of files into a sin-gle file. An alternative to 
tarthat you may run into less frequently is cpio , which
uses a very different syntax to accomplish the same task. There’s also the POSIXstandard archive utility 
pax, which can understand tarfiles, cpio files, or its own
format. I have never seen anything distributed in paxformat, but I mention it here
for completeness.
One last archive utility worth mentioning is ar, which is most frequently used to
create object code libraries used in software development, but it is also used to cre-ate package files used by the Debian distribution.
TABLE 1-1 Most Common Archive Tools
Tool Notes
tar Most popular.
cpio Used internally by the RPM format; not used extensively elsewhere.
ar Used internally by Debian packager; otherwise, used only for software
development libraries. arfiles have no path information.1.4 An Introduction to Archive Files 5
You can also find utilities to handle .zip files created with PKZip as well as some
lesser-known compressed archive utilities, such as lha. Open source programs forLinux are virtually never distributed in these formats, however. If you see a
.zip
archive, it’s a good bet that it’s intended for a Microsoft operating system.
For the most part, you need to know two things about each format: how to
query the archive for its contents and how to extract files from the archive. UnlikeWindows archivers, which have all kinds of dangerous bells and whistles, Linuxarchivers focus on the basics. So it’s generally safe to query and extract files from anarchive, especially if you are not the root user. It’s always wise to query an archivebefore extracting files so that you don’t inadvertently overwrite files on your systemthat may have the same names.
1.4.1 Identifying Archive Files
When you download an archive from the Internet, it most likely has been com-pressed to save bandwidth. There are some file-naming conventions for compressedfiles; some of these are shown in Table 1-2.
When in doubt, remember the 
file command. This tool does a good job of
identifying what you are looking at when the filename gives you no clue. This isuseful when your Web browser or other tool munges the filename into somethingunrecognizable. Suppose that I have a compressed tar archive named 
foo.x , for6 Chapter 1 • Downloading and Installing Open Source Tools
TABLE 1-2 Archive Naming Conventions
Extensions Type
.tar tar archive, uncompressed
.tar.gz .tgz tar archive, compressed with gzip
.tar.bz2 tar archive, compressed with bzip2
.tar.Z .taz tar archive, compressed with the UNIX compress command
.ar .a ar archive, generally used only for software development
.cpio cpio archive, uncompressed
example. The name tells me nothing about the contents of this file. Then I try the
following command:
$ file foo.x
foo.x: gzip compressed data, from UNIX, max compression
Now I know that the file was compressed with gzip , but I still don’t know
whether it’s a tarfile. I can try unzipping it with gzip and try the file command
again. Or I can just use the -zoption of the command:
$ file -z foo.x
foo.x: tar archive (gzip compressed data, from UNIX, max compression)
Now I know exactly what I’m looking at.
Normally, people follow some intuitive file-naming conventions, and the file-
name does a good job of identifying the archive type and what processing hasbeen done.
1.4.2 Querying an Archive File
Archive files keep track of the files they contain with a table of contents, which(conveniently enough) is accessed with a 
-tflag for all the archivers I mentioned
earlier. Following is a sample from a tar file for the Debian cron installation:
$ tar -tzvf data.tar.gz
drwxr-xr-x root/root         0 2001-10-01 07:53:19 ./drwxr-xr-x root/root         0 2001-10-01 07:53:15 ./usr/drwxr-xr-x root/root         0 2001-10-01 07:53:18 ./usr/bin/-rwsr-xr-x root/root     22460 2001-10-01 07:53:18 ./usr/bin/crontabdrwxr-xr-x root/root         0 2001-10-01 07:53:18 ./usr/sbin/-rwxr-xr-x root/root     25116 2001-10-01 07:53:18 ./usr/sbin/cron
The example above added the –voption to include additional information sim-
ilar to a longlisting from the lscommand. The output includes the file permissions
in the first column, followed by the ownership in the second column. The file size(in bytes) is shown next, with directories listed as having a size of 
0. When inspect-
ing archives, you should pay careful attention to the ownership and permissions ofeach file.
The basic commands to list the contents of an archive for the various formats are
listed in Table 1-3. All three formats produce essentially the same output.1.4 An Introduction to Archive Files 7
TABLE 1-3 Archive Query Commands
Format Command Notes
tar tar -tvf filename
tararchive compressed with gzip tar -tzvf filename
tararchive compressed with bzip2 tar -tjvf filename
cpio archive cpio -tv < filename cpio uses stdin
and stdout as
binary streams.
Reading the symbolic representation of a file’s permissions is fairly straightfor-
ward when you get used to it. You should be familiar with the tricks that are usedto represent additional information above and beyond the usual read/write/executepermissions.
Let’s start with the permission string itself. This is represented with a ten-
character string. The first character indicates the type of file, whereas the remainingthree groups of three characters summarize the file owner’s permission, the groupmembers’ permissions, and everyone else’s permissions, respectively.
The type of file is indicated with a single character. The valid values for this char-
acter and their meanings are listed in Table 1-4.
The next nine characters can be grouped into three groups of three bits. Each bit
represents the read, write, or execute permissions of the file, respectively, repre-sented as 
r, w, and x. A -in a bit position indicates that that permission is not set.
A -in the wposition, for example, indicates that the file is not writeable. Some
examples are shown in Table 1-5.
The last things to know about permissions are the setuid, setgid, and sticky bits.
These bits are not listed directly, because they affect the file’s behavior only whenexecuting. 
When the setuid bit is set, the code in the file will execute, using the file’s owner
as the effective user ID. This means that the program can do anything that the file’sowner has permission to do. If a file is owned by root and the setuid bit is set, thecode has permission to modify or delete any file in the system, no matter which userstarts the program. Sounds dangerous, doesn’t it? Programs with the setuid bit havebeen the subject of attacks in the past.8 Chapter 1 • Downloading and Installing Open Source Tools
The setgid bit does the same thing, except that the code executes with the privi-
leges of the group to which the file belongs. Normally, a program executes with theprivileges of the group of the user who started the program. When the setgid bit isset, the program runs with privileges as though the user belonged to the same group.
You can recognize a file with the setuid or setgid bit set by looking at the 
xbit in
the permissions string. Normally, an xin this position means that the file is exe-
cutable, whereas a -indicates that the file is not executable.1.4 An Introduction to Archive Files 9
TABLE 1-4 File Types in an Archive Listing
Character Meaning Notes
- regular file Includes text files, data files, executable, etc.
d directory
c character device A special file used to communicate with a
character device driver. These files traditionally
are restricted to the /dev directory; you usually
don’t see them in archives.
b block device A special file used to communicate with a 
block device driver. These files traditionally are
restricted to the /dev directory; you usually don’tsee them in archives.
l symbolic link A filename that points to another filename. The
file it points to may reside on a different file
system or may be nonexistent.
TABLE 1-5 Examples of File Permission Bits
Permissions
rwx File is readable, writeable, and executable.
rw- File is readable and writeable but not executable.
r-x File is readable and executable but not writeable.
--x File is executable but not writeable or readable.
The setuid and setgid bits add two more possible values for this character. A low-
ercase sinstead of an xin the owner’s permissions means that the file is executable
by the owner and the setuid bit is set. An uppercase Smeans that the setuid bit is
set, but the owner does not have execute permission. It seems odd, but it is allowedand just as dangerous. The file could be owned by root, for example, but root hasno permission to execute the file. Linux gives root execute permission if anyone has
execute permission. So even if the execute bit for root is not set, as long as the cur-rent user has execute permission, the code will execute with root privileges.
Like the setuid bit, the setgid bit is indicated by modifying the 
xposition in the
group permissions. A lowercase shere indicates that the file’s setgid bit is set and
that members of the group have permission to execute this file. An uppercase Sindi-
cates that the setgid bit is set, but members of the group do not have permission toexecute the file.
You can see in the 
cron package output, shown earlier in this chapter, that the
crontab program is a setuid program owned by root. Some more permissions and
their meanings are shown in Table 1-6.
TABLE 1-6 Some Examples of Permissions and Their Meanings
Permission Execute Effective Effective
String Permission User ID Group ID
-rwxr-xr-x All users can execute this file. Current user Current user
-rw-r-xr-x Current user Current user
-rwsr-xr-x All users can execute this file. File owner Current user
-rwSr-xr-x File owner Current user
-rwxr-sr-x All users can execute this file. Current user Group owner
-rwsr-sr-x All users can execute this file. File owner Group owner
-rwsr-Sr-x File owner Group owner All users can execute this file,
including the owner, but notmembers of the file’s group.Everyone except the ownercan execute this file.All members of the file’sgroup can execute this fileexcept the owner; everyoneelse except the owner canexecute this file.10 Chapter 1 • Downloading and Installing Open Source Tools
The sticky bit is something of a relic. The original intent of the sticky bit was
to make sure that certain executable programs would load faster by keeping thecode pages on the swap disk. In Linux, the sticky bit is used only in directories,where it has a completely different meaning. Normally, when you give write andexecute permission to other users in a directory that you own, those users are freeto create and delete files in that directory. One privilege you may not want themto have is the ability to delete other users’ files in that directory. Normally, if auser has write permission in a directory, that user can delete any file in that direc-tory, not just the files he owns. You can revoke this privilege by setting the stickybit on the directory. When the directory has the sticky bit set, users can deleteonly files that belong to them. As usual, the directory’s owner and root can deleteany files. The 
/tmp directory on most systems has the sticky bit set for this
purpose.
A directory with the sticky bit set is indicated with a tor a Tin the execute per-
mission for others. For example:
-rwxrwxrwt All users can read and write in this directory, and the stickybit is set.
-rwxrwx--T Only the owner and group members can read or write, andthe sticky bit is set.
1.4.3 Extracting Files from an Archive File
Now that you know how to inspect an archive file’s contents, it’s time to extract thefiles to have a closer look. The basic commands are listed in Table 1-7.
Although it’s generally safe to extract files from an archive, you need to pay atten-
tion to the pathnames to avoid clobbering any data on your system. In particular,
cpio has the ability to store absolute paths from the root directory. This means that
if you try to extract a cpio archive that happens to have a bunch of files in /etc ,
you could clobber vital files inadvertently. Consider a cpio archive that contains a
copy of /etc/hosts , among other things. If you try to extract files from this
archive, it will try to overwrite your copy of /etc/hosts . You can see this by query-
ing the archive
cpio -t < foo.cpio
/etc/hosts1.4 An Introduction to Archive Files 11
The leading / is your clue that the archive wants to restore thecopy of
/etc/hosts and not some other copy. So if you are extracting files for inspection,
you probably don’t want to overwrite your copies of the same file (yet). You willwant to make sure that you use the GNU option 
--no-absolute-filenames so
that the hosts file will be extracted to
./etc/hosts
Fortunately, the only time you are likely to encounter a cpio archive is as part of
an RPM package file, and RPM always uses pathnames relative to the current direc-tory, so there is no chance of overwriting system files unless you want to.
Note that the version of tar found in some versions of UNIX also allows absolute
pathnames. The GNU version of tar found in Linux automatically strips the lead-ing 
/from files extracted from a tar archive. So if you happen upon a tar file that
comes from one of these other flavors of UNIX, GNU tar will watch your back.GNU tar also strips the leading 
/from the pathnames in archives that it creates.
1.5 Know Your Package Manager
Package managers are sophisticated tools used to install and maintain software onyour system. They help you keep track of what software is installed and where the12 Chapter 1 • Downloading and Installing Open Source Tools
TABLE 1-7 Archive Extraction Commands
Format Command Notes
tar tar -xf filename This command extracts
files to the currentdirectory by default.
tar -xzf filename
tar -xjf filename
cpio archive cpio -i -d < filename Beware of absolute
pathnames.
ararchive ar x filename Files have no path
information.tararchive compressed
with bzip2tararchive compressed
with gzip
files are located. A package manager can keep track of dependencies to make sure
that new software you install is compatible with the software you have alreadyinstalled. If you wanted to install a KDE package on a GNOME machine, forexample, the package manager would protest, indicating that you don’t have therequired runtime libraries. This is preferable to installing the package only toscratch your head trying to figure out why it won’t work.
One of the most valuable features that a package manager offers is the ability to
uninstall. This allows you to install a piece of software and try it out, and then unin-stall it if you don’t like it. After you uninstall the package, your system is back tothe same configuration it had before you installed the package. Uninstalling a pack-age is one way to upgrade it. You remove the old version and install the new one.Most package managers have a special upgrade command so that this can be done
in a single step. 
The package manager creates a centralized database to keep track of installed
applications. This database is also a valuable source of information on the state ofyour system. You can list the applications currently installed on your computer, forexample, or you can verify that a particular application has not been tampered withsince installation. Sometimes, just browsing the database can be an educationalexperience, as you discover software you didn’t know you had.
T wo of the most common package formats are RPM (RPM Package Manager
2)
and the Debian Package format. Some additional examples are listed in Table 1-8.As you might guess, RPM is used on Red Hat and Fedora distributions, but also onSuse and others. Likewise, the Debian format is used on the Debian distributionand also on several popular distributions (Knoppix, Ubuntu, and others). Otherpackage managers include 
pkgtool , which is used by the Slackware distribution,
and portage , which is used by the Gentoo distribution. 
The decision about which package manager to use is not yours to make (unless
you want to create your own distribution). Each Linux distribution chooses a sin-gle tool to manage the installed software. It makes no sense to have two packagemanagers in your system. If you don’t like the package manager your distributionuses, you would be well advised to choose a different distribution rather than try toconvert to a different package manager.1.5 Know Your Package Manager 13
2. Formerly the Red Hat Package Manager.
TABLE 1-8 Some Popular Linux Distributions and the Package Formats They Use
Distribution Package Format
Red Hat RPMFedora RPMDebian DebKnoppix DebUbuntu DebGentoo portageXandros DebMandriva (formerly Mandrake) RPMmepis Deb
Slackware pkgtool
When you’ve identified the format you want to download, there usually is one
more choice. Because this is open source we’re talking about, after all, it only makessense that you have the choice of downloading the source.
1.5.1 Choosing Source or Binary
If you are running Linux on an Intel-compatible 32-bit processor, you are likely tohave the opportunity to download software in the form of precompiled binaries.Most often, binaries are available in a package format and less frequently in 
tar
archive format. If you choose to download and install software from precompiledbinaries, you won’t need to touch any source code unless you want to. 
If you are running Linux on anything other than an Intel-compatible processor,
your only choice may be to download the source and build it yourself. On occasion,you may want to build from source even when a compatible binary is available.Developers deliberately generate binaries for the most compatible architecture toreach the widest audience possible. If you are using the latest and greatest CPU, youmay want to recompile the package to target your machine instead of using an oldercompatible architecture, which may not run as fast.14 Chapter 1 • Downloading and Installing Open Source Tools
Using Intel as an example, you are likely to find plenty of binary executables for
the i386 architecture. The i386 refers to the 80386, which is the lowest commondenominator of 32-bit Intel architectures. These days, when a package has beenlabeled i386, it more likely means Pentium or later. Many packages use the i586label, which more specifically refers to the Pentium processor. Nevertheless, codecompiled and optimized for a Pentium won’t necessarily be optimal on a Pentium4 or Xeon.
Whether or not you will see a performance increase by compiling for a newer
processor depends on the application. There is no guarantee that the performanceadvantages of compiling for a newer processor will be perceptible in every application.
Table 1-9 lists some of the most common architecture names used for RPM
packages. Although these labels are in many cases identical to the labels used by theGNU compiler, don’t assume that they are the same. The label is often arbitrarilychosen by the packager and may not reflect the actual build contents. Most often,you will see packages labeled as i386 when they actually are compiled for Pentiumor Pentium II. Because few people actually run Linux on an 80386 these days, noone complains.
TABLE 1-9 Overview of Architectures
Label Description
i386 Most common architecture you will find, although in gcc, i386
refers specifically to 80386. When you see this in a package,you should assume that it requires at least a Pentium I CPU.
i486 Not very common. It probably is safe to assume that a packagelabeled with this architecture is compatible with an 80486 (orcompatible).
i586 Becoming more common. The GNU compiler uses i586 todescribe the Pentium I CPU. Expect this to work on anyPentium or later processor.
i686 The GNU compiler uses i686 to describe the Pentium ProCPU, which served as the basis for the Pentium II and laterprocessors. Assume that this requires a Pentium II or later CPU.
ix86 Not very common, but it should be safe to assume Pentium orbetter.1.5 Know Your Package Manager 15
continues
TABLE 1-9 Continued
Label Description
x86_64 AMD Opteron and Intel Pentium 4 with EM64T extensions.
These are the latest processors that have both 32-bit and 64-bitcapability. This code is compiled to run in 64-bit mode, whichmeans that it will not be compatible with a 32-bit processor;neither will it run on an Opteron or EM64T CPU running a32-bit Linux kernel.
IA64 This refers specifically to the 64-bit Itanium processor. This is a one-of-a-kind architecture from Intel and Hewlett-Packard,found only on very expensive workstations and supercomputers.
ppc PowerPC G2, G3, and G4 processors found in some AppleMacintosh and Apple iMac computers.
ppc64 PowerPC G5, found in the Apple iMac.
sparc SPARC processor, used in Sun workstations.
sparc64 64-bit SPARC processor, used in Sun workstations.
mipseb MIPS processor, most often found in SGI workstations.
Building from source is not necessarily difficult. For a relatively simple project,
such as a text-based utility, building from source can be easy. For more complexprojects, such as a Web browser or word processor, it can be a real headache.Generally speaking, the larger the project, the more supporting developmentlibraries are required. Large GUI projects, for example, typically rely on several dif-ferent development libraries, which most likely are not installed on your system.T racking down the right versions of all these packages can be a time-consumingexercise in futility. In Chapter 2, I discuss more about how to build projects fromsource. In general, when looking for software, you will want to take the binary whenyou can get it.
1.5.2 Working with Packages
Many newer Linux distributions try to make things easy on the user, so it is possi-ble to run Linux without being aware that there is a package manager behind thescenes. Nevertheless, it makes sense to understand how these tools work if you plan16 Chapter 1 • Downloading and Installing Open Source Tools
to venture outside the sandbox provided by your distribution. Knowing your way
around the package management tools is very useful when things go wrong. Thebasic features that you can expect to find in a package manager include
• Installs new software on your system
• Removes (or uninstalls) software on your system• Verifies—makes sure installed files have not been corrupted or tampered with• Upgrades the installed versions of software on your system• Queries the software installed (for example, “What package installed this file?”)• Extracts—inspects the contents of a package before you install
1.6 Some Words about Security and Packages
Just about every computer user has had some firsthand experience with malware
(malicious software). If you are a Linux user, surely you’ve received some strange e-mails from your friends using Windows, the result of the latest Microsoft virusspreading across the Internet.
Malware includes more than just e-mail viruses, though: It also includes just
about any software that is introduced to your system and executes without yourconsent. It includes viruses, spyware, and any other destructive software that findsits way into your system. Microsoft Windows defenders argue that Windows is tar-geted by malware writers because there are many more Windows machines outthere than Linux machines. Although that may be true, it’s also true that Windowsis simply a much easier target. A key vulnerability in Windows 98, Windows ME,and the “home” version of Windows XP , for example, is that any user can touch anyfile and make systemwide changes. So just by clicking an e-mail attachment, anunseasoned Windows user can turn the computer into part of a zombie horde in adistributed denial-of-service attack or just delete the contents of drive C.
The motives for malware are varied and range from organized crime to revenge
to just plain vandalism. Don’t assume that you are not a target. 
Linux users tend to think that their systems are immune to malware, but that is
not true. The JBellz MP3, for example, was a T rojan horse that exploited a vulnera-bility in the 
mpg123 program—an open source MP3 player for Linux. In this case,
the malware wasn’t even an executable file but a music file in MP3 format. When the1.6 Some Words about Security and Packages 17
user tried to play this file in a different program, it appeared as though the file had
been corrupted and would not play. In actuality, it was a clever piece of malware thattargeted a specific vulnerability in the 
mpg123 program. The mpg123 program con-
tained a buffer overflow such that a corrupted MP3 file could contain arbitrary scriptcode that would then be executed. In this particular instance, the author decided thatit would be clever to delete the contents of the user’s home directory. 
Although your Linux machine is not likely to spread viruses on the scale of
Microsoft Windows, there are still vulnerabilities. It’s basically impossible to seesomething like the JBellz T rojan coming, so the only thing you can do is pay atten-tion to security alerts and take them seriously. In the case of JBellz, the damage wasrestricted to a single user, but at least the system was not compromised, although itcould have been if that user had been the root user.
There have been other instances of malware creeping into the source code of
widely used packages. One such instance was the OpenSSH source code.
3In this
instance, the OpenSSH source code was compromised on the host site, created aback door for someone to get in, using the privileges of the person who compiledthe source. So if you downloaded the compromised source for OpenSSH, built it,and installed it, a back door would be open that would allow an intruder to executecode with your privileges.
Linux has no equivalent of an antivirus program to scan programs for viruses;
instead, it relies on trust and authentication. It’s the difference between being proac-tive and reactive to virus threats. The Windows paradigm is decidedly reactive, butthere is still a good deal of trust involved. When you download a Windows pro-gram, you trust that your virus definitions are up to date and that you are not oneof the first people to encounter a new virus. By contrast, the Linux approach is to“trust but verify,” using authentication and trusted third parties. (I discuss authen-tication in the next section.)
In the interest of good security, Linux does not allow unprivileged users to install
system software. When you install software on your system, you must do so withroot privileges, which means that you log in as root or use a program like 
sudo . This
is when your system is vulnerable, because most programs require scripts to executeduring installation and removal. Whether you realize it or not, you are placing agreat deal of trust in the package provider that the scripts will not compromise thesecurity of your system. Authenticating the package author is a key step in makingsure the software is legitimate.18 Chapter 1 • Downloading and Installing Open Source Tools
3. Refer to http://www.cert.org/advisories/CA-2002-24.html
1.6.1 The Need for Authentication
With security, it’s not just what you run, but also when you run it. A Linux user can
create any kind of malicious program he wants, but without superuser privileges, hecan’t compromise the whole computer. So it’s important to know that all packageformats allow package files to contain scripts that execute during package installa-tion and removal. These typically are Bourne shell scripts that execute with rootprivileges and, therefore, can do absolutely anything. Such scripts are potential hid-ing places for malware in a T rojan-horse package, which is one reason why youshould always authenticate software before you install it, not just before you run it.
You should always be reluctant to use any tool that requires superuser privileges,
but the package manager is one exception. The package database is the central hubof a typical distribution, and as such, it is accessible only with root privileges. Thereare several ways to authenticate a package. The 
rpmtool, for example, has authenti-
cation features built in. For others, such as Debian, authentication is a separate step.
1.6.2 Basic Package Authentication
The most basic form of package authentication involves using a hashing function.The idea is similar to a checksum, which tries to identify a set of data uniquely byusing the sum of all the bytes in the data. A simple checksum of all the bytes in thefile is not enough to guarantee security, however. Many different datasets can havethe same checksum, and it is easy to manipulate the data while preserving thechecksum. Simple checksums are never used to authenticate packages, because sucha signature is easy to forge.
The value produced by a hashing function is called a hash. Like a checksum, the
hash characterizes an arbitrarily large dataset with a single fixed-length piece of data.Unlike a checksum, the hash output is very unpredictable, making it extremely dif-ficult to modify the data and produce the same hash. Most hashing algorithms usea large key (for example, 128 bits), so the probability of producing the same hashtwice is so remote that it isn’t worth trying. If you download a file from an unknownsource but have the hash from a trusted source, you can have faith in two things:
• The chance of getting a modified file that has the same hash is extremely
remote.
• The chance that a malicious programmer can take that file, modify it accord-
ing to his wishes, and produce the same hash is infinitesimal.1.6 Some Words about Security and Packages 19
One popular tool for creating hashes is md5sum , based on the MD5 algorithm,4
which produces a 128-bit hash. The md5sum program can generate hashes or verify
them. You generate the hashes by specifying the names of files on the commandline. Then the program produces one line for each file containing the resulting hashand the filename:
$ md5sum foo.tar bar.tar
af8e7b3117b93df1ef2ad8336976574f *foo.tar2b1999f965e4abba2811d4e99e879f04 *bar.tar
You can use the same data as input to the md5sum program to verify the hashes:
$ md5sum foo.tar bar.tar > md5.sums$ md5sum --check md5.sumsfoo.tar: OKbar.tar: OK
Each hash is represented with a 32-digit hexadecimal number. (For you
nongeeks, each hexadecimal digit is 4 bits.) You can check this hash against a valueposted by a trusted source to see whether your data is correct. If the MD5 hashmatches, you can be virtually certain that the file is unchanged from the time whenthe MD5 hash was created. The only catch is whether you can trust the MD5 hashthat you checked it against. Ideally, you should get the MD5 sum from a trustedsite that is not the same place from where you downloaded the package. If the fileyou downloaded is a T rojan from a compromised site, you can rest assured that anyMD5 hashes from this site have also been modified.
Suppose that you want to download the latest and greatest version of OpenOffice
from OpenOffice.org. The official site will refer you to one of many mirrors, so howdo you know that the site you were referred to has not been compromised and thatthe package you download has not been replaced with a T rojan-horse package? Thetrick is to go back to the official site and look for the MD5 sum for the file. In thisexample, the OpenOffice.org site posts the MD5 sums for all the files available fordownload, so after you download the file, you can verify it against the MD5 hashposted on the OpenOffice.org site. Sometimes, you can download a file for inputto the 
md5sum program, or you must make one yourself by cutting and pasting from20 Chapter 1 • Downloading and Installing Open Source Tools
4.MD5 stands for Message Digest algorithm number 5 .
your Web browser. In this example, I cut and pasted the sum for the file I down-
loaded into a file called md5.sum , as follows:
cf2d0beb6cae98acae81e4d690d63094  Ooo_1.1.4_LinuxIntel_install.tar.gz
Note that the md5sum program is a little picky about white space. It expects no
white space before the hash and exactly two spaces between the hash and the file-name. Anything else will cause it to complain.
When you have your md5.sum file, you can check the file you downloaded as
follows:
$ md5sum --check md5.sum
OOo_1.1.4_LinuxIntel_install.tar.gz: OK
As you can see in the example above, the program prints an unambiguous mes-
sage indicating that the MD5 sum is correct. This means that the file you down-loaded from an unfamiliar mirror matches the MD5 sum that was posted on theOpenOffice.org site. Now you can rest assured that you have authenticated yourcopy of the file (provided that the OpenOffice.org site hasn’t been compromised).
1.6.3 Package Authentication with Digital Signatures
A digital signature is another kind of hash like MD5, except that you do not needto know anything unique about the data you want to authenticate. All you need toauthenticate a digital signature is a single public key from the person or organiza-tion you are trying to authenticate. When you have the public key, you can authen-ticate any data signed by that person. So even though each signature produced bythat person is a unique hash, you need only a single public key to verify the authen-ticity of any data signed by that person.
The person who wants to sign data produces a pair of keys: one public and one
private. The keys are based on a passphrase that only the originator knows. Theoriginator keeps the private key and the passphrase secret, while the public key ismade available to whoever wants it. If either the private key or the data changes,authentication will fail. The odds of creating a valid signature with the same publickey but a different passphrase and private key are extremely remote. The chance ofbeing able to forge a signature for a legitimate public key is infinitesimal.1.6 Some Words about Security and Packages 21
This method is based on trust. You trust that certain individuals and organiza-
tions will not sign data that is infected with malware. You trust that they will takeadequate measures to keep their private keys and passphrases secret. You also haveto trust the sources of the public keys you use. Given that your trust is well placed,you can rest assured of the authenticity of data you validate with the public keys.
The tool most often used for digital signatures of Open Source code is GNU
Privacy Guard (GPG). The process of signing data with GPG is depicted inFigure 1-1.
1.6.4 GPG Signatures with RPM
The RPM package format allows the option of a GPG signature for authentication.The RPM format also uses other hashes, including MD5, for each file in the RPM.These hashes can be used to verify that the RPM hasn’t been corrupted duringtransfer and to confirm that the files haven’t been tampered with since installation,but they do not provide any authentication. Only the GPG signature authenticatesthe RPM. Alternatively, you could authenticate the RPM manually, using the MD5hash for the package file provided by a trusted source.22 Chapter 1 • Downloading and Installing Open Source Tools
FIGURE 1-1 GPG Signing ProcessSigned
DataPublic
Key
Private
KeyValid
Signature?
Yes or NoData GPG GPGProvider Co nsumer
The rpmutility has a --checksig flag, which unfortunately lumps all the hashes
together in one line. So if a file does not have a GPG signature, rpmwill still report
that it has a good MD5 sum. If it does have a GPG signature, it simply includes anadditional 
gpgok in the output line. Consider the following output:
$ rpm –checksig *.rpm
abiword-2.2.7-1.fc3.i386.rpm: sha1 md5 OKabiword-plugins-impexp-2.2.7-1.fc3.i386.rpm: sha1 md5 OK
abiword-plugins-tools-2.2.7-1.fc2.i386.rpm: sha1 md5 OK
firefox-1.0-2.fc3.i386.rpm: (sha1) dsa sha1 md5 gpg OK
dpkg-1.10.21-1mdk.i586.rpm: (SHA1) DSA sha1 md5 (GPG) NOT OK (MISSING KEYS:GPG#26752624)
Notice that five RPM package files are listed, but only the firefox and dpkg
packages have a GPG signature. I have a public key for firefox but not for dpkg .
Therefore, firefox is the only package in this group that can be considered
authenticated, but rpmdoesn’t highlight that fact in any way. Too bad. The dpkg
RPM is signed with a GPG signature, but I don’t have a public key for it. So eventhough it is signed, I don’t have any way to know whether the signature is valid. Inthis case, at least 
rpmdoes produce a more ominous warning.
The signature for the firefox package above was recognized because it was
signed by Red Hat and I ran it on a Fedora installation. The Fedora distributionincludes several public keys that are used by Red Hat to sign the packages that itmakes available. These are copied to the hard drive when you install the distribu-tion. If you download an RPM, and the signature can be verified using one of thesekeys, you can be assured that the package is the same one that is provided by RedHat and not infected with any malware. The public key for the dpkg RPM was notfound because the RPM came from a Mandrake distribution, so the public key isnot available on my Fedora installation. I’ll have to track that key down myself.
If you download a package that has an invalid signature or requires an unknown
public key, like the 
dpkg package above, rpmwill warn you when you install it.
Unfortunately, even with an unauthenticated signature, rpmversion 4.3.2 lets you
install the package anyway without any challenge. This is unfortunate, becauseyour system is vulnerable during the install process when you are running scriptsas root. GPG does not try to distinguish between a forged key and a missing pub-lic key. It cannot. Just as with real signatures, two people can have the same namebut don’t have the same signature, but that doesn’t make one of them a forger.Likewise, two people with the same name will have unique public keys. The onlything GPG will tell you when it can’t authenticate a signature is that it does nothave a public key for it.1.6 Some Words about Security and Packages 23
24 Chapter 1 • Downloading and Installing Open Source Tools
Tracking Down a Missing Public Key
There are several resources on the Web for tracking down GPG public keys, but usu-
ally, it’s best to go to the source. In this example, I am missing a public key from theMandrake distribution. A simple query confirms this:
$ rpm -qip dpkg-1.10.21-1mdk.i586.rpm
Name        : dpkg                         Version     : 1.10.21                  Vendor: MandrakesoftRelease     : 1mdk                     Build Date: Thu May 20 07:03:20 2004
Host: n1.mandrakesoft.com
Packager    : Michael Scherer <misc@mandrake.org>URL         : http://packages.debian.org/unstable/base/dpkg.htmlSummary     : Package maintenance system for Debian 
There are a few clues here for finding a trusted public key. The URLs mandrake-
soft.com and mandrake.org are good starting points. The package shows that it wascreated in 2004, which is a long time ago in Internet years. Since Mandrake becameMandriva, these sites have been taken offline. This is not going to be easy. I need onemore piece of information to know what I am looking for: the key ID.
$ rpm --checksig dpkg-1.10.21-1mdk.i586.rpm
dpkg-1.10.21-1mdk.i586.rpm: ... 
... (GPG) NOT OK (MISSING KEYS: GPG 78d019f5 )
This shows me the ID that I am looking for.
The next stop is the Mandriva Web page. A Google search takes me to
http://mandriva.com. A site search for public keys shows me some hashes and a sin-
gle public key, which is not the one I want. A few more searches turn up no leads. Itlooks as though Mandriva’s Web page is a dead end.
Next, I try a Google search for public keys , and this turns up several sites that main-
tain public keys. After some trial and error, I finally find the key at www.keys.pgp.netby doing a key search for 0x78d019f5
,which turns up the missing key.
Search results for '0x78d019f5'
Type bits/keyID    cr. time   exp time   key expirpub  1024D/78D019F5 2003-12-10            uid MandrakeContrib <cooker@linux-mandrake.com>
sig  sig3  78D019F5 2003-12-10 __________ __________ [selfsig]sig  sig3  70771FF3 2003-12-10 __________ __________ Mandrake Linux<mandrake@mandrakesoft.com>
Be wary of any package that is signed but has a signature that your system doesn’t
recognize (that is, it doesn’t have a public key for it). If you need to search for a pub-lic key, get it from a different site from the one where you got the RPM—one thatis not referenced by that site. You shouldn’t trust any keys from a site that theprovider points you to, because those sites could be shams. This is common sense.You wouldn’t hire a contractor just because he has a license hanging on the wall; youneed to check his credentials for yourself. Likewise, you shouldn’t trust his refer-ences, either; for all you know, they’re his partners and family members.
1.6.5 When You Can’t Authenticate a Package
Users should think twice before installing any package that isn’t authenticated, butI’m willing to admit that I’m not religious about this when it comes to installing1.6 Some Words about Security and Packages 25
sig  sig3  26752624 2003-12-10 __________ __________ MandrakeCooker <cooker@linux-
mandrake.com>sig  sig3  45D5857E 2004-09-22 __________ __________ Fabio Pasquarelli (Lavorro)<fabiopasquarelli@tin.it>sig  sig3  17A0F9A0 2004-09-22 __________ __________ Fafo (Personale)<rec.r96@tin.it>
sub  1024g/4EE127FA 2003-12-10            
sig sbind  78D019F5 2003-12-10 __________ __________ []
A search for Mandrake also turns up this key in a list of several other Mandrake
keys. Clicking the hyperlinked 78D019F5 takes me to the PGP key, which is plaintext. To import this key, I must save this to a file named 78D019F5.txt, and I canimport it to the RPM keyring as follows:
rpm --import 78D019F5.txt
If the text contains a valid public key, I should see no errors. Finally, I can check
the validity of the original package as follows:
rpm --checksig dpkg-1.10.21-1mdk.i586.rpm
dpkg-1.10.21-1mdk.i586.rpm: (sha1) dsa sha1 md5 gpg OK
The understated “gpg OK” is what I’m looking for.
Before I leave this topic, I’ll discuss trust. I got the signature from the pgp.net
domain. I am trusting that the proprietors of this database have taken reasonable pre-cautions to verify that the public key came from a legitimate source. In the end, it all
comes down to trust.
packages on my old clunker hobby PC. It’s a different story when you are deploy-
ing packages across a large enterprise. With my clunker, there’s not much to lose ifI get burned, but in a large enterprise, the result could be disastrous.
Sometimes, the author doesn’t provide any authentication information. The odds
of getting a T rojan Linux package these days are fairly low, but that can change.Here are some practical steps you can take when you can’t authenticate a packagewith a signature:
• Build from source.
This is not foolproof, as mentioned in the discussion of the OpenSSH inci-
dent earlier in this chapter. Nevertheless, understand that building from sourcemay be easy, or it could be difficult. Every project is different, and you won’tknow until you try. The initial download usually is small enough, but giveyourself a deadline, after which you will resort to other means. Simply tryingto build an unfamiliar project can become a time vacuum as you search for allthe required development packages. I discuss building from source in detail inChapter 2.
• Inspect the install scripts.
These are the scripts that pose the most immediate danger, because they run
with root privileges when you install the package, before you ever run any ofthe installed software. I’ll discuss how to do this for each package format.
• Inspect the contents.
Look at the binary files that are being installed. A typical user application
should not need binaries in /usr/sbin or /sbin, because these directories arereserved for system daemons and system administration tools. Be wary of anyfiles with the setuid or setgid bit set, especially if they are owned by root. Thesefiles can execute with the permissions of the file owner, which may be a hid-ing place for malware. Only a few system programs need this capability; any-thing else is suspicious.
Whatever technique you choose, keep in mind that this is still a matter of trust.
Instead of trusting an authenticated source, you are trusting your skills at identify-ing malware by inspection.26 Chapter 1 • Downloading and Installing Open Source Tools
1.7 Inspecting Package Contents
There are a few basic things you may want to inspect in any package you download
before installing it. Most package formats consist of the following key pieces:
• An archive of files that will be installed on your system. This may be in tar
format, cpio format, or something else.
• Scripts that will execute during the installation and removal of the package.• Dependency information for the install tool to determine whether your system
meets the requirements for this package.
• Some textual information about the package itself.
The amount of descriptive information in a package is largely up to the person
packaging the file. Typically, it will include some basic information about theauthor, the date of packaging, and the licensing terms. A thoughtful packager willinclude some information about what the software actually does, but too often, thisis not the case.
Package dependencies may be extensive, or they may be sparse or nonexistent.
Packages for Slackware-based distributions, for example, don’t have any dependencyinformation in them. You install the package and cross your fingers as to whetherit’s going to work. RPM is at the other extreme. When you’re building an RPMpackage, the tools can automatically detect dependencies that will be listed in thepackage. The packager can also opt to specify exact dependencies or to specify nodependencies at all (à la Slackware).
Each package format provides some method of running scripts at installation and
removal time. Installation scripts should be scrutinized closely. Even if you don’t sus-pect malware, an immature project may contain some defective install scripts thatcould damage your system. If the script is too complex for you to understand, I rec-ommend that you find some way to authenticate the package before installing it.
Installation scripts usually are broken down into these categories:
•Preinstall —This script is run before any data is unpacked from the archive.
•Postinstall —This script is run after the data is unpacked from the archive.
Typically, these scripts will do minor tasks to customize the installation, suchas patching or creating configuration files.1.7 Inspecting Package Contents 27
•Preuninstall —This script will run when you choose to remove the package
but before any files are removed from the system.
•Postuninstall —This script runs after the primary files have been removed
from the system.
The textual information that comes with a package varies from one format to the
next. Often, it contains additional authenticating information, such as a projectpage on SourceForge.net. Dependency information contained in the package alsovaries from one package format to the next. This may be names of other packagesor names of executable programs that the package requires.
1.7.1 How to Inspect Packages
You probably will need to inspect a package both before and after it is installed.Before it is installed, you will be inspecting a package file, which may have anylegitimate Linux filename. The filename usually, but not always, is derived fromthe official package name—that is, the name that will show up in the package data-
base after it is installed. The package name is encoded inside the package file andshould be visible with a basic package file query. Although package creators arecareful to include the package name as part of the filename, don’t assume that thefilename and the package name are the same. After the package is installed, it canbe referred to only by the name specified inside the package file, so querying apackage file for its official name is a good first query. You may be installing a 
gcc
compiler RPM, for example, but for some reason, the file was named foo.rpm .
You can query the contents of this RPM file with
$ rpm -qip foo.rpm
but after you install it, the same query becomes
$ rpm -qi gcc
The rpmcommand normally queries the RPM database, but the -poption spec-
ifies that a package fileis the target of the query. The same package can have any
filename, but when it’s installed in the database, it has only one name.
As mentioned, the basic information contained in a package includes things such
as its name, version, author, copyright, and dependencies. Additional informationincludes the list of files to be installed, as well as any scripts that will run during installand removal. You usually are interested in these things before you install the package.Table 1-10 shows a list of basic queries for both RPM and Debian package files.28 Chapter 1 • Downloading and Installing Open Source Tools
There are several reasons why you might want to query the package database. You
may want to list all the packages that are installed in the system, for example, or youmay want to know what version of a particular package is installed. A useful thingto do is verify the contents of an installed package to make sure that none of thefiles has been tampered with since installation. The query format changes slightlyon installed packages versus package files. Some examples are shown in Table 1-11.
TABLE 1-10 Queries on Package Files
Query RPM Debian
Basic information rpm -qpi filename dpkg -s filename
List of files to be installed rpm -qpl filename dpkg -L filename
dpkg -e
not available
dpkg -I
dpkg -I
TABLE 1-11 Queries of Installed Packages
Query RPM Debian
rpm -qi name dpkg -s name
rpm –qa dpkg --list
rpm -ql name dpkg -L name List all files installed by a particular
packageList all the packages installedBasic information about a particularpackagerpm -qp –-provides filename Show what packagethis file provides (forexample, the name andversion as they willappear in the database)rpm -qp –-requires filename Show what otherpackages this packagerequiresrpm --checksig filename Verify authenticationinformationrpm -qp -scripts filename Dump install/uninstallscripts1.7 Inspecting Package Contents 29
continues
TABLE 1-11 Continued
Query RPM Debian
rpm -V name cd /;
md5sum -c </var/lib/dpkg/info/name.md5sums
rpm -qf
filename dpkg -S filename
rpm -q X dpkg-query -W X
1.7.2 A Closer Look at RPM Packages
RPM is one of the most comprehensive package formats you are likely to find in
Linux. An RPM package can contain a great deal of information, but it can be dif-ficult to extract. To help you get at this additional data, the 
rpmtool comes with a
--queryformat option (--qf for short). Most of the tags used by the --qf option
are not documented in the manual, but you can get a list of them by typing
$ rpm –-querytags
HEADERIMAGEHEADERSIGNATURESHEADERIMMUTABLEHEADERREGIONSHEADERI18NTABLESIGSIZESIGPGPSIGMD5SIGGPGPUBKEYS...
Note that query tags are case insensitive, although the output from rpmlists them
all in uppercase. If you want to get an idea of who provides the packages for yourdistribution, for example, try this query:
$ rpm -qa --qf '%{vendor}' | sort | uniq -c
1 Adobe Systems, Incorporated
12 (none)
1 RealNetworks, Inc
838 Red Hat, Inc.
1 Sun Microsystems, Inc.What version of
package X is currentlyinstalled?Which package doesthis file belong to?Verify files installedby a particularpackage30 Chapter 1 • Downloading and Installing Open Source Tools
This query on my Fedora Core 3 system shows that 838 packages are provided
by Red Hat, and 12 are provided by unidentified sources. It turns out that theunidentified packages are actually GPG public keys. Each public key shows up as aseparate package in the database, and typically, these have no “vendor” ID. 
Another useful query is to check the install scripts that come with an RPM pack-
age. An example is shown below:
$ rpm -qp --scripts gawk-3.1.3-9.i386.rpm
postinstall scriptlet (through /bin/sh):
if [ -f /usr/share/info/gawk.info.gz ]; then
/sbin/install-info /usr/share/info/gawk.info.gz
/usr/share/info/dirfipreuninstall scriptlet (through /bin/sh):if [ $1 = 0 -a -f /usr/share/info/gawk.info.gz ]; then
/sbin/install-info --delete /usr/share/info/gawk.info.gz 
/usr/share/info/dirfi
The output includes a single line identifying the purpose of the script (post-
install, etc.) and the type of script (for example, /bin/sh ). This allows you to
inspect the scripts visually before they execute.
You can get at the contents of the archive file in an RPM by using a command
called rpm2cpio . This converts any RPM package file you give it to a cpio archive,
which is what the RPM format uses internally. cpio is an archive format like tar
with a slightly different syntax. The output of rpm2cpio goes to stdout by default.
This is how cpio normally works, unlike tar. To extract the files in an RPM to the
current directory without installing the package, use the following command:
rpm2cpio filename.rpm | cpio -i –-no-absolute-filenames
Notice that I use the --no-absolute-filenames option to cpio to ensure that
I don’t clobber any valuable system files. In fact, RPM packages don’t allow absolutefilenames in the 
cpio archive. In any case, you can never be too safe.
1.7.3 A Closer Look at Debian Packages
Debian packages have a simpler format than RPM, and the dpkg tool lacks many
of the features that the rpmutility has. As a result, this requires some more effort on
your part to inspect these packages. A Debian package filename typically has a .deb
extension, although it is actually an archive created with the arprogram. You can1.7 Inspecting Package Contents 31
inspect the contents of a Debian package with the arcommand, but that doesn’t
tell you much. For example:
$ ar -t cron_3.0pl1-72_i386.deb
debian-binarycontrol.tar.gzdata.tar.gz
The file named debian-binary contains a single line of ASCII text indicating
the version of the format used for the package. The file named control.tar.gz is
a compressed tar archive that contains the install scripts as well as some other use-ful information. The file named 
data.tar.gz file is a compressed tar archive that
contains the program install files. To extract these files for further inspection, usethe 
arcommand:
$ ar -x filename.deb
Now let’s look at some more details from the sample file above. The file
data.tar.gz contains the files required for the program to operate. Sometimes,
you can just extract these files and have a working installation, but I don’t recom-mend it. In this example, the list looks like the following:
$ tar -tzf data.tar.gz
././usr/./usr/bin/./usr/bin/crontab./usr/sbin/./usr/sbin/cron./usr/sbin/checksecurity./usr/share/./usr/share/man/./usr/share/man/man1/...
The control.tar.gz file contains more files required for package installation,
removal, and maintenance. You can extract these files by using the dpkg command
with the -eoption, for example:
$ dpkg -e cron_3.0pl1-72_i386.deb
$ ls ./DEBIAN/*./DEBIAN/conffiles./DEBIAN/control./DEBIAN/md5sums./DEBIAN/postinst32 Chapter 1 • Downloading and Installing Open Source Tools
./DEBIAN/postrm
./DEBIAN/preinst./DEBIAN/prerm
As you might have guessed, the preinst and postinst files are the preinstall
and postinstall scripts described earlier in this chapter. Likewise, the prerm and
postrm files are the preuninstall and postuninstall scripts, respectively.
The md5sums file contains the list of MD5 hashes that can be used to check the
integrity of the files in data.tar.gz . This file can be used as input to the md5sum
program, but these hashes are verification only—not authentication. You can use the
md5sums file to verify that the package has not been corrupted before you install it
and to verify that the installed files were not tampered with after installation, but ittells you nothing about the authenticity of the source of the files. Nevertheless, peri-odically verifying the contents of an installed package via its 
md5sums is a good idea.
The md5sums file does not include all the files that are installed, because often, a
package requires configuration files that are intended to be modified after installa-tion. It is expected that these files will not match the original contents after instal-
lation. Such files are excluded from checking by listing them in 
conffiles . Any
file listed in conffiles is ignored when the integrity of the installation is checked.
1.8 Keeping Packages up to Date
A package updater helps take some of the work out of tracking down package filesand their dependencies one by one. Suppose that you want to install package 
X, but
it requires three other packages that you don’t have installed. You will have to installthese three packages before you can install package 
X. But it’s also possible that these
packages require other packages you don’t have, and those in turn could require oth-ers, and so on, and so on.
This is where package updaters come in handy. With a package updater, you sim-
ply request package 
X; the tool determines what other packages are required to
install the package and then downloads and installs those as well. 
The package updater works by keeping a list of package repositories that it can
search when you request a package. Typically, these repositories reside on theInternet and are maintained by the distributor (for example, Red Hat). The repos-itory is a distributor’s way of making fixes and security updates available, but theyusually include general updates as well. 
A repository can also reside on a local file system, such as a CD or another com-
puter located inside your firewall, which is useful if you have to maintain many1.8 Keeping Packages up to Date 33
machines on a LAN. You can retrieve required packages from the Internet and make
them available internally for faster updating of your client machines, for example.
For Debian-based distributions, the tool of choice is Apt, which stands for
Advanced Package Tool. This is actually a set of command-line tools used to main-
tain the packages in your distribution. Apt has been ported for use on RPM-baseddistributions. It remains to be seen whether Apt will become the preferred tool forpackage management for RPM users.
For RPM-based distributions, two major tools are worth mentioning. The first is
up2date , which is designed by Red Hat for its Enterprise Server and Fedora Core
distributions. The other is yum, which stands for Yellowdog Updater Modified.5
Some claim that Apt and yumcan upgrade an entire installation—for example,
take it from Red Hat 8.0 to Red Hat 9.0 without having to reinstall the OS. I wouldbe extremely hesitant before trying this myself.
6
Don’t expect the package updater to do everything you need. Because package
updaters rely on a select few repositories to search for packages, you can expect yourchoices of software and versions to be somewhat limited. Official repositories tendto favor established tools with stable versions. They may have a bias for particulartools or versions, based on the distributions they support or on the whims of therepository’s maintainers. Don’t believe anyone who says, “If it’s not in my reposi-tory, you don’t need it.” There is a lot of good work going on that is not part of adistribution or repository. If you want to work with the latest bleeding-edge versionof a package, or to try something new or unusual, you probably will have to bypassthe package updater.
If you do some searching, you are likely to find bug reports and complaints about
every package updater. T rying to keep hundreds of interdependent software pack-ages up to date and functioning is an extremely complicated task. Bugs are unavoid-able while developers gain more experience with the problem. The good news is thatthere is a great deal of activity in this area, so things can only get better with time.
1.8.1 Apt: Advanced Package Tool
Apt is one of the more mature tools for managing packages in Debian distributionsand is now available for RPM distributions. An excellent feature of Apt is that34 Chapter 1 • Downloading and Installing Open Source Tools
5. It was called yup when it was part of the Yellowdog distribution for PowerPC, but since then, it has been
adapted by other RPM-based distributions and modified.
6. Possible side effects include headaches, ear infections, anxiety, nausea, and vomiting.
unlike the basic dpkg tool, Apt will automatically authenticate packages signed with
GPG signatures. Remember that RPM already supports GPG signatures.Furthermore, Apt will check with you before installing a package that it cannotauthenticate, so you don’t need to worry about repositories being compromised andloaded with T rojan-horse packages.
Just like 
dpkg , Apt is not a single command but a set of commands. The two
most commonly used Apt commands are apt-get and apt-cache . To get started,
you probably will be interested in the apt-get command. This is the workhorse
that will retrieve and install packages for you. The apt-cache command allows you
to query the list of available packages downloaded to the local cache, which is fasterthan repeatedly querying repositories on the Internet. This list includes all availablepackages, including those you have not installed as well as updates to what you haveinstalled. 
The 
apt-key command allows you to add public signatures from a trusted
source to your database, as well as to inspect the ones you already have. This allowsyou to authenticate packages that are signed by that source. The 
apt-setup com-
mand allows you to specify the preferred repositories that Apt should search whenlooking for packages. On my Ubuntu distribution, 
aptsetup allows only Ubuntu
mirrors; Debian repositories are not allowed. In this case, you can still edit the
/etc/apt/sources.list file by hand to include more agnostic repositories.
/etc/apt/sources.list can point to sites on the Internet or to local directories
on your system or on your LAN. The only thing Apt requires is that the files beavailable via a URL.
1.8.2 Yum: Yellowdog Updater Modified
Yum currently is the tool of choice for RPM-based systems. It is a command-lineutility that functions much like Apt. Like Apt, Yum keeps a cache containing infor-mation about available packages. Unlike Apt, yum queries each repository every timeit runs by default. This is much more time consuming than using a cache like Apt. 
The 
yumcommand is used to install, query, and update packages. The -Coption
tells yumto use the cache for the initial request. If yum decides to install software
based on the request, it will update the cache before doing so.
With Yum, authentication is optional via GPG signatures. This is controlled
for each repository via the configuration files in /etc/yum.conf and
/etc/yum.repos.d . If the flag gpgcheck=1 is set, the yum command will not1.8 Keeping Packages up to Date 35
install unauthenticated packages. Just as with Apt, you can create your own repos-
itory in a directory anywhere that can be accessed via a URL.
The options for the yumcommand are fairly intuitive. To show all the packages
currently installed that have updates available, for example, the command is
$ yum list updates
This produces a simple list of packages that are available for update. The closest
equivalent for Apt is the less intuitive apt-get --dry-run -u dist-upgrade ,
which produces a lot of cluttered additional output.
1.8.3 Synaptic: The GUI Front End for APT
Synaptic isn’t even at version 1.0 at this writing, yet it is an extremely useful GUIfor maintaining packages via Apt. On my Ubuntu machine, 861 packages areinstalled as I write this. At any given time, dozens of these are available for update.This is a task that cries out for a GUI. Synaptic groups packages by category so thatyou can easily locate and inspect updates for the software you use the most. As adeveloper, you probably want to know when gcc goes from version 3.3 to 3.4, butperhaps you don’t care that FreeCell has been updated from version 1.0.1 to version1.0.2. You can also use the categories to look for software that may be new or forsomething you just never installed. As a software developer, I regularly browse thedevelopment tools to see whether any cool new projects are available. You can seean example of Synaptic in action in Figure 1-2.
Like Apt, Synaptic will not install an unauthenticated package without your
explicit consent. One nice feature of Synaptic is that it is easy to see the potentialconsequences of my actions before having to endure a long wait while the tool down-loads dozens of packages I didn’t ask for. If I browse to the Games and Amusementssection, and select 
kasteroids for installation, there is a little problem: Ubuntu
uses Gnome by default, and kasteroids is a KDE application. So if I want to install
kasteroids , I will need to install ten more packages as well. Although Synaptic will
gladly install these packages for me, it warns me first that there are ten more pack-ages required to install, requiring several megabytes to download. So now I knowthat if I’m in a hurry, it’s probably not a good idea to install 
kasteroids right now.
Unfortunately, there is no way to identify which package updates include security
fixes. In general, how do you know whether you really want to upgrade gcalctool
from version 5.5.41-0 to 5.5.41-1? Intuitively, this looks like a minor change, maybea bug fix, but is it a security fix? Who knows? This is something that the open sourcecommunity should deal with eventually.36 Chapter 1 • Downloading and Installing Open Source Tools
Another useful feature is the filter, which allows you to query the volumes of
update data you are presented with so that you can find the updates you are inter-ested in. Synaptic is still early in development (version 0.56 at this writing), and thisfeature still needs work. Currently, there is no way to filter out major from minorchanges, for example. A minor change most likely includes bug fixes and securityfixes; major changes typically include new features.
Synaptic is still very useful and should be the GUI of choice for Debian-based
systems. It could become popular on RPM-based distributions as well. Currently,very few RPM repositories are compatible with Apt and Synaptic.
1.8.4 up2date: The Red Hat Package Updater
Red Hat provides the up2date GUI for use with Yum repositories. This tool can
operate from the command line as well. With no options, up2date presents a list
of files available for update, which can number in the hundreds, and asks you toselect which ones to update from this list.1.8 Keeping Packages up to Date 37
FIGURE 1-2 An Example of the Synaptic GUI

The default output does not include packages that you don’t have installed, so if
a new tool is available, up2date won’t tell you about it. Just like Synaptic, up2date
will authenticate packages via GPG signatures and won’t install packages that it can’tauthenticate.
The GUI for 
up2date leaves much to be desired. The function is minimal, in
that it only updates existing packages; it does not show you new ones, and it doesnot allow you to browse and uninstall currently installed packages. This is a shame,because on the command line, it is quite useful and intuitive.
up2date tries to be a chameleon by allowing access to Yum, Apt, and up2date
repositories. The default configuration that comes with the Fedora Core 4 distribu-tion directs you to a comprehensive list of Yum repositories. This seems like a goodidea, but it causes your updates to take longer—much longer. A better approach isto track down a few repositories yourself and add them directly to the file
/etc/sysconfig/rhn/sources . A nice feature of up2date is that you can also
point it to a directory full of RPMs and let it figure out all the messy dependencies.As long as your directory contains all the required RPMs, it works nicely. You canmount your installation DVD on 
/mnt/dvd and add the following line to
/etc/sysconfig/rhn/sources :
dir fc-dvd /mnt/dvd/Fedora/RPMS
Now you can install packages from your CD and let up2date worry about the
dependencies. From the command line, an example might look like this:
$ up2date --install gcc
I have not had any luck using the up2date GUI. I once thought I would try
up2date on my Fedora Core 3 system to update a handful of carefully selected
packages from the list of 200 or more that were available for update. I clicked Ok,and the GUI went to sleep to ponder my selections over my broadband connection.Unresponsive, and with no indication of any progress, it looked like the tool wasstuck. About 15 minutes later, the tool came back and told me that I had the wrongkernel for two of the packages; therefore, none of my selected packages would beupdated. Thank you; come again! 
One of the main reasons for the slowness appeared to be that
/etc/syscsonfig/rhn/sources pointed to the yum repositories located in38 Chapter 1 • Downloading and Installing Open Source Tools
/etc/yum.repos.d , which contained about six repository entries, each with a
mirror list. The mirror list, which resides on the repository host, appears to slowthe tool to a crawl. One list numbered 65 mirrors! It looks as though the GUI isusing this information very inefficiently, whereas specific command-line interac-tions don’t. So before you get discouraged using 
up2date , try the command line.
1.9 Summary
This chapter covered some of the basics of open source software. Specifically, Ilooked at the various distribution formats and tools to use them. I discussed atlength the archive file, which is at the core of every distribution format and in somecases isthe distribution format.
I reviewed some of the basic security measures that are available to ensure that
you do not download malware when you look for software. I discussed the basics ofauthentication and the common-sense measures you can take to protect yourself.
Finally, I looked at some of the tools that are built on top of the packaging tools.
These are tools to manage all the packages in the system. Each has advantages anddrawbacks.
1.9.1 Tools Used in This Chapter
•dpkg —the main tool used to install and query packages in the Debian pack-
age format used by the Debian distribution and its derivatives, such asUbuntu.
•
gpg—the GNU encryption and signing tool. This is a general-purpose tool
that is used with packages to enhance security with digital signatures.
•gzip , bzip2 —GNU compression utilities, most often used in conjunction
with archive files.
•rpm—the main tool for installing and querying software packaged in the Red
Hat Package Manager (RPM) format. In addition to Red Hat, RPM is usedby Suse and other distributions.
•
tar, cpio , ar—the UNIX archive tools that are at the core of most package
formats.1.9 Summary 39
1.9.2 Online References
• www.debian.org—the home page of the Debian distribution, including a FAQ
that discusses the packaging format, among other things
• www.gnupg.org—the home page of the GNU Privacy Guard project, which
created the gpgtool
• www.pgp.net—a repository of public keys used by gpgand others
• www.rpm.org—the home page of the RPM project40 Chapter 1 • Downloading and Installing Open Source Tools
2.1 Introduction
In this chapter, I discuss the tools used to build software as well as the things you
need to know to build software distributed in source code form. Despite its short-comings, the 
make program is still the core tool used to build Linux software. Every
developer has had at least some exposure to the make program, but I cover some
important details that not every developer knows.
I look at how GNU source is distributed with the GNU build tools, which are
used on many other open source projects. I also touch on some emerging buildenvironments that are emerging as alternatives to 
make .
I’ll look at some common errors and warnings you will encounter while working
with build tools.
2.2 Build Tools
Developing software is an iterative process. You edit source code, compile, run thecode, find some bugs, and start all over again. Although this is not an efficient
2
41Building from Source
process, your build tools should make it as efficient as possible. The make program
is the workhorse of the Linux build environment. It was the first tool to support theiterative build process used by software developers. Although many programmershate it, to date, there has been no compelling alternative to 
make . As a result, every
Linux programmer should be intimately familiar with it.
2.2.1 Background
In its original form, make is a rather primitive tool. The UNIX version of make has
no support for conditional constructs and does not support any language to speakof. 
make relies instead on a few simple expression types that control its interaction
with the shell. During the build, the shell does the real work. make is just a
supervisor.
The simplicity of make is both a feature and a drawback. The lack of any real
scripting ability is a problem for developers who want to deploy common sourcecode to multiple platforms. As a result, you sometimes see source distributions thatcontain several 
Makefile s—one for each target. Besides being messy and inelegant,
it’s a maintenance headache.
Several variants of make have sprouted up over the years to address its shortcom-
ings. These keep the basic make syntax but often add keywords to support features
that are missing. Unfortunately, there is no guarantee that a script built for one
make variant will work on another make variant, so this is a disincentive for devel-
opers to switch to a new flavor of make . GNU make is one of these variants—
which, being the version of make on Linux, has a lot going for it. I will look at some
of GNU make ’s features in detail.
There are also tools that have been built on top of make to work around its short-
comings. These tools generate Makefile s from a higher-level description of the
project.
2.2.1.1 Imake
One of the first tools developed to generate Makefile s is the imake program. imake
came out of the X Window project as a way to build X Window source on various
UNIX platforms. It uses the C preprocessor to parse its build script (called an
imakefile ) and generates a Makefile for use by make . This enhances portability
by encapsulating system-specific gobbledygook in preprocessor macros and condi-tional constructs. A side benefit is that the 
imakefile s are concise and simple. 42 Chapter 2 • Building from Source
imake never really caught on, however, and has shortcomings of its own. One of
these is the fact that each build target requires a detailed set of macros. If your targetsystem does not have a set of these descriptions, you can’t use 
imake . This prevents
projects using imake from being deployed on diverse and cutting-edge systems.
Although it worked well for building X Window on UNIX systems, the only
place you are likely to encounter an imakefile today is in a legacy X Window
project.
2.2.1.2 GNU Build Tools
Seeking a portable way to build software on various architectures, the GNU Project
created its own suite of build tools to enhance the make program. The approach is
similar to imake except that instead of the C preprocessor, the GNU build tools use
the m4program, which has more capabilities than the C preprocessor. The GNU
build tools are used to create source distributions that can be built on a wide vari-ety of machines. An individual building the project from a source distribution needsonly a working shell and 
make program. The GNU build tools have become the
de facto standard for distributing source.
Although these tools make life easy for the open source consumer, they are diffi-
cult for the developers who create the distributions. For one thing, the m4syntax is
unfamiliar to most programmers, and syntax errors are common. In addition, thetools continue to evolve and mature as new features are added, and some featuresbreak along the way.
Another drawback to the GNU build system (as well as 
imake ) is that it adds an
extra step to the build process. With the GNU build tools in particular, this con-figuration step can consume more time than the build itself. This has inspired somedevelopers to think of better ways to build source.
2.2.1.3 Alternative Approaches
Alternative build tools are at a disadvantage, because most people don’t have the
patience to deal with a build tool that is still under development. A developer hasenough to worry about without dealing with the aggravation of bugs in his buildtool or wondering whether his build scripts will work with the next release of thetool. Using the GNU build tools as the benchmark, any alternative should be sim-pler to use and as fast or faster.2.2 Build Tools 43
One project that is up to the challenge is Cons.1Cons is based on Perl,2which is
where its build scripts borrow their syntax. So it’s easier to use than the GNU buildtools, because a developer is more likely to be familiar with Perl. It replaces all theGNU build tools, and the sundry files that are associated with them, with a singlefile used by a single tool in a single step. A disadvantage of this approach is that thetime penalty incurred by the 
configure stage of a GNU build is incurred every
single time you run the build. For someone downloading the source, who will onlybuild it once, this is no big deal. But for a developer who has to build repeatedly, itis a problem. Another drawback is that the individuals downloading the source dis-tribution must have the correct version of Perl installed, as well as the appropriateCons tool. This forced developers to stick with older versions of Perl, which can bedifficult. Although Cons introduced some good ideas, it never really caught on, andthe project appears to be dead today.
Some programmers decided that the ideas behind Cons were solid, but that
shortcomings of Perl were holding it back. They took the same design and imple-mented it in Python.
3This is Scons.4Python has some advantages over Perl, includ-
ing the fact that it is object-oriented by design and is more strongly typed. AlthoughPython is in its second major version as of this writing, the basic syntax and gram-mar are virtually unchanged from the original, so writing backward-compatiblescripts is not as painful as it is in Perl. Scons developers target Python 1.5, whichcovers a very wide audience, so Python compatibility is not a big issue. Scons suf-fers from the same time penalty as Cons. Although there are many favorable reviewsof Scons, and a handful of projects use it in their source distributions, it has yet tocatch on in a big way.
Perhaps an alternative to 
make will catch on, but until then, it behooves you to
understand make .
2.2.2 Understanding make
make is an excellent tool to enhance your productivity. A well-written Makefile
can make a huge difference in the speed of your development. Unfortunately, manydevelopers have learned to use 
make through trial and error without ever reading44 Chapter 2 • Building from Source
1.Cons (www.gnu.org/software/cons/dev/cons.html) is short for Construction System .
2. www.perl.org3. www.python.org4. www.scons.org
any documentation. These individuals rely a great deal on intuition and luck, and
that shows in their Makefile s. Chances are that you are one of them, so I will start
with some basics and work into some very useful GNU extensions.
2.2.2.1 Makefile Basics: Rules and Dependencies
Unlike traditional scripts, which execute commands sequentially, Makefile s con-
tain a mixture of rules and instructions. Makefile rules have the following very
simple form:
target: prerequisite
commands
The rule asserts a dependency that says that the target depends on the prerequi-
site. In the simplest form, the target consists of a single filename, and the prerequisitecontains one or more filenames. If the target is older than any of its prerequisites, thecommands associated with the rule are executed. Typically, the commands containinstructions to build the target. A trivial example looks like the following:
foo: foo.c
gcc -o foo foo.c
Here, foois the target, and foo.c is the prerequisite. If foo.c is newer than
foo, or if foodoesn’t exist, make executes the command gcc-ofoofoo.c . If foo
is newer than foo.c , nothing happens. This is how make saves you time—by not
building targets that don’t need to be built.
In a more complicated example, the prerequisite of one rule can be the target in
another rule. In that case, those other dependencies must be evaluated before thecurrent dependency is evaluated. Consider this contrived example:
# Rule 1
program: object.o
gcc -o program object.o
# Rule 2
source.c:
echo 'main() {}' > $@
# Rule 3
object.o: source.c
gcc -c source.c -o $@
# Rule 4
program2: program2.c
gcc -o program2 program2.c2.2 Build Tools 45
Starting with an empty directory, if you run make with no arguments, you will
see the following output:
$ make
echo 'main() {}' > source.cgcc -c source.c -o object.ogcc -o program object.o
make evaluates the first rule it encounters in the Makefile and stops when the
dependency is satisfied. Other rules are evaluated only if they are required to satisfyrule 1. The parsing takes place something like this:
• rule 1:
program requires object.o .
•object.o is the target of rule 3; evaluate rule 3 before checking the
file date.
•I f  object.o is newer than program , build program with gcc.
• rule 3: object.o requires source.c .
•source.c is the target of rule 2; evaluate rule 2 before checking the
file date.
•I f  source.c is newer than object.o , build object.o with gcc.
• rule 2: source.c doesn’t have any prerequisites.
•I f  source.c doesn’t exist, build it with an echo command.
Notice that the dependencies determine the order in which the commands exe-
cute. Other than the first rule, the order of the rules in the Makefile have no
impact. We could, for example, swap the order of rules 2 and 3 in the Makefile ,
and it will make no difference in the behavior. 
Notice also that rule 4 has no impact on the build. When the target for the first
rule is determined to be up to date, make stops. Because this does not need any-
thing from rule 4, program2 is never built, and program2.c is not needed. That
doesn’t mean that rule 4 is superfluous or useless. You could just as easily type make
program2 to tell make to evaluate rule 4. Makefile rules can be independent of
one another and still be useful.
make can also build specific targets in the Makefile , which allows you to bypass
the default rule. This technique is the preferred way to build a single object in a46 Chapter 2 • Building from Source
project during development, for example. Suppose that you’ve just modified
program2.c and want to see whether it compiles without warnings. You could type
$ make program2.o
Assuming the dependencies for program2.o are straightforward, this compiles
program2.c and nothing else.
make can also build so called pseudotargets , which are targets that do not repre-
sent filenames. A pseudotarget can have any arbitrary name, but there are conven-tions that are commonly followed. A common convention, for example, is to usethe pseudotarget 
allas the first rule in a Makefile . To understand the need for
pseudotargets, consider this example, in which you have two programs you want tobuild as part of your 
Makefile :
program1: a.o b.o
gcc -o program1 a.o b.o
program2: c.o d.o
gcc -o program2 c.o d.o
If you ran make with no arguments, it would build program1 and stop. To get
around this, you could require the user to specify both programs on the commandline, as follows:
$ make program1 program2
This works, but being able to run make with no arguments is a nice way to make
your code easy to build. To fix this, add a pseudotarget named allas the first rule
in the Makefile with program1 and program2 as the prerequisites, as follows:
all: program1 program2
program1: a.o b.o
gcc -o program1 a.o b.o
program2: c.o d.o
gcc -o program2 c.o d.o
This allows you to type make or makeall , which will build both program1 and
program2 . Although all is a common convention, you could have called your
pseudotarget fred or anything else. As long as it’s the first target, it will be evalu-
ated when you run make with no arguments. Because it is a pseudotarget, the name
of the target is irrelevant. 2.2 Build Tools 47
Typically, no commands are associated with a rule that contains a pseudotarget,
and the name of the pseudotarget is chosen so that it doesn’t conflict with any ofthe files in the build. Just in case, GNU 
make has a built-in pseudotarget that you
can use to give make a clue, as follows:
.PHONY: all
This tells make not to search for a file named alland to assume that this target
is always obsolete.
2.2.2.2 Makefile Basics: Defining Variables
make allows build options to be specified using variables. The syntax for defining a
variable is straightforward:
VAR = value
VAR := value
The two types of definitions are equivalent except that the := form allows vari-
ables to reference themselves without recursion. This comes up when you want toappend text to a variable:
VAR = value
# Wrong!  Causes infinite recursion
VAR = $(VAR) more
# Okay, the := prevents recursion
VAR := $(VAR) more
# A GNU extension that does the same thing
VAR = valueVAR += more
GNU make allows an alternative syntax for defining variables using two key-
words: define and endef . The GNU equivalent syntax looks like the following:
define VAR
valueendef
The newlines before and after the keywords are required and are not part of the
variable definition.48 Chapter 2 • Building from Source
The convention for variable names is to use uppercase letters, although this is
not required. The value of the variable may contain any ASCII text, but the text isstored literally and has no meaning until it is used in context. One common pat-tern, for example, is to use backticks to switch the contents of the variable with theoutput from a shell command. The backticks are a shell trick and have no specialmeaning inside the 
Makefile , so you are not assigning the variable with the con-
tents of a shell command. Instead, the variable contains literal text that can bepassed to the shell. The following variable works as expected because it’s used inthe shell:
SOMEFILE = `date +%02d%02m%02y`.dat
all:
@echo $(SOMEFILE)
$ make
290505.dat
Because the variable is used in a shell context, the backticks behave as expected,
echoing today’s date followed by the .dat extension. It’s a clever trick, but it would
fail miserably if you tried to use it as a prerequisite or a target. For example:
all: $(SOMEFILE)
@echo $(SOMEFILE)
290505.dat:
touch 290505.dat
$ make
make: *** No rule to make target ``date', needed by `all'.  Stop.
This fails because make isn’t looking for a target named 290505.dat . Because the
backticks have no meaning to make , the white space in this variable makes it look
like two ugly targets:
0      `date
1      +%02d%02m%02y`.dat
The first target is the one that causes make to fail, and this appears in the cryp-
tic error message. GNU make provides a fix for this specific situation, which I will
discuss later.2.2 Build Tools 49
White Space and Newlines in Variables
make automatically removes leading and trailing white space from your variable val-
ues when you use the traditional syntax. Specifically, any spaces following the = donot appear in the variable contents, and any spaces before the newline are removed aswell. By contrast, when you use the 
define syntax, leading and trailing spaces are
preserved, as the following Makefile illustrates:
TRAD=                         Hello World
define DEFD
Hello World
endef
all:
@echo "'$(DEFD)'"@echo "'$(TRAD)'"
The extra quotes in the echo command help illustrate in the output just where
your spaces are (or aren’t). When you run make , you will see the following:
$ make
'                             Hello World    ''Hello World'
Notice that the value of $(DEFD) contains spaces following Hello World , which
you can’t see in the Makefile listing. If you want to preserve spaces with the tradi-
tional syntax, you can use the built-in empty variable $()as follows:
TRAD=$()                      Hello World     $()
Limiting the length of your lines keeps your Makefile readable. This can be dif-
ficult using the traditional syntax, because the entire declaration must appear on oneline. Fortunately, 
make allows you to break long lines using the backslash character
(\) as the last character on a line. But beware, because make compresses white space
before and after the backslash. Consider the following variable declaration:
FOO=Hello                \
World
When used in the Makefile , the value of FOO will contain simply
Hello World
As you may have noticed, it is not possible to embed newlines in a variable using
the traditional syntax. GNU make allows you to embed newlines with the define50 Chapter 2 • Building from Source
syntax, but beware. Variables with embedded newlines have limited uses, as the fol-
lowing example illustrates:
define FOO
HelloWorldendef
This particular variable is essentially useless. If you try to use it as a command, the
newline will break it into two commands. So if you wanted to echo the contents tothe screen, for example, it would not work. The error you get comes from the shell(not 
make ), as follows:
echo Hello
HelloWorldmake: World: Command not found
Variables with embedded newlines can be used to encapsulate sequences of inde-
pendent shell commands. But because each command in a rule is its own shell, youcan’t write an entire script in a variable that contains embedded newlines. The fol-lowing trivial bit of shell script does not work as written:
define FOO
if [ -e file ]; then 
echo file exists
fienddef
all:
$(FOO) # Does not work!
$ make
if [ -e file ]; thenSyntax error: end of file unexpected (expecting "fi")
Here are some points to remember about white space in variables:
• Leading and trailing white space does not appear in variables defined using the
traditional syntax.
• White space before and after a backslash is compressed when using the traditional
syntax.
• Leading and trailing white space can be preserved with the traditional syntax by
using the predefined $() variable.
• The define syntax preserves all white space, including newlines, but variables
with newlines have limited uses.2.2 Build Tools 51
2.2.2.3 Makefile Basics: Referencing and Modifying Variables
As you have seen, the syntax for referencing a variable requires that you enclose the
name in parentheses or braces, as follows:
$(MACRO) # this is most common
${MACRO} # less common but still okay
Strictly speaking, this applies only to variable names with more than one letter,
which ideally is the case for all your variables. A variable named with a single letterdoes not require parentheses or braces.
Unlike variables in a script, which change value during the course of execution,
Makefile variables are assigned once and never change value during the course of
the build.5This trips up many people working with Makefile s, who think intu-
itively that variable assignments take place in order in which they appear in the
Makefile or that they have a scope and lifetime. All variables in a Makefile have
global scope, and their lifetime is the duration of the build. A variable can bedefined many times in the 
Makefile , but only the last definition in the Makefile
determines its value. Consider the following Makefile :
FLAGS = first
all:
@echo FLAGS=$(FLAGS)
FLAGS = secondother:
@echo FLAGS=$(FLAGS)
You might think that the variable FLAGS will have one value for the alltarget
and another value for the other target, but you would be wrong. The definition of
FLAGS is fixed before any rules are evaluated. When FLAGS is defined, make will dis-
card old definitions as new ones are encountered so that the last definition in thefile is the only one that matters.
2.2.2.4 Minimal Makefiles Using Implicit Rules
As you write more 
Makefile s, it quickly becomes apparent that many rules follow
a few simple patterns. In large Makefile s, it is possible to have many rules that are
identical except for the target and prerequisite. This leads to a great deal of copying52 Chapter 2 • Building from Source
5. The only exceptions are the so-called “automatic” variables, which I discuss later.
and pasting with the text editor—which, as every developer knows, leads to dumb
mistakes.
Fortunately, make provides implicit rules that allow you to describe these patterns
without having to copy and paste rules. make comes with numerous predefined
implicit rules that cover many patterns commonly found in Makefile s. Thanks to
the built-in implicit rules, it is possible to write rules without any instructions. It’seven possible to use 
make without a Makefile ! Just for fun, try this in an empty
directory:
$ echo "main() {}" >  foo.c
$ make foocc     foo.c   -o foo
Just by using implicit rules, make is able to figure out that you wanted to create
a program named foofrom foo.c .
Implicit rules are there to keep your Makefile s short and make your job easier.
GNU make comes with many implicit rules to do almost everything you need. You
should exploit implicit rules every chance you get.
One way to define implicit rules is to use suffix rules. As an example, the suffix
rule GNU make uses to create to object files from C source files looks like this:
.c.o:
$(COMPILE.c) $(OUTPUT_OPTION) $<
This says, “If you see a target with an .oextension, and there is no explicit rule
for it, look for a file with the same base and the .cextension. If you find it, run
these commands.” As shown here, the commands for implicit rules typically areenclosed in variables, which allow you to exploit these rules further.
You can examine all the built-in variables and implicit rules by typing 
make-p .
Using this to examine COMPILE.c , you can see that it is defined by using additional
variables:
COMPILE.c = $(CC) $(CFLAGS) $(CPPFLAGS) $(TARGET_ARCH) -c
CC = gcc
Interestingly, only one of these additional variables ( CC) is actually defined; the
others are quietly replaced with empty strings. make allows you refer to undefined
variables without producing an error. You can exploit this in a minimal Makefile
just by including the following line:
CFLAGS = -g2.2 Build Tools 53
You can use this one-line Makefile to compile objects with debugging enabled.
But because make allows you to define variables on the command line, you could
skip the Makefile or override CFLAGS on the command line as follows:
$ make CFLAGS=-g foo
gcc -g    foo.c   -o foo
Variables provided on the command line override all definitions in the Makefile .
So whether you have a complicated Makefile or none at all, you can exert a great
deal of control over the build from the command line. Table 2-1 lists several variablescommonly used in implicit rules that can be overridden on the command line.
Given the flexibility of implicit rules for object code, you may never need to write
explicit rules for object files. There are implicit rules for C, C++, Fortran, and manyothers. I list some of the most common ones in Table 2-2, but you can use 
make
-pto check, or check the make manual for others.
TABLE 2-1 Common Variables Used in Implicit Rules
Variable Default Description
CC gcc C compiler.
CXX g++ C++ compiler.
CFLAGS none Flags passed to the C compiler with implicit rules.
CXXFLAGS none Flags passed to the C++ compiler with implicit rules.
CPPFLAGS none Flags passed to the C preprocessor used with implicit
rules, including C++, C, some assembly language rules,
and several others. Typical flags include -I, -D, and -U.
TABLE 2-2 Default Suffix Rules to Create Object Code
Language Extensions Command
C .c $(CC) -c $(CPPFLAGS) $(CFLAGS)
C++ .cpp .cc .C $(CXX) -c $(CPPFLAGS) $(CXXFLAGS)
Assembler .s $(AS) $(ASFLAGS)
Assembler6.S $(CPP) $(CPPFLAGS)54 Chapter 2 • Building from Source
6. This rule doesn’t create object code directly but preprocesses the .Sfile into an .sfile, which is then
compiled by the rule above.
Language Extensions Command
Pascal .p $(PC) -c $(PFLAGS)
Fortran .f $(FC) -c $(FFLAGS)
Fortran .F $(FC) -c $(FFLAGS) $(CPPFLAGS)
Defining Custom Implicit Rules
Although GNU make defines numerous implicit rules that cover the most common
cases, there are likely to be cases where you can’t find an implicit rule that works foryou. In this case, you can write an explicit rule or define your own implicit rule.
Suppose that you have a bunch of C++ modules to compile, all of which have the
.cxx extension. This is not one of the three C++ extensions recognized by GNU
make .7You could rename all the files or create explicit rules for each of them, but cre-
ating a new implicit rule is trivial. In this example, you simply need two lines:
.SUFFIXES: .cxx # let make know that this is a new extension
.cxx.o:
$(CXX) -c $(CPPFLAGS) $(CXXFLAGS) $<
Without the .SUFFIXES: pseudotarget, make will not use your implicit rule,
because it does not recognize files that end with .cxx . The suffix rule I used above is
compatible with other versions of make , as are the variables I used to define it. GNU
make offers an alternative syntax, which is more flexible and less picky about suffixes,
called pattern rules . A pattern rule for my .cxx source files would look like this:
%.o : %.cxx
$(CXX) -c $(CPPFLAGS) $(CXXFLAGS) $<
Pattern rules produce less clutter than suffix rules because they don’t require the
.SUFFIXES: pseudotarget. It also looks more like a normal rule than a suffix rule
because it contains a target on the left and a prerequisite on the right. Instead of spe-cific filenames for targets and prerequisites, the pattern rule uses a 
%character to
match filenames or targets in the Makefile , much like a wildcard character used 
to match filenames in the shell. When make encounters a target (or filename) that
matches the pattern, it uses the part of the target name matched by %to look for a
prerequisite.2.2 Build Tools 55
7. The extensions for C++ that make recognizes are .C, .cpp , and.cc.continues
Defining Custom Implicit Rules (Continued)
Let’s look at an example using a Makefile that contains only the previous two-
line pattern rule. If we try to build a file that doesn’t exist, make behaves as though
the pattern rule did not exist:
$ make foo.o
make: *** No rule to make target `foo.o'.  Stop.
Notice that make doesn’t say that it can’t find foo.cxx ; it just says there’s no rule.
That’s because although our target ( foo.o ) matches our pattern rule, there is no file
or target that matches the prerequisite ( foo.cxx ). There may be dozens of other pat-
tern rules that match the same target, but without any explicit targets or filenames towork with, 
make cannot make any assumptions. So the answer make provides is the
safe answer. 
Now suppose that we create a file named foo.cxx in our current directory, and
we run this command again:
$ make foo.o
g++ -c   foo.cxx
This time, make is able to apply the implicit rule, because it sees that the target
(foo.o ) satisfies our target pattern and is able to find a file ( foo.cxx ) that matches
our prerequisite pattern. The implicit rule is satisfied, so the associated commandscan run. 
The matching prerequisite could have also been an explicit target instead of a file.
Let’s remove 
foo.cxx and add a single explicit rule to our Makefile , as follows:
foo.cxx:
@echo Hello World
This rule does nothing but print a message to let us know it triggered. It does not
create any files, so expect make to fail. Now when we run the same make command,
we get a different result:
$ make foo.o
Hello Worldg++ -c   foo.cxxg++: foo.cxx: No such file or directoryg++: no input filesmake: *** [foo.o] Error 156 Chapter 2 • Building from Source
Because there is no file named foo.cxx, gcc fails as expected. The rule that
should have created foo.cxx simply prints Hello World to the screen instead. The
point is that an explicit target was enough to satisfy our implicit rule without any filesat all.
Although the previous example used the GNU pattern rules, 
make behaves exactly
the same way with traditional suffix rules. Each time make searches for a specific tar-
get, it searches the explicit rules first. Only if it cannot find a match does it attempt
to apply the implicit rules.
2.2.2.5 Using Automatic Variables to Improve Readability
Because implicit rules and pattern rules work without specific filenames, make pro-
vides automatic variables that help define the commands for these rules. In earlier
examples, I used $<, which is an automatic variable that takes on the value of the
first prerequisite in the rule. Unlike other make variables, automatic variables have
unique values for every rule. Table 2-3 shows some of the most useful automaticvariables used in implicit rules.
As you have seen, user-defined variables can contain other variables. What may
not be obvious is that variables can also contain automatic variables, which means2.2 Build Tools 57
TABLE 2-3 Some Useful Automatic Variables in GNU make
Variable Value
$@ Filename of the target.
$^ The names of all prerequisites with duplicate names removed.
$+ The names of all prerequisites, including duplicates.
$< The name of the first prerequisite in the rule.
$? The names of all prerequisites newer than the target.
$* With a suffix rule, this is the base name (or stem) of the target. If the
rule is .c.o: , for example, and the matching target is foo.o , this value
is foo. With some pattern rules, this value may not be what you expect.
that their value can change over the course of the build. Automatic variables are vital
for implicit rules but can be very useful in explicit rules as well. Consider this com-mon 
Makefile pattern for creating a program:
program: $(OBJS)
$(CC) -o $@ $^
The command portion consists of a predefined variable ( CC) as well as two auto-
matic variables. Unfortunately, there is no way to encapsulate this in an implicit rulebecause there isn’t a clear pattern. The 
program can be any name, and the OBJS can
contain any number of arbitrarily named objects. As far as make is concerned, there
is no pattern to follow. 
If you want to use this same rule to create additional programs in the same
Makefile , you are resigned to cutting and pasting these commands for every pro-
gram, as follows:
program1: $(PROG1_OBJS)
$(CC) -o $@ $^
program2: $(PROG2_OBJS)
$(CC) -o $@ $^
# etc...
At least with the use of automatic variables, you can cut and paste with no mod-
ifications to the command portion of the rule. It is a little cryptic, but you can dobetter. By embedding the automatic variables in a more intuitively named variable,you can 
make the pattern easier to follow. For example:
build_C_program=$(CC) -o $@ $^
program1: mod1.o mod2.o
$(build_C_program)
program2: mod3.o mod4.o
$(build_C_program)
Now the ugly automatic variables are part of the definition for
build_C_program , but they behave as expected when build_C_program appears
as a command. The rule is a little easier to follow, and other developers don’t needto know what 
build_C_program looks like unless they want to.58 Chapter 2 • Building from Source
2.2.2.6 Manipulating Variables
To understand why you need to manipulate variable values, let’s look at a typical
example of a text substitution pattern used in UNIX make :
SRCS=foo.c bar.c
OBJS=$(SRCS:.c=.o)
This syntax is called a substitution reference and is available in all flavors of make .
Here, we take a list of source files and use it to create a list of object files. In thisexample, we take the value of the 
SRCS variable, substitute .ofor all the .cexten-
sions, and store the result in the OBJS variable. Because we have two source mod-
ules, named foo.c and bar.c , we would like OBJS to contain foo.o and bar.o .
The text substitution is preferable to typing every name again on a second line,which would be tedious and error prone. 
Makefile s usually contain many such
lists, making text substitutions indispensable.
The substitution reference works in this example because all the source modules
have the same extension. What if you have mixed C and C++ sources in your SRCS
variable? For example:
SRCS=foo.c bar.cpp
OBJS=$(SRCS:.c=.o)
The substitution reference syntax does not allow more than one file extension
pattern. So in our example, the substitution does not take place for bar.cpp
because it doesn’t match the .cpattern. OBJS then contains foo.obar.cpp , which
is not what you want.
A typical solution in UNIX make would be to create an additional variable as
follows:
CSRCS=foo.c
CXXSRCS=foo.cppOBJS=$(CSRCS:.c=.o) $(CXXSRCS:.cpp=.o)
This technique tends to increase clutter, especially when you are dealing with
numerous lists.
Next, I will look at the GNU function extensions, which are much more
flexible.2.2 Build Tools 59
2.2.2.7 Manipulating Variables with Functions
As you have seen, the substitution reference syntax can be very handy, but it is a bit
limited. GNU make ’s function extensions are versatile tools for manipulating vari-
able contents. Here are two ways to do the same thing:
OBJS=$(SRCS:.c=.o)
OBJS:=$(patsubst %.c, %.o, $(SRCS)))
The first line uses the familiar substitution reference. The second line is the
equivalent of the substitution reference using the patsubst function. The
patsubst function has the same limitations as the substitution reference: Only one
extension at a time is supported. Functions can be nested, however, which is some-thing that you can’t do with a substitution reference. To include additional exten-sions, for example, you can use the following syntax:
OBJS:=$(patsubst %.cpp, %.o, $(patsubst %.c, %.o, $(SRCS)))
Now we can transform multiple file extensions without having to create extra
variables.
Here is another technique using different functions. It is a bit more concise and
generic:
OBJS:=$(addsuffix .o, $(basename $(SRCS)))
The basename function takes a list of filenames (in this case, the contents of
SRCS ) and strips the extension from each of them. It doesn’t matter what the exten-
sion is; basename just strips off all the characters following the last period in the
text. We take the output and glue .oto the end of each basename , using the
addsuffix function. This gives us the list of objects we want.
Another useful function is the shell function. Recall the earlier example in
which we used backticks to create a filename based on the date and time:
NAME = `date +%02d%02m%02y`.dat
You saw how this pattern fails when it is used as a prerequisite or a target. GNU
make provides the shell function as an alternative to do what we want:
NAME = $(shell date +%02d%02m%02y).dat60 Chapter 2 • Building from Source
Unlike the backticks, this variable is safe to use as a prerequisite or a target
because GNU make evaluates the shell command when the variable is initialized,
before any rules are evaluated. In this particular example, this is exactly what wewant. There may be situations in which you still want to use the backticks. Considera rule like the following:
CURRENT_TIME=$(shell date +%T)
something:
@echo started  $(CURRENT_TIME)sleep 5@echo finished $(CURRENT_TIME)
When you run this, you will see the following:
$ makestarted  9:49:45sleep 5finished 9:49:45
This probably is not what you had in mind. The problem is that the shell com-
mand `date +%T` is evaluated only once, at the beginning of the build, so every
time we use it during the build, it has the same value. If you want the value tochange over time, you will have to call the shell each time, which is what the back-ticks do. For example: 
CURRENT_TIME=`date +%T`
This gives us the output we expect:
$ make
started  9:49:45sleep 5finished 9:49:50
Note again that the value of CURRENT_TIME is a variable and that its value
remains fixed in both examples. In the first example, CURRENT_TIME contains the
output of a shell command that ran at the beginning of the build. In the secondexample, 
CURRENT_TIME contains the text of a shell command that is executed dur-
ing the build. It is the output from the shell command that is changing, not thecontents of the variable.
A complete list of functions can be found in the info 
page for make . Table 2-4
contains a list of some of the more useful functions.2.2 Build Tools 61
TABLE 2-4 Functions for Manipulating Text
Function Usage Description
subst $(subst from, to, text )Returns the contents of the text
argument with all occurrences of the
from argument replaced with the
contents of the toargument.
patsubst Returns a white space–separated listof filenames with the filename patternsreplaced. The pattern syntax is thesame syntax used for pattern rules. Thefunction is intended for filenames,but it will work for any text.
strip $(strip string) Removes leading and trailing whitespace from the 
string argument.
findstring Returns the match argument if it can
be found in the string argument;
otherwise, it returns an empty string.
filter Returns a white space–separated listof files found in the 
filenames
argument that match the patterns
argument. The patterns argument
may contain multiple patternsseparated by white space and uses thesame syntax used for pattern rules.
filter-out Returns a white space–separated listof files from the 
filenames argument
that do not match the patterns
argument. The patterns argument
may contain multiple patternsseparated by white space and uses thesame syntax used for pattern rules.
sort $(sort text) Returns the white space–separated listin the 
text argument sorted in
lexical order.$(filter-out patterns,
filenames )$(filter patterns,
filenames )$(findstring match,
string)$(patsubst from-
pattern, to-pattern,
filenames )62 Chapter 2 • Building from Source
Function Usage Description
word $(word n, text) Returns the nth word found in the
text argument. The word count
starts with 1.
wordlist Returns the set of words in the text
argument starting with first and
ending with last . 
words $(words text) Returns the number of words in the
text argument.
error $(error message) Used within a conditional (discussedlater). Stops the build with the given
error message.
2.2.2.8 Defining and Using Your Own Functions
In the unlikely event that you cannot find a built-in function to suit your needs,
GNU make allows you to define your own functions. User-defined functions are
accessed via the call built-in function, as follows:
$(call myfunction, arg1, arg2)
Notice that the user-defined function call don’t look like other function calls. You
must use the call built-in function with your function name passed as an argu-
ment. Additional arguments are separated by commas. The definition of
myfunction looks just like a variable except that it now has access to positional
arguments. Here’s a more complete example:
myfunction = @echo $(1) $(2)
all:
$(call myfunction,hello,world)
When you run this Makefile , it produces the following output:
$ makehello world$(wordlist first,
last, text )2.2 Build Tools 63
The term function is a little misleading. make is not a programming language,
and it provides only minimal error checking. Just like variables, make is happy to
return empty strings when you make mistakes. Because a function definition looksno different from a variable definition, for example, you can still use a function asa variable. This seems weird, but it is consistent, given the syntax used for functions.When you use a function as a variable, it behaves as though the function were calledwith no arguments. For example:
$(call myfunction) # is the same as...
$(myfunction)
In both cases, the positional arguments— $(1) , $(2) , and so on—are replaced
with empty strings.
2.2.2.9 Conditional Constructs
Conditional constructs are another useful GNU extension for controlling make
behavior. There are only four types of conditionals supported in GNU make , and
these are listed in Table 2-5.
There really are only two tests and their complements: if equal and if
defined . Because make does not have a notoperator, it is necessary to have sepa-
rate keywords for equal and notequal , as well as for defined and notdefined .
Each conditional block may have an optional else clause but must be termi-
nated by an endif . The basic syntax is as follows:
conditional test
makefile text evaluated when test is true
else
makefile text evaluated when test is false
endif64 Chapter 2 • Building from Source
TABLE 2-5 Conditionals Available in GNU Make
Conditional Usage
ifeq Test if two values are equal
ifneq Test if two values are not equal
ifdef Test if a variable is defined
ifndef Test if a variable is not defined
The text that appears inside the conditional clause may be any valid make lines, but
conditionals may not be used inside the command section of rules or inside variable/function definitions. This places some restrictions on how conditionals may be used.
Let’s start with a simple example using the 
ifeq conditional:
ifeq ($(shell uname -o),GNU/Linux)
CPPFLAGS += -DLINUX
endif
Here, we use the ifeq conditional to test for the type of OS. It uses the output of
the shell command uname -o to indicate the type of operating system, which is
GNU/Linux on my Linux system. So when this appears in a Makefile on a Linux sys-
tem, the CPPFLAGS variable will include the -DLINUX flag. Recall that CPPFLAGS is
used in implicit rules for targets that are built with tools that use the C preprocessor.This is one technique for using conditionals to support multiple build environments.
Adding an 
else clause is straightforward:
ifeq ($(shell uname -o),GNU/Linux)
CPPFLAGS += -DLINUX
else
CPPFLAGS += -DOS_UNKNOWN
endif
You can also wrap conditional clauses around rules as well, so the following syn-
tax is valid:
ifeq ($(shell uname -o),GNU/Linux)
all: linux_programs
else
all: bsd_programs
endif
The command portion of the rule can be inside or outside the conditional clause.
One thing worth noting is that conditionals are evaluated in line with variable
assignments, as illustrated by this silly example:
ifeq ($(A),0) # A isn't defined yet
all:
@echo $(A) == 0
elseall:
@echo $(A) != 0
endif
A=02.2 Build Tools 65
Now when you run this Makefile , you see the following illogical output:
$ make
0 != 0
The problem is that the macro A is not defined until after the conditional, so the
ifeq test fails (it’s false). See what I mean about no error checking?
The ifdef clause tests for variable definitions, which can be useful for using vari-
ables as command-line options. For example:
ifdef (debug)
CFLAGS += -g
else
CFLAGS += -O2
endif
Recall that CFLAGS is the built-in variable used for implicitly building an object
from C source code. In this example, the default setting for CFLAGS is to include
optimization with the -O2switch. If you want to compile for debugging, invoke
make with debug defined on the command line as follows:
$ make debug=1
In large, complicated Makefile s, you might use ifdef to make sure that a nec-
essary variable is set:
ifndef (A_VITAL_VARIABLE)
$(error A_VITAL_VARIABLE is not set)endif
2.2.2.10 Pulling Source from Various Places
Very often, projects with many source files will locate them in a single directory and
build them in a different one. This reduces clutter by keeping one type of file in adirectory. T raditionally, this is accomplished with the 
VPATH variable. As usual,
however, GNU make extends this functionality in many useful ways.
In principle, the VPATH variable works much like the shell PATH variable. It con-
tains a list of directories separated by colons, in which it searches for targets and pre-requisites. You might have a project with two directories: 
srcfor source files and
binfor objects. In this case, the Makefile usually would reside in the ./bin direc-
tory and would look something like this:66 Chapter 2 • Building from Source
VPATH=../src
foo: foo.c
$(CC) -o $@ $^
VPATH indicates that make should search for targets and prerequisites in the
../src directory as well as the current directory. It’s not necessary to specify the
location of foo.c when it appears as a prerequisite. make will substitute the correct
path. So assuming that foo.c resides in the ../src directory, the output would
look like this:
$ make
gcc -o foo ../src/foo.c
If you have a copy of foo.c in the current directory, of course, make will still
pick up that one, which can be a problem. make will always search the current
working directory first. So if you are in the unfortunate situation of having two dif-ferent files with the same name in two places, 
VPATH may not solve your problem.
GNU make adds an extension to the VPATH with the vpath directive (note the
use of lowercase). With the vpath directive, you can specify filename patterns so
that make will search only for particular type of files in particular directories. As a
contrived example, you could put C++ source in a directory named /cxxsrc and
C source in a directory named /csrc . The Makefile for this could look like this:
vpath %.cpp ./cxxsrc
vpath %.c ./csrc
In general, most projects of modest size do not need to resort to the vpath direc-
tive or even use a VPATH variable. Things can get very confusing when you use these
features. These features are used with very large projects or to work around someinherited limitations of the build environment. Some of the source distributed withGNU projects require you to build in an empty directory. These projects rely on the
vpath feature to read the source from a read-only location.
2.2.3 How Programs Are Linked
Most developers are blissfully unaware of the linker, because the compiler normallydoes most of the work. Although there is a linker command (
ld), this command is
almost never called directly. All we worry about are the names and locations of thelibraries we want to link with. In fact, many little details must be provided to thelinker. Fortunately, the compiler already knows about these and passes these argu-ments behind the scenes.2.2 Build Tools 67
To get a peek at the sausage grinder, you can use the -voption of the C com-
piler. Here is what it looks like on my machine with the clutter removed (if you canbelieve that):
$ gcc -v -o hello hello.c
/usr/libexec/gcc/i386-redhat-linux/4.0.1/cc1 -quiet -v hello.c -quiet -dumpbase
hello.c -auxbase hello -version -o /tmp/ccoxYr4k.s
as -V -Qy -o /tmp/ccw3IciH.o /tmp/ccoxYr4k.s/usr/libexec/gcc/i386-redhat-linux/4.0.1/collect2 --eh-frame-hdr -m elf_i386 
-dynamic-linker /lib/ld-linux.so.2 -o hello /usr/lib/gcc/i386-redhat-linux/4.0.1/../../../crt1.o /usr/lib/gcc/i386-redhat-linux/4.0.1/../../../crti.o/usr/lib/gcc/i386-redhat-linux/4.0.1/crtbegin.o -L/usr/lib/gcc/i386-redhat-linux/4.0.1 -L/usr/lib/gcc/i386-redhat-linux/4.0.1 -L/usr/lib/gcc/i386-redhat-linux/4.0.1/../../.. /tmp/ccw3IciH.o -lgcc --as-needed -lgcc_s --no-as-needed -lc -lgcc --as-needed -lgcc_s --no-as-needed /usr/lib/gcc/i386-redhat-linux/4.0.1/crtend.o /usr/lib/gcc/i386-redhat-linux/4.0.1/../../../crtn.o
There are three stages that the compiler goes through. The first stage converts the
source code to assembly language. The second stage runs the assembler to create anobject file. The third, and ugliest, stage is the linker. Although the command is
collect2 , it actually calls ldwith the same options.
The C and C++ compilers have two basic modes of operation: with and without
linking. When you specify the -cflag, you are compiling without linking. The
compiler produces object files, and that’s it. Without the -coption, the compiler
produces object files and calls the linker to link the resulting objects into a program.
It should be apparent from the previous example that there is more to your pro-
gram than the objects you create. Several objects that you never heard of, withnames that begin with 
crt,8are linked with your program in addition to your
object files. In addition, the C standard library is linked by default. This goes onbehind the scenes, of course, so you don’t have to link with these things explicitly.When you are compiling with dynamic libraries (which I will discuss shortly), theevidence is there to see. For example:
$ ldd hello
libc.so.6 => /lib/tls/libc.so.6 (0x009f4000)/lib/ld-linux.so.2 (0x009db000)68 Chapter 2 • Building from Source
8.crtin this context is an abbreviation for C Runtime .
The ldd9command shows you the dynamic libraries that a particular program
needs. Here, you can see that my hello program requires libc.so.6 —also known
as the C standard library. The other requirement is /lib/ld-linux.so.2 , which
is the dynamic linker itself.
2.2.4 Understanding Libraries
A library is a container that holds reusable object code. When you make a POSIX
function call, for example, you have to link your code with the POSIX library for itto work. Many libraries are available for different functions. Some, like the C stan-dard library, are required; most others are optional. By encapsulating functions inlibraries, developers can keep the program footprint (and the executable file) smallby picking only the libraries that are absolutely necessary for the program to run.
Libraries come in two flavors: dynamic and static. A static library is just an archive
of object files created with the 
arcommand. The linker reads the object files con-
tained in the archive one by one when it tries to resolve external symbols. The linkeris free to discard any object files that are not required. So although a static librarymay contain hundreds of objects and megabytes of object code, your code may pullin only one or two objects and have a much smaller footprint. If you are everinclined to peek inside a static library, you are likely to find that it contains manymodules with only one function per module. This gives the linker maximum oppor-tunities to remove unnecessary code. The key feature of static libraries is that onlythe person building the executable needs a copy of the library. To achieve this, someor all of the object code in the library will be copied to your executable file. If anexecutable is linked only with static libraries, we say that it is statically linked. Thismeans you can take a copy of that executable to another system and execute it with-out requiring any additional files.
Unlike a static library, which is just an archive of object files, a dynamic library is
itself an object file. Dynamic libraries are created by the linker, unlike static libraries,which are created by the compiler and the 
arcommand. When you link with a
dynamic library, the compiler does not copy any object code into your executablefile. Instead, it records information about the dynamic library in your executable sothat it can be dynamically linked at runtime. This means that part of the link process
is deferred until runtime. This has several advantages over a static library. For one2.2 Build Tools 69
9.lddstands for List Dynamic Dependencies.
thing, it makes your executable file smaller, because the linker is not copying object
code into it. Another advantage is that only one copy of the library code needs toreside in memory. This saves memory, because all the programs that use the librarypoint to the same physical memory.
Patching is easier with shared libraries because you can update the shared library
with a bug fix and simultaneously fix every program in the system that uses thelibrary. If the same library were static, you would have to rebuild every program thatuses it to deploy the same bug fix.
When a dynamically linked program executes, the program performs the final
linking stages before it enters main. It searches for the required dynamic librariesand exits with an error if it can’t find any of them. This can be a problem when youwant to ship binary files to different machines, because all the machines must haveall the required shared libraries; otherwise, the program won’t run.
Let’s create a dynamically linked executable to illustrate some of these issues.
We’ll need two modules: 
hello.c for the main program and empty.c , which will
be an empty shared object.
$ cat <<EOF > hello.c
#include <stdio.h>int main() { printf("Hello World\n"); }EOF$ cat /dev/null > empty.c
empty.c is an empty file, which can be compiled in to an object or shared object
just like any other source file. We don’t need any code here to illustrate our point,but we will create a shared object:
$ gcc -shared -fpic -o empty.so empty.c
The -shared flag tells the compiler to link the object but to create a shared
object instead of an executable object. To link hello with this shared object, spec-
ify it on the command line like any other object file, but the result is different:
$ gcc -o hello hello.c empty.so
There’s a surprise in store when you try to run this:
$ ./hello
./hello: error while loading shared libraries: empty.so: cannot open sharedobject file: No such file or directory70 Chapter 2 • Building from Source
The dynamic linker is looking for empty.so and can’t find it. We have to help
it out with an environment variable:
$ LD_LIBRARY_PATH=. ./hello
Hello World
This illustrates one of the downsides to shared objects: If the object is not in a
predefined location, the program will not execute. There are other downsides aswell. For one thing, if your shared library takes up large amounts of space for code,so will any process that runs your executable. Recall that static libraries are justarchives of object files. When the linker sees a static library, it is free to pull in onlythe object files that your program actually needs, which keeps the executable assmall as possible. When your code links with a shared object, however, it’s all ornothing. The dynamic linker cannot discard pieces of the shared object, so yourprocess’s memory will map all the code contained in the shared object, whether itcalls it or not.
With a virtual memory system, this is not such a big problem, because code that
is never used may never consume any physical memory. Regardless, the additionalcode consumes virtual addresses, which can be a problem in large applications thatrequire large data sets, particularly on 32-bit architectures. Shared libraries can robyou of virtual memory that you could put to better use.
Static and Dynamic Linking: A Closer Look
For this example, we will create a new shared object that does nothing but consume
memory. Let’s call it piggy.c :
$ cat <<EOF > piggy.c
char bank[0x100000];EOF
This declares a global buffer of 1MB. For the main module, we’ll use the trivial
hello.c program that just waits for us to press Ctrl+C so we can examine it while
it runs:
$ cat <<EOF > hello.c
#include <unistd.h>int main(){ pause(); }EOF2.2 Build Tools 71
continues
Static and Dynamic Linking: A Closer Look (Continued)
There are several ways to link these two modules into a program, each with advan-
tages and disadvantages. The simplest way is to link them as two objects:
$ gcc -o hello hello.c piggy.c
$ size hello
text    data     bss     dec     hex filename1117     264 1048608 1049989  100585 hello
$ nm -S hello | grep bank080496c0 00100000 B bank
When the modules are linked like this, the memory consumption is visible with
the size command. The nmcommand shows that the array bank is present and con-
suming 1MB of space. When piggy.o is part of a static library that is linked with
the executable, the situation changes:
$ gcc -c piggy.c
$ ar clq libpig.a piggy.o$ gcc -o hello hello.c -L ./ -lpig$ size hello
text    data     bss     dec     hex filename1117     264       4    1385     569 hello
$ nm -S hello | grep bank
This time, the size of the executable is noticeably smaller because the bank array
is not included, which tells us that the piggy.o module was excluded. The linker is
able to determine that there are no references to piggy.o , so it does not link it with
the executable.
Finally, we’ll create a shared object:
$ gcc -shared -o piggy.so piggy.c
$ gcc -o hello hello.c ./piggy.so$ size hello$ size ./hello
text    data     bss     dec     hex filename1349     272       4    1625     659 ./hello
$ nm -S hello | grep bank
The size is roughly the same as the executable linked with the static library, and
there is no trace of the bank array. This time, we’ll run it using the pmap command
to get a look at the process’s memory map:
$ ./hello &
$ jobs -x pmap -q %17382:   ./hello08048000      4K r-x--  /hello08049000      4K rw---  /hello72 Chapter 2 • Building from Source
b7dab000      4K rw---    [ anon ]
b7dac000   1160K r-x--  /libc-2.3.2.sob7ece000     36K rw---  /libc-2.3.2.sob7ed7000      8K rw---    [ anon ]b7ee7000      4K r-x--  /piggy.sob7ee8000      4K rw---  /piggy.sob7ee9000   1032K rw---    [ anon ]b7feb000     84K r-x--  /ld-2.3.2.sob8000000      4K rw---  /ld-2.3.2.sobffeb000     84K rw---    [ stack ]ffffe000      4K -----    [ anon ]
It looks like piggy.so is consuming only 8K, but tucked away in this output is an
anonymous mapping of 1032K. This is storage that was allocated for the bank array.
So linking with a shared object can force you to consume virtual memory that you
otherwise would not use. If this data is truly unused, it may never actually consumephysical memory or swap, although this range of virtual addresses is not usable byyour process. This can become a problem if your process needs to allocate large blocks
of memory.
By default, the compiler uses dynamic linking, which means that if it can choose
between a static library and a dynamic library, it will choose the dynamic library. Ifyou want to link exclusively with static objects, you must specify the 
-static
option to gcc/g++ . Most development libraries come with copies of both a
dynamic library and a static library for this purpose. The linker cannot staticallylink an executable with a dynamic library, or vice versa.
Library Naming Convention
Dynamic libraries typically are identified by the .so extension of the filename. In
Linux (and UNIX), dynamic libraries are also known as shared libraries or, moregenerically, as shared objects. This is where the .so extension comes from. Staticlibraries use the 
.aextension, which they inherited from the ararchive program with
which they are created. To differentiate an ar archive (which is hardly used else-
where) from a static library, library filenames are prefixed with the string "lib" . So
to create a static library named foo, name the file libfoo.a . 
When the linker sees an argument such as -lfoo , it automatically searches for a
file named libfoo.a in a predefined search path. In keeping with tradition, dynamic
libraries also use the libprefix, so the linker will search for either libfoo.a or lib-
foo.so .2.2 Build Tools 73
2.3 The Build Process
Now that you have an understanding of the tools used to create projects, let’s take
a step back and examine how these tools work together. To keep it simple, we willfocus on projects built with the C and/or C++ programming languages. C is mostcommon because it generally is more portable than C++. Although C++ is suitablefor most projects, developers who want to reach the widest range of target systemswill choose C. Believe it or not, there are even some programmers out there whodon’t know C++.
Although the GNU compilers are commonly available on Linux systems, they are
not necessarily required to build from source. Open source projects, and GNU proj-ects in particular, emphasize portability, so they don’t necessarily require GNU com-pilers or Linux. The same code that builds and runs on Linux can also build and runon Solaris, IRIX, Free BSD, and even Windows. Portability isn’t just a nice-to-havefeature for anal-retentive developers; it’s also vital to the survival of an open sourceproject. More platforms means more users and more developers looking at the code.That is a recipe for a better application.
2.3.1 The GNU Build Tools
The GNU build system is the de facto standard build tool for open source projects.It is a complex set of tools used by developers to generate source distributions thatare easy to build and install. A typical source distribution created with the GNUbuild tools includes a 
configure script, which is used by the end user to create a
Makefile . As an end user, you don’t need to know that these tools exist. You need
only remember three simple steps to build any GNU program:
$ ./configure
$ make$ make install
It isn’t always that simple. Sometimes, you will need to customize something to
meet your needs. Worse, any of these three steps could fail. I will focus on the GNUbuild tools from an end-user perspective. A complete understanding of the GNU buildtools is out of the scope of this book, but many fine resources on the Web can helpyou, as well as documentation that comes with the GNU tools distribution.
2.3.2 The configure Stage
The configure stage deals with all the messy details that are too complicated to
deal with in a Makefile alone. GNU build tools are designed to create tools that74 Chapter 2 • Building from Source
install on any POSIX-compliant system, so it makes very few assumptions about
your system. The configure script can take a long time to run, as it checks for
installed libraries and tools. It may also generate C header files to communicate sys-tem information to the compiler.
For some small projects, the developers may choose to provide a simple
Makefile instead of using the configure script. You can recognize this by the
presence or absence of a Makefile in the source archive. If there is no Makefile ,
there will be a file named Makefile.in that is read by the configure script to cre-
ate the Makefile .
The configure script looks at your system and determines whether you have the
necessary tools to compile the project, such as a working compiler. The GNU toolsused to create the 
configure script give the developer a great deal of flexibility to
check for all kinds of little details. This reassures you that the project will build beforeyou invest a lot of time trying to build it. It can be a drawback, because the 
configure
stage can take a considerable amount of time, particularly for smaller projects.
The configure script produced by the GNU build tools has several options that
are common to all projects. Developers can add more options, but there is norequirement that these be documented, which is unfortunate. This can make build-ing a project something of a black art, but it doesn’t need to be so.
Some of the basic options common to all 
configure scripts focus on fine-tuning
the installation of the program (Table 2-6).
TABLE 2-6 Common Options for the configure Command
Option Usage
--prefix Determines the root of the installation, typically /usr/local .
--bindir Determines the location of files that otherwise will be installed
in ${prefix}/bin . This directory is for executables that will
be run by regular users.
--sbindir Determines the location of files that otherwise will be installedin 
${prefix}/sbin . This directory is for executables that will
be run by administrators or system daemons.
--datadir Determines the location of ancillary data files; defaults to
${prefix}/share .
--libdir Determines the location of shared libraries; defaults to
${prefix}/lib .2.3 The Build Process 75
continues
TABLE 2-6 Continued
Option Usage
--mandir Determines the location to install manpages; defaults to
${prefix}/man .
--program-prefix Adds a prefix to executable files stored in ${bindir} and
${sbindir} . Sometimes, this is used to install multiple
versions of the same program, because it allows the use of a unique prefix for each version.
--program-suffix Like –-program-prefix except that it adds a suffix to the
program.
There are various versions of the GNU build tools in circulation, and the options
tend to vary slightly. To see what options are available with a particular project, youcan always type
$ ./configure -–help
In addition to the common options listed in Table 2-6, configure scripts always
allow user-defined options of the form
--enable-X
--disable-X--with-X--without-X (also –-with-X=no)
The Xcan be anything the developer wants, but these options are not always doc-
umented. To make matters more confusing, the configure script will accept any
Xeven if there is no such feature. configure –-with-fries , for example, runs
without errors on any project, but don’t open the ketchup just yet.
Consider the gccproject. If you download the source from the Free Software
Foundation, you will need to follow the same three steps I discussed earlier. The
configure step in particular has a large number of options that are not docu-
mented by the help function and are scarcely documented anywhere else. Someimportant options include the ability to enable only certain languages with the 
--enable-languages option. If you are looking for only a C and C++ compiler,
for example, you will want to configure it with --enable-languages=c,c++ .
Without this, gccbuilds in support for Fortran, Java, and whatever else that ver-
sion supports, which makes the build take longer and consume more space. You76 Chapter 2 • Building from Source
won’t find this in the README or any of the FAQs that come with the source,
although it is widely documented in other sources on the Web.
One way to check for yourself is to examine configure.in (sometimes
configure.ac ) and look for script code that uses variable names beginning with
enable_ or with_ . These are Bourne shell variables that are set by the configure
options --enable-X and --with-X . In the gcc configure.in script, for exam-
ple, you will find several references to variables named enable_languages ,
with_newlib , and others. Generally speaking, when these are not documented in
a README file or help message, they are not intended to be used by nondevelopers.
But sometimes, you have to improvise.
2.3.3 The Build Stage: make
The build stage is started by running make with no arguments. This builds all the
object files and links them into one or more programs. The default target in thiscase is a pseudotarget named all, but the GNU build tools create other pseudotar-gets that can be used on the command line (Table 2-7). Normally, you don’t needto know about these, but in the event that your build fails and you find yourself fix-ing source code, these are good to know about.
TABLE 2-7 Some Useful Targets in an automake-Generated Makefile
Target Usage
all Default target; builds all libraries and executables.
clean Removes object files and libraries that are built by the alltarget.
distclean Removes all files created during configure and make . Ideally,
what you are left with is only the files that were in the original
tararchive. After building the distclean target, you will not
have a Makefile and will have to run configure again before
you can rebuild.
mostlyclean Like clean , except that selected libraries are not removed,
including those that the developers determined are seldomchanged and should not need rebuilding.
install Installs programs in the directories determined when configure
was run. Be careful; there is not always an uninstall target.
uninstall If present, removes programs installed by the install target.2.3 The Build Process 77
When your build completes without errors, you are ready for the next stage in
the process, which is the install stage.
2.3.4 The Install Stage: make install
If you want to install a program for all to use, the default configuration usually issufficient. You will have to have root privileges to do so. The default location usu-ally is 
/usr/local/bin , which is relatively safe. It won’t conflict with or overwrite
system programs that usually reside in /usr/bin . Still, /usr/local is used by
many other programs, so it’s always wise to check the installation first by installingin a test directory that you own. To do this, you will have to re-run the 
configure
script with the --prefix option. One idea is to use ~/usr as the prefix, in which
case you would configure the package as follows:
$ ./configure –prefix ~/usr
This allows you to try out new software before making it publicly available to
other users on your system. This technique can also be useful if you are working ona multiuser machine on which you are not the administrator. If you want to installthe program just for yourself, perhaps this is all you need to do. 
You should use this technique to test the 
uninstall target to make sure that 
it does what it is supposed to do before you put files in a public directory. The
uninstall target is important. Without it, you can accumulate clutter in your sys-
tem that can interfere with other programs as time goes on. For simple projects,
install and uninstall are trivial, but for large projects, they can get quite com-
plicated. You should be very reluctant to install a program in public directoriesunless you are certain that the 
uninstall target works correctly.
2.4 Understanding Errors and Warnings
In this section, I discuss how to interpret errors and warning messages that comefrom the various build tools and how you can fix them. I look at some commonerrors that may not have an error or warning associated with them. 
Makefile s are
especially prone to this type of error, causing many a developer to struggle to under-stand why his build doesn’t behave the way it is supposed to.78 Chapter 2 • Building from Source
2.4.1 Common Makefile Mistakes
The language of dependencies and prerequisites that make uses is not always intu-
itive. The build flow is not sequential and not always obvious. Makefile rules can
be simple, but the interactions of rules and dependencies can get wildly compli-cated. Although understanding the basics I discussed earlier goes a long way, evenexperienced developers get tripped up once in a while.
2.4.1.1 Shell Commands
The default shell used by 
make in the command portion of a rule is the Bourne
shell. To be more specific, Makefile commands are executed using the command
listed in the SHELL variable, which defaults to /bin/sh . In theory, you could
change this to whatever shell you like, but no one does. That’s probably because allthe implicit rules are written for the Bourne shell. If you change the shell, you maynot be able to use the implicit rules.
Another good reason for not mucking with the default shell is less obvious. When
make executes the command portion of a rule, each line of the commands is
spawned as a new shell. So when you assign a shell variable on one line of your com-mands, it is forgotten on the next line. This makes it impossible to write a scriptunless you can fit the entire script on one line. Fortunately, one-line scripts are aBourne shell specialty.
10
Programmers run into trouble, however, when they try to compress as much stuff
as they can on one line. If it’s a script you want, create a script file and call it fromyour 
Makefile . You can write the script in the language you’re most comfortable
with; ideally, it will be more readable. Just make sure that your script returns a validexit status so that 
make will be informed when your script encounters an error.
2.4.1.2 Missing Tabs
It’s unfortunate that the designer of make decided to give an invisible character an
important meaning. The tab is required before all commands in the command por-tion of the rule. 
make uses this to differentiate among commands, targets, and vari-
able assignments.2.4 Understanding Errors and Warnings 79
10. Note that you can use line-continuation characters, but no newlines are passed to the shell. This can be
tricky to get right.
You cannot replace the leading tab with spaces; that’s an error. You may put spaces
after the tab but not before it. Because tabs are invisible, it’s not obvious when theyare replaced by spaces, as some text editors are apt to do. What is worse is the unin-formative error you get when this happens, as in the following 
Makefile :
all:
@echo Look Ma! No Tabs!
Here is what GNU make has to say:
$ makeMakefile:2: *** missing separator.  Stop.
Not the most helpful message, is it?
The problem is that there is no requirement to have commands following a tar-
get. make must infer whether the line following a target contains commands, a vari-
able assignment, or another target. The separator is what make uses to determine
the type of line it’s looking at. This could be a tab, colon, equal sign, etc. make
doesn’t try to guess what ought to be there, but when you see this error, 99 percent
of the time it’s a command line that hasn’t been prefixed with a tab.
That was lucky. Our command didn’t happen to have a separator, so at least we
got an error message. Consider this case, in which the command is missing a lead-ing tab but contains a different separator:
all:
env FOO="Look Ma! No Tabs!" printenv FOO
Guess what happens when you run this Makefile ?
$ makemake: Nothing to be done for 'all'.
The problem is that make saw the separator = and then determined that this is a
variable assignment. In case you’re wondering, the variable that was assigned isnamed 
$(env FOO) . Yes, variable identifiers can contain white space, which is why
you need those parentheses.
2.4.1.3 VPATH Confusion
Earlier, I introduced the VPATH feature of make . This feature is very useful for large
projects but can be a source of confusion. The main problem occurs when yourproject has name clashes, such as multiple source files with the same name (a bad80 Chapter 2 • Building from Source
idea, but it happens). When this happens, you can end up pulling in a different
module than the one you intended.
No matter what you set VPATH to, make always searches the current directory
before anyplace else. When make finds a match in the current directory, it does not
search any other directories. VPATH is always the second choice for make .
Another problem that can arise with VPATH is actually a C preprocessor issue. It’s
natural to assume that the C preprocessor will automatically search the currentworking directory for 
#include files, but the search path depends on the syntax.
When the filename appears in quotes, such as
#include "foo.h"
the preprocessor automatically searches the directory where the source file resides.Under normal circumstances, both files reside in the current directory, but with a
VPATH , the source modules may reside somewhere else. In this case, the preproces-
sor will search that directory, not the current one. If the #include filename appears
in brackets, such as
#include <foo.h>
the preprocessor searches the standard include directories as well as any directories
specified with -Iflags, but nowhere else. When you are using a VPATH , this may be
the preferable syntax, because you can exert more control over where the sourcecomes from.
Consider a case in which you have a name clash among your header files. If the
include statement uses quotes, the C preprocessor will always prefer the copy that
resides where the source file resides, regardless of the -Ioptions you use.11Suppose
that you have multiple builds of the same source code. The source code resides in adirectory named 
/src , and each build is in a separate subdirectory ( build1 ,
build2 , and so on). Each subdirectory could contain a file named config.h that
determines the options for each build. Each directory would also have a Makefile
with a VPATH that points to ../src . If the /src directory contains a file named
config.h , none of the other config.h files will be used. The C preprocessor will
always use the one that resides where the source resides.2.4 Understanding Errors and Warnings 81
11. Even GNU’s -nostdinc and -include options don’t change this behavior.
VPATH gets even more confusing when you have many paths, presenting many
opportunities for name clashes. This is where the GNU vpath syntax is preferable,
because it allows you to pick only specific source patterns from specific directories.
2.4.2 Errors during the configure Stage
In a project that uses the GNU build tools, the most likely step to fail is the con-
figure step. At least, this is the way it is supposed to work. The idea is to prevent
you from wasting time trying to build when you don’t have the required tools andlibraries. When the 
configure stage passes, you have a much higher confidence
level that your build will complete without errors. When it fails, it should tell youwhat you need instead of requiring you to guess.
A well-written 
autoconf script can provide a clear message indicating that you
need xyzto build this project and even where to find it. More typically, you will
see something like can'tfindxyz . If you want to build the project, you have lit-
tle choice but to track down xyzand install it on your system.
In large projects with many configurable options, certain development tools may
be required for one feature but not for others. In this case, it’s up to the developer todefine the 
configure script so that it correctly identifies the tools that are needed.
Because this requires human input, you can rest assured that there will be mistakes.When this happens, you are not likely to realize it unless you are intimately familiarwith the project. If you are not an expert in the project (or in 
autoconf ), it’s usu-
ally easier to appease the configure script than to try to debug it.
There are some occasions where configure complains that you don’t have xyz
when in fact you do. This can happen if xyzis located in an unexpected place.
Usually, configure does a good job of searching for these things. If a required tool
is in your path, it will find it. If you have required library or include files installedin a nonstandard place, you must inform the 
configure script. For some reason,
the only way to inform configure about nonstandard directories is via environ-
ment variables rather than command-line options. The environment variables usedfor this purpose are:
CPPFLAGS —indicates nonstandard include paths with additional -Iflags
LDFLAGS —indicates nonstandard library directories with additional -Lflags
Let’s look at an example. Suppose that you are an unprivileged user on a univer-
sity network, trying to build an open source project named Homer , which uses the82 Chapter 2 • Building from Source
GNU build tools. You run the configure script, and it informs you that it requires
a particular development library named Marge . You track down the source for the
Marge library—which, luckily, is another GNU build project. Because you are an
unprivileged user, you cannot simply install the Marge library using the default
installation settings. You have to install it under your home directory, so you chooseto install it in 
~/usr/local/lib . The entire process for building the Marge library
would look something like this:
$ configure –-prefix=~/usr
$ make$ make install
Now to build the Homer project, you have to inform Homer ’s configure script
where to find the Marge library. You do this with the LDFLAGS environment vari-
able, as follows:12
$ LDFLAGS='-L ~/usr/lib' ./configure
Typically, a development library provides header files that you need to build code
that uses the library. The compiler needs to know where to find these files. Giventhe prefix from before, 
Marge ’s include files would reside in ~/usr/include . You
should indicate this via the CPPFLAGS environment variable, as follows:
$ LDFLAGS='-L ~/usr/lib' CPPFLAGS='-I  ~/usr/include' ./configure
Notice that LDFLAGS contains options that are passed to the linker—in this case,
the -Loption. Likewise, CPPFLAGS contains flags used by the preprocessor.
2.4.3 Errors during the Build Stage
The job of the configure script is to check for all the required development tools
and libraries, and depending on the project, it may generate the Makefile . An
advantage of a project that uses the configure script to generate the Makefile is
that the Makefile is not touched by human hands, so in theory, it should be free
of syntax errors. You can identify such a project by looking at the distributionarchive. When the 
configure script generates the Makefile , the archive will not
contain a Makefile ; instead, it will have a file named Makefile.in .2.4 Understanding Errors and Warnings 83
12. Note that the space before the tilde is required for the shell to expand it properly. The presence of the
space requires the use of quotes to set the environment variables properly.
Errors during the build can be broken into two categories: errors in the
Makefile and errors detected during the build.
2.4.3.1 Errors in the Makefile
Although GNU build tools generate Makefile s that are unlikely to have syntax
errors, there are ways to introduce syntax errors into an automatically generated
Makefile . A developer can use the include keyword to insert text into the out-
put Makefile verbatim, for example. This presents an opportunity to introduce
syntax errors into the output Makefile .
If the project you are building is not using an automatically generated Makefile ,
you should consult any README files in the project to see what magic is required to
build the project. Sometimes, developers use nondefault targets to build the projector require you to specify a variable on the command line manually.
It’s usually safe to assume that the person packaging the source code made sure
that it built before posting it on the Web. If you can’t find a logical explanation fora syntax error in the 
Makefile , perhaps it’s an indicator of the quality of the rest of
the code. It may not be worth your time to continue in this case.
2.4.3.2 Errors Detected by make
When your Makefile is free of syntax errors, any build errors will be in the com-
mands section of the rules. make checks the return status of every command it exe-
cutes and stops when it encounters an error. Developers can choose to ignore errorsfrom specific commands, but this is the exception.
The errors you will see during a build can be broken down as shown in Table 2-8.I discuss compiler and linker errors in detail later; now I focus on the “other”
category.
TABLE 2-8 Build Errors by Category
Error Category Sources
Compiler errors Preprocessor errors, syntax errorsLinker errors Unresolved symbolsOther command errors Write permission errors, file not found, shell syntax errors,
latent syntax errors84 Chapter 2 • Building from Source
Shell syntax errors are not uncommon in Makefile s, especially when developers
try to cram as much script as they can into a single line. Detecting these errors canbe difficult. Consider this defective bit of script inside a 
make rule:
t1 t2 t3:
@if [ "$@"="t1" ]; then echo $@ is target 1; else echo $@ is not target 1;
fi
This combines three targets in a single rule— t1, t2, andt3—with no prerequi-
sites. A funny thing happens when you run this:
$ make t1 t2 t3
t1 is target 1t2 is target 1t3 is target 1
Can you spot the error in the script? Because this is not a lesson in scripting, I’ll
give you the answer: The problem is inside the brackets. The brackets are a shellalias for the 
test command, which requires white space between every operator
and tokens. Because there is no white space around the = in this example, the test
command sees one continuous string but no operators. In this case, test returnszero, which is what 
ifconsiders to be true .
Thanks to the @in front of the script, you won’t see this script printed on stdout
when the rule executes. You can change this behavior in two ways. One is to use the
-noption of make , which shows you a dry run—that is, it prints the commands but
does not execute them. Even commands hidden with @are printed, so the preced-
ing Makefile produces the following output:
$ make -n t1 t2 t3
if [ "t1"="t1" ]; then echo t1 is target 1; else echo t1 is not target 1; fiif [ "t2"="t1" ]; then echo t2 is target 1; else echo t2 is not target 1; fiif [ "t3"="t1" ]; then echo t3 is target 1; else echo t3 is not target 1; fi
Now you can see the offending script, which might otherwise be embedded
inside several variables and difficult to find inside the Makefile . By using the -n
option, you can see the text that gets passed to the shell verbatim. For tricky prob-lems, you can also redirect this output to a file and run it as a plain script. This ishelpful when your compile lines have grown huge, and you need to track down anoption that is causing the compiler grief. You can use the output to create a scriptfile that you can tweak until you get it right. When the problem is fixed, you canmake that same change inside the 
Makefile .2.4 Understanding Errors and Warnings 85
Another technique for debugging shell commands from a Makefile is to change
the shell options to include -x. This prints out commands as they are executed. You
can do this on the command line by modifying the SHELL variable as follows:
$ make SHELL="/bin/sh -x" t1 t2 t3
+ [ t1=t1 ]+ echo t1 is target 1t1 is target 1+ [ t2=t1 ]+ echo t2 is target 1t2 is target 1+ [ t3=t1 ]+ echo t3 is target 1t3 is target 1
This causes the shell to print out commands as they are being executed. It shows
you only the parts of the script that are being executed, minus the shell keywords.In this example, the test inside the brackets tends to stand out a little more, whichmay clue you in faster than looking at the raw script. Every case is different, so ifone technique doesn’t work for you, try the other.
2.4.4 Understanding Compiler Errors
Compilers have gotten better over the years at reporting errors and warnings. In theold days, a simple thing like a missing semicolon in a C program would produce atotally misleading error message. Modern C/C++ compilers are much better at pro-ducing error messages that can be parsed automatically. 
Let’s start with a trivial example of a defective program:
1 void foo()
2 {3       int x,y,z;45       x=16       y=2;7       z=3;8 }
The assignment in line 5 is missing a semicolon. The basic error output from the
compiler looks like the following:
$ gcc -c foo.c
foo.c: In function 'foo':foo.c:6: error: parse error before "y"86 Chapter 2 • Building from Source
The message format is designed for easy parsing from within an Integrated
Development Environment (IDE) or from a text editor, such as Emacs and Vim.The error message consists of three parts separated by colons: the filename, the linenumber, and a textual message. This is how IDEs and text editors can direct you tothe offending source code when it detects an error. The output is human readable,so you don’t need any tools to parse this output. As you noticed earlier, the error ison line 5, but the compiler sees an error on line 6. That’s because the compilercouldn’t determine that the semicolon was missing until line 6. 
Interestingly enough, the same code compiled by the C++ compiler produces a
more helpful error message:
$ g++ -c foo.c
foo.c: In function 'int foo()':foo.c:6: error: expected ';' before "y"
Well-written C code should compile fine with the C++ compiler, which can pro-
vide better warnings and strictly enforce the use of prototypes. You could do a quickswitch with the offending object and see whether you get a better error message. Forexample:
$ make CC=g++ foo.o
Open source projects usually don’t use the C++ compiler to compile C code. One
reason is that there is no guarantee in the C++ standard that code written in C++can be called from C. This rules out using C++ to compile development librariesthat will be used by C clients. There’s also an implicit expectation that code writtenin C should require only a C compiler.
Errors in the source have a tendency to cascade down, so a single error can cause
the compiler to report errors on subsequent lines of code that otherwise would becorrect. A best practice when tracking down errors is to start with the first errorreported in the module and work your way down. Often, the first error is the onlyerror you have to fix.
A classic example of this is the missing include file, which is something you will
encounter often when you build open source code. Every development library usesheader files to define the interface, so if you have the wrong version of a requiredlibrary or none at all, you are likely to encounter a missing include file error.
When the C compiler encounters a missing include file, it produces only one
error message indicating a missing include file. This is often followed by dozens or2.4 Understanding Errors and Warnings 87
even hundreds of error messages caused by the missing declarations that otherwise
would be found in the missing include file. The following code illustrates the error:
1 // foo.h contains the following two lines:
2 // typedef int type1;3 // typedef int type2;45 #include "foo.h"67 void foo()8 {9      type1 x;
10      type2 y;1112      x = 1;13      y = 2;14 }
If you compile this file without access to foo.h , the compiler spews out the fol-
lowing errors:
$ gcc -c foo.c
foo.c:5:17: foo.h: No such file or directoryfoo.c: In function 'foo':foo.c:9: error: 'type1' undeclared (first use in this function)foo.c:9: error: (Each undeclared identifier is reported only oncefoo.c:9: error: for each function it appears in.)foo.c:9: error: syntax error before "x"foo.c:10: error: 'type2' undeclared (first use in this function)foo.c:12: error: 'x' undeclared (first use in this function)foo.c:13: error: 'y' undeclared (first use in this function)
If you start with the first error, you see that it reports that it can’t find foo.h .
There is no point in debugging any further errors until you resolve this one. In thiscase, you know that the only error is that 
foo.h is missing, but the compiler doesn’t
know that. The compiler tries to continue as far as possible in the presence of errors.In practice, a preprocessor error is worth stopping for.
2.4.5 Understanding Compiler Warnings
Sometimes, warnings are a welcome opportunity to improve your code. At othertimes, they can be just a nuisance. In the old days, programmers had to run a pro-gram called 
lint just to get warnings. lint would examine your source code and
produce a list of warnings that read like a phone book. The fact that lint was an
extra step meant that the user probably was committed to following up on the88 Chapter 2 • Building from Source
warnings. Most of these same warnings can now be generated by compiler, so pro-
grammers tend to take them for granted or to view them as a nuisance. Perhapsthat’s why warnings are disabled by default in the GNU compiler.
One excuse for not turning on warnings is as old as the 
lint program. The com-
plaint is that for every warning message that points to a serious problem, there areten or more messages that are just nitpicking. The nitpicky warnings include thingslike unused variables, unsigned versus signed integer comparisons, and 
printf for-
mats. Many warnings raise portability concerns, which leads some programmers tothink that these warnings don’t apply to them. Even if you don’t ever plan to supportanything but Linux on IA32, you may want to upgrade your compiler at some point.The same portability issues that occur when moving between processors or operat-ing systems can be problems when upgrading from one version of a compiler to thenext. Warnings are not hard to fix, and fixing them makes your code more robust.
The easiest way to turn on warnings with the GNU C/C++ compiler is to use the
-Wall option, which turns on a select set of warnings chosen by the GNU team.
These represent a conservative set of warnings, none of which will generate an error.There are dozens of command-line options to control warnings that the GNU com-piler supports. Virtually every individual type of warning produced with 
-Wall can
be disabled, and additional warnings can be enabled as well.
Typically, projects built with the GNU build tools ship with warnings turned off.
Perhaps this is because the options to turn on warnings are not standard across com-pilers. This is unfortunate, but you can turn on warnings in a project that uses theGNU build tools with the following 
configure command:
$ CFLAGS=-Wall ./configure
You usually can assume that a mature project compiled without warnings when
it was released. It’s possible that although it compiled cleanly for IA32, there couldbe warnings when it compiles for x86_64. Also, different versions of the compilerwarn about different things, so you may see new warnings pop up when you com-pile with a newer compiler than the developers are using. If you are digging into thesource and trying to fix a bug, turning on warnings is a good place to start.
2.4.5.1 Warnings: Implicit Declarations
With 
-Wall , the compiler warns you when it encounters implicitly defined vari-
ables or functions in C code. (In C++, this is simply an error.) Most experienced2.4 Understanding Errors and Warnings 89
programmers know not to compile code with these warnings. If you see one of these
warnings, it is probably a sign of some very old legacy code that has not beentouched in ages or of an inexperienced programmer at work.
An implicitly defined variable is a variable that appears on the left side of an
assignment expression before it appears in a declaration. Such a variable is implic-itly defined to be an 
int. T raditional C allows this for global variables, and gccis
very permissive about it. For example:
$ echo "x=1;" > foo.c
$ gcc -c -Wall foo.cfoo.c:1: warning: type defaults to 'int' in declaration of 'x'foo.c:1: warning: data definition has no type or storage class
Even with the -ansi option, this still compiles. The only way to force an error
for implicit variable declarations is to use the -pedantic option, but this could
cause the compiler to complain about other code. Use this option carefully.
Likewise, an implicitly defined function is one that is used without a preceding
function declaration or prototype. When a function is defined implicitly, the com-piler assumes that it returns an 
intand makes no assumptions about the number
or type of arguments the function takes. This is equivalent to the following func-tion declaration:
int foo(); // Function declaration – no info about arguments
Note that a function declaration is not the same as a function prototype. A func-
tion declaration is a legacy that predates the ANSI standard. It informs the compilerabout the return type, but tells the compiler nothing about the arguments. Whenthe compiler sees a function with no prototype, it accepts any number of argumentsof any type. This allows legacy C code to compile but has no place in modern code.A prototype specifies the exact number and type of arguments for a function inaddition to its return value.
The problem is that either a function prototype or a function declaration will sat-
isfy the C compiler when using 
-Wall . If you want to see warnings when a func-
tion is used without a prototype, use -Wstrict-prototypes . When the compiler
encounters a function declaration, gcc(3.4.4) prints the following message:
foo.c:2: warning: function declaration isn't a prototype90 Chapter 2 • Building from Source
Now that you know all about function declarations and prototypes, you will
actually understand what this warning means when you see it.
2.4.5.2 Format Warnings
One of the checks turned on with the -Wall flag is format checking. The GNU
compiler checks the format strings of the printf family of functions, looking for
the correct number of arguments and verifying that the argument type matches theformat. Format checking is done for all the functions listed in Table 2-9.
Format checking can uncover very-hard-to-find errors. The 
%sformat in a
printf statement, for example, can cause a segmentation violation if you give it an
invalid pointer. The compiler warns you, for example, if you inadvertently use a %s
format with an argument that isn’t a char * . This kind of mistake is easy to make,
particularly with format strings that have many arguments. The following trivialexample illustrates the warning:
1 #include <stdio.h>
2 void foo(int x)3 {4       printf("%s", x);5 }
When you compile this with -Wall , you see the following message:
$ gcc -c -Wall foo.cfoo.c: In function 'foo':foo.c:4: warning: format argument is not a pointer (arg 2)2.4 Understanding Errors and Warnings 91
TABLE 2-9 Functions That Have Format Checking
printf Variants scanf Variants
printf vprintf scanf vscanf
fprintf vfprintf fscanf vfscanfsprintf vsprintf sscanf vsscanf
snprintf vsnprintf
This tells you that the format calls for a pointer, but the argument (in this case,
the second argument to printf ) is not a pointer. A %sformat requires a char *
pointer specifically, If you pass the wrong kind of pointer—for example, int * —
you will see a warning as well. Such warnings are core dumps waiting to happen.
There are similar messages for other format mismatches, but not all such mis-
takes can crash your code. In particular, the IA32 architecture rarely crashes whenthe wrong type of argument is passed to an integer format. The IA32 is very per-missive about data alignment; typically, the worst thing that happens is that you seegarbled output. Other CPU architectures are not so forgiving, and mismatching thetypes in your 
printf formats can cause your code to crash.
It’s not unusual to see format warnings in code, especially if you are turning
warnings on for the first time. IA32 programmers can be complacent and mayignore warnings that they think don’t apply to them. In particular, many have got-ten used to substituting 
longint and inttypes as equivalent. The compiler warns
when you use pass a long integer to a %dformat (the correct format for a long inte-
ger is %ld). On an IA32, this is not a problem, because a long is the same size as
an int. On a 64-bit machine, these are not the same size. On the x86_64 platform
(for example, Opteron and Xeon/EM64T), an intcontinues to be 32 bits, and a
long is 64-bits. This is becoming more important as 64-bit architectures become
more popular.
Although printf is a relatively safe function to call, scanf is much more dan-
gerous. Whatever the architecture, a bad scanf argument can crash your program.
Because the arguments are pointers into program memory, a type mismatch herecan result in a buffer overrun and corrupt memory. All warnings about 
scanf for-
mats should be taken seriously.
T wo other options can be added for format checking beyond those turned on
by -Wall :
•-Wformat-nonliteral —This issues a warning when the compiler encoun-
ters a format string that is a variable instead of a string literal. This meansthat the format cannot be checked. Worse, because the format can change at runtime, this presents opportunities at runtime for bugs that can crash your code.
•
-Wformat-y2k —Applies to the strftime and strptime functions, which
use a format syntax similar to that of printf . You will see a warning when-
ever it sees a two-digit year format.92 Chapter 2 • Building from Source
2.4.5.3 Other Warnings the GNU C Compiler Emits
In addition to format warnings, the GNU compilers can produce many other warn-
ings. Each type of warning can be turned on individually or turned on whenselected features are enabled:
•Null pointer arguments— Check pointer arguments of selected functions for
null at compile time. This does not guarantee that null arguments will not bepassed at runtime. As of 
glibc 3.4.4, there is almost no checking for this in
the C/C++ standard library.
•Missing parentheses— Warn you when an assignment is used in a Boolean
context, such as an ifstatement. Often, this is unintentional. For example:
if ( x = y ) //versus...
if ( x == y )
Line 1 is both an assignment and a Boolean test, but it could also be a typo-
graphical error. Perhaps line 2 is what you intended. This is a very commonform of typographical error, so the compiler will warn you about it. An extraset of parentheses clears up the ambiguity, as follows:
if ( (x = y) )
•Missing braces— Warn you when your if/else statements do not use braces.
This can be a source of errors, as follows:
1 void foo(int x, int y, int *z)
2 {
3       if ( x == y )4               if ( y == *z ) *z = 0.0;5       else6               *z = 1.0;7 }
The indentation in this example suggests that the else statement on line 5
belongs to the ifstatement on line 3, but the lack of braces dictates that this
else statement belongs to the ifon line 4. The warning suggests that you use
braces in case this is an error:
$gcc -c -Wall foo.c
paren.c: In function 'foo':paren.c:3: warning: suggest explicit braces to avoid ambiguous 'else'2.4 Understanding Errors and Warnings 93
To correct this, add braces to each ifand else clause, as follows:
1 if (x == y) {
2     if (y == *z) {3         *z = 0.0;4     }5 }6 else7 {8     *z = 1.0;9 }
•Uninitialized variables— Warn you when variables may be used before being
initialized. These can be false alarms, but the problems are easy to spot whenthe compiler points them out. You see these warnings only when compilingwith optimization. For example:
1 int foo(int y)
2 {3       int x;4       if ( y > 0 ) {5               x = 1;6       }7       return x;8 }
Here, the variable xis not initialized if y <= 0 . In this case, the function
returns an undefined value. Compiling without optimization produces nowarning, but adding optimization (with 
-O1, -O2, and so on) produces a
warning, as follows:
$ gcc -Wall -c -O2 uninit.c
uninit.c: In function 'foo':uninit.c:3: warning: 'x' might be used uninitialized in this function
The code is just as defective without optimization, but the compiler doesn’t
have access to the same information when it isn’t optimizing. The informationit needs comes from analyzing the code flow, which is part of the optimizationprocess.
The list of warnings the 
gcccompiler is capable of showing is long. Most warn-
ings can be enabled and disabled on an individual basis. It is worth studying the gcc
manual to learn more.94 Chapter 2 • Building from Source
2.4.5.4 C++ Specific Warnings from the GNU Compiler
C++ has increased type safety over C, and many of the warnings in C (such as miss-
ing prototypes) are errors in C++. Despite this, there are plenty of new and object-oriented ways to screw up your code. The GNU compiler provides several warningsspecific to C++. Some notable warnings that are turned on with 
-Wall include the
following:
•Nonvirtual destructors— Warn you when a polymorphic class has a nonvir-
tual destructor. This can be a source of runtime errors due to memory leaksand resources not being released as a result of the wrong destructor beingcalled. For example:
1 class Base {
2      char *m_ptr;3 public:4      Base() : m_ptr(0) { m_ptr = new char[1024]; };5      virtual char *ptr() { return m_ptr; };6      ~Base() { delete [] m_ptr; } ;7 };89 class Foo : public Base {
10      char *foo_ptr;11 public:12      Foo() { foo_ptr = new char[1024]; };13      virtual char *ptr() { return foo_ptr; };14      virtual ~Foo() { delete [] foo_ptr; } ;15 };
$ gcc -Wall -c polymorph.cpp
polymorph.cpp:1: warning: 'class Base' has virtual functions but non-virtual destructor
The destructor in class Base must be declared virtual, or it will never be called
when we delete a pointer of type Foo * . Note the fact that the derived class
destructor ~Foo is virtual does not help. The Base class is still defective.
•Reordered initializers— Warn you when you define your initializers in a dif-
ferent order from the order in which they were declared. The standard calls forthe compiler to initialize members in the same order in which the membersare declared. The order in which the initializers appear in the code is irrelevant,2.4 Understanding Errors and Warnings 95
which can be misleading. Here’s an example of how the initializers can fool
you into creating a bug:
1 struct Foo {
2      int m_two; // declaration3      int m_one;4      Foo( int one )5      : m_one(one), // initializer6        m_two(m_one+1) {};7 };89 #include <iostream>
1011 int main(int argc, char **argv[])12 {13      Foo f(1);14      std::cout << f.m_one << std::endl;15      std::cout << f.m_two << std::endl;16 }
$ gcc -c -Wall order.cpp
order.cpp: In constructor 'Foo::Foo(int)':order.cpp:3: warning: 'Foo::m_one' will be initialized afterorder.cpp:2: warning:   'int Foo::m_two'order.cpp:6: warning:   when initialized here
The initializers appear on lines 5 and 6 above. If you look at only the two ini-
tializers, you may see nothing wrong. Look again, and you realize that m_two
was defined first and therefore is initialized first. This is a problem, because
m_two is initialized using the value of m_one , which is uninitialized when
m_two is initialized. To solve the problem, you must reorder the declarations
so that m_two is declared after m_one .
•Deprecated features— Emit warnings for modules that use deprecated features
such as using old-style Standard Template Library (STL) headers (for example,
iostream.h instead of <iostream> ). The warning you will see looks like the
following:
warning: #warning This file includes at least one deprecated or antiquated
header. Please consider using one of the 32 headers found in section17.4.1.2 of the C++ standard. Examples include substituting the <X> headerfor the <X.h> header for C++ includes, or <iostream> instead of thedeprecated header <iostream.h>. To disable this warning use -Wno-deprecated.96 Chapter 2 • Building from Source
This warning pops up even when you compile without warnings enabled. If
you follow the suggestion, you will change the #include statements to use the
new-style headers—but not so fast. Along with the new headers comes muchstricter namespace enforcement. In particular, any references to symbols innamespace 
stdwill not compile unless you add the appropriate namespace
qualifier or a using statement. You are likely to see this only in legacy code
that predates the C++ standard or in code you may be porting from a systemwith a more permissive C++ compiler. You have three choices here:
1. Easiest—Ignore it and/or turn it off using 
-Wno-deprecated .
2. Easy—Fix the offending headers and add the statement using namespace
stdto the module body (but not in any header files).13
3. More work—Fix the offending headers and use appropriate namespace
qualifiers.
•Incompatible ABI —This warning is not part of -Wall but is turned on with
the -Wabi flag. ABIstands for Application Binary Interface , and it is the con-
vention used for calling functions that allows multiple programming languagesto live in the same program. You may be writing a C++ program that calls alibrary written in Fortran, and even though you don’t have a Fortran compiler,your program works. This is possible because the Fortran compiler follows aparticular ABI. The ABI tells the compiler how to call a function and how topass arguments. 
The ABI is unique for each combination of processor and operating system,
because each has unique requirements and capabilities. Most procedural lan-guages have no issues conforming to a common ABI, so ABIs typically are lan-guage neutral. C++, however, is sufficiently complicated that it requires theABI to be extended in ways that are unique to the C++ language. The lack ofa standard ABI requires each compiler to make arbitrary extensions to the CABI to support C++. As a result, for example, you may not be able to link codecreated by GNU C++ with a library compiled by a commercial vendor because2.4 Understanding Errors and Warnings 97
13.Never put using namespace std in a header file. This pollutes the namespace of any module that uses
it and can cause difficult-to-find bugs.
the ABIs may not be compatible. For that matter, you cannot link C++ code
compiled with version 3. xof the GNU compiler with a library created with
version 2. x, because the ABI changed between those two versions.
Today, there still is no universally standard C++ ABI, but GNU follows a pub-
lished ABI that allegedly is vendor neutral.14As of g++ 3.4.4, the GNU com-
piler itself has some issues with this common ABI. This warning informs youwhen your code has run into one of these issues. Here’s an example from theg++ 
info page:
$ cat <<EOF > foo.cpp
struct A {};
struct B {
A a;virtual void f ();
};EOF$ gcc -Wall -Wabi -c foo.cppfoo.cpp:4: warning: 'B::a' contains empty classes which may cause baseclasses to be placed at different locations in a future version of GCC
If you see this warning, it means that you may have issues if you try to link
with C++ libraries that are compiled with a third-party compiler or perhaps aslightly different version of g++.
Finally, there is one more C++ warning feature worth mentioning: The 
-Weffc++ flag warns when your code violates style guidelines from Scott Meyer’s
Effective C++ book. You don’t need the book to learn from the warnings. T ry this
feature on your own code when you have time to improve your skills. This producessome very informative warnings about your coding style that could lead to bugsdown the road.
2.4.6 Understanding Linker Errors
Given a correctly formatted linker command line, the most likely linker error youwill encounter is a missing symbol. A missing symbol can be a function or a global98 Chapter 2 • Building from Source
14. www.codesourcery.com/cxx-abi/abi.html
variable that an object file refers to but does not declare. This can happen, for
example, when you try to build a project using an incompatible version of alibrary. A missing symbol is reported as an undefined reference and looks like thefollowing:
$ g++ -Wall -o main main.o -lfoo
main.o(.text+0x11): In function 'main':: undefined reference to 'bar()'
When an undefined reference occurs in a C program, it means that you are miss-
ing a library or object file on your link line or that you have an incompatible ver-sion of the library that doesn’t define the particular symbol.
Undefined references can also show up with C++ programs and libraries because
of a difference in name mangling between your compiler and the one that built thelibrary. Name mangling is a technique that C++ compilers use to rename functions
based on their signature. Because the linker knows nothing about C++, the com-piler must append data to function names to give them unique signatures that thelinker can understand. Typically, the compiler appends extra characters to the func-tion names to indicate the number and types of arguments.
When this happens, it can be frustrating, because you usually see an unresolved
symbol like the preceding example. You don’t have any clue that the name man-gling is incompatible. The 
nmcommand can help here. Using the above example,
you can see what the mangled name of bar() looks like by using the following
command:
$ nm -u main.o | grep bar
U _Z3barv
The string _Z3barv is how g++ 3.3.5 mangles the function signature void
bar(void) . If libfoo is compiled with a different incompatible compiler, the sig-
nature might be completely different. This is due to the fact that there is no stan-dard way to mangle names in C++. If you installed a binary package that was builtwith an incompatible compiler, you may see such errors. Unfortunately, the onlyway to resolve this is to compile both your code and the library with the samecompiler.2.4 Understanding Errors and Warnings 99
Another thing to be aware of is the fact that the linker is picky about the order
of libraries specified on the linker command line. I will illustrate this by creatingtwo libraries and one main module, as follows:
main.c
1 extern void two();2 int main(int argc, char *argv[]) { two(); }
one.c
1 void one(void) { }
two.c
1 extern void one(void);2 void two(void) { one(); }
Now I create two libraries and try to link as follows:
$ ar -clq libone.a one.o$ ar -clq libtwo.a two.o$ gcc -o main main.c -L. -lone -ltwo./libtwo.a(two.o)(.text+0x7): In function 'two':/home/john/src/linker/two.c:5: undefined reference to 'one'collect2: ld returned 1 exit status
In this example, the function two() calls one() , and these functions reside in
libtwo.a and libone.a , respectively. The linker sees libone.a first, because it is
the first -loption on the command line. At this point, it is looking to resolve the
reference to two() from main() . Because there are no references to one() yet, the
linker assumes that it is safe to discard the module containing one() . Next, it
processes libtwo.a and resolves the reference to two() from main() . But now 
it has an unresolved reference to one() . Because it has already discarded that mod-
ule, the link will fail with an unresolved reference to one() .
The solution is simple. Just switch the order of the libraries on the command line
as follows:
$ gcc -o main main.c -L. -ltwo -lone
Now the linker is satisfied. The dependencies are resolved from left to right—that
is, the libraries on the left require the libraries on the right.
2.5 Summary
This chapter introduced the tools used to build open source code. The make tool is
the most commonly used among open source projects written in C and C++. Forthis reason, I devoted quite a bit of coverage of 
make and how to use it.100 Chapter 2 • Building from Source
2.5.1 Tools Used in This Chapter
•configure —a script that comes with each GNU project, which generates the
Makefile to be used by make .
•cons —a build system based on the Perl scripting language that attempted to
replace make .
•make —the most common UNIX build tool for C and C++ code.
•scons —an attempt to reimplement cons in the Python scripting language
and overcome cons ’s shortcomings. 
2.5.2 Online References
• www.gnu.org/software/autoconf—the GNU autoconf project used to dis-
tribute GNU source
• www.gnu.org/software/make—the GNU home page for the make project
• www.python.org—the home page for the Python scripting language• www.scons.org—the home page of the 
scons tool2.5 Summary 101
This page intentionally left blank 
3.1 Introduction
There’s an old joke that goes something like this: “Programmers shouldn’t docu-
ment their code. It should be as difficult to use as it was to write.” Intentional ornot, the fact is that open source code often lives up to these expectations.
Although most tools do a good job of providing documentation and making it
easy to find, many don’t. Sometimes, you just need to know where to look.
3.2 Online Help Tools
An important innovation of UNIX was the use of online1help. This was a welcome
alternative to keeping a shelf full of printed manuals at your disposal, which was
3
103Finding Help
1. The term online here refers to documentation in electronic format as opposed to hard copy, not the
modern definition that implies the Internet.
also common at the time. The manprogram was, and still is, the main interface for
online help. The GNU project, looking for a more flexible online format, inventedthe Texinfo format, which generates the documentation used by the 
info program.
Some tools may neglect to provide manpages or info pages, and instead provide
some plain text or other format tucked away in the distribution, just waiting for youto find it.
Although 
manand info are the formats of choice for command-line tools, many
GUIs choose to use their help menus as the only source of online documentation.
3.2.1 The man Page
The idea of viewing documentation in a text window may seem archaic in thesedays of HTML and PDF , but there are advantages to reading documentation in aterminal window. This is especially true if, like many Linux developers, that is whereyou do most of your work. Getting the information from a 
man page is much
quicker than opening a browser or a PDF viewer. When you want answers fast, the
manpage is what you want.
man pages have a concise format that is meant to be read in a linear fashion.
Simple tools can get by with a single manpage as their only source of documenta-
tion; more complicated tools and programming libraries rely on a suite of manpages
for documentation. Perl even created its own manual section containing hundredsof pages covering various aspects of Perl.
In the past, 
manpages were limited to ASCII (or ISO-8859) characters, which
limited the selection of languages in which they could be written. This was largelydue to the limitations of the text terminals used to display them. In the old days,text terminals were capable of handling only 7- or 8-bit character encodings, andmemory constraints limited the number of available fonts. Those dumb terminalsare largely a thing of the past. GUI-based terminals such as 
xterm are capable of
handling many fonts and character encodings with no issues. The tools that create
manpages allow more of a selection of character encodings, such as UTF-8, so man
pages can now be written in any language.
manpages are written in an ancient markup language called troff . The troff
text-formatting language, although quite old, is still very powerful. Like HTML,
troff output is formatted to fit the device on which it is being presented. So in
addition to viewing in a text window, the output can be formatted for printout or104 Chapter 3 • Finding Help
transformed into HTML or PDF . To format a manpage for PostScript printout, for
example, you can use the -toption as follows:
$ man -t man | lp -Pps
You can pipe this output to any tool that understands PostScript to manipulate
the output any way you like.
There are two flavors of manin use in various distributions. Red Hat and many
other RPM-based distributions use the traditional manprogram, whereas Debian-
based distributions use a package named man-db . The difference between the two
is primarily in the database that is used to index and catalog the manpages. The
man-db approach has some advantages over the traditional manpage database, but
for the most part, both sets of tools behave the same way.
3.2.2 man Organization
The Linux manual is broken up into sections. This follows the Filesystem HierarchyStandard,
2which, among other things, specifies the contents of each mansection. A
summary of these sections is listed in Table 3-1.
TABLE 3-1 Linux Manual Sections
Section Description
1 User commands available from the shell2 System calls available to programs via library functions3 Library functions available to programs4 Devices available in 
/dev directory
5 Miscellaneous system files (for example, /etc )
6 Games, if any7 Miscellaneous information
8 Commands available to administrators3.2 Online Help Tools 105
2. www.pathname.com/fhs
The division of the manual into sections allows manpages to avoid name clashes.
A command named sync is documented in section 1, for example, and a function
named sync is documented in section 2. Both manpages are named sync. If they
weren’t documented in different sections, the system would have to resort to someodd naming conventions to distinguish between the two. Occasionally, you have tojump through a few hoops to get the 
manpage you want. If you want to see the man
page for the sync command, the following will do on most systems:
$ man sync
If you want the manpage for the sync function , you need to know that sync is
a system call and that it is documented in section 2. To look at this specific man
page, you specify the section number before the page name, as follows:
$ man 2 sync
Another confounding factor is that some distributions take the liberty of creat-
ing their own sections. Perl is one example mentioned earlier. Another is section 3p,which comes with Fedora. This contains POSIX functions (hence, the p). This sec-tion also contains a 
manpage for the sync function, which is virtually identical to
the manpage in section 2. If you are not sure, and you just want to see all the man
pages the system knows about, you can type
$ man -a sync
This brings up the manpages in order, and each time you press q to exit the man
page, you are presented with the next matching manpage.
The convention for referring to a manpage in a specific section is to put the sec-
tion number in parentheses following the page name. The sync function in section
2, for example, would be referenced as sync(2) , whereas the sync function in sec-
tion 3p would be referenced as sync(3p) . This notational convention is used
throughout manpages, and I follow this convention in the footnotes of this book.
You might expect that when the section is not specified, each section is searched
in sequence. But most distributions choose to search for commands before func-tions. That means that 
mansearches section 8 (System Administration Commands)
before section 3 (Programming Libraries). If you are a system administrator (andwho isn’t?), that makes sense. But as a programmer, if I am writing a socket programand need the 
manpage for the accept function located in section 3, typing man
accept will give me the manpage for the accept command from the Common106 Chapter 3 • Finding Help
UNIX Printer System package documented in section 8. If you are like me, you
probably don’t remember which section is which off the top of your head. That iswhere the 
whatis command comes in handy:
$ whatis accept
accept (8)           - accept/reject jobs sent to a destinationaccept (2)           - accept a connection on a socket
This shows you the manpages in the order in which they are found. In this case,
it’s apparent that section 8 is searched before section 2. It also shows you that the
manpage you are looking for is in section 2.
The manpage search order is determined in a system configuration file, which varies
based on installation. Fedora and Ubuntu (Debian-based) use /etc/man.config ,
whereas Knoppix (also Debian-based) uses /etc/manpath.config . The mandb
version allows you to override the settings for yourself in ~/.manpath . The tradi-
tional package, however, does not allow you to override the defaults without a com-mand-line option or environment variable.
The command-line manual tools leave a few things to be desired. It would be
nice, for example, to browse section 3 of the manual. Unfortunately, the command-line tools do not allow casual browsing of the manual; neither do they allow yousimply to list all the entries in a particular section. The old 
xman tool, which was
part of the original X11 distribution, would let you browse by section, but mostnewer distributions no longer include this tool. One tool that will let you browsethe 
manpages by section is part of the KDE project. The khelpcenter program
allows you to browse not only KDE documentation, but the plain old manpages as
well, as you can see in Figure 3-1.
You might be asking, “What about GNOME?” As of this writing, the 
gnome-help tool does not support browsing of the manpages.
3.2.3 Searching the man Pages: apropos
If you can’t browse the manuals, the next-best thing you can do is search them.There are two basic tools for this purpose: 
apropos and whatis . apropos is an
unusual name for a UNIX command, not just because it has more than three let-ters, but also because this is a word most native English speakers haven’t seen sincetheir last high-school vocabulary quiz. The word apropos means relevant , which is
the idea behind the command. You give a keyword, and it comes up with relevantresults (ideally).3.2 Online Help Tools 107
Unfortunately, the apropos command searches only the NAME section of the
manual entries, which contains only a brief one-line description of the topic. So ifyour keyword doesn’t show up in the 
NAME section, apropos won’t help. Only one
keyword at a time is searched. So if you specify two keywords, you get all thematches for the first keyword, followed by all the matches for the second keyword.By default, matching occurs anywhere in the text, so the word 
manmatches the
words manual, command, etc., as well as the word man.This is not a search engine.
The results you get may be anything but apropos.108 Chapter 3 • Finding Help
FIGURE 3-1 Using khelpcenter to Browse the manSections

You also have to be careful that your keyword is not too restrictive. Searching for
the word compression, for example, ought to show you something about compres-
sion utilities, but when I type aproposcompression , the results are missing two
of the most popular ones: bzip2 and gzip . Here are the results from Fedora
Core 4, for example:
$ apropos compression
Compress::Zlib       (3pm)  - Interface to zlib ...pbmtopsg3            (1)  - convert PBM ...SSL_COMP_add_compression_method (3ssl)  - handle ...zlib                 (3)  - compression/decompression ...
The problem is that the term compression does not match the words compress and
compressor , which, as it turns out, are the words used to describe gzip and bzip2 ,
respectively. So our keyword search comes up with nothing. You can see this sum-mary for yourself with the 
whatis command as follows:
$ whatis gzip bzip2
gzip (1)             - compress or expand files
bzip2 (1)            - a block-sorting file compressor , v1.0.2
The traditional version of apropos , found in Red Hat distributions, takes no
options. The mandb version of apropos , found in Debian distributions, takes sev-
eral, including options to restrict matches to exact matches or whole words. Bothversions allow you to use regular expressions.
3For example:
$ apropos 'mag[tn]'
mt (1)               - control magnetic tape drive operationmt-gnu (1)           - control magnetic tape drive operationrmt (8)              - remote magtape protocol modulermt-tar (8)          - remote magtape protocol modulexmag (1x)            - magnify parts of the screen
The expression mag[tn] matches magtape and magnify , so your search results
include anything that has either word. The man-db version of apropos found in
Debian distributions allows you to restrict output to exact matches with the -e
option. This allows you, for example, to look for the word compress without also
matching the word compressor . Table 3-2 shows a summary of features by package.3.2 Online Help Tools 109
3. If you need a primer on regular expressions, see regex(7) in the manual.
TABLE 3-2 Search Features by Package
Search Features Traditional mandb
Regular expressions Yes YesExact matches Yes
Brute-force matching Yes
The traditional version of manallows you to do a brute-force search for keywords
by looking at every word in every single manpage. The -Koption (note the capi-
talization) of man does this and is guaranteed to be slow even on the fastest
machines. When a thorough search is in order, this is the best you can do.
3.2.4 Getting the Right man Page: whatis
We saw how different sections can have manpages with the same name, which can
cause you to get the wrong manpage when you don’t know what section to look in.
If you want to look up the usage of the readdir function, for example, typing man
readdir will take you to section 2 of the manual, which says:
This is not the function you are interested in.  Look at readdir(3) for the
POSIX conforming C library interface.  This page documents the bare kernelsystem call interface, which can change, and which is superseded by getdents(2).
Luckily, in this case the manpage is helpful enough to let you know that you are
looking at a system call and not a POSIX function. It even tells you what section tolook in. In the more likely event that the 
manpage is not so helpful, the whatis
command can help:
$ whatis readdir
readdir              (2)  - read directory entryreaddir              (3)  - read a directory
Because section 3 contains the manpage we are looking for, we can then specify
the correct manpage as follows:
$ man 3 readdir
The traditional version of whatis takes no arguments, so matching is permissive.
The mandb version allows regular expressions and shell-style wildcard matching. If110 Chapter 3 • Finding Help
you can’t remember a command but remember that it ends with zip, for example,
you can try the following search:
$ whatis -w "*zip"
funzip (1)           - filter for extracting from a ZIP ..gunzip (1)           - compress or expand filesgzip (1)             - compress or expand filesunzip (1)            - list, test and extract compressed ...zip (1)              - package and compress (archive) files
A less elegant way to get the manpage you are looking for is to read all of them
until you find the one you like. This is done with the -aoption to man, which says,
“Show me all the manpages that match.”  The sections are presented in the order
determined by the local configuration, which can get tedious. On my FedoraCore 4 installation, for example, 
man -a read brings up eight manpages. Sifting
through eight manpages might provide some incentive for you to remember the sec-
tion number for next time.
3.2.5 Things to Look for in the man Page
A Linux manpage follows the conventions that are documented in section 7 of the
manual.4A minimal manpage has a NAME section, which consists of the name of
the program or topic being documented, followed by a brief description. This is the only text that is searched by the 
apropos command.
Most manpages consist of more than just a NAME section, of course. There are sev-
eral standard sections defined by convention, but programmers are free to defineadditional sections as appropriate.
The 
SEEALSO section is the closest thing you will get to cross-referenced docu-
mentation in a manpage. Don’t be surprised if this refers you to a manpage that
does not exist on your system (or perhaps anywhere else). Occasionally, some older
manpages refer to programs that don’t exist anymore. It’s also possible that the man
page refers to a program that just isn’t installed on your system. You should alwayslook at the 
SEEALSO section and try to find the cross references. You can learn a lot
more than you would by reading one manpage alone.
The ENVIRONMENT section provides valuable information about how environ-
ment variables affect the program’s behavior. Usually, this section covers locale3.2 Online Help Tools 111
4. See man(7) .
issues, but occasionally, there are some nice shortcuts that can be encapsulated in an
environment variable. Some programs, for example, allow you to put lengthy com-mand-line options in an environment variable to save you the trouble of typingthem every time. The 
ENVIRONMENT section is always worth a peek.
Another section worth looking at is the CONFORMINGTO section, which tells you
what standards apply to the command or function. This is important when you arewriting portable programs. There are many examples of multiple functions that dothe same thing in Linux. This conformance should be the tie-breaker when it comesto deciding which one to use. The 
bcopy function does the same thing that memcpy
does, for example, but if you check the CONFORMINGTO section for bcopy , you will
see that it conforms to 4.3BSD. The memcpy function also conforms to 4.3BSD,
but it is also part of the ISO C standard (ISO 9899), so memcpy should be preferred
for portability.
Near the bottom of the manpage, you may see a BUGS section. Perhaps this is a
misnomer, because some people say that the only difference between a bug and afeature is the documentation. Regardless, the 
BUGS section usually documents
design limitations of the program or features that aren’t fully functional.Sometimes, it contains a “to do” list to let you know what features are planned. It’san optional section, but if someone took the time to write the section, it behoovesyou to read it.
3.2.6 Some Recommended man Pages
Every section of the manual has an intro page that can be helpful if you forgetwhich section is which. Though the intro pages themselves don’t have much usefulinformation,
5they can serve as a quick reminder about section names, as follows:
$ whatis intro
intro                (1)  - Introduction to user commandsintro                (2)  - Introduction to system callsintro                (3)  - Introduction to library functionsintro                (4)  - Introduction to special filesintro                (5)  - Introduction to file formatsintro                (6)  - Introduction to gamesintro                (7)  - Introduction to conventions and miscellany
section
intro                (8)  - Introduction to administration and
privileged commands112 Chapter 3 • Finding Help
5.intro(2) is one exception here. There is a useful introduction to system calls.
If the idea of casually reading the manual seems weird, that probably means you
have never spent much time reading it. There are several pages with useful infor-mation and tutorials that you would never find unless you looked for them. Manyof these are in section 7, which is probably the least-read section of the manual.Table 3-3 lists a few of the selections from section 7 you may want to read.
TABLE 3-3 Recommended Reading from Section 7 of the Manual
Name Description
ascii(7) A short, handy page that simply lists ASCII codes in tables—
very handy when you need to know silly details like the octalconstant for Ctrl+G.
boot(7) A nice overview of the kernel boot sequence. If you are tryingto build distribution from scratch or just want to learn howLinux boots, this is a good read.
bootparam(7) A nice summary of the low-level options that the kernel cantake on the command line. This list is long, and it is mostassuredly not complete, but it is very informative.
charsets(7) A nice overview of character sets used in Linux, with a briefoverview of the features of each. If you work on internation-alized programs (i18n), this is a must read.
hier(7) A good overview of the Filesystem Hierarchy Standard, whichdescribes the conventions used to lay out the directories in aLinux system. It answers burning questions like “What’s thedifference between
/bin and /usr/bin ?” If you are devel-
oping an application for distribution, read this before youdecide where to install your files.
man(7) So you want to write a manpage? This section tells you how,
giving you just enough troff to get by as well as an expla-
nation of the conventions used in Linux manpages.
operator(7) Lists the C operators and their precedence. If you hate usingextra parentheses in your code, you should study this 
man
page. After reading this page, you should be able to find thebug in the following C statement:
if ( 1 & 2 == 2 ) printf("bit one is set\n");3.2 Online Help Tools 113
continues
TABLE 3-3 Continued
Name Description
regex(7) An introduction to regular expressions, which is a topic every
programmer should master.
suffixes(7) In Linux, file suffixes are conventions for the user. The OSrelies on more reliable techniques to determine a file type.This 
manpage describes many of the known conventional
suffixes in use. It is not complete but can be helpful.
units(7) Lists standard unit multipliers defined by SI.6Did you know
that according to SI rules, a megabyte is actually 1,000,000bytes and not 1,048,576 bytes (2
20bytes)? Disk drive
manufacturers know this, and you should too. We softwaredevelopers tend to use the prefixes for decimal multiplierswhen we mean to use the binary prefixes.
The SI defines unique prefixes for such binary values that 
no one uses. Perhaps we should. By the way, 1,048,576 bytesis properly called a mebibyte (abbreviated MiB). Although this
terminology is not likely to catch on any time soon, you mustbe aware of the potential for confusion when you are talkingrequirements with a scientist, hardware engineer, or disk drivevendor.
There’s also some nifty trivia in here, such as the prefix for a
million billion billion. Impress your friends.
uri(7), url(7), Three keys for the same manpage, describing the components 
urn(7) of a URL and what they mean. You may think you know allthe things that can go into a URL, but check this page out
and see whether you learn anything.
Remember that all the manpages listed in Table 3-3 are in section 7 and that
some of these names clash with those in other sections. Be sure you specify the sec-tion in order to get the correct 
manpage.114 Chapter 3 • Finding Help
6.SIis an abbreviation for Système International d’unités , the international standard for scientific
measurements.
3.2.7 GNU info
When you’re looking for documentation for GNU tools, the info program is the
preferred tool. Very often, GNU manpages contain an abridged version of the doc-
umentation that refers you to the info program for more details.
The fact that GNU chose to use a unique tool to document its tools is something
of a nuisance to UNIX folks, who are used to manpages, but info provides several
features that you don’t get in a manpage. For one thing, info files are hyperlinked
and heavily cross referenced. So when you browse the documentation for ssh, for
example, it will refer to other relevant tools, such as ssh-keygen . You can follow
the link in the documentation to go to the documentation for ssh-keygen and go
back again, just like on a Web page. Another feature of info pages is that they are
indexed so that you can locate relevant documentation more effectively than on a
manpage.
GNU info pages are written in a markup language called Texinfo. Like troff ,
Texinfo predates HTML and XML but is built upon earlier work. Texinfo evolvedfrom the TeX formatting language (still in wide use today) and another projectcalled scribe, which lives on today as scheme scribe. The goal of Texinfo was to markup content, not format, which makes Texinfo documents easier to write fromscratch than those in some other markup languages. Like 
nroff , Texinfo format
can be translated into hard copy, HTML, or plain text.
Although Texinfo is a very flexible format, most binary distributions don’t
include Texinfo source files. Normally, a binary distribution ships with speciallyprocessed text files that are suited only for input to the 
info viewer. These files are
created from Texinfo source with the makeinfo program. Because they do not con-
tain any typesetting information, they don’t produce the best hard copy. If you wantto translate the documentation into hard copy, or if you are looking to create PDFor HTML output, you should locate the Texinfo source. It usually can be found inthe source distribution or on the project’s home page.
3.2.8 Viewing info Pages
The GNU info system is tightly integrated with Emacs, the flagship GNU text
editor. Figure 3-2 shows what info looks like in the Emacs editor.
If you are not an Emacs user, there are a couple of alternatives. One of these is
the text-based info program. People who are used to manpages have a hard time3.2 Online Help Tools 115
using the info browser, mainly because it does not behave anything like the less
pager used by man. This can be very irritating when you have to transition between
manpages and info pages. Programmers who use the vitext editor are comfort-
able with the less program because its key bindings are very similar to those of vi.
The fact is that info borrows heavily from Emacs and behaves much like it. info
provides vikey bindings for Vi users with the --vi-keys option, but it doesn’t
behave much like vior less .
So you want to use the info program? You can start with the manpage, which
covers only the command-line options. If you want more in-depth information, youneed to look at the 
info page—but not so fast. If you type info info , you get
information about the info format, not the info program. For that, you have to116 Chapter 3 • Finding Help
FIGURE 3-2 Using Emacs to View info Pages

type info info-stnd , which will give you the correct documentation. The man
page ought to tell you that, but for some reason, it doesn’t.
So you actually like the info program? You can use it as your preferred tool to
look at both manpages and info pages. By default, info will try to find the infor-
mation in info format and fall back to a manpage if it does not.
There is another text-based alternative for programmers who don’t use Emacs
and don’t care for info : the pinfo program.7Users who prefer manpages will have
an easier time learning to use it. Like info , this is a text-based program, but it uses
more advanced terminal capabilities, including color highlighting for hyperlinks.Perhaps most important, it uses 
vikeys for cursor movement.
In the realm of GUI tools are some more choices for viewing info pages. Some
Web browsers will accept a URL of the form info:topic . In a GNOME system,
this simply launches the gnome-help browser, which as of this writing does not
support manor info pages. How rude!
A KDE browser sends you to khelpcenter , which is an excellent browser for
info pages. You can see for yourself in Figure 3-3.
An info document is usually broken into sections, which makes it easier to read
but can make finding exactly what you are looking for tedious. If you know theexact section you are looking for, you can bring it up with a lengthy command linelike the following:
$ info "(make)Quick Reference"
There’s nothing quick about having to type all that, but if it’s something you refer
to often, you can use a shell alias or a script to save typing. Both pinfo and info
allow you to jump to specific sections with this syntax, but as of this writing,
khelpcenter does not. If what you want is a single continuous stream of output,
like a manpage, you can coerce the info program to do so as follows:
$ info --subnodes some-topic | less
The --subnodes option forces info to dump the entire contents of the docu-
ment to stdout . Piping the output to less gives it the look and feel of a very long
manpage.3.2 Online Help Tools 117
7. http://pinfo.alioth.debian.org
3.2.9 Searching info Pages
You can search info pages like manpages by using the --apropos option to info .
Each info file has an index, which can be searched with the --apropos option, so
your search is more likely to produce output with info than with the apropos
command used for manpages. Like man, the search is case insensitive and matches
anywhere in the text. Unlike with man, you cannot use a regular expression. So
although searching an index is likely to get you more relevant results, this benefit islargely offset by the lack of any filtering via regular expressions or word-basedsearches.
info also has a -woption that behaves like the whereis command, but because
there are relatively few info files, you are unlikely to encounter the frequent name
clashes that you often can with man.118 Chapter 3 • Finding Help
FIGURE 3-3 Using khelpcenter to View info Pages

3.2.10 Recommended info Pages
Although most GNU tools have only terse manpages that refer you to the info
documentation, GNU has made an effort to provide comprehensive manpages for
some tools, such as gcc. Even these, however, are outdone by their Texinfo coun-
terparts. The man page for gcc 3.4.4, for example, is around 54,000 words,
whereas the info page comes in at nearly 158,000 words. The info file contains
much more background and historical information, whereas the man page is
strictly business.
When you have some time to read and learn, several topics in the info pages
are worth  a read. Table 3-4 lists some selected topics for you to explore in the
info pages.
Although info and mancover a large number of tools, there are many more ways
to document code.
TABLE 3-4 Recommended Reading
info Page Comments
coreutils This is some very informative light reading. It contains many of the
sundry two- and three-letter commands that UNIX programmersknow and love but organizes them by function. There is even areprint of an article that Arnold Robbins wrote for Linux Journal
in the node named “Opening the software toolbox.”
cpp This probably is one of the most misunderstood tools that every Cprogrammer uses. The 
manpage leaves a great deal to be desired.
Given that it’s a complex topic, the info format is preferred.
gcc Provides much more information than the gcc man page, including
implementation details, and is well organized for online reading.
ld A behind-the-scenes tool that every developer uses, but developersprobably don’t have a clue how it works. Again, this is a complextopic that is well suited to the 
info format.
libc An in-depth reference to the C standard library, with lots of back-
ground information.3.2 Online Help Tools 119
3.2.11 Desktop Help Tools
In the Linux world, there are two major players in the desktop environment:
GNOME and KDE. Although each desktop environment has its fans and detrac-tors, no desktop environment would be complete without online help. GNOME’shelp system is provided by the 
gnome-help command (also known as yelp ) and as
of version 2.10.0 is focused exclusively on GNOME tools. What help it providesusually is not relevant to programmers. Although 
gnome-help hasn’t much to offer
in the way of content, it still has its uses. It makes an excellent lightweight browserfor a simple HTML file, such as a 
README file written in HTML. Firing up Mozilla
or Firefox to view a simple HTML file is like using an aircraft carrier to go water-skiing. Besides, who wants to do all that typing? Mozilla and Firefox require you totype a fully qualified URL as follows:
$ firefox file:///usr/share/doc/someproject/README.html
Remember, that’s three slashes after the file: (and the number of slashes shall
be three). yelp is smart enough to find files in the current working directory, and
it starts up much faster than Firefox or Mozilla. So in the very likely event that youhappen to be working in the directory where your documentation resides, you canuse a refreshingly simple command such as:
$ yelp README.html
Because it has no plug-ins to speak of, yelp can pull up a simple HTML docu-
ment almost as fast as mancan pull up a manpage.
Just like yelp , khelpcenter makes an excellent lightweight Web browser for sim-
ple HTML files. As a bonus, khelpcenter provides a much less myopic view of the
tools than yelp . Although khelpcenter is thesource of information on KDE tools,
it also allows you to view manpages and GNU info pages. The info pages are hyper-
linked, just as expected, but less expected is the fact that manpages are hyperlinked as
well. Scroll down to the SEE ALSO section of a manpage and click any of the cross-
referenced manpages to view that document. This is a nice feature. khelpcenter also
provides search capability, as well as a glossary. The search is a little flaky, and the glos-sary is sparsely populated, but it is sure to get better as the tool matures.
3.3 Other Places to Look
If you are working with a new project, chances are that it has little or no docu-mentation in the form of a 
manor info page. Documentation may exist but may120 Chapter 3 • Finding Help
be hard to find. Things like README files have a way of getting lost in the packag-
ing when a project is packaged in a binary distribution. Chances are that the helpis somewhere; it’s just up to you to find it.
3.3.1 /usr/share/doc
This subdirectory is part of the Filesystem Hierarchy Standard and is where many
manpages and info pages are stored. It is also the preferred place to store the elec-
tronic equivalent of the writing on the back of a napkin that we call the README file.
A binary distribution typically includes a subdirectory named after each project,
where it may place all sorts of sundry information. Typically, you will find releasenotes, change logs, copyright information, and some sort of 
README file.
Occasionally, developers distribute only plain text or HTML documentation
without any manpages. This is the place you will usually find it. You might even
find Texinfo source or some other markup, like DocBook.
Debian provides the doc-linux package, which contains many of the HOWTO
documents from The Linux Documentation Project8(discussed later in this chap-
ter) stored in /usr/share/doc . These files are not coupled with any installed pack-
ages; they’re documentation for the sake for documentation.
3.3.2 Cross Referencing and Indexing
Cross referencing turns your documentation from a dictionary into a thesaurus. Adictionary shows you only what words you explicitly look for, whereas a thesaurusleads you to words you may not have thought of. Likewise, cross-referencing Linuxdocumentation is important, because there is usually more than one way to do whatyou want, and you may not know it unless you check the cross references. Linuxdocumentation does a fairly good job of referring to other tools despite the fact thatthe documentation is accumulated from many diverse sources.
For 
manpages, the cross-reference information can be found in the SEEALSO sec-
tion. Suppose that you are setting up a DHCP server, and you look at the dhcpd
manpage. This is the logical place to look, because dhcpd is the name of the DHCP
daemon. If you don’t look at the cross references, you may miss the dhcpd.conf
page, which has more vital information. Many complex tools and servers are3.3 Other Places to Look 121
8. www.tldp.org
documented by more than one manpage, and these elements are cross referenced in
the SEEALSO section. A typical manpage cross reference looks like the following:
SEE ALSO
dhclient(8), dhcrelay(8), dhcpd.conf(5), dhcpd.leases(5)
I described the convention for referencing manpages earlier. You can see here that
the manpages themselves use this notation.
Another vital tool for cross referencing man pages is the apropos command,
which I have already discussed. Beware that apropos works only when the database
is indexed. This normally is done for you with a fresh installation, but as you accu-mulate new software with new 
manpages and remove others, the index gets out of
sync. As a result, relevant documentation may not show up in an apropos search,
or you may get hits for software that is no longer installed. To fix this problem, youneed to run the appropriate indexer for the 
maninstallation. The traditional man
tool uses a tool called makewhatis that is usually run as a cron job. This is a fairly
time-consuming process, as the tool has to visit every manpage in the system. If you
have a laptop that you power up and down often, this job may never get a chanceto run. When it does, you’ll probably want to kill it if you are running on batteries.
The 
mandb version of manuses a tool named (appropriately enough) mandb . One
advantage of the mandb package is that indexing with the mandb program takes
much less time than makewhatis . This, too, typically is set up as a cron job. info
files do a fairly good job of cross-referencing other info files via hyperlinks. The
info tool also has an apropos -like function with the --apropos option. Although
it is not as flexible as the apropos command, it is still useful.
GNOME and KDE use HTML and XML extensively to document their tools,
and these usually are cross referenced well. Unfortunately, there is no universallyaccepted convention for storing and indexing HTML files to document text-basedcommands. These files can refer you to a 
manpage, but they can’t link to it. So when
you encounter an HTML file for a command-line tool, it is not likely to havehyperlinks to locally installed documentation. Links in these HTML files are likelyto point to other HTML files in the same project or to sources on the Web.
3.3.3 Package Queries
In Chapter 1, I discuss in detail how to use packages. Here’s how you can put thatknowledge to use. For starters, you may want just the basic information about apackage. This is useful in situations when you are wondering what a particular122 Chapter 3 • Finding Help
command does. Suppose that you notice the program named diffstat in
/usr/bin and wonder what it does. Perhaps you tried man diffstat and got
nothing. The next thing you should try is a basic query, such as:
$ rpm -qf /usr/bin/diffstat
diffstat-1.38-2
This query tells you the name of the package that installed /usr/bin/diffstat .
In this case, it reports that you have diffstat-1.38-2 installed. Then you could
query the package information to get a basic description of the package that this toolcame from, such as:
$ rpm -qi diffstat
...Summary     : A utility which provides statistics based on the output
of diff.
Description : ...
Sometimes, this is enough to tell you what you need to know, but now that you
know the name of the package, you can query the package contents to see whetheryou can glean any other information from it. In the list of installed files, you shouldlook for 
README files or HTML documents that might give you more information.
You might be on the lookout for misplaced manpages as well. These may be located
in some unconventional places that you may not have checked otherwise. In thecase of 
diffstat-1.38-2 , someone put the manpage in the wrong place:
$ rpm -ql diffstat
/usr/bin/diffstat/usr/share/man/man1/ man1
/usr/share/man/man1/ man1/diffstat.1.gz
The packager mistakenly created an extra man1 subdirectory under the man1 sub-
directory, so that the mancommand doesn’t find it. Although this problem was fixed
in a later version of diffstat , it is a useful illustration of how package queries can
be helpful. Packagers are humans, too. In case you are wondering, you could stillread this 
manpage with the following command:
$ man /usr/share/man/man1/man1/diffstat.1.gz
In most distributions, when a manpage cannot be found, you get nothing. In a
Debian-based distribution you may encounter undocumented(7) , which is a  generic
manpage describing some of the same topics covered in this section.3.3 Other Places to Look 123
3.4 Documentation Formats
Linux documentation comes in many formats. Although plain text is common, sev-
eral markup languages have been used over the years to produce prettier printoutsor more browser-friendly output. One of the first attempts was 
troff , which is the
language used to create manpages. troff generates output that can be typeset or
viewed in a terminal window. Other formats such as LaTeX were designed withtypeset printout in mind, but they are capable of producing browsable documen-tation. The preferred format of GNU, Texinfo, is designed to produce hypertextoutput that can be browsed or printed. Finally, there’s the ubiquitous HTML whichis designed exclusively for the browser. Which one are you most likely to encounter?All of them.
3.4.1 TeX/LaTeX/DVI
TeX and LaTeX markup go back a long way in UNIX history. LaTeX is an exten-sion of the TeX markup system. Most TeX documents these days are actually LaTeX,but the two names are often used interchangeably. Primarily intended for hard-copyprintout, TeX has extensive support for the special typesetting requirements ofresearch papers that contain mathematical symbols and formulas. TeX made thiseasy for authors at a time when word processors were in their infancy, thereby cre-ating a niche for itself among researchers. Despite advances in conventional wordprocessing applications, TeX is still widely used today.
The native output format of TeX is called DVI (for Device Independent). By
itself, it can be viewed using 
xdvi , kdvi , or evince , but the intended use of DVI
was as an intermediate format. As a result, there are many mature tools to convertDVI files to any format under the sun. Many open source applications prefer to takeadvantage of this rather than reinvent the wheel, so they provide DVI or TeX out-put that can be manipulated further. Some tools include compressed DVI files withthe binary distribution in 
/usr/share/doc . The Debian package of gdb, for exam-
ple, includes a nifty refer ence card in DVI format.
Both TeX and LaTeX use the .tex extension for source files, which can be con-
fusing. Documents written for TeX may not compile with the latex command, in
which case you should try the texcommand. In many cases, a LaTeX document
consists of more than one source file, so make sure that you have all the source filesbefore you try to create a document from LaTeX source. Sometimes, the documentis complicated enough that it requires a makefile.124 Chapter 3 • Finding Help
The tetex-bin package contains many programs to convert DVI files to many
other formats. To keep things confusing, numerous wrapper programs allow you toshortcut the process. One such tool is 
pdflatex , which converts LaTeX source
directly to PDF . It looks direct to you, but the process is essentially the same as ifyou compiled with LaTeX and converted the DVI file to PDF with 
dvipdf . Aside
from requiring less typing, pdflatex will clean up some of the many intermediate
files that LaTeX creates in the process of compiling.
Most desktop distributions include support for TeX and LaTeX, but some
smaller ones will strip these tools out to save space, and others may not includesome of the various tools. Keep this in mind when you work with TeX and LaTeXdocumentation.
3.4.2 Texinfo
Texinfo is the preferred format for documentation of GNU programs. The basicdocumentation browser for GNU documentation is the 
info program or Emacs,
depending on who you ask. The files used by info are stored in the
/usr/share/info directory, but these are not Texinfo files. These files are created
from Texinfo source using the makeinfo program. Most binary distributions do
not include Texinfo source files but instead provide the preformatted info files.
These are indicated with the .info extension, whereas Texinfo source files usually
are indicated by the .texi or .texinfo extension. As you might expect, a source
distribution will include Texinfo source as well, but some binary distributions alsoinclude Texinfo source. If any Texinfo source files are provided with a binary distri-bution, they are most likely to be found under 
/usr/share/doc . It’s worth look-
ing, because occasionally, you will find more comprehensive documentation than isin the manual or 
info pages.
Texinfo source code is formatted with the makeinfo program. This allows you
to convert it to info , HTML, DocBook, XML, or plain text. With the -- html
flag, for example, makeinfo creates a working Web document in a subdirectory
under the current working directory. A simple example might look like thefollowing:
$ makeinfo –-html foo.texi
$ yelp foo/index.html3.4 Documentation Formats 125
makeinfo produces several formats, including DocBook, XML, and HTML,
but not DVI. For DVI, you can use the TeX formatter as long as it includes this lineat the top:
\input texinfo
This line is a directive for the TeX formatter (ignored by makeinfo ) that
instructs it to import the texinfo package, which is what allows the TeX compiler
to format Texinfo source. With this line, you can use the texcommand to format
a simple Texinfo source file to produce DVI. For most Texinfo source documents,you should use 
texi2dvi instead of the texcommand.
3.4.3 DocBook
DocBook is an SGML9format for authoring documentation used by some tools.
Strictly speaking, DocBook is not a file format but an SGML Document TypeDefinition (DTD). SGML is the predecessor and a superset of the ubiquitous XMLformat. DocBook follows the goals of SGML, which are to provide a storage formatfor documentation that separates content from style. This allows content to besearched electronically without style and markup information corrupting the search.
The DocBook DTD is valid in both SGML and XML, although DocBook files
usually have the 
.sgml extension. DocBook is not a user-friendly format for writ-
ing documents from scratch. Authors are encouraged to use other tools to createformatted documentation and convert it to DocBook. 
makeinfo will generate
DocBook-formatted SGML, for example. The purpose of this would be to strip thecontent from the Texinfo markup for the purpose of searching. The DocBooksource could still be used to produce the typeset documentation in whatever formatyou want. KDE, for example, uses DocBook for 
khelpcenter documents.
DocBook source files can be identified by the .sgml or .docbook extension. In
a binary distribution, these are most likely to be found in /usr/share/doc , but
these may not be suitable as stand-alone documents. A DocBook file may be partof a larger scheme for online help that requires many other supporting files.
If you want to manipulate DocBook source, the 
docbook-utils package con-
tains several programs to transform DocBook source into other formats. Thisrequires the 
jadetex package to do its transformations using the TeX language.126 Chapter 3 • Finding Help
9.SGML stands for Standard Generalized Markup Language .
This gives you access to DVI output, which means you can transform DocBook
using any tool that understands DVI.
Before trying to manipulate DocBook source by hand, you should look around
and see whether the files were created from other source that’s easier to manipulate,such as Texinfo or 
troff .
3.4.4 HTML
Because HTML is so pervasive, many authors are more comfortable with it than withother markup languages, such as Texinfo, DocBook, and 
troff . Web pages used for
project documentation are usually created by hand in a text editor like vior Emacs.
Often, this is only one or two plain text files with no images, JavaScript, or flashy con-tent. Thanks to lightweight browsers like 
yelp and khelpcenter , you don’t have to
start up your bloated Web browser to read one of these HTML files. Here again, theconventional location for HTML documentation is in 
/usr/share/doc .
Some tools use HTML exclusively, without any manpages. The NTP (Network
Time Protocol) package, for example, has extensive HTML documentation, a sam-ple of which is shown in Figure 3-4.3.4 Documentation Formats 127
FIGURE 3-4 A Sample of HTML Documentation from the NTP Package

The downside to HTML documentation is that there are no formatting conven-
tions to guide authors on style or content. I don’t know about you, but picturesfrom Alice in Wonderland don’t help me much. Don’t look for HTML to replace 
man
any time soon. There is also no convention for storing HTML documentation in
any centralized fashion, so your ability to search HTML documentation is deter-mined by the whims of the author. You won’t find much, if any, cross referencingto other documentation, either.
Despite the downside, a good deal of high-quality documentation is provided in
HTML format. The ability to present data with proportional and fixed-width fontscan go a long way toward making a document more readable than plain text. HTMLis also better suited to internationalized documentation than plain text. Figure 3-5shows an HTML file in Japanese from the 
udev package that happened to be
installed on my system.128 Chapter 3 • Finding Help
FIGURE 3-5 A Sample of HTML Documentation in Japanese from the udev Package

Depending on your system setup, your terminal may have trouble presenting
plain text encoded in a non–ISO-8859 encoding. Most browsers, on the otherhand, can present other encodings without much fuss.
3.4.5 PostScript
The PostScript language, created by Adobe, is the predecessor to the PortableDocument Format (PDF) in wide use today. PostScript was once the dominantprinter language in the UNIX world. It was common for software packages (bothcommercial and open source) to include documentation in PostScript format,much the same way that we use Portable Document Format (PDF) today.
At that time, PostScript printers were the preferred printers for UNIX systems.
To typeset without PostScript, you would need a driver that could typeset a docu-ment on the local machine and understand how to send the proper commands tothe particular printer.
10Typically, this involved sending a large amount of data
through a low-speed interface such as a serial port, parallel port, or shared Ethernet,which made printing typeset documents a very time-consuming task. As an alter-native, PostScript provided a relatively concise language that allowed you to offloadthe typesetting task onto the printer. This made PostScript print jobs much fasterbut made the printers significantly more expensive.
Things have changed since then, and PostScript has faded from popularity. For
one thing, the idea of a smart printer that can offload typesetting tasks is not asappealing as it used to be. The world is now full of inexpensive, dumb printers capa-ble of producing high-quality output. A typical low-cost printer has a high-speedUSB interface and is connected to a computer with horsepower and memory tospare. Although there are still top-of-the-line workgroup printers that supportPostScript, it is no longer dominant in the UNIX world.
The GNU Ghostscript tools were a big enabler for using PostScript on UNIX.
Ghostscript allowed those of us who couldn’t afford expensive PostScript printers toprint PostScript documents on less expensive printers (albeit very slowly). Onething Ghostscript allows you to do is view a PostScript file onscreen instead of3.4 Documentation Formats 129
10. You can still see evidence of this legacy in the DVI and groff tools, which still provide output in HP
PCL and other printer languages.
printing it. This is done via the gscommand. The default output is to an X win-
dow, for example:
$ gs somedoc.ps Displays the contents of somedoc.ps onscreen
When Ghostscript was introduced, viewing typeset documentation onscreen still
was fairly novel. Only TeX users had that luxury with xdvi . With Ghostscript, any-
one could view typeset documentation in PostScript format onscreen.
Ghostscript is primarily a printing tool; there are better ways to view a PostScript
document onscreen. One such tool is GNOME’s evince11document viewer, which
can be used to view PostScript and PDF files. KDE uses konqueror as a front end
for Ghostscript, which is a little more user-friendly than Ghostscript by itself.
3.4.6 Portable Document Format (PDF)
Although PDF is excellent for producing typeset output for printing, it is equallysuitable for browsing online. One reason why it is so popular is that it saves vendorsmoney by requiring users to print their own manuals. Although it’s inconvenient tothe user, this is preferable to having a single sheet of paper printed in ten languagesor no manual at all.
On the more positive side, PDF is a nice way to encapsulate hyperlinked content
in a single file. A well-done PDF file has a hyperlinked table of contents and index,which can make browsing the manual online much more effective than using aprinted manual.
Thanks to Adobe, PDF is an open format, so open source tools can be created to
view and create PDF files without royalties. Despite this, PDF documentation isnot found in open source packages as commonly as it is in commercial packages.This is probably due to the tools, which have been lacking for some time, but thatis changing.
Ghostscript can display PDF files but lacks the refinements of Adobe Acrobat
Reader. There are no hyperlinks or table of contents, for example. The open source
xpdf12program is still maturing but is slow at rendering pages. GNOME’s evince
is relatively new but is much faster at rendering pages than xpdf . GNOME has big
plans for evince as a tool for viewing PostScript, PDF , and DVI.130 Chapter 3 • Finding Help
11. www.gnome.org/projects/evince
12. www.foolabs.com/xpdf
Adobe has made Acrobat Reader available for Linux for a long time, although
historically, the Linux version has lagged the Windows version considerably in fea-tures. Adobe recently ported Acrobat Reader 7.0 to Linux, which brings it up todate with the latest Windows version. Unfortunately, it also has much of the bloatof the Windows version. For this reason, it may be preferable to use one of the opensource viewers.
3.4.7 troff
This is the native markup language of manpages. It has a long history that predates
UNIX.13The tool used for this purpose is groff , which is the GNU version of
troff , but you don’t need to know that, because the manprogram will do most of
what you need. The following two commands, for example, are equivalent:
$ man intro
$ gzip -dc /usr/share/man/man1/intro.1.gz | groff -man -Tascii | less
The manprogram handles the messy tasks of finding the manpage and uncom-
pressing it with gzip , of course.
Although groff can generate DVI files from troff source, it also can generate
several other formats on its own. Many of these are of the printer-language variety,such as HP PCL and PostScript. No doubt this is part of its UNIX legacy. Becauseyou can also convert your output to DVI, you can do what you like with it fromthere.
3.5 Internet Sources of Information
When you can’t find the answers you need on your hard drive, you may be inclinedto use the Internet to search for help. Using a search engine can lead you downmany blind alleys and consume a great deal of your time. In this section, I discusssome resources you can use to search for information more effectively.
3.5.1 www.gnu.org
In addition to providing the source code for many of the tools that enable Linux,this site provides the manuals. Here, you will find the manuals that come with your3.5 Internet Sources of Information 131
13. Interested readers are encouraged to read roff(7) in the online manual. Be advised that there are some
inaccuracies.
packages, as well as some that don’t. The GNU C library ( glibc ), for example, is
fully documented in a manual published in two volumes and spanning more than1,300 pages. You could purchase the two volumes for $60 each, or you could down-load the Texinfo source from GNU. The Texinfo source is quietly tucked awayunder the 
glibc link amid all the other documentation that GNU provides. You
might never know that such extensive documentation was here unless you were tolook. The Web page gives you no clue as to how much documentation is in eachproject or how good the documentation; you just have to explore and find that outfor yourself. So visit the site and have a look around.
3.5.2 SourceForge.net
Many open source projects that are not sponsored by GNU are hosted bySourceForge.net, including many of the tools in your distribution that you useevery day. This isn’t always obvious even from the packaging information. The
strace package that comes with the Ubuntu distribution, for example, does not
give credit to the SourceForge.net Web page.
$ dpkg -s strace
Package: straceMaintainer: Roland McGrath <frob@debian.org>Description: A system call tracer
strace is a system call tracer, i.e. a debugging tool ...
In fact, the only reference to SourceForge.net you will find is an email address for
a mailing list on the last line of the man page. That’s a shame, because
SourceForge.net is a valuable resource for users as well as developers. RPM packagesseem to do a better job of attributing credit to the original authors via the URL fieldof the RPM header:
$ rpm -qi strace
Name        : stracePackager    : Red Hat, Inc. <http://bugzilla.redhat.com/bugzilla>URL         : http://sourceforge.net/projects/strace/Summary     : Tracks and displays system calls ...
If you have an RPM distribution, it pays to look at the information section of the
RPM to see whether there is a place you can look for information. If you are usinga Debian distribution, you may want to go directly to SourceForge.net and lookaround.132 Chapter 3 • Finding Help
Each project on SourceForge.net has its own home page and the ability to host
forums where users can discuss issues and ask questions. The forums are where youcan communicate with other users and power users about a particular project youare interested in. These forums typically have very low traffic, so the “signal-to-noise” ratio is high. Just browsing the archives will be educational, and the sub-scribers aren’t likely to flame you too badly if you post a dumb question.
3.5.3 The Linux Documentation Project
If you have never visited www.tldp.org before, you owe it to yourself to visit. TLDPis the source for the 
manpages that most Linux distributions use. Much of the other
information here is of a more general nature, so you are unlikely to find informa-tion about a specific open source tool, for example. There is some very good read-ing here nonetheless. The material is arranged into a few major buckets, including
HOWTO s, Guides, FAQs, and of course manpages.
Be sure to check the dates of the documents before you invest too much time
reading them. Linux changes so fast that the documentation has a relatively shortshelf life. These documents are written and maintained by volunteers, so they arelikely to go out of date before someone has time to update them.
The 
HOWTO documents are often very useful, because they cover very specific top-
ics and generally walk you through a process without requiring a great deal of back-ground knowledge. The topic of the 
HOWTO is usually enough of a clue to tell you
whether you want to read it. Because these are very specific topics, it’s hard to findone that will waste your time.
The FAQ comes from Usenet newsgroups, where the same questions are often
posted over and over.
14The FAQ was an attempt to cut down on the noise of the
same questions being posted so often. Unfortunately, FAQ is a misused term thesedays, because FAQs often contain questions that the author wished had been asked,rather than ones that have actually been asked. Unlike the lame, contrived FAQsyou may encounter on a run-of-the mill Web page, most of the TLDP FAQs arecreated from real questions asked by real people (some even frequently). If you havea specific question, the FAQ is worth a look.
A good place to start is The Linux Documentation Project FAQ, located at
www.tldp.org/FAQ/LDP-FAQ/index.html.3.5 Internet Sources of Information 133
14. In case you didn’t know, FAQ stands for Frequently Asked Questions.
3.5.4 Usenet
Usenet has a long history that predates the Internet. Usenet “newsgroups” are often
more gossip and blather than news. Here, anyone can say anything, and the onlyrepercussions are the so-called “flames” of postings by other angry readers. You willfind many strong opinions and a general lack of manners.
Browsing Usenet archives via Google or another search engine is advised before
actually posting a question on one of these newsgroups. There’s a chance that yourquestion has been asked before. It also helps to get to know some of the regularposters to see whether anyone is actually getting help.
Before you use any information you get from Usenet, make sure to verify it with
some other source. Be skeptical of everything you read. There is no guarantee thatthings you read are accurate or even safe to try.
Be advised that there is no attempt to hide your e-mail address in Usenet postings,
so it is out there for the world to see. Usenet groups are gold mines for spammers.
3.5.5 Mailing Lists
Of the many forum formats, this is the one that I have found to be the most valu-able. What you get in a mailing list is a group of people who are passionate aboutthe topic and generally eager to help (otherwise, they wouldn’t subscribe). As withany forum, it is always advisable to do your homework before posting a question.Although people on mailing lists usually have good manners, it’s rude to waste peo-ple’s time with simple questions you could look up yourself.
A valuable resource for finding mailing lists is the mailing-list archive (MARC)
located at http://marc.theaimsgroup.com. This site contains archives of hundreds ofmailing lists on Linux as well as many other computer topics.
3.5.6 Other Forums
Many Linux Web sites host forums similar to Usenet that are targeted to Linux top-ics. These usually require some kind of subscription before you can post, but manyallow you to read their archives without signing in. The quality of postings can varywildly.
3.6 Finding Information about the Linux Kernel
Documenting the Linux kernel is a huge task. Several good books have been writ-ten on the topic, but the kernel is a moving target. This makes just about any book134 Chapter 3 • Finding Help
on the kernel obsolete by the time it is published. The most up-to-date documen-
tation you are going to find is located in the kernel source tree. The kernel source,as it is distributed from http://kernel.org, contains quite a bit of documentation. Inaddition, many kernel modules provide helpful information embedded in the ker-nel objects themselves.
3.6.1 The Kernel Build
The kernel build process is well documented in many places on the Internet.Although the 2.6 series has simplified the build process significantly, there areplenty of gotchas and little details worth knowing about. The first place to look, ofcourse, is the 
README file located at the root of the kernel source tree. It starts with
“What Is Linux,” which may make your eyes glaze over and cause you to give upprematurely. But if you stick with it, you will find that somewhere in the middle, ittalks about all the targets you can build with the top-level makefile. Some of theseare quite useful, and a summary can be printed at any time by typing
$ make help
Cleaning targets:
clean           - remove most generated files but keep the configmrproper        - remove all generated files + config + various
backup files
Configuration targets:
config          - Update current config utilising a line-oriented
program
menuconfig      - Update current config utilising a menu based
program
xconfig         - Update current config utilising a QT based front-
end
gconfig         - Update current config utilising a GTK based front-
end
oldconfig       - Update current config utilising a provided .config
as base
randconfig      - New config with random answer to all optionsdefconfig       - New config with default answer to all optionsallmodconfig    - New config selecting modules when possibleallyesconfig    - New config where all options are accepted with yesallnoconfig     - New minimal config
Other generic targets:
all             - Build all targets marked with [*]
* vmlinux         - Build the bare kernel* modules         - Build all modules3.6 Finding Information about the Linux Kernel 135
modules_install - Install all modules
dir/            - Build all files in dir and belowdir/file.[ois]  - Build specified target onlyrpm             - Build a kernel as an RPM packagetags/TAGS       - Generate tags file for editorscscope          - Generate cscope index
Static analysers
buildcheck      - List dangling references to vmlinux discarded
sectionsand init sections from non-init sections
checkstack      - Generate a list of stack hogsnamespacecheck  - Name space analysis on compiled kernel
Kernel packaging:
rpm-pkg         - Build the kernel as an RPM packagebinrpm-pkg      - Build an rpm package containing the compiled
kernel & modules
deb-pkg         - Build the kernel as an deb package
Documentation targets:
Linux kernel internal documentation in different formats:xmldocs (XML DocBook), psdocs (PostScript), pdfdocs (PDF)htmldocs (HTML), mandocs (man pages, use installmandocs to install)
Architecture specific targets (i386):
* bzImage       - Compressed kernel image (arch/i386/boot/bzImage)
install       - Install kernel using
(your) ~/bin/installkernel or(distribution) /sbin/installkernel orinstall to $(INSTALL_PATH) and run lilo
bzdisk       - Create a boot floppy in /dev/fd0fdimage      - Create a boot floppy image
make V=0|1 [targets] 0 => quiet build (default), 1 => verbose build
make O=dir [targets] Locate all output files in "dir", including
.config
make C=1   [targets] Check all c source with $CHECK (sparse)make C=2   [targets] Force check of all c source with $CHECK
(sparse)
Execute "make" or "make all" to build all targets marked with [*]
For further info see the ./README file
Additionally, each selectable feature of the kernel is documented in the Kconfig
files that you find in many subdirectories of the kernel source tree. These files con-
tain the text of the help messages you see when you create a new kernel configura-tion via 
make config , make menuconfig , etc. The Kconfig files are plain text
files, so you can look at them with any text editor.136 Chapter 3 • Finding Help
3.6.2 Kernel Modules
Linux provides features for kernel modules to do a modest amount of self docu-
mentation. The modinfo command allows you to see information in the module
that the author may have placed there. Specifically, modules can take parameterswhen loaded, much like command-line parameters. These options are shown by the
modinfo command, along with whatever documentation the author provided. For
most modules, the information is minimal, but for some, it is quite extensive. If themodule takes a kernel parameter, it will be listed by 
modinfo , whether it’s docu-
mented or not. Even if it’s not documented, this at least gives you some directionas to what to look for in the source.
One example of how extensive the 
modinfo documentation can be is the aic79xx
module used with Adaptec SCSI controllers. The modinfo output is shown below:
aic79xx:period delimited, options string.
verbose       Enable verbose/diagnostic loggingallow_memio   Allow device registers to be memory mappeddebug         Bitmask of debug values to enableno_reset      Suppress initial bus resetsextended      Enable extended geometry on all controllersperiodic_otag Send an ordered tagged transaction
periodically to prevent tag starvation.This may be required by some older diskor drives/RAID arrays.
reverse_scan  Sort PCI devices highest Bus/Slot to lowesttag_info:<tag_str>    Set per-target tag depthglobal_tag_depth:<int> Global tag depth for all targets on all busesrd_strm:<rd_strm_masks> Set per-target read streaming setting.dv:<dv_settings>      Set per-controller Domain Validation Setting.slewrate:<slewrate_list>Set the signal slew rate (0-15).precomp:<pcomp_list>  Set the signal precompensation (0-7).amplitude:<int>   Set the signal amplitude (0-7).seltime:<int>     Selection Timeout:
(0/256ms,1/128ms,2/64ms,3/32ms)
Sample /etc/modprobe.conf line:
Enable verbose loggingSet tag depth on Controller 2/Target 2 to 10 tagsShorten the selection timeout to 128ms
options aic79xx 'aic79xx=verbose.tag_info:{{}.{}.{..10}}.seltime:1'Sample /etc/modprobe.conf line:
Change Read Streaming for Controller's 2 and 3
options aic79xx 'aic79xx=rd_strm:{..0xFFF0.0xC0F0}'3.6 Finding Information about the Linux Kernel 137
The driver takes only one parameter, named aic79xx , but the variable contains
text that encodes dozens of details. Indeed, the modinfo output for the aic79xx
module reads almost like a manpage, but this is an extreme case. Most modules take
no parameters and contain little or no information, but you will never know unlessyou try.
3.6.3 Miscellaneous Documentation
The kernel source distribution contains a treasure trove of reference material in the
Documentation directory. The Documentation directory has more than 700 files,
most of them plain text with a few DocBook files thrown in for good measure. TheDocBook source is actually part of the kernel build. If you want to view it, there areseveral targets you can use to create the format you want:
$ make pdfdocs # Generate PDF files
$ make mandocs # Generate man pages
$ make psdocs # Generate PostScript output
There is a great deal of diverse information here, including information for ker-
nel hackers as well as system administrators.
3.7 Summary
This chapter introduced the main sources of documentation for Linux tools andhow to use them. It looked at the various tools used to retrieve documentation andexamined some of the sources of documentation. Along with the sources, the chap-ter looked at the formats that Linux documentation comes in. Finally, the chapterlooked specifically at the kernel and the documentation that is available for variouskernel modules and for the rest of the kernel.
3.7.1 Tools Used in This Chapter
•man—the original UNIX help tool
•apropos —searches the manheadlines for keywords
•whatis —searches the whatis database of manpages (created by makewhatis )
for keywords
•info —the GNU help tool that supports more complex documents138 Chapter 3 • Finding Help
•yelp —the GNOME help tool that is also a functional Web browser
•khelpcenter —the KDE help tool
•xdvi , kdvi —tools for viewing documents in DVI format
•evince —the GNU all-purpose viewer for DVI, PDF , PostScript, and other
formats
•makeinfo —used to render Texinfo-formatted documentation into other for-
mats (HTML, DocBook, and others)
•gs—the command-line front end for Ghostscript, the GNU PostScript viewer
•xpdf —an open source PDF viewer for the X Window System
3.7.2 Online Resources
• www.troff.org—dedicated to the troff formatting language
• www.pathname.com/fhs—the Filesystem Hierarchy Standard• http://marc.theaimsgroup.com—archives of various mailing lists• www.foolabs.com/xpdf—the 
xpdf project home page
• www.gnome.org/projects/evince—the evince project home page
• pinfo.alioth.debian.org—the pinfo home page3.7 Summary 139
This page intentionally left blank 
4.1 Introduction
The text editor is one tool every good developer should be intimately familiar with.
Many editors have zealous followers, ready to tout their favorite text editor as thebest thing since sliced bread. In this chapter, I help you sort out some of the hypefrom the facts and make the best choice for you.
Editing source files is only part of your task, however. A good software develop-
ment process requires revision control for source files. Several open source tools andcommercial tools are available for this purpose, and the number is growing. I lookat the most common open source tools and some emerging ones as well.
Finally, I look at some tools to help you navigate and manipulate source files.
These are particularly useful for working in teams and working with source codethat is not yours.
4
141Editing and Maintaining
Source Files
4.2 The Text Editor
Perhaps the single most important tool you work with as a software developer is the
text editor, which directly affects your productivity and quality of life. Being famil-iar and comfortable with your text editor can make the difference between codingfor fun and coding as a chore. Because of this, the subject of text editors oftenevokes visceral reactions from developers. Most have strong opinions as to whatdefines a good text editor.
Some developers believe that a modern editor must have a GUI and be part
of an Integrated Development Environment (IDE) to be useful for softwaredevelopment. Others want to keep their hands on the keyboard while they’retyping and give the mouse a rest. What makes a good text editor is whateverworks for you.
One thing you want to avoid is getting attached to a tool that is hard to find or
that works in only one environment. When you work on your own system, you arefree to install whatever tools you like and run whatever distribution you like. Youcould find an excellent editor, install it, and invest a great deal of time mastering it,only to find out that it doesn’t work in your new favorite distribution. Worse,maybe it’s no longer under development, and you can’t get someone to work onporting it. That’s an extreme case.
A more likely scenario would be if you work as a contractor for a large company.
You may not have the liberty to install your favorite distribution with your favoritetext editor. You may have fallen in love with KDE’s Kate editor, only to find outthat your new employer uses a GNOME distribution. What then? Perhaps you areworking on a newly installed system you put together for a client, but you havegrown so accustomed to your own editor macros and settings that you become afish out of water without them. The client is watching, and your fumbling at thekeyboard is not filling him with confidence.
Surely, there is no shortage of text editors to choose among. If anything, there are
too many. As a competent developer, you should build your skills in the mainstreameditors so that you can be productive in any environment. Linux text editors can bebroken into the four basic categories listed in Table 4-1.142 Chapter 4 • Editing and Maintaining Source Files
You owe it to yourself to master one of the terminal-based editors—preferably vi
or Emacs. viis part of the POSIX standard, so if you master it, you will be able to
work productively on any Linux or UNIX system. Emacs is a standard part of anyopen source distribution and usually will be installed on any Linux system.Although Emacs is available for most major UNIX clones, it may not be part of thestandard install.
4.2.1 The Default Editor
By default, many tools that require input from a text editor bring up vifor you. As
a result, most developers have some basic competency in vi. You usually can change4.2 The Text Editor 143
TABLE 4-1 Basic Editor Categories
Editor Description
vi/Vim The grandfather of UNIX text editors, viis part of
the POSIX standard. It is designed for people whoknow how to type, allowing you to accomplish allyour tasks with your hands remaining in the homeposition on the keyboard at all times. Vim is the mostcommon open source clone of 
vithat comes with
most Linux distributions.
Emacs and clones Emacs is the flagship GNU editor that never met a
feature it didn’t like. The Emacs program is extensibleand has accumulated many features beyond textediting over the years. Emacs is available for everyGNU/Linux distribution.
Other terminal-based clones This category includes essentially every other terminal-
based editor. Some of these editors are clones of olderprograms that are no longer available, such as WordStar.
GUI text editors It may seem unfair to lump all GUI text editors into
one category, but there is a good reason for this. Whatall GUI editors have in common is that they are allintuitive to use and modeless. The keyboard is for
typing, and the mouse is for everything else.
the default editor to your editor of choice by setting the EDITOR environment
variable. Some tools also use the VISUAL environment variable, if EDITOR is not set.
Most tools will respect the choice you make here, provided that the indicated toolis installed and in the path.
4.2.2 What to Look for in a Text Editor
Ask three developers what the most important feature in a text editor is, and youare likely to get three different answers. This may partly explain why there are somany text editors out there.
Some editors seem to be created by programmers who want to scratch a particu-
lar itch at the expense of other features. If such an editor satisfies your needs, it’sprobably just a coincidence. Other editors are just toy projects created for the pur-pose of promoting one programming language or another. The text editor seems tobe the project of choice for this purpose. The programming language an editor iswritten in shouldn’t be a factor in your decision to use it.
1
If you are looking to evaluate a new text editor or clone, you should look for sev-
eral features that are geared specifically to developers. A few of these features arelisted in Table 4-2.
TABLE 4-2 Common Text Editor Features
Feature Description
Brace matching Place your cursor on a bracing character, such as {}()[], and
the editor will highlight the matching brace. Some editorsallow you to move the cursor to the matching brace, which is also useful. This is extremely useful for code that is poorlyindented or for writing complex expressions with multipleparentheses.
Syntax highlighting This does more than just make the code pretty to look at; it
also can alert you visually to many common types of errors,such as comments or quotes that have not been closed andpreprocessor syntax errors.144 Chapter 4 • Editing and Maintaining Source Files
1. The programming language should be a factor only when you have to build from source. In that case,
you need to have a compiler in that language.
Feature Description
Autocompletion This can be a real time-saver, especially for the typing
impaired. Type the first few letters of a word, and the editorwill fill in the rest based on previously typed words. Somedevelopers can’t live without this feature.
Regular expressions A regular expression is a precise syntax for searching and
modifying text. It’s an important feature for editing sourcecode, particularly when you find yourself making surgicalchanges to a large set of source files. A less precise syntaxmight make unintended changes.
Automatic indenting This seems like such a small feature that most programmers
take it for granted. But extra keystrokes required to indentcode really interrupt your flow and slow you down. Typically,the editor will simply indent as you type, but some editorsallow you to reformat previously typed sections of code thatare poorly indented.
Code browsing Tools such as 
ctags and etags generate indexes for a set 
of source files. The editor can use this index to navigate thecode. Emacs and 
viuse unique formats for tags, and most
editors that support tags will use one of these formats.
Code building The ability to build code from within the editor is more than
just convenience; it’s also an important development tool. Agood text editor will not just run your build, but also help
you track down warnings and errors.
Although the features listed in Table 4-2 can be found in most of the popular ter-
minal-based and GUI text editors, some features that are not listed here may beimportant to you, as well as some that we take for granted.
One feature that we take for granted is the ability to work on multiple files. Every
editor can do this, each in a different way. The generic term for this ability is buffers,
which is used because the text you are editing need not belong to a file. The abilityto open such anonymous buffers can be very useful; consequently, every editor sup-ports this feature in some fashion.4.2 The Text Editor 145
Another feature you may be interested in and that not all Linux editors have is
Windows compatibility. If you work in a Windows environment as well as in Linux,being able to work on the same text editor in both environments can improve yourproductivity and should be a factor in your choice. The two most popular editorsfor Linux (
viand Emacs) both have excellent full-featured Windows versions.
One more feature that may be important to you is internationalization (i18n for
short). The ability to display text in other character sets is largely a function of yourGUI, but the text editor has to be able to deal with the particular encoding as well.Most Linux text editors have no problem with Unicode UTF-8 encoding.
4.2.3 The Big Two: vi and Emacs
viand Emacs have legacies that date back before UNIX (not just Linux). These are
terminal-based editors that were around before X, when the terminal was the onlyinteractive interface to a computer. Some programmers today resist using terminal-based editors (sometimes called text-mode editors) because they think those editorsare archaic. This is not true. 
viand Emacs have evolved over the years to include
all the features you expect in a modern text editor, including a GUI. Nevertheless,text mode is still very important; it gives you something to work with when youcan’t bring up a GUI.
2
4.2.4 Vim: vi Improved
viis part of the POSIX standard and part of every UNIX distribution. Until
recently, however, it was not open source. This led to numerous open source clonesover the years. Vim is the clone that you will find in all major Linux distributions.Vim (short for 
viImproved ) implements all the features of vi, adds numerous
enhancements, and addresses some of vi’s shortcomings.
4.2.4.1 Vim Features
Not surprisingly, Vim has all the features listed in Table 4-2, which explains why it
is the most common viclone chosen for Linux distributions. One benefit of using
an open source text editor is that the user base consists almost entirely of program-mers. The people developing the code are actually using the tool as well. This isalways a recipe for good software. It also means that if a useful new feature shows146 Chapter 4 • Editing and Maintaining Source Files
2. If you’ve never been in this situation, you’re not trying hard enough.
up in another text editor, chances are that the same feature will be implemented in
short order.
Table 4-3 lists a small set of features comparing viand Vim. Vim actually has
many more features—too numerous to mention here. When talking about featuresthat are common to both 
viand Vim, I will use the term vi, and I will use Vimto
refer to features that are unique to Vim.
A key feature of viis that it does not rely on the mouse. You accomplish every-
thing you need to do with your hands in the home position on the keyboard insteadof reaching for the mouse, as you would with a GUI editor, or contorting your fin-gers to press three keys at once, as you might with Emacs. To accomplish this, 
vi
uses three basic modes: command mode , insert mode , and Ex mode . Perhaps this is
something only a programmer could love (or comprehend). The modal nature of
viis both loved and despised by programmers.
4.2.4.2 Modes, Modes, Modes
vistarts in command mode, which means “Don’t start typing yet!” In command
mode, every key on the keyboard performs a particular function. Press a key, andsomething happens (maybe). Command mode is used to move the cursor, cut andpaste, or do just about anything else that does not involve typing text. This is onething that makes 
viunlike any other text editor and turns away many new viusers.
It’s only natural to expect to be able to type after you start your editor. Before youcan start typing in 
vi, however, you must be in insert mode.
Figure 4-1 illustrates the modes in viand the methods used to change 
between them.4.2 The Text Editor 147
TABLE 4-3 vi vs. Vim
Feature vi Vim
Undo Only one level Multiple levelsTab expansion No YesNumber of buffers per session T wo Limited only by system resourcesGUI No OptionalSyntax highlighting No Yes
Autocompletion No Yes
Notice that there are numerous ways to get from command mode to insert mode,
but the only way to get out of insert mode is to press the Esc key. Also notice thatall paths go through command mode. You can’t go from insert mode directly intoEx mode, for example.
4.2.4.3 Command Mode
Command mode can be unnerving for new 
viusers. There is no status message, and
there is no “Are you sure?” pop-up message. The command is executed the instantyou touch the key. Press the j key in command mode, for example, and the cursormoves down one line. That’s it. No fanfare or congratulations—just results. Thistakes some getting used to, but as you become proficient, you will find it to be a hugeproductivity boost.
The key mapping in command mode is a balance between easy to remember and
easy to use. Most of the commands that take you to insert mode fall into the easy-to-remember category. These include commands like 
ifor insert and ato append.
Commands that involve cursor movement, on the other hand, may be more cryp-tic, because they are designed to be easy to use. The 
jkhl keys, for example, per-
form the same function as the arrow keys in command mode, which allows you tomove the cursor with the index and middle fingers of your right hand.
Commands can be repeated by typing the number of repetitions before the
command. Here again, no status is printed to the screen, and no warning is given.Just type 
40jin command mode, and the cursor simply moves down 40 lines.
This, too, takes some getting used to. Unintended repeats can produce some veryodd behavior, which can often scare off a new 
viuser. Fortunately, Vim has mul-
tiple levels of undo.148 Chapter 4 • Editing and Maintaining Source Files
FIGURE 4-1 Modes in viIllustratedEx
ModeComm and
ModeInsert
Mode:
Enter
EscInsert Commands
Esc
4.2.4.4 Cursor Movement Commands
Table 4-4 lists some essential commands used to move the cursor in command
mode. The arrow keys on the keyboard work as you would expect; in fact, each oneis a unique command. But using the arrow keys requires you to move your handfrom the home position. By using the 
jkhl keys, you can keep your hands in the
home position.
TABLE 4-4 Cursor-Motion Commands That Can Be Used Alone or with Other Commands
Command Description
j Move cursor down one line.
k Move cursor up one line.
h Move cursor left one column.
l Move cursor right one column.
Enter Move cursor down one line.
G Go to line. With no repeat count, it jumps to the end of the file. If
you provide a repeat count, it jumps to that line. 50G, for example,
jumps to line 50 of the file.
+ Move cursor down one line and position it at the first nonblankcharacter on the line.
- Move cursor up one line and position it at the first nonblankcharacter on the line.
% Jump to matching brace. Cursor most be positioned on a bracingcharacter such as (){}[]; otherwise, the terminal beeps, and nomovement is made.
[[ A two-character command that takes you backward to the first { inthe first column. This is useful for moving between functions.
]] Same as [[ except that it moves you forward.
'{mark} Jump to line marked by the mcommand with the specified mark.
Marks are user defined with the mcommand and can be any
character. 4.2 The Text Editor 149
continues
TABLE 4-4 Continued
Command Description
'' Jump (return) to the last line you jumped or searched from. 
This command is very useful for when you accidentally jumpsomewhere you didn’t expect, as well as for peeking at some othersection of a file and returning to what you were doing.
/{expr} Search forward. Press /, and you are prompted for a regularexpression to search for. The search begins when you press Enter.The search takes place from the current line and continues until thefirst match. By default, the search will continue from the top of thefile (that is, wrap around). See also the 
wssetting in Table 4-18.
?{expr} Search backward. This command is the same as / except that thesearch take place from the current line to the top of the file. This,too, will wrap around and continue the search from the bottom.
n Repeat last search in the same direction starting from the currentcursor position.
N Repeat last search in the opposite direction starting from thecurrent cursor position.
w Move cursor forward one word.
b Move cursor backward one word.
There is a method to having all these cursor-movement commands. As you shall
see, all the cursor-movement commands listed in Table 4-4 can be combined withother commands to make them very useful. There also are some useful movementcommands that cannot be combined with other commands. Some of these com-mands are listed in Table 4-5.
The fact that cursor movement is done almost exclusively in command mode is
a gotcha for first-time 
viusers. It’s natural for many users to reach for the arrow
keys while typing, only to be punished by a bunch of garbage spewed on the screenwith each keypress. Older versions of 
vicould not handle arrow keys in insert
mode. Vim, on the other hand, allows you to use the arrow keys while you’re ininsert mode, which is a nice improvement. Without this feature, you would have togo back to command mode just to move the cursor.150 Chapter 4 • Editing and Maintaining Source Files
4.2.4.5 Insert and Change Commands
Aside from cursor movement, the most important commands you need to know to
get started are the ones that let you start typing. These are the commands that takeyou into insert mode. There are many ways to get into insert mode, but most pro-grammers pick one and stick with it. There are always circumstances where one ofthe other commands is preferred, but the 
icommand is enough for most cases.
Table 4-6 lists several of the most common commands for entering insert mode.
TABLE 4-6 Basic Insert Commands
Command Description
i Enter insert mode starting before the character under the cursor.
a Enter insert mode starting after the character under the cursor.
I Enter insert mode starting before the first non blank character on 
the line.
A Enter insert mode starting after the last non blank character on the line.
o Enter insert mode starting on a new line below the current cursorposition.
O Enter insert mode starting on a new line above the current cursor
position.4.2 The Text Editor 151
TABLE 4-5 Stand-Alone Cursor-Motion Commands
Command Description
Ctrl+F Move forward one screen.
Ctrl+B Move backward one screen.
Ctrl+] Jump to the tag that the cursor points to. This is used with ctags
and can also be accomplished in Ex mode with :ta.
Ctrl+T Return from the previous tag jump (like the Back button in a Web
browser).
Although it may seem unintuitive, insert commands are single commands like
everything else. The entire insert, starting from the moment you press the com-mand key until the point you press Esc, is treated as one single command regard-less of the number of words. Normally, this is not important until you realize thatlike all other commands, inserts can be repeated, perhaps inadvertently. Becausethere is no feedback when you enter a repeat count, as I pointed out earlier, this typeof error is easy to make. Suppose that you want to enter the following line of text,but you forget to enter insert mode:
50 dollars per hour is my rate, and that's a bargain at any price
You start typing 50, and nothing appears. Oops! You’re not in insert mode. No
problem; you just press ito take you into insert mode. But there’s a surprise in
store. You finish typing, and press Enter and then Esc to return to command mode.Suddenly, the same line appears 49 more times. What happened was that the com-mand you entered was not simply 
i, but 50i, because you forgot that you were in
command mode when you started typing. vidutifully replicates the insert you did
49 more times, for a total of 50 “inserts.”
This is a kind of time bomb, because you really don’t know you made the mis-
take until after you exit insert mode, which may be some time later. Fortunately,you have the undo command (
u). It may be useful to remember that using Ctrl+C
instead of Escto exit insert mode will cancel any repeats that you specified. The
insert remains intact, but you will see only one copy. Before you enter an insertcommand, you can clear any repeat that you may have inadvertently typed by press-ing 
Ctrl+C or Escfirst.
Without repeats, insert commands are straightforward. You press the key,
enter insert mode, type to your heart’s content, and then exit insert mode bypressing 
Esc or Ctrl+C . Change commands behave the same way, except that
they are preceded by an initial deletion. The basic change commands are shownin Table 4-7.
There are no patterns to speak of for using insert commands except for the
repeats I mentioned earlier. Change commands can be repeated as well. When yourepeat a change command, however, 
virepeats the deletion but the insert occurs
only once. This behavior is more instantaneous and intuitive. There are no timebombs ticking when you use change commands.152 Chapter 4 • Editing and Maintaining Source Files
4.2 The Text Editor 153
The patterns in Table 4-8 are identical to the patterns for the delete command ( d),
which I will introduce in Table 4-12. In fact, in Vim, the only difference between achange and a delete is that the change leaves you in insert mode. This is anotherdifference between 
viand Vim. When you run a command like 4sto change the
next four characters, for example, viwill return you to command mode after you
type the fourth character. Vim, on the other hand, will remain in insert mode.
TABLE 4-7 Change Commands
Command Description
C Delete everything from the cursor to the end of the line; then enter
insert mode. This command is the same as D(see Table 4-10) followed
by A.
c{motion} Delete text starting with the character under the cursor as determined by the motion and then enter insert mode. Valid motion commandsinclude any of the commands listed in Table 4-4. 
{N}s Change (substitute) the next N characters from the current cursorposition. This command is similar to the 
ccommand except that you
specify an exact number of characters instead of a motion command.The command 
5sis the same as c5l.
S Change (substitute) the current line in its entirety. This commanddeletes all text on the current line (except the newline) and puts you in
insert mode.
TABLE 4-8 Some Patterns for Using Change Commands
Pattern Description
2cw Delete two words and enter insert mode; alternatively, change the nexttwo words.
cta Delete everything up to the next occurrence of the letter a and then insert.This is a combination of the 
ccommand and a tamotion command.
5cta Delete everything up to the fifth occurrence of the letter a and then insert.
5S Delete the current line and the subsequent four lines, and enter insert mode.
4.2.4.6 Other Commands in Command Mode
You should know several miscellaneous commands that are available from com-
mand mode. Perhaps the most important is the undo command ( u), because it’s
easy to make mistakes with vi. Vim enhances the undo feature over vi’s. Standard
viallows you to undo only the most recent insert or change, whereas Vim allows
numerous undo levels, like a modern word processor. In traditional vi, repeating
an undo twice would simply redo the change. Because Vim allows multiple levels ofundo, it had to introduce a new redo command, which is 
Ctrl+R in command
mode. Table 4-9 lists these and other commands you should know.
Control keys are used sparingly as commands in vibecause they are part of the
ASCII character set. Some control keys are vitally important, such as Ctrl+M ,
which is a carriage return.3Other control keys are relics of the teletype era, such as
Ctrl+R , which ASCII defines as Device Control 2. That’s probably why Vim uses
Ctrl+R for undo, because it isn’t likely to clash with any pseudoterminal features.
TABLE 4-9 Miscellaneous Commands
Command Description
u Undo the most recent change or insert. This command can be used
multiple times to undo multiple changes.
Ctrl+R Redo the most recent undo (Vim only).
m{letter} Set a bookmark at the current line, using the given letter. The lettercan be any lowercase letter as defined by the current locale.
. Repeat the last change or insert.
Ctrl+L Redraw the screen. This command is useful when a backgroundprocess prints text to the screen and garbles the output.
zt Redraws the screen, placing the current line and the cursor at thetop of the screen.
zz Redraws the screen, placing the current line and the cursor in themiddle of the screen.
zb Redraws the screen, placing the current line and the cursor at the
bottom of the screen.154 Chapter 4 • Editing and Maintaining Source Files
3. See the ascii(7) man page for more details. Note that Ctrl+A is ASCII \001, Ctrl+B is ASCII \002,
and so on.
4.2.4.7 Cut, Paste, and Delete Commands
Historically, vidocumentation uses the term register to refer to what is conven-
tionally called a clipboard today. No doubt this is part of its legacy as a program-mer’s text editor. Similarly, what we call cutand paste today are referred to in 
vi
documentation as yank and put. Luckily, the term for delete is delete .
One difference between these commands and the change commands is that none
of them uses insert mode. When you execute one of the commands listed inTable 4-10, you return to command mode. Like most 
vicommands, these com-
mands can be repeated, so typing 5p, for example, will paste the contents of the
clipboard five times.
TABLE 4-10 Delete, Cut, and Paste Commands
Command Description
D Delete from the current cursor position to the end of the line.
Data is saved in the default clipboard (register).
d{motion} Delete some number of characters starting at the current cursorposition. The number of characters is determined by the 
motion
argument. Type ddto delete the current line. Data is saved in the
default clipboard (register). Valid motion commands include anyof the commands listed in Table 4-4.
y{motion} Copy (yank) some number of characters to the clipboard(register). The number of characters is determined by the
motion argument. Type yyto copy the current line to the
default clipboard (register). Valid motion commands includeany of the commands listed in Table 4-4.
p Paste (put) the characters in the clipboard at the current cursorposition, starting with the character after the character under thecursor.
P Paste (put) the characters in the clipboard at the current cursorposition, starting with the character before the character under
the cursor.
The yank and delete commands take a single argument labeled motion , which
tells vihow much to delete or copy. The motion argument can be any of the4.2 The Text Editor 155
cursor-movement commands listed in Table 4-4, as well as the additional motion
commands listed in Table 4-11.
The commands in Table 4-11 seem a bit arbitrary until you put them to use.
Believe it or not, the names are mnemonic. Table 4-12 lists some basic patterns for the
dand ycommands that show you how to combine them with motion commands.156 Chapter 4 • Editing and Maintaining Source Files
TABLE 4-11 Additional Movement Commands Used Only with Commands
Command Description
f{char} Position the cursor under the first column to the right that matches
the given character.
t{char} Same as fexcept that the cursor is positioned one column to the left
of the matching character.
F{char} Position the cursor under the first column to the left that matchesthe given character.
T{char} Same as Texcept that the cursor is positioned one column to the
right of the matching character.
TABLE 4-12 Some Cut-and-Paste Patterns
Pattern Description
dfa Delete characters from the current cursor position to the right, up toand including the first a. Stated mnemonically, delete until you find a .
dta Delete characters from the current cursor position to the right, up to
but not including the first a. Stated mnemonically, delete everything
up to a . 
5yta Copy characters from the current cursor position up to the fifth
occurrence of the letter a.
yy4p Copy the current line and paste four more copies to the buffer. Thisis two commands: (1) 
yy(yank the current line) and (2) 4p(paste
the default register four times).
Pattern Description
dn Delete characters from the current position until the first match of
the most recent search. This is the dcommand with nas the motion
command.
d'a Delete characters from the current cursor position until the positionmarked 
a. This is a combination of a dcommand and 'aas the
motion command.
yG Copy all lines from the current line to the end of the file. This is a y
command combined with the motion command G. Recall that the G
command positions you at the end of the file.
y50G Copy all lines between the current line and line 50, including thecurrent line and line 50. This is a 
ycommand combined with the
motion command 50G.
d5l Delete the next five characters, starting at the current cursor position(that’s a lowercase L, not a one). This is a combination of 
dwith 5l
as the motion command.
5dd Delete the current line and the following four lines. This is a dd
command preceded by a repeat count of five.
In the patterns shown in Table 4-12, the commands yand dare interchangeable.
The patterns are the same for both commands, although the effects are different.
Finally, the delete, yank, and put commands can be used with multiple registers
(clipboards), which I left out for clarity. Each register is user defined and designatedby a single lowercase letter. The desired register is indicated by a leading quote fol-lowed by the register name. To yank (copy) the current line into a register named a,for example, type the following:
"ayy
POSIX states that register names are limited to lowercase characters as defined by
the locale, which means in English that you have 26 named registers available plusa default register. When you use an uppercase letter for the register name, the textthat you yank or delete is appended to the register.4.2 The Text Editor 157
To put (paste) the contents of register a into the buffer, follow the same pattern:
"ap
Before Vim, the user registers were the only way to copy and paste data from one
file to another. T raditional viclears the default register when you switch buffers, so
anything you yanked or deleted is lost when you switch buffers. Vim preserves thedefault buffer when switching buffers, which is another enhancement over 
vi.
4.2.4.8 Ex Mode
This is where vistarts to look a bit like a Frankenstein’s monster. Ex is the name of
the line-oriented editor that viis built upon, and many of the commands in Ex
mode are from the original editor. It’s hard to imagine that ex was once a text edi-tor that people used to get their work done, but it was. Ex is a line-oriented editor,which means that it works with files one line at a time. It seems as though this oughtto be forgotten dinosaur DNA, but many important tasks are still done in Ex mode.Because Ex is a fully functional editor, many of the tasks you can do in commandmode can also be done in Ex mode. Ex mode is used to provide complex commandsor commands that take arguments. In general, Ex commands fall into the easy-to-remember category.
In Figure 4-1, you can see that you enter Ex mode from command mode by
pressing the colon (
:). All the text that follows, up until you press Enter , is inter-
preted as an ex command. When you press Enter , the command is executed, and
vireturns to command mode.
Ex commands all have the same basic form, which is an optional line number or
range of line numbers followed by a command, as follows:
:[firstline][,lastline]command
The most frequently used commands typically are one or two letters, but others
can be longer. The line numbers are optional. If you don’t specify any, the commandapplies only to the current line. If you specify only one line number, the commandapplies to the specified line. If you want the command to affect a range of lines, youmust specify a starting line and an ending line. To delete lines 25 through 30 inclu-sive, for example, you can use the 
dcommand as follows:
:25,30d
Several shortcuts are available as alternatives to entering specific line numbers.
Some of the most common ones are shown in Table 4-13.158 Chapter 4 • Editing and Maintaining Source Files
Note that the shortcuts in Table 4-13 can apply to both the start address and the
end address. You could delete a block of text starting with the word Begin and end-
ing with the word Endby using the following form:
:/Begin/,/End/d
When you are using a search instead of a line number, the search begins at the
current line. In this example, I use a forward search, so the command deletes thefirst line that has the word Begin following the current line. You can mix and match
line numbers and searches in commands. You could delete everything from line 1to the first line that has the word Endwith the following command:
:1,/End/d
Like all Ex commands, the delete command may apply to one or more lines, but
each line is treated equally. You can’t delete from line 3, column 5 up to line 10, col-umn 17 with a single command, for example.
A very useful shortcut listed in Table 4-13 is the marker. The apostrophe is the
same key used to jump to marked lines in command mode, so it’s not hard toremember when you get used to using it.
Table 4-14 lists a few of the most essential Ex commands you are likely to use 
in 
vi. Note that almost every command can take a range of addresses as I have4.2 The Text Editor 159
TABLE 4-13 viShortcuts for Specifying Line Numbers in Ex Mode
Char Shortcut
. The current line number.
$ The last line number in the file.
% A shortcut specifying the entire file—the same as typing 1,$.
'a Location of tag a. Recall that tags are set in command mode with the m
command.
/{expr}/ The next line that matches the regular expression.
?{expr}? The previous line that matches the regular expression.
\/ The next line that matches the most recent regular expression.
\? The previous line that matches the most recent regular expression.
\& The next line that matches the most recent substitution.
specified here, whether it makes sense or not. The :wcommand, for example, is
used to update the current file on disk. This command can take an argument towrite the current buffer to a different file, but it can also take a range of addresses,such as:
10,20w foo.dat
This writes lines 10 through 20 to a file named foo.dat . Not every command
allows an address or range of addresses, but most commands do. It may seem unnec-essary or unusual for some commands, but it comes in handy on occasion.
TABLE 4-14 Essential Ex Commands
Command Short Form Description
write :w {filename} Write the current buffer to the given filename.
The filename is optional. Without it, viupdates
the current file on disk. Use :w!to force writing
to a file that is marked read-only. Use :woften
to update your work on disk.
quit :q Quit vi(does not comply if the buffer is not
saved). Use :q!to force vito quit and discard
modifications. See also :e.
xit :x Quit viand save unsaved data. This fails if the
file is read-only. Use :x!to force writing to a
file marked read-only. The :wqcommand can
also be substituted.
edit :e {filename} Open the named file for editing in a new buffer.The current file is not closed. When no filenameis specified, 
vireopens the current file without
saving any changes but will not discard editswithout your permission. So if you want to loseyour edits and start over, use 
:e!.
delete :d Delete the current line or range of lines.
map :map {a} {b} Remap the keys used in command mode. Withno arguments, it prints out the current settings.
set :set {argument} Change default settings for Vim (see Table 4-18).This command is most useful in your 
.vimrc file.160 Chapter 4 • Editing and Maintaining Source Files
Command Short Form Description
help :help Enter the Vim help system. Help can take a keyword
argument and does a decent job of finding relevantinformation. The help is hyperlinked using 
vi’s tags
capability, which means you have to be somewhat
competent with vito get help from Vim.
It’s interesting to note that many of the commands that Ex understands are also
understood by the sed(stream editor) command. What you learn here can help you
elsewhere in scripting tasks.
4.2.4.9 Vim Enhancements in Insert Mode
While you’re in insert mode, the keyboard behaves much as you would expect. You
type, and text appears. It’s as simple as that—almost. This is where Vim has addedsome significant improvements over 
vi. Vim makes several commands available in
insert mode, which is a capability that vidoes not have. The list is too numerous
to mention here, but Table 4-15 describes some of the most useful ones.
TABLE 4-15 Vim Commands in Insert Mode
Command Description
Ctrl+N/Ctrl+P Complete the word from existing words in the document. Unlike some
editors, Vim doesn’t require you to provide any letters to guess at amatch, although it helps to narrow the search. Press 
Ctrl+N again to
produce the nextmatch. Press Ctrl+P to go back to the previous match.
Ctrl+T/Ctrl+D (Also valid in vi) Shift current line right or left by one shiftwidth
setting. This defaults to eight columns but can be changed with the
set shiftwidth command.
Ctrl+R Insert the contents of one of the special registers used by Vim into
the current document. Type :help registers to see what registers are
available. Each register has a single, arbitrarily chosen character fora name. These include things like % for the current filename and .(period) for the most recent insert.
Ctrl+V (Also valid in vi) Enter nonprintable characters, such as control
characters, that otherwise might have be interpreted as a vicommand.
It’s not a good idea to put control characters in scripts, but see the
sidebar for a useful application.4.2 The Text Editor 161
Example Use of Ctrl+V
One customization I use in my visessions is to remap the Tkey in command
mode so that it will switch between the current buffer and the alternate buffer(think toggle). In other words, I define the 
Tkey to be equivalent to the following
excommand:
:e#
To define (or map) keys in command mode, you use the map command in Ex
mode. To map the Tkey the way I want, the command looks something like this:
:map T :e#
Unfortunately, this does not work. The problem is that I am trying to use an ex
command to describe another excommand, but excommands don’t take effect
until you press Enter . When I press Enter to complete the map command, it
maps the Tkey to an incomplete ecommand. I need an additional Enter key to
be included with the mapping for the Tkey. This is where Ctrl+V comes in
handy.
When you type a control character preceded by Ctrl+V , it is escaped , which
means that it is interpreted literally and not interpreted by the terminal. viuses the
^character to represent escaped control characters. An escaped Ctrl+C character, for
example, is displayed as ^C.
For this problem, I need to know that visees the Enter as an ASCII carriage
return (\015 or Ctrl+M ). With this knowledge, I can enter the same mapcommand
followed by Ctrl+V and Ctrl+M as follows:
:map T :e#^M
Notice that the Ctrl+V character does not appear in the output; only the charac-
ter that follows Ctrl+V appears. Now the mapping for Tcontains the additional car-
riage return that exrequires.
The complete list of commands includes some that seem a bit silly, such as
Ctrl+E , which inserts the character below the cursor at the current position. This
seems silly only until you find a use for it, of course.162 Chapter 4 • Editing and Maintaining Source Files
4.2.4.10 Search and Replace
Although you can do searches in command mode, search-and-replace commands
are accessible only in Ex mode. This is because Ex mode is the only mode forlengthy arguments, and regular-expression search-and-replace operations can getquite lengthy.
The basic Ex command to replace text is the 
substitute command, which can
be abbreviated as subst or, more often, s. Like all Ex commands, it takes a line
number or a range of line numbers with all the abbreviations listed in Table 4-13available. The basic 
substitute command looks like this:
:s/search/replacement/flags
The search string is the only required argument. The replacement and flags
parameters are optional. With no replacement , the search string is replaced by an
empty string. By convention, all arguments are delimited with a forward slash (/),which can be a problem when you are working with filenames. 
virequires you to
escape the slashes with a backslash, for example, which is a nuisance. This is what a
substitute command with a pathname looks like in vi:
:s/\/usr\/bin\/file1/\/usr\/bin\/file2\//
This is what Larry Wall, author of the Perl programming language, calls “the
leaning toothpick syndrome” because of its appearance. Aesthetics aside, it is justplain difficult to type. 
viwill allow you to use just about any punctuation charac-
ter as a delimiter.4It assumes that the first character following the command is the
delimiter, and it looks for that character to parse arguments. In Vim, you can usethis much-easier syntax to work with filenames:
:s#/usr/bin/file1#/usr/bin/file2/#
In this case, I chose to use the pound sign ( #) to delimit the arguments, which
allows me to use forward slashes in the string arguments.
By default, the subst command replaces the first match found on each line spec-
ified. If you want to replace every occurrence of the search string on a line, use the
gflag to the substitute command as follows:
:s/some text/some other text/g4.2 The Text Editor 163
4. Actually, the entire family tree of tools built on edallows this as well, but it’s not used often.
One restriction that may be apparent is that the search string must be on one
line. This is a restriction of Ex commands in general, because there is a one-line-per-command rule. Embedded newlines are not allowed in search strings, althoughthe replacement may contain multiple lines by using escaped carriage-return char-acters (
Ctrl+V Ctrl+M ).
This section does not have enough space to do justice to the power of the regu-
lar expressions used in the search and replace commands. For further information,you can start with the 
regex(7) man page.
4.2.4.11 Browsing and Building Code
Vim works in conjunction with a program called ctags , which creates an index of
your source files that can be read by Vim. Running ctags can be as simple as
$ ctags -R
which automatically recognizes source files in the current directory and subdirecto-ries, and creates a single index file named 
tags . This is what Vim looks for when it
starts. POSIX requires only that ctags and viwork with C and Fortran files, but
Exuberant ctags , which comes with most Linux distributions, can index source
files in many languages, including C++, Java, and Python.
Essentially, ctags makes your editor (in this case, vi/Vim) behave something
like a Web browser, with your source behaving like a Web page. ctags does not
modify your source. The only output is a single index file. But when visees this
index, function calls and variable names become hyperlinks. If you are looking at afunction reference, you can follow the link and be taken to the declaration of thatfunction. Follow a link for a class instance (in C++), and you will be taken to thedefinition for that class. Just as you can in a Web browser, you can back out of linksthat you jump to, returning to the place you started. This is an excellent tool fordevelopers.
Code browsing in 
vican be done in command mode or Ex mode but not in
insert mode. Table 4-16 lists the most useful commands for browsing code. Severalof these commands are unique to Vim, which enhances 
vi’s code-browsing facili-
ties significantly. T raditional viallows only the basic jumps and has no support for
multiple tag matches, which can occur in C++ code that uses function overloadingor namespaces. Note that Vim maintains a tag “stack” so that you can back out ofyour jumps as though you were clicking the Back button in your browser.164 Chapter 4 • Editing and Maintaining Source Files
TABLE 4-16 Commands for Browsing Code
Command Short Form Function
Ctrl+] Jump to tag under cursor.
:tagname :ta Jump to specified tag. If no tag is specified,
jump to the tag under the cursor.
Ctrl+T Return from current tag to most recentjumping-off point.
:pop :po (Vim only) Same as Ctrl+T except that you
can specify a count to go up multiple levelswith a single command.
:tnext :tn (Vim only) Jump to next match when a tagproduces more than one match, such as anoverloaded C++ function. This does not affectthe tag stack.
:tprevious :tp (Vim only) Same as :tnext but moves to
previous match.
:tselect name :ts (Vim only) Show a list of matching tags youcan select when a tag produces more than onematch.
:tags (Vim only) Show the current tag stack with one
line for each tag.
The GUI version of Vim even has some pretty buttons to perform most of the
functions listed in Table 4-16, which makes it look even more like a Web browser.
Seamlessly moving between building code and editing code is one of the key fea-
tures of an IDE. Many developers find this to be an essential productivity tool. Vimdoesn’t claim to be an IDE but does have features to edit and build code simulta-neously. Rather than try to take over your whole project, as many IDEs do, Vimrequires you to provide a 
Makefile . Then you can call make from Vim with the
:make command, which doesn’t seem like much of a feature. But when you do it
this way, Vim will save the output of the compilers and allow you to visit each line4.2 The Text Editor 165
of source that produced an error or warning. To build your code, use the :make
command in Ex mode as follows:
:make arguments
Use the :make command as you would call make from the shell. You can specify
additional flags or targets, if you like. After it runs, Vim saves the warnings anderror messages to allow you to navigate the source. The commands for this are listedin Table 4-17. Note that this is a Vim feature. POSIX 
vidoes not have a make com-
mand or any of the commands listed in Table 4-17.
By default, Vim understands the errors and warnings produced by gccand g++,
but you can tweak it to understand other compilers with the errorformat setting.5
Likewise, if you don’t use make to run your builds, you can change the program that
the :make command runs via the makeprg setting.
4.2.4.12 Customizing vi Settings
Many settings control the way vibehaves and are modified in Ex mode via the
:set command. Historically, viuses a file in your home directory named .exrc
to read your personalized settings, but Vim prefers to use .vimrc . If both files are
present, Vim will ignore your .exrc and read only your .vimrc . This distinction
is important if you use more than one viclone. You can expect all clones to read166 Chapter 4 • Editing and Maintaining Source Files
TABLE 4-17 Vim Code Build Commands
Command Short Form Function
:make arguments :mak Run make in the current directory and
capture errors and warnings.
:cnext :cn Jump to the source line of the next error orwarning in the most recent build.
:cprev :cp Jump to the source line of the previouserror or warning in the most recent build.
:cfile filename :cf Read a list of errors from the file forprocessing with :cnext and :cprev. This is
an alternative to using :make.
5. Type :help errorformat for more information.
your .exrc file, but probably none of them besides Vim will look for .vimrc . For
this case, it’s a good idea to keep your vanilla visettings in your .exrc file and then
use the source command in your .vimrc to read your .exrc as follows:
:so ${HOME}/.exrc
In addition to this line, you can use the .vimrc file to include commands that are
understood only by Vim. Table 4-18 contains a list of some useful settings that youmay want to modify in your 
.vimrc or .exrc file.
TABLE 4-18 User-Modifiable Settings
Short Example
Setting Form Usage Description
tabstop ts set ts=4 Set the number of columns per tab stop
(default is 8). This affects how textcontaining tabs is displayed or expanded.
shiftwidth sw set sw=4 Set the number of columns to shift withusing the shift commands (default is 8).Notice that this is independent of the
tabstop setting.
autoindent ai set ai T urn automatic indenting on or off: aifor
on and noai for off(default is off).
expandtabs et set et Do not insert tabs; instead, use the numberof spaces defined by 
tabstop . The default
is to use hard tabs (ASCII code \011).
wrapscan ws set ws Change the search behavior: When thisoption is turned on (the default), a forwardsearch may find a match on preceding lines,and a backward search may find a match ona subsequent line. This applies to searches inboth command mode and Ex mode.
When turned off, a forward search searches
only from the current line to the end of the file. Likewise, a backward searchsearches only from the current line to thebeginning of the file. 4.2 The Text Editor 167
continues
TABLE 4-18 Continued
Short Example
Setting Form Usage Description
syntax sy sy on (Vim only) T urn syntax high-
lighting on or off. Notice that thisdoes not use the 
setcommand.
makeprg mp set mp=ant (Vim only) Choose an alternative to make to run your build.
errorformat efm set efm=%f\ %d (Vim only) Specify a scanf -like
string for Vim to use to parse theerror output from the compiler. Fora full description of the format, see
:help efm in the Vim help system.
Notice that some settings are made via the :set command; other settings are
themselves commands (such as :syntax ).
4.2.4.13 GUI Mode
Vim has a GUI available, usually as a separate program named gvim in GNOME
systems and kvim in KDE.6The GUI is an excellent enhancement and a great way
to learn vi. If you despise modes and for some reason still want to use Vim, use the
modeless option of gvim (-y). This makes Vim behave like a typical modeless GUI
editor for the faint of heart.
Each menu in GUI mode has reminders for the equivalent vicommand to help
you learn commands you may not be familiar with. The menu shows the keystrokesrequired to use the command without the GUI, so it is an excellent learning tool.Finding the right packages to install the GUI can be tricky, depending on your dis-tribution. Table 4-19 lists a couple of packages for KDE and GNOME.168 Chapter 4 • Editing and Maintaining Source Files
6. Interestingly, although many KDE and GNOME apps coexist peacefully, kvim and gvim cannot be
installed at the same time.
TABLE 4-19 Package Names for GUI-Enabled Vim
Distribution Package Name
Knoppix (KDE) vim-gtk
Ubuntu (KDE) kvim
Ubuntu (GNOME) vim-gnome
Fedora (GNOME) vim-X11
4.2.4.14 The Bottom Line on Vim
Recalling the list of features in Table 4-2, let’s look at how to use them. Table 4-20
presents a summary of the features and how to access them.
Many more features are available in Vim. The most comprehensive help infor-
mation is in Vim’s own help menus. These are accessible as read-only, tagged docu-ments, which can be opened in command mode by typing 
:help keyword .
Unfortunately, navigating Vim’s help system requires some knowledge of viand
tags. Just remember that to close a help page, you type :q.
TABLE 4-20 How to Access Core Features in Vim
Feature How
Brace matching In command mode, type %.
Syntax highlighting Normally on by default. You can enable and disable it
manually by typing :syn on or :syn off in Ex mode.
Autocompletion Ctrl+N / Ctrl+P in insert mode.
Regular expressions Available during search in command mode and via the
substitute command in Ex mode.
Automatic indenting Enable and disable from command mode with :set ai
and :set noai .
Code browsing In command mode, Ctrl+] to follow the tag of the text
under the cursor and Ctrl+T to return from the tag.4.2 The Text Editor 169
4.2.5 Emacs
Emacs is the flagship text editor of the GNU project. As an alternative to vi,
Emacs has a loyal following. Emacs comes with a script processor based on the Lispprogramming language so that anyone can program extensions (anyone whoknows Lisp, that is). Because the primary users of Emacs are programmers, theresult is that Emacs has become something of a sandbox for developers over theyears, having accumulated many features that have little or nothing to do with textediting. If these happen to be features you are looking for in a text editor, Emacswins hands down.
4.2.5.1 Emacs Features
As you might expect, Emacs has all the features listed in Table 4-2, but finding them
can be difficult. Along with the basics, Emacs has several tools for manipulatingsource code that you are not likely to find elsewhere. In most instances, these areshell commands that have been integrated into the editor.
4.2.5.2 Modes? What Modes?
Emacs claims to be modeless, which is what 
videtractors consider to be vi’s ugliest
wart. Because Emacs is by nature a terminal-based editor, modelessness is some-thing of an illusion. Emacs has modes. The difference is that the modes that Emacsuses typically are transient—that is, you enter the mode when you input a com-mand, which may require additional arguments or interaction on your part. Whenthe job is done, you return to the default mode. The Emacs default mode variesbased on the type of file.
If you are a 
viuser, you can think of Emacs being in insert mode all the time.
There is no command mode or Ex mode. Instead, Emacs relies on key combina-tions using the 
Ctrl and Meta keys on the keyboard.7This technique does have
drawbacks, because control keys map to valid ASCII characters, some of which haveimportant functions. 
Ctrl+G , for example, maps to the ASCII BEL character
(\007), which causes the terminal to beep. T ry it for yourself by pressing Ctrl+G in
your terminal window. Ctrl+G happens to be what Emacs uses to abort a command
sequence.170 Chapter 4 • Editing and Maintaining Source Files
7. On a PC, the meta key is labeled Alt. 
Meta keys are also a problem if you are using a GUI terminal window like
gnome-terminal , because your terminal may map the Meta key to some other
purpose. The gnome-terminal happens to trap the Meta key to allow keyboard
access to the GUI menus, which supersedes Emacs making it unusable in textmode.
8In general, when you run Emacs in its own GUI window, these problems
don’t exist.
Another thing to know about Emacs is that the default mode can vary based on
the type of file you are editing. As a programmer, you probably are interested in CC
Mode, which is the mode Emacs uses to edit C, C++, Java, and others. CC Mode
handles automatic indenting and is complex enough to have its own info page. By
default, Emacs starts in CC Mode when it recognizes the file you are editing assource code. By contrast with 
vi, in which automatic indentation is a gentle hint,
CC Mode automatic indentation is in your face. It works hard to make sure youdon’t deviate from your chosen indentation style. The supported styles are docu-mented in the CC Mode 
info page and are listed in Table 4-21.
TABLE 4-21 Emacs Styles Used for Autoindentation
Style Description
gnu The default style used by Emacs, “blessed” by the Free Software
Foundation
K&R The style used in the Kerninghan and Ritchie examples
bsd Also known as Allman Style; similar to K&R
whitesmith Based on the style used with examples from the Whitesmith Ccompiler—a commercial compiler used on PDP-11
Stroustrup C++ style used by Stroustrup
ellemtel Named for Ellemtel Telecommunication Systems Laboratories,which published this style
linux Style used in the Linux kernel
python Style used for writing C extensions to Python
java Style used for Java code.4.2 The Text Editor 171
8. You can override this by disabling the keyboard shortcuts listed in the Edit menu in gnome-terminal .
By default, Emacs uses the GNU style, which is the style found in the Emacs
source code. This style is not found in many other places however, so you probablywill want to change the style. I demonstrate how to do that in a later section.
4.2.5.3 Emacs Commands and Shortcuts
Emacs relies heavily on nonprintable key sequences to implement commands.
These are keys that use the 
Ctrl , Alt, and Esckeys on the keyboard. I will use the
Emacs convention for documenting commands summarized in Table 4-22. Notethat Emacs documentation refers to the PC’s 
Altkey as the Meta key. Some legacy
systems do not use a PC keyboard and do not have an Altkey. The Esckey is used
differently from the Ctrl and Meta keys, because pressing Escproduces an ASCII
character (\033). By comparison, the Ctrl and Altkeys by themselves do not pro-
duce and output. Commands that use Esc, therefore, require two keystrokes. Note
that on systems where there is no Meta key (or the Meta key has been assigned forother purposes), you can use the 
Esckey instead. M-xis equivalent to Esc x , for
example.
Each Emacs command has a name, and most commands have a shortcut. Every
command can be executed by typing Esc x or M-x followed by the command name.
Because command names tend to be descriptive, they also tend to be rather long.The Backspace key, for example, is mapped to the 
delete-backward-char com-
mand. If you are a glutton for punishment, you could press the following keysequence instead of the Backspace key:
M-x delete-backward-char
TABLE 4-22 Emacs Convention for Documenting Commands
Notation Description
C-character Hold down the Ctrl key while you press the indicated
character.
M-character Hold down the Altkey (aka Meta key) while you press the
indicated character.
Esc character Press the Esckey and then press the indicated character or
control sequence; same as M-character .172 Chapter 4 • Editing and Maintaining Source Files
I will list commands by their shortcuts for the most part, especially for commands
that you are not ever likely to type in this way (such as delete-backward-char ).
When you type M-x, Emacs enters what it calls minibuffer mode. In minibuffer
mode, Emacs allows you to save typing with tab completion. Type the first few let-ters of the command name, and press Tab. If there is an unambiguous match,Emacs will complete the command and allow you to press 
Enter . If there is more
than one match, you are prompted with a list of possible matches. Minibuffer modealso keeps a history of commands, so 
M-xfollowed by an up or down arrow allows
you to scroll through the history of your most recently used commands.
4.2.5.4 Cursor Movement
Like vi, Emacs allows the user to move the cursor without moving from the home
position on the keyboard. The basic movements are listed in Table 4-23.
On a PC keyboard, the cursor keys also work, as well as Page Up and Page Dn .
Emacs also provides the ability to repeat commands a given number of times.Repeats in Emacs are done by preceding a command with 
C-uand the number of
repeats. This sequence moves the cursor five characters to the right:
C-u 5 M-f
You can apply this pattern to any Emacs command.
TABLE 4-23 Basic Cursor Movement in Emacs
Keys Movement
C-b Left one column (mnemonic: back)
C-f Right one column (mnemonic: forward )
C-n Down one column (mnemonic: next)
C-p Up one column (mnemonic: previous )
C-v Down one screen
M-v Up one screen
M-f Right one word (mnemonic: forward )
M-b Left one word (mnemonic: backward )4.2 The Text Editor 173
4.2.5.5 Deleting, Cutting, and Pasting
Because of its age, Emacs does not use the contemporary terms for describing fea-
tures such as the clipboard, cut, copy, and paste, although it has all these features.Likewise, it does not use the same terms as 
vi. The basic cut operation in Emacs is
called kill,and a paste operation is called yank, as in “yank text from the clipboard.”
viusers note that this is the opposite direction from a viyank.
To cut or copy an arbitrary region of text, you first have to set a mark. You do
this by moving your cursor to one end of your region and marking it with the C-@
command. The other end of your region is defined by wherever your cursor hap-pens to be when you call the appropriate cut or copy command. The basic com-mands are listed in Table 4-24.
4.2.5.6 Search and Replace
Emacs searches have two basic forms, illustrated in Table 4-25. Notice that the basic
form does not use regular expressions. To get a regular expression, you precede thesearch command with an 
Esccharacter.
TABLE 4-24 Basic Cut and Paste Commands in Emacs
Keys Movement
Set a mark to define a region of text to be used with kill and
copy commands
C-k Cut text (aka kill) from the cursor position to the end of the line
M-k Cut text from the current cursor to the end of the sentence
C-w Cut a region of text from current cursor position to themark set with 
C-@
Copy a region of text from the current cursor position tothe mark set with 
C-@
C-y Paste (aka yank) text from the clipboard beginning at the
current cursor position
Undo C-_ C-x uESC w C-InsC-@ C-Space174 Chapter 4 • Editing and Maintaining Source Files
TABLE 4-25 Emacs Search-and-Replace Commands
Keys Movement
C-s Search forward using exact matching (no regular expressions)
C-r Search backward using exact matching (no regular expressions)
Esc C-s Search forward using a regular expression
Esc C-r Search backward using a regular expression
Emacs uses an incremental mode for searching that is very useful. When you press
C-sto start a forward search, you are prompted for a search string. As you type the
string, Emacs finds the closest text that matches your string and moves the cursor whileyou type. In addition, it highlights all the matching text that is visible onscreen. Whenyou press Backspace, the cursor moves backward as well, so if you delete the search,you are left where you started. Emacs remains in this mode until you press 
Return .
Before you press Return , you can press either C-sor C-rto move to the next
match or previous match, respectively. Leaving the search mode leaves you at thefirst matching string and removes all the highlighting. After you have left searchmode, you can repeat the last search again by entering the command twice—forexample, 
C-sfollowed by C-s.
4.2.5.7 Browsing and Building Code with Emacs
Emacs uses the same principle as vifor browsing code. You create an index of your
source code using a utility like etags , which comes with the Exuberant Ctags pack-
age. etags takes a list of source files that you want to index and creates an index
file named TAGS in the current directory. This file is what Emacs uses to find its way
through the source. Some useful commands for browsing code in Emacs are shownin Table 4-26.
Another useful feature is the bookmark, which allows you to mark a point in a
text file with a meaningful name. To set a bookmark named review, the sequencewould look like this:
C-x r m review
The name review is now saved for later reference. To go back to that line, you
would type the following sequence:
C-x b m review4.2 The Text Editor 175
TABLE 4-26 Emacs Commands for Browsing Code
Keys Movement
M-.    Esc-. Jump to a tag. This command prompts you to enter a tag.
Press Return to jump to the tag under the cursor.
M-*    Esc-* Return from the current tag to the most recent jumping-offpoint.
C-u M-. Find next alternative tag for most recent tag (for overloadedC++ functions, for example).
C-u – M-. Go back to previous alternative tag found.
C-x r name Set a bookmark at the current cursor position with the given name.
C-x b name Jump to the bookmark with the given name.
Bookmarks are saved to disk. You can quit Emacs, and the next time you run it,
it will remember the bookmarks that you set earlier.
4.2.5.8 Text Mode Menus
Emacs commands can be not only difficult to remember, but also downright diffi-
cult to type. The key sequences required to type Emacs commands can be especiallydifficult for people with repetitive-strain injuries, such as carpal tunnel syndrome.With the GUI version of Emacs, this is not a problem, because the GUI menusallow you to access most commands via the mouse. Recent versions of Emacs alsoallow you to access menus in text mode. To use the menu, press F10 on the key-board, and you will be prompted for further input. It’s not as convenient as a mousebut more intuitive than remembering a bunch of control sequences, and it’s easieron the wrists. Figure 4-2 shows what this looks like.
GNOME users beware 
gnome-terminal intercepts the F10 key. If you want to
use text-mode menus in Emacs you must disable this feature in the KeyboardShortcuts
9section of the Edit menu of gnome-terminal .
9. You might ask, “Why use emacs in text mode if you are running GNOME?” One occasion where this
would be necessary is if you are running emacs remotely on an embedded target with no X libraries.176 Chapter 4 • Editing and Maintaining Source Files
4.2.5.9 Customizing Emacs Settings
One drawback to using Emacs is that it compels you to learn some Lisp program-
ming. That is, if you want to customize even the simplest settings you must use Lispsyntax. When Emacs starts up it looks for a file in your home directory named
.emacs . This file contains Lisp statements and functions that can be used to change
the default settings of Emacs.
An introduction to Lisp is beyond the scope of this book, but there are numer-
ous resources in the Emacs info pages and on the Web. Let’s look at a simple exam-ple to demonstrate how settings are customized in Lisp, which will also illustrate thehurdles involved. You will see what it takes to change the default indentation style,which is perhaps the first thing you will want to change. I’ll set it to K&R style, andwhile I’m at it, I’ll set the tab stops at four spaces (instead of the default eight) andturn on syntax highlighting. The 
.emacs file for this can look like the following:
1. (defun my-c-style ()
2.   (c-set-style "k&r")3.   (turn-on-font-lock)4.   (setq c-basic-offset 4))5. 6. (add-hook 'c-mode-common-hook 'my-c-style)7. (setq indent-tabs-mode nil)4.2 The Text Editor 177
FIGURE 4-2 Emacs Editor in Text Mode Using Menus

The file starts by defining a Lisp function named my-c-style , which will be
called when we enter CC Mode. The first line of this function on line 2 is straight-forward and sets style to 
k&r. Line 3 turns on syntax highlighting font-lock is
what Emacs calls it. Finally set the number of spaces per indent to four by settingthe value of 
c-basic-offset on line 4. That’s it for the my-c-style function,
which ends with a closing parenthesis. Line 6 installs my-c-style as a hook func-
tion to be called whenever Emacs sets the mode to CC Mode. This is a global set-ting and therefore can be done in a stand-alone expression. Finally, line 7 containsanother stand-alone expression that disables the use of tabs in indentation of codeby setting the value of 
indent-tabs-mode to nil.
As you can see, changing the default behavior in Emacs is not trivial. The basics
of Lisp expressions are not hard to master, but can be intimidating if you are notfamiliar with it. Often you can find what you need on the Web ready to cut andpaste into your 
.emacs file. Advanced Emacs users (who by definition are Lisp pro-
grammers) have created and accumulated many Lisp libraries to customize Emacsextensively. This cuts both ways, however. If you get too accustomed to nonstan-dard customizations, you could find yourself a fish out of water should you have towork on a system without access to your files. Custom hooks that make you pro-ductive are great. Just don’t get too attached to them.
4.2.5.10 Emacs for 
viUsers
There is another feature of Emacs for viusers called Viper, which is the name for
the vicompatibility mode for Emacs. In fact Viper has its own info page and is
documented as though it were a separate editor. Viper can be enabled by modify-ing your 
.emacs file as follows:
(setq viper-mode t)
(require 'viper)
The first time you start Emacs in Viper mode, you will be greeted with a lengthy
help message describing the features of Viper and asking you if you want to dis-able the message next time you start Viper. The next thing you are asked is tochoose a level from 1 to 5. Level is closest to 
viwith virtually no Emacs features
each increment brings you closer to Emacs nirvana. Viper does not read your .exrc
or .vimrc files like viand Vim. Instead, Viper stores changes in your home direc-
tory in a file named .viper , which contains (you guessed it) Lisp statements.178 Chapter 4 • Editing and Maintaining Source Files
Although Viper claims to make some improvements over vi, Viper is not Vim,
and many of the features listed in the previous section are not available in Viper. Ifyou are a 
viuser who wants to explore Emacs, Viper is a useful alternative.
4.2.5.11 GUI Mode
Most major Linux distributions include the GUI version of Emacs (technically
called Xemacs) as their default Emacs editor. Xemacs was a fork of Emacs, but nowit seems these two programs have merged into one big happy executable. Whencompiled with a GUI, Emacs can still function in text mode but will not do sounless you explicitly specify the 
-nw (no windows) option (you can also run
emacs-nox) . Note that even if you aren’t running an X server, this version of
Emacs still tries to bring up the GUI, and instead of falling back to text mode, itquits with an error message.
The GUI behaves exactly like text-mode Emacs. All the same commands are
accepted, but if you have trouble remembering them, there are always the mouseand menus. Emacs could be mistaken for any other GUI editor except that itdoesn’t typically have the look and feel of the desktop environment it is running on.The Emacs GUI is not quite as polished as some of the more modern GUI editorswe will discuss.
4.2.5.12 The Bottom Line on Emacs
Emacs has much to offer a programmer looking for a text editor, but the learning
curve can be difficult. If you don’t mind learning something about Lisp and you liketools like CC Mode to police your editing, you really should learn Emacs. Even ifyou despise Emacs, it still behooves you to learn the basics. The GNU 
info browser
borrows heavily from Emacs, particularly for the searching and navigation options.These are good skills to master the 
info pages have much more information than
the corresponding manpages. Occasionally you may encounter a tool that borrows
the Emacs command set as well. Learning the basics of Emacs will help you go along way to being productive in other tools.
4.2.6 Attack of the Clones
Emacs and vihave loyal followings, but even the most loyal minions can point to
some shortcomings in their favorite editor. The danger of creating a tool for pro-grammers is that there is always someone out there who thinks he can do it better.4.2 The Text Editor 179
With vi, what spurred the clones was that although viis part of the POSIX stan-
dard, the original source was proprietary until recently. This meant that viusers on
operating systems other than UNIX needed an alternative. As a result several vi
clones sprouted up, the most successful of which was Vim, which is available manyplatforms in addition to Linux. Most of the other 
viclones lack the features that Vim
adds but do a good job of being compatible with the original vi. They have names
like Elvis, Vile, and Nvi. One unique and interesting variant is called bvifor Binary
vi. This tool allows you to view and edit binary data files with a viinterface. The
binary data is presented in hexadecimal bytes and you can use the familiar vicom-
mands to navigate and modify the data. An example of bviis shown in Figure 4-3.
In the case of Emacs, the one shortcoming that motivates cloning is its memory
footprint. It’s too big. Because Emacs is extensible, it makes no sense to create a cloneof Emacs that has more features. It’s easier to extend Emacs with Lisp programs. Soit should be no surprise that all the Emacs clones are light versions of Emacs. A typ-ical user is someone who is comfortable with the Emacs interface but who happensto be working in a low-memory environment, like an embedded system.
A few other interfaces are also popular and worth mentioning. One is WordStar,
which was to DOS what 
viwas to UNIX. WordStar was thetext editor for devel-
opers working on DOS machines. The interface was cloned in several other productssuch as Borland’s T urbo Pascal and T urbo C. Many Linux users got their start onDOS-based machines and came to like the WordStar interface. It’s only natural that180 Chapter 4 • Editing and Maintaining Source Files
FIGURE 4-3 bvi in Action

this interface would find its way into some clones. You can find the WordStar inter-
face provided by the Joe text editor. Joe is an interesting editor because it can changeits behavior to be an Emacs clone or a clone of another editor called Pico, whichbrings us to next popular interface.
Pico was part of the very popular Pine email client, which was available on UNIX
systems before the era of Web browsers. As part of an email client you might guessthat Pico is easy to use, and it is. Many users came to be comfortable with it,although it lacks many features required for programming. Nevertheless, program-mers use it. You can get the original source for Pine from the University ofWashington
10or you can get the GNU clone called Nano. GNU cloned Pico because
the Free Software Foundation determined that the source license for Pico was notcompatible with the GPL. Nano follows the same interface as Pico and adds severalenhancements. Some popular clones are listed in Table 4-27. With the exception ofVim, all these clones run exclusively in text mode (there is no GUI). I will look atsome GUI text editors in the next section. Table 4-28 presents a summary of avail-able features in each clone. The list of features comes from Table 4-2.
TABLE 4-27 Some Popular Editor Clones
Editor 
Name Emulates Notes
Vim vi Adds many enhancements to vi.
Joe Emulation is selected by the command name. The 
joeand jstar commands emulate WordStar; jmacs
emulates Emacs; and jpico emulates Pico. These
commands point to the same executable.
Zile Emacs Zilestands for Zile Is Lossy Emacs . Zile does not have
text menus, so it’s probably better suited for experi-enced Emacs users.
Jed Emulation is selected in your 
.jedrc file. Jed uses text
menus that are the same in all modes, so it’s suitablefor beginners.
Nano Pico GNU clone of the Pico text editor with enhancements.Emacs, WordStar,OthersEmacs, Pico,WordStar4.2 The Text Editor 181
10. www.washington.edu/pine
TABLE 4-28 Emulator Feature Summary
Version Brace Syntax Auto- Regular Automatic Code Code
Editor Tested Matching Highlighting completion Expressions Indenting Browsing Building
Vim 6.3.71 Yes Yes Yes Yes Yes Yes Yes
Joe 3.1 Yes Yes Yes Yes Yes YesZile 2.2 Yes Yes YesJed 0.99.16 Yes Yes Yes Yes Yes Yes Yes
Nano 1.2.4-3 Yes Yes Yes
Later I look in depth at one more feature that these editors bring to the table. All
the clones use less memory than their predecessors, which is one reason why theyare exclusively text based. A GUI by nature consumes more memory than a text-based editor. But before I can discuss that topic, I need to take a closer look at theGUI editors.
4.2.7 Some GUI Text Editors at a Glance
Emacs and Vim aside, what all GUI text editors have in common is that they aremodeless. They can do this because the mouse and GUI are used for all features thatdon’t involve typing. There is no shortage of GUI editors available. The number offeatures each provides varies greatly. Some GUI editors are not intended for codedevelopment and don’t have the features you would look for in a programmer’s texteditor. Others specifically target programmers.
In this section I look at the default editors provided with Gnome and KDE, as
well as some other popular examples. There are many other fine editors including acouple written purely in Java. I chose not to include these, because most Linux dis-tributions do not come with a Java installation. If you have a Java installation, a Java-based editor may be worth looking at. An editor written in Java is attractive if youwork in Windows and Linux, you can run the same editor in both environments.Because Java uses Unicode internally, you can expect to find excellent support forinternationalization.182 Chapter 4 • Editing and Maintaining Source Files
Keep in mind that all these editors are constantly under development, so this is
only a snapshot of the features available.
4.2.7.1 Kate, Kwrite
Kate is the featured text editor for the KDE environment and Kwrite is its light
cousin. Both have all the features listed in Table 4-2 except code browsing. One fea-ture Kate have not covered before is folding allows you to hide sections of code or
comments to cut down on clutter while you work. You can see how this works inFigure 4-4 and Figure 4-5.
Kate has a plug-in mechanism to support additional features. The autocomple-
tion feature, for example, is available as a plug-in.
Although Kwrite is supposed to be the light version of Kate, the only significant
difference I have found is that Kate will open multiple files in the same windowwhen you use tabs. By comparison, Kwrite will open one window for each file.4.2 The Text Editor 183
FIGURE 4-4 Kate Editor Showing the Folding Controls (Unfolded)

4.2.7.2 Gedit
This is the default text editor for the GNOME environment. Compared with
KDE’s Kate and Kwrite, Gedit comes up short on features. Of the features listed inTable 4-2, however, it lacks only code browsing and autocompletion. Syntax-high-lighting implementation is very complete and supports many languages besides Cand C++, including markup languages such as LaTeX and HTML and hardwaredesign languages such as VHDL and Verilog. You can see an example of Gedit inFigure 4-6.
4.2.7.3 NEdit
NEdit is one of the older GUI text editors around and is not married to any par-
ticular desktop environment, such as GNOME or KDE. Because it uses only Xlibraries, it will run on any distribution that has X without many extra libraries. Thecontrols look a little primitive by today’s standards, as you can see in Figure 4-7.184 Chapter 4 • Editing and Maintaining Source Files
FIGURE 4-5 Kate Editor Showing the Folding Controls (Folded)

4.2 The Text Editor 185
FIGURE 4-6 Gedit Editor
FIGURE 4-7 NEdit Editor

Don’t let the simple controls fool you. NEdit is a full featured programmer’s edi-
tor with all the features you have come to expect. Of the features in Table 4-2, theonly thing lacking is integrated builds. NEdit will compile your code and save theerror messages in an output window, but unlike other editors, it will not visit faultylines of code reported by the compiler. The output is for display only.
4.2.7.4 SciTE
SciTE is a relatively new text editor based on the Scintilla library, available from
www.scintilla.org. (The name SciTE is short for Scintilla T ext Editor. ) Like NEdit,
SciTE is not married to any particular desktop environment, but unlike NEdit, SciTEhas a modern look and feel, thanks to the Scintilla library.
As a newcomer, the bar is already set fairly high in terms of features, and SciTE
delivers. SciTE has a couple of unique features going for it that make it worth alook. For one thing, SciTE produces some of the best-looking syntax highlightingyou will find. Like other text editors, it uses colors to enhance the output, but it alsouses proportional fonts for comments, which improves the output further (usually).You can see an example in Figure 4-8.186 Chapter 4 • Editing and Maintaining Source Files
FIGURE 4-8 SciTE Editor

As if to show this off, SciTE allows you to export the text with syntax highlight-
ing (typefaces and all) to PDF , RTF , LaTeX, HTML, or XML. You can export entiredocuments or just the text selected with the mouse.
Another nice feature is a split window that allows you to look at compiler out-
put and code in a single window. You can use the mouse to click warnings, andSciTE will highlight the appropriate line of code.
Finally, SciTE has an excellent Windows version, which looks and feels exactly
like the Linux version. You won’t find Kate or Gedit for Windows, and NEdit willrun only if you have an X server for Windows.
4.2.8 Memory Usage
Memory footprint is a big deal in some environments. On a desktop machine withhundreds of megabytes of memory, you probably don’t worry too much about howmuch memory your text editor uses. But in an embedded system with limited mem-ory, a slow CPU, and no swap disk, efficient memory usage is essential.
Embedded targets running Linux are becoming more common. These are sys-
tems with very little memory and often no keyboard or display. They may have justenough memory to run a small text editor but not a GUI. When working onembedded systems, it’s usually most productive to do as much as you can on the tar-get system. So any tool that conserves memory is welcome.
Even on a desktop machine, the more memory an editor uses, the more sluggish
it is. If your system is low on memory, excessive memory usage causes swapping,which makes everything run slowly. Another issue is startup and shutdown time.Often, developers need to edit things in “just in time” fashion, which makes longstartup times annoying and unproductive. A text editor with a small footprint usu-ally starts up very quickly and exits just as fast.
Figure 4-9 shows the memory footprint of the editors I have discussed so far. The
editors that have the biggest footprint are, not surprisingly, GUI based, whereas thesmallest footprint can be found on the terminal-based editors.
These measurements were taken immediately after startup with an empty file.
What is harder to compare is how these editors manage memory while running andediting large files. Suppose that you need to modify one line of a 100MB file. Doesyour editor read in the entire file and consume an extra 100MB, or does it read inonly the pieces you need to see so as to conserve memory? That’s an extremely con-trived example, but you don’t need such large files to run into efficiency issues onembedded targets. Some common sense as well as trial and error should help youfind the right editor for the job.4.2 The Text Editor 187
4.2.9 Editor Summary
I have focused on text editor features for programmers, and the choice of features
was perhaps a little arbitrary. There are many other choices offering many more fea-tures. Perhaps you would like a GUI editor that you can use in both Windows andLinux. Emacs and Vim will work, but so will SciTE and many other editors I didn’tcover here.
Another feature I left out that may be important to you is internationalization
(i18n for short). The i18n support in all the editors tested was surprisingly good.It used to be that terminal-based editors could not support the fonts or encod-ings required for i18n, but that’s not true anymore. Thanks in large part to theinternational community of open source developers, most popular text editors,including terminal-based editors, can handle multiple encodings. All the editors188 Chapter 4 • Editing and Maintaining Source Files
FIGURE 4-9 Memory Usage by Text EditorEditor Memory Us age (MiB)
0510152025303540
zile
Vi-mi nimal
joe
jed
Emacs-nox
vim
Emacs-nw-kde
nedit
Emacs-x-kde
Emacs-nw-gnome
Emacs-x-g nome
SciTE
Vim-x-g nome
gedit
Vim-x-kde
kwrite
kate
tested support UTF-8, a versatile Unicode encoding that is a superset of ASCII and
that can represent any written language on Earth.
Don’t just look at the features listed here; also consider what special requirements
you have. If you do any searching on the topic of text editors, you will discover thatthis has been the topic of “holy wars” in the past. T ry to keep an open mind, andalways be skeptical of anyone who claims that his favorite editor is the only oneyou’ll ever need. As a programmer, your goal is to be productive, not fashionable.
4.3 Revision Control
Any good software process requires revision control. It is a key metric for any organ-ization that claims to be mature. The ability to control what goes into a release of soft-ware and to keep track of it after it is released is vital to quality software development.
Good revision control is necessary not only for large organizations, but also for
individual developers. It can be a chore, but more often than not, it is a vital toolin your development process. From a text-editing perspective, you can think of revi-sion control as being a super undo function. It allows you to checkpoint your devel-
opment at certain stages where features are stable, before you start to implementnew features that may affect the whole project.
This section cannot do justice to all the tools that are available for revision con-
trol. Indeed, you can find several books on each of these tools. Instead, I look atsome of the basic concepts of revision control that are common to all tools. Then Icover some of the features of several popular tools. You can find more informationabout these tools in the Online References section at the end of this chapter.
4.3.1 Revision Control Basics
Suppose that Figure 4-10 illustrates the revisions of a module you created for anopen source project. Revisions A through F are the main branch, where new fea-tures are added and debugged. At revision C, you decided to release the code. Whileusers were getting used to the features, you continued to work on new features inrevisions D through F . Sometime during this development, your users discoveredbugs that needed to be addressed. In order to fix these bugs without releasing unfin-ished features, you created a branch from revision C. This allowed you to fix thereleased code and work on new features at the same time. At revision F , you wereable to merge the bug fixes into the main code, which is typical before a new release.4.3 Revision Control 189
The pattern illustrated in Figure 4-10 is a bit oversimplified, but it is the same for
virtually all revision control tools.
Until recently, there weren’t many open source tools available for revision con-
trol. That has changed, and some new tools are maturing and competing for newprojects. Some of the most popular tools are listed in Table 4-29.
TABLE 4-29 Some Popular Revision Control Tools
Name Description
RCS Revision Control System—the ancestor of CVS (Concurrent
Version System), which served as its basis originally. RCS does notsupport projects and requires that files be locked to be modified.
CVS Concurrent Version System—built on RCS; allows files to be
grouped in projects and does away with the locking requirement.Developers must resolve conflicts with a merge before they areallowed to commit changes.
Subversion A successor to CVS fixes many of its shortcomings while preserving
the basic user interface.
GNU arch An alternative to CVS that developed at about the same time as
Subversion. It addresses many of the same issues that Subversiondoes. It remains to be seen whether it will catch on.
monotone Takes some new approaches to revision control that may be a little
controversial; uses SHA1 hashes to record changes and versions.190 Chapter 4 • Editing and Maintaining Source Files
FIGURE 4-10 Simple Branching ExampleMain BranchABCDEFC2 C1
Release! Development
Continues…Merge bug fixes with
our new features.Bug Fixes
Create a 
new branch
New
Release!
4.3.2 Defining Revision Control Terms
The terminology varies from one tool to the next, so I’ll define some neutral terms
for comparing one tool with another.
4.3.2.1 Project
Generally, this is an arbitrary grouping of files determined by the developer,
although very often in practice, one project produces a single executable or library.Each open source tool, for example, has its own revision control “project.”
Each file under the project has an independent history, but the project itself may
have a history as well. In CVS and Subversion, a project is called a module, whereas
GNU arch calls it an archive and monotone calls it a working copy .
4.3.2.2 Add/Remove
This is a basic feature. Developers need to be able to add and remove source files as
the project evolves. All tools allow this. Most tools keep a history of removed filesso that they can be restored for old versions of the source.
4.3.2.3 Check In
This is the ability to create new versions of a file. Each time you check in a file, it
is a snapshot of what the file looked like at that point. No matter how many addi-tional changes you make, you can always reproduce the file exactly as it was whenyou checked it in.
4.3.2.4 Check Out
This is the ability to recall a file that was checked in earlier. Depending on the tool,
you may check out a file or an entire project. Most tools default to a nonlockingscheme for checking out files, with locks supported only for special circumstances.When a file is locked, no one else can work on it (check it out).
Consider RCS, which uses a locking scheme. To modify a file, you need to check
it out with a lock. This gives you a writable copy of the checked-out file that youcan modify. Until you check the file in, no one else can lock the file. In principle,the lock means that no one else can edit the file. This ensures that when you docheck the file in there will be no other changes to your file. In this case, the check-in operation is little more than a copy operation.
Locks can be a nuisance in projects with many developers and many source files.
If two developers need to modify the same source file, they cannot do it at the same4.3 Revision Control 191
time, no matter how trivial the change. This impairs the productivity of the whole
team, because developers may have to synchronize their efforts.
All modern revision control tools use nonlocking schemes that allow multiple
developers to modify the same file. They accomplish this by merging changes when
the files are checked in (see below). The assumption is that two developers workingon the same file will be working on different lines, such that a merge will not be dif-ficult. When this is not the case, the merge must be done manually. It’s a small priceto pay for enhanced productivity.
4.3.2.5 Branch
Branching is a vital tool for revision control. It enables numerous useful patterns.
One pattern for creating bug fixes is illustrated in Figure 4-10 earlier in this chap-ter. A branch allows you to create new versions of old source files that don’t conflictwith the latest development and that don’t pick up any unwanted changes thataren’t ready for release.
4.3.2.6 Merge
Another key enabler, this allows tools like CVS to work without locks. Instead, the
tools rely on merges. The idea is that when you change a file, the tool checks to seewhether anyone else made a change since you started working on it. If so, you arerequired to perform a merge before you can check in.
If you and another developer check out the same file, and the other developer fin-
ishes first, you are required to merge his changes with yours before you can submitthem. The merge is done with the help of a merge tool and is facilitated by the revi-sion control tool. If you circumvent the merge step, your changes will overwrite theother developer’s.
4.3.2.7 Label
Revision control tools typically pick arbitrary labels for versions of files checked into
the system. When you have more than one source file, odds are that they aren’t allthe same revision. The tools use labels like 1.1, 1.2, and so on. monotone, for exam-ple, uses a 40-character hash. These are arbitrary and probably not what you wantto use to communicate release versions. A label allows you to give a bunch of files acommon name like 
release_1.0 so that you can keep track of all the files that
went into a release with one key.192 Chapter 4 • Editing and Maintaining Source Files
4.3.2.8 In Summary
If you are contributing to a revision-controlled project, very likely the choice of tools
is out of your hands; you will have to use whatever the project maintainer decidedon. If you plan to create an open source project, you should become familiar withyour choices. Understand that CVS has been the workhorse of open source projectsfor a long time and that many developers are comfortable with it. Subversion is rap-idly catching on, as it uses nearly the same syntax as CVS and addresses many ofCVS’s flaws.
Creating a project with an unfamiliar revision control tool is likely to discourage
some developers from joining your project. Even if you think a particular tool is thebest, if you want to recruit developers, you need to think about what they find mostproductive. T rying to get developers to change tools may create an unnecessary hur-dle for you.
4.3.3 Supporting Tools
Managing changes in source code is a difficult job, and it only gets more difficultwhen more developers are involved. It should come as no surprise that several toolsare available to make this job easier.
4.3.4 Introducing diff and patch
Most users are familiar with the basic diff command, which compares pairs of
files. Most often, we are interested only in knowing whether two files are identical.In that case, you can use the 
cmpcommand. When you want to see the changes, the
output from diff is not the most user friendly. That’s because the real usefulness
of the diff command is for creating patches that can be applied by the patch com-
mand. As you shall see, there are better tools than diff for looking at file changes,
but let’s look at what diff is good for. I’ll start with an example of diff output
with a few trivial changes:
$ cat -n before.txt
1  This is a line to be deleted2  This is a line that will be changed3  This is a line that will be unchanged
$ cat -n after.txt
1  This is a line that has been changed2  This is a line that will be unchanged3  This is a line that has been added4.3 Revision Control 193
$ diff before.txt after.txt
1,2c1< This is a line to be deleted< This is a line that will be changed---> This is a line that has been changed3a3> This is a line that has been added
Although not particularly user friendly, the output is readable. By default, lines
that are changed or deleted from the first file are indicated with a <character. Lines
that have been added or changed in the second file are preceded by a >character.
The extra information indicates the line numbers where the change took place,which can be used by the 
patch command.
Let’s see how this output is used by the patch command. Suppose that you have
a file identical to before.txt that you want to patch (call it new.txt ). You want
to take the differences between before.txt and after.txt and apply them to
new.txt . You can do this with a patch as follows:
$ cp before.txt new.txt
$ diff before.txt after.txt > mypatch.txt$ cat mypatch.txt | patch new.txt
After the patch is applied, new.txt is identical to after.txt . This is not a very
interesting application, because you could just as easily have overwritten new.txt
with after.txt . But that’s not the point. The point is that if all you have is a copy
of before.txt , all you need to create after.txt is mypatch.txt . Instead of
keeping two copies of the same file, which may be very large, all you need are theoriginal and the difference (which could be very small) in the form of a patch. Thiswas the intended use for patches, which was very important back when a great dealof networking was done with slow modems and disk drive sizes were a couple oforders of magnitude smaller. Patches allowed you to update your source code usingmuch smaller files instead of downloading an entire project. Despite the increasingpopularity of broadband Internet and huge disks, patches are still used widely todayto disseminate changes to the Linux kernel source. Many open source projects dis-tribute changes in the form of patches as well.
Another useful feature of patches is that they can be reversed. To undo the
changes you just made to 
new.txt with the patch command, you can use the same
patch file as follows:
$ patch -R new.txt < mypatch.diff194 Chapter 4 • Editing and Maintaining Source Files
The -Rflag tells patch to apply the differences in the reverse direction or to undo
the patch. When using the default output of the diff command, you have to tell
patch exactly what files to patch. In this example, I gave the filename new.txt as
an argument to patch ; otherwise, it would have prompted for a file to patch. A
more versatile way to create a patch is to use the -uoption (for unified format),
which looks like this:
$ diff -u before.txt after.txt | tee mypatch.diff
--- before.txt  1994-07-02 12:34:56.000000000 -0500+++ after.txt   2004-07-02 12:34:56.000000000 -0500@@ -1,4 +1,4 @@-This is a line to be deleted.-This is a line that will be changed.+This is a line that has been changed.
This is a line that will be unchanged.
+This is a line that has been added.
Notice that the output is now quite a bit different and includes both filenames
as well as their creation date and time (with the time zone). It is also a bit more read-able than the default 
diff output. Although the patch has two filenames, a given
patch will modify only one file. The patch command chooses one of these two files
to modify, depending upon which one exists. If only one file exists, it patches thatfile. If both files exist, it patches the second file (
after.txt , in this example). If no
files exist, the patch command fails, because there is nothing to patch.
When you use the unified format, multiple differences can be combined into a
single large patch. In this way, you can patch numerous files with a single patch file.This is how many open source tools, including the Linux kernel, distribute patches.
The most common way to create a large patch is to use the 
diff command with
the -roption to walk through directories recursively. This requires you to have two
identical directory trees with identical filenames. The only differences are in the filesthemselves. By looking for matching pathnames, 
diff is able to determine which
pairs of files to compare to produce the differences. When diff encounters a file
that exists in only one of the two trees, the default behavior is to skip that file andprint a warning to 
stderr . This behavior can be changed with the -Noption,
which causes diff to assume that a missing file is actually there but empty. In this
way, a patch can include files that were created. Then applying the patch will createnew files.4.3 Revision Control 195
Let’s look at another trivial example that ties this all together. First, you need a
set of files to work with:
# Create before and after directory trees
$ mkdir old new
$ echo "This is one. It's unchanged." | tee old/one new/one$ echo "This is two. It will change." > old/two$ echo "This is two. It changed." > new/two$ echo "This is three. It's new" > new/three
This creates two directories for the demonstration. The newdirectory contains
what you want, or the latest versions of the files. The olddirectory contains what
you started with, or the old versions of the files. You can create a patch as follows:
$ diff -Nur old new > mypatch.diff
This produces a patch that looks like the following:
diff -Nur old/three new/three
--- old/three   1969-12-31 18:00:00.000000000 -0600+++ new/three   2005-10-30 20:40:56.296875000 -0600@@ -0,0 +1 @@+This is three. It's newdiff -Nur old/two new/two--- old/two     2005-10-30 20:40:56.265625000 -0600+++ new/two     2005-10-30 20:40:56.281250000 -0600@@ -1 +1 @@-This is two. It will change.+This is two. It changed.
Note that patches have a direction, which is determined by the order of the files
given to the diff command. In this case, the direction is from oldtonew. So now
that you have a patch, you can transform old into new, as follows:
$ patch –-dir old < mypatch.diff
patching file threepatching file two
After patch runs, the contents of oldand new are identical. You can also reverse
the direction of the patch and transform new into old,as follows:
$ patch --dir new -R < mypatch.diff
patching file threepatching file two196 Chapter 4 • Editing and Maintaining Source Files
This is a small example of what you might do on a larger scale with the Linux
kernel source or a large open source project. In the kernel, unofficial patches areavailable all the time, offering features that are not part of the kernel. Usually, thereis good reason for this, but some good features are not ready for wide distribution.You can use a patch and be assured that if the feature turns out to be broken, youcan undo the change to your source and get your kernel back the way it was.
I glossed over some details in these examples. First is the 
--dir option to patch ,
which tells the patch command to do a chdir to the specified directory before
applying the patch. Another detail is the fact that the patch command automati-
cally removes the leftmost directory element of the patch before applying the patch.In this case, it’s the old or new, which means that the 
patch command will not look
for a directory named oldor newwhen applying the changes. You can exert more
control over this behavior with the -poption (see patch(1) for details).
The .diff extension is one common convention for naming patch files. The
Linux kernel uses filenames that start with patch . These files can include changes
to hundreds of files in the Linux source tree.
4.3.5 Reviewing and Merging Changes
The diff command leaves something to be desired when it comes to reviewing
changes. You could argue that the output is not intended for human consumption,but for small changes, it’s adequate. The GNU 
diff command has many options
to make the output more readable, but in a text terminal, there’s only so much youcan do.
For large changes, it’s often more helpful to see formatted output so that you can
zero in on exactly what has changed. When 
diff sees a single character changed on
one line, it prints out the entire line (twice) to indicate the change. For example:
$ diff src1 src2
1c1< const char *somechars=":,-;+.({)}";---> const char *somechars=":,-;+.{()}";
Only two characters changed on this line. Can you spot the change? This is where
some other tools are more helpful. Vim, for example, is capable of showing differ-ences that highlight single-character changes, as follows:
$ vim -d src1.c src2.c4.3 Revision Control 197
A slightly nicer alternative is the GUI version, gvimdiff , which is shown in
Figure 4-11. This illustrates how single-character changes are highlighted in addi-tion to line changes. Now the difference is much easier to spot. Both 
vim and
gvimdiff highlight changes, but the limitations of your terminal capabilities may
make the GUI preferable.
Another open source GUI tool for reviewing differences is xxdiff ,11available
from sourceforge.net. This tool adds some nice features, including a merge utility.A GUI is especially nice to have in a merge utility, which you shall find out.
Eventually, the time comes when things get more complicated and a merge is in
order. A merge situation comes up most often when you’re working under revisioncontrol. Usually, the need for a merge arises when you are working with other teammembers or on multiple branches. To help you understand merges better, I’ll intro-duce the GNU command-line merge tool.
The GNU merge tool used by revision control tools such as CVS and Subversion
is called 
diff3 . The diff3 command gets its name because it requires three file-
name arguments, as follows:
$ diff3 myfile original yourfile
The order of myfile and yourfile is interchangeable, but the second filename
must be the common ancestor. Figure 4-12 shows a graph of what the revision treelooks like.
To illustrate, we need a file to work with. Let’s consider this trivial example:
1  void foo(void)
2  {3          printf("This will be changed by me.\n");45          printf("This will be unchanged.\n");67          printf("This will be changed by you.\n");8  }
Now suppose that I change my copy of this file so that line 3 reads
printf("This was changed by me.\n");
and you change your copy of the file so that line 7 reads
printf("This was changed by you.\n");198 Chapter 4 • Editing and Maintaining Source Files
11. http://sourceforge.net/projects/xxdiff
4.3 Revision Control 199
FIGURE 4-11 The Same Difference Shown with gvimdiff (Highlighted s Is the Cursor)
FIGURE 4-12 Graphic Illustration of a Merge using diff3original
myfile
diff3yourfile
Now we have two different changes that need to be merged. Luckily, they’re on
different lines of the same file, so merging is quite easy. With no arguments, diff3
produces output that illustrates the changes but is readable only if the changes aresmall, as in this example:
$ diff3 me.c orig.c you.c
====11:3c
printf("This was changed by me.\n");
2:3c3:3c
printf("This will be changed by me.\n");
====31:7c2:7c
printf("This will be changed by you.\n");
3:7c
printf("This was changed by you.\n");
Differences are delimited by ====1 or ====3 , indicating which of the modified
files caused the difference against the original. Numbering is based on the argu-ment order, so in this example, 1 is 
me.c , 2 is orig.c , and 3 is you.c . The line
numbers, as well as the types of changes, are indicated on the left. This is not themost useful output from 
diff3 , however. The more useful output comes 
with the merge option, where diff3 will attempt to do the merge for us. Because
the changes are trivial in this example, diff3 produces straightforward results:
$ diff3 -m me.c orig.c you.c  | cat -n
1  void foo(void)2  {3          printf("This was changed by me.\n");45          printf("This will be unchanged.\n");67          printf("This was changed by you.\n");
8  }
Notice that both your and my changes show up in the output in the right place.
Very often in a large source file, it’s possible to have such trivial merges that requireno input from the user, but sometimes, it’s not so simple. Let’s look at anotherexample:200 Chapter 4 • Editing and Maintaining Source Files
1  void foo(void)
2  {3          printf("This will be changed by both of us.\n");4  }
In this case, both you and I modify the same line of code. Instead of showing you
the listings, let’s see what diff3 says:
$ diff3 -m me.c orig.c you.c
1  void foo(void)2  {3  <<<<<<< me.c4          printf("This was changed by me.\n");5  ||||||| orig.c6          printf("This will be changed by both of us.\n");7  =======8          printf("This was changed by you.\n");9  >>>>>>> you.c
10  }
Clearly, this output is not ready to compile. Now we have some choices to make,
as indicated by the delimiters. The conflict starts with the <<<<<<< characters and
ends with the >>>>>>> characters. We must use a text editor to clean up everything
in between, deciding which changes to keep, and which ones to loose.
CVS and Subversion look for conflicts when you try to put changes back in the
repository via the commit command. If the tool detects that someone else has
changed a file since you retrieved your copy, it will not allow you to commit your
changes. To resolve this, you have to do an update , which is the command to bring
your local copy up to date with the repository. When you do the update, changedfiles that have not been modified by you are overwritten with the new changes. Atthe same time, files that have been changed in the repository and by you require amerge to bring them up to date in your local copy. When you run the 
update com-
mand, the tool runs diff3 to do whatever merges are necessary. Then it is up to
you to resolve any remaining conflicts with your text editor.
Let’s take one more look at xxdiff , which can be very helpful with merges. The
same merge is shown in Figure 4-13.4.3 Revision Control 201
Here, you are presented with all three files and can choose which change to take
with the click of a mouse. You can even select all three changes, which can produceoutput that looks just like 
diff3 . Or you can tell xxdiff to wrap each change in
an #ifdef statement. For example:
1     void foo(void)
2     {3     #if defined( ME )4       printf("This was changed by me.\n");5     #elif defined( ORIG )6       printf("This will be changed by both of us.\n");7     #elif defined( YOU )8       printf("This was changed by you.\n");9     #endif10    }
One thing to notice with CVS and most revision control tools is that the person
doing the merge has the power of choice. That is, if I were merging this change toa revision control system, I would get the opportunity to choose which change getschecked in: yours or mine.202 Chapter 4 • Editing and Maintaining Source Files
FIGURE 4-13 Using xxdiff to Do a Merge

4.4 Source Code Beautifiers and Browsers
I discussed how Emacs’ CC Mode enforces rigid indentation rules that are hard to
break. If everyone used Emacs, and used the same indentation style, there wouldbe no issues. In reality, everyone has his or her own favorite editor with his or herown settings. As more people touch the same source file with all these different set-tings, what you are left with can be a mess. Some editors expand tabs to spaces;others mix tabs and spaces; and all make different assumptions about how manyspaces are in a tab. What may look pretty in one editor may look like avant-gardepoetry in another editor.
There are tools that will indent the code for you, but beware: Reformatting an
entire module can cause problems in revision control systems. That’s because youare touching virtually every line of code. Even though you are only rearranging thecode, the merge tool does not know that. So when someone makes a change in anunpretty version of the file, trying to merge those changes with the pretty version
could be unwieldy. Let’s consider another contrived example. Suppose that you hada source file with a bunch of declarations on one line, as follows:
int i; int j; int k; int l; int m; int n; int o; int p; int q; int r;
You run a beautifier, which places each declaration on its own line. Now another
developer checks out the same file and notices that the variable mis unused; he
deletes that declaration to remove the warning. This developer is not interested inbeautifying the code but just wants to commit a simple change to fix a warning.Now when you commit your beautified code, what should have been a one-linechange is now a ten-line change, as follows:
$ diff3 -m -E me.c orig.c you.c
<<<<<<< me.cint i;int j;int k;int l;int m;int n;int o;int p;int q;int r;=======int i; int j; int k; int l; int n; int o; int p; int q; int r;>>>>>>> you.c4.4 Source Code Beautifiers and Browsers 203
This is an easy example, of course; it only gets uglier from here. The only advice
to offer is to make sure that no unbeautified code gets merged with beautified code,which may be difficult or impossible to guarantee. Resist the temptation to beau-tify entire modules unless you are certain that there will be no merges after thatpoint. If you can’t be certain, you may be able to beautify small sections of code,which are less likely to conflict with other merges.
In the end, this should illustrate the importance of coding standards, in particu-
lar when it comes to indentation. If you work on an open source project, you prob-ably will have to comply with a required indentation style. Even if you don’t likesomething about the style, it is important to comply. Any indentation style is bet-ter than none at all.
4.4.1 The Indent Code Beautifier
UNIX had a command named cbthat could reformat C source code. It was imple-
mented as a filter that operated exclusively on standard input and output. This wasannoying to some people, because you couldn’t just turn it loose on your sourcecode. This approach has its advantages, particularly for 
viusers, because viis able
to take advantage of filters. You can indent only the code between two braces, forexample, as follows:
!%cb
There is another good reason for keeping code beautifiers at bay. Consider the
earlier example of beautified code that has to be merged with unbeautified code.Filtering a block of code allows you to make incremental changes to a large modulethat might be in work by many users. Instead of reformatting an entire module andclashing with everyone else, you can fix up a single function or block of code with-out causing too much grief when it comes time to merge.
The Linux equivalent of the 
cbfilter is the indent command, which is much
more versatile. For one thing, indent can reformat C++ as well as C. It can oper-
ate as a filter like cbbut can also indent files in place. Although doing so is not
recommended, you could reformat a bunch of files with a single command, asfollows:
$ indent *.c204 Chapter 4 • Editing and Maintaining Source Files
Although indent does not support all the styles that Emacs supports (listed in
Table 4-21 earlier in this chapter), it does include K&R, GNU, and BSD styles. Youcan exert precise control over every aspect of reformatting with more than 80 com-mand-line options, however, so whatever style you like, you can tweak 
indent to sup-
port it.
Let’s look at some examples. Listing 4-1 shows a pathologically indented
Fibonacci function—a classic example from programming class.
LISTING 4-1 Fibonacci on Drugs
unsigned int fibonacci(unsigned int n)
{
if ( n < 2 ) {
return n;
}
else
{
return
fibonacci( n - 1 )
+
fibonacci( n -2 );
}
}
Now let’s run this through a few styles with indent . You can see two examples
in Listing 4-2 and Listing 4-3.
LISTING 4-2 An Example of GNU Style Using indent
unsigned int
fibonacci (unsigned int n){
if (n < 2)
{
return n;
}
else
{
return fibonacci (n - 1) + fibonacci (n - 2);
}
}4.4 Source Code Beautifiers and Browsers 205
LISTING 4-3 Berkeley Style Using indent
unsigned int
fibonacci(unsigned int n){
if (n < 2) {
return n;
} else {
return fibonacci(n - 1) + fibonacci(n - 2);
}
}
indent is fairly aggressive in its reformatting output, so it merges and breaks
lines as it sees fit. There are dozens of options to control the finer details, so youusually can start with one of the basic styles and tweak it with additional options.For example:
$ indent -kr -bl -bli0 -nce
This takes the K&R style and tells indent to put braces on their own line ( -bl)
with no additional indentation ( -bli0 ). Finally, it tells indent to keep the else
on its own line ( -nce ). These options can be combined in a file named
.indent.pro , which may reside in the current directory or your home directory.
All you do is place the same options in a text file. For example:
-kr -bl -bli0 -nce
If this file is in your home directory, these will be the default options, whenever
you run indent . Alternatively, if you contribute to multiple projects with differ-
ent indentation styles, you could put a unique .indent.pro in each project
directory.
4.4.2 Astyle Artistic Style
Another promising open source beautifier is called astyle .12Like indent ,
astyle understands C and C++, but it also understands Java and (gasp) C#. Here
again, astyle does not support all the formats that Emacs does, but it does have
predefined formats for K&R, GNU, and Linux styles, as well as something it callsANSI style.206 Chapter 4 • Editing and Maintaining Source Files
12. http://sourceforge.net/projects/astyle
astyle is less aggressive than indent when it comes to reformatting. It will not
break lines unless you explicitly tell it what kind of lines it can break. It will not consolidate statements that span more than one line into a single line. Thisdoes not work well with Listing 4-1, for example. Listing 4-4 shows the resultsafter Listing 4-1 has been processed with 
astyle .
LISTING 4-4 Example of ANSI Style with astyle
unsigned int fibonacci(unsigned int n)
{
if ( n < 2 ) {
return n;
}else{
return
fibonacci( n - 1 )+fibonacci( n -2 );
}
}
Notice that this style is almost identical to the modified K&R style I created in
the last section except that the second return statement still occupies three lines.
4.4.3 Analyzing Code with cflow
When you have to work on code that you didn’t create or haven’t looked at in a longtime, just looking at the source is not always enough to understand the code.Fortunately, there are tools to help.
The POSIX 
cflow command translates your source code into a call graph that
allows you to see an overview of program flow. This is very useful for looking atunfamiliar code. GNU has a version of the POSIX 
cflow13command that,
although not fully POSIX compliant, is still very useful. Let’s use Listing 4-5 as anexample.4.4 Source Code Beautifiers and Browsers 207
13. www.gnu.org/software/cflow
LISTING 4-5 ex4-5.c
1  void zfunc(void) { afunc(); }
23  void xfunc(void) { zfunc(); }45  void afunc(void) { afunc(); }67  void recurs(void) { recurs(); }89  void mainfunc()
10  {11          xfunc();12          recurs();
13  }
This module is simple enough to illustrate how cflow works. Using the POSIX
format of cflow, you get the following:
$ cflow --format=posix ex4-5.c
1 afunc: void (void), <ex4-5.c 5>2     afunc: 1
afunc() calls itself
3 mainfunc: void (), <ex4-5.c 9>4     xfunc: void (void), <ex4-5.c 3>
mainfunc() calls xfunc()
5         zfunc: void (void), <ex4-5.c 1> xfunc() calls zfunc()
6             afunc: 1 zfunc() calls afunc()
7     recurs: void (void), <ex4-5.c 7> etc...
8         recurs: 79 recurs: 7
10 xfunc: 411 zfunc: 5
Notice that the output looks something like an outline. Functions are listed first
in alphabetical order. Under each function, cflow lists the functions called by that
function, with one level of indentation for each level of call depth. In this example,you can find 
mainfunc() listed in alphabetical order on line 3, followed by the
functions it calls. Notice that call trees are shown only once—for example, xfunc()
is called by mainfunc() , and this is shown in the call tree beginning on line 3.
cflow lists xfunc() on line 10 but does not show its call tree, because that was
shown under mainfunc() .
The POSIX output format is the most concise. The default output format
includes function signatures and redundant call trees, making the output a littlemore cluttered. For example:208 Chapter 4 • Editing and Maintaining Source Files
$ cflow ex4-5.c
afunc() <void afunc (void) at ex4-5.c:5> (R):
afunc() <void afunc (void) at ex4-5.c:5> (recursive: see 1)
mainfunc() <void mainfunc () at ex4-5.c:9>:
xfunc() <void xfunc (void) at ex4-5.c:3>:
zfunc() <void zfunc (void) at ex4-5.c:1>:
afunc() <void afunc (void) at ex4-5.c:5> (R):
afunc() <void afunc (void) at ex4-5.c:5> (recursive: see 6)
recurs() <void recurs (void) at ex4-5.c:7> (R):
recurs() <void recurs (void) at ex4-5.c:7> (recursive: see 8)
recurs() <void recurs (void) at ex4-5.c:7> (R):
recurs() <void recurs (void) at ex4-5.c:7> (recursive: see 10)
xfunc() <void xfunc (void) at ex4-5.c:3>:
zfunc() <void zfunc (void) at ex4-5.c:1>:
afunc() <void afunc (void) at ex4-5.c:5> (R):
afunc() <void afunc (void) at ex4-5.c:5> (recursive: see 14)
zfunc() <void zfunc (void) at ex4-5.c:1>:
afunc() <void afunc (void) at ex4-5.c:5> (R):
afunc() <void afunc (void) at ex4-5.c:5> (recursive: see 17)
Another useful format is the reverse call tree, which is something like a cross ref-
erence. Instead of listing each function and showing you the functions it calls, itshows you each function followed by a list of functions that call it. Using the moreconcise POSIX format, the reverse call tree of our example looks like this:
$ cflow --format=posix ex4-5.c -r
1 afunc: void (void), <ex4-5.c 5> afunc defined on line 5
2     zfunc: void (void), <ex4-5.c 1> afunc called by zfunc
3         xfunc: void (void), <ex4-5.c 3> zfunc called by xfunc
4             mainfunc: void (), <ex4-5.c 9> etc...
5     afunc: 16 mainfunc: 47 recurs: void (void), <ex4-5.c 7>8     recurs: 79     mainfunc: 4
10 xfunc: 311 zfunc: 2
You can also get a less cluttered, flat cross reference by using the --xref option
to cflow . This produces a simple list of functions with one line for each time the
function appears in the source. For example:
$ cflow  --xref ex4-5.c
afunc * ex4-5.c:5 void afunc (void) afunc defined on line 5, denoted by “*”
afunc   ex4-5.c:1 afunc is referenced on line 1
afunc   ex4-5.c:5 etc...4.4 Source Code Beautifiers and Browsers 209
mainfunc * ex4-5.c:9 void mainfunc ()
recurs * ex4-5.c:7 void recurs (void)recurs   ex4-5.c:7recurs   ex4-5.c:12xfunc * ex4-5.c:3 void xfunc (void)xfunc   ex4-5.c:11zfunc * ex4-5.c:1 void zfunc (void)zfunc   ex4-5.c:3
The output lists the functions in alphabetical order, as well as the filename and
line where they were encountered. If the line is a function declaration, the outputincludes an asterisk along with the function prototype.
4.4.4 Analyzing Code with ctags
While we’re talking about cross-referencing, let’s revisit Exuberant Ctags. Although
ctags normally produces output for your text editor, it can also produce a human-
readable cross reference using the -xoption. Using our trusty example again, you
can see the output as follows:
$ ctags -x ex4-5.c
afunc            function      5 ex4-5.c         void afunc(void) { afunc(); }mainfunc         function      9 ex4-5.c         void mainfunc()recurs           function      7 ex4-5.c         void recurs(void) { recurs(); }xfunc            function      3 ex4-5.c         void xfunc(void) { zfunc(); }zfunc            function      1 ex4-5.c         void zfunc(void) { afunc(); }
Notice that the difference between cflow and ctags is that ctags focuses exclu-
sively on definitions, not references. Although cflow gives you additional informa-
tion about references, ctags has more features. For one thing, ctags supports
numerous languages besides C and C++, whereas cflow supports only C. Another
feature of ctags is that it lets you filter the output for C code using the --c--kinds
option. Suppose that you wanted to see all the global variables declared in a set ofmodules and nothing else. You could limit the output using the following command:
$ ctags -x --c--kinds=v –-file-scope=no
The --c--kinds option indicates that you want variables only ( v), which nor-
mally would show every variable defined at file scope, including static definitions.The 
--file-scope=no flag tells ctags to exclude variables that are not global.
Note that the -xoption works with all the languages that ctags supports, but the
--c--kinds flag applies only to C code.
Finally, the cross reference from ctags is tab separated so you can import it into
a spreadsheet or table easily. This can be useful for performing simple code metrics.210 Chapter 4 • Editing and Maintaining Source Files
4.4.5 Browsing Code with cscope
cscope is a text mode browser for looking at code. It creates its own database from
a list of source files that you provide and then enters into an ncurses text menu
system. You need a functional terminal emulator to use cscope . An example is
shown in Figure 4-14.
The screen is broken into two halves. Each line on the top half of the screen pre-
sents a line of code that matched the most recent query. The bottom half of thescreen contains entry fields for several types of queries that are supported. Eachquery is preceded by a straightforward description, such as “Find this C symbol.”Most programmers should find these queries self explanatory. The Tab key takesyou between the top and bottom halves of the screen, and the up-arrow and down-arrow keys move between lines in each half. Enter a query in the appropriate fieldof the bottom half of the screen, and the results appear in the top half. Move thecursor to one of the hits in the top half, and 
cscope will call your favorite editor
and take you to that line of code.
Needless to say, this is a very interactive tool. cscope has only limited support
for static output. The >character will save the current list of matches to a file, and
you can append to it with >>. Make no mistake— cscope is intended to be used
interactively.4.4 Source Code Beautifiers and Browsers 211
FIGURE 4-14 Cscope Menu System

4.4.6 Browsing and Documenting Code with Doxygen
Doxygen is a great tool, primarily intended for generating documentation for soft-
ware projects. It is able to parse your C and C++ code and produce hyperlinked doc-umentation (typically, HTML) for browsing. In addition, you can add some verylightweight markup to your code, and Doxygen will include it in the documenta-tion. The following is sufficient documentation for a function:
/**
** This is a function.*/
void func(void){}
Java programmers will recognize this as javadoc syntax. In fact, Doxygen borrows
heavily from javadoc. This is a lightweight markup syntax that lets you write com-ments that can be read in a text editor but can produce quality typeset text outputfor documentation.
Doxygen can generate output in HTML, LaTeX, PDF , RTF , and even 
manpages
(for example, troff ). It can also use the Graphviz14tool (dot) to generate complex
UML diagrams for C++ classes. This is an excellent tool for verifying designs thatuse the UML syntax. If you start with a design in UML, you should be able to gen-erate the same diagrams from the source code.
You start with Doxygen by creating a Doxyfile, which is the file that contains all
of your preferences for a given project. A minimal Doxyfile could read as follows:
INPUT                  =.
FILE_PATTERNS          = *.cpp
This tells Doxygen to pick up all the .cpp files in the current directory. By
default, it will produce HTML and LaTeX output in separate subdirectories. Themore typical way to create a Doxyfile is to use the skeleton created by the programvia the 
-goption, as follows:
$ doxygen -g212 Chapter 4 • Editing and Maintaining Source Files
14. www.graphviz.org
Inside the Doxyfile, you will find a lengthy list of tags that control the output.
There are 127 tags in the skeleton Doxyfile that is generated by Doxygen 1.4.0.With comments, the file is 1,200 lines long. That’s a lot of information. You can geta good feel for what Doxygen is capable of just by reading the comments in theskeleton. Most of these tags take a 
YESor NOvalue. The skeleton file is completely
usable after you fill in the INPUT and FILE_PATTERNS tags, which are two of the
few that don’t take a simple YESor NOvalue.
Some more useful settings for your Doxyfile are shown in Table 4-30.Doxygen is a very useful tool for generating documentation from source code.
Because the documentation is the source, the odds of it being up to date are greaterthan keeping documentation in separate files. By keeping documentation in thesource, the document revisions track the source revisions. It’s still up to developersto update the contents of the documentation with each revision, but since the doc-umentation is in plain sight, there are fewer excuses to neglect it.
TABLE 4-30 Some Useful Doxygen Tags
Tag Purpose
USE_PDFLATEX Produce PDF output from LaTeX; requires
GENERATE_LATEX (default NO)
PDF_HYPERLINKS Produce PDF output with hyperlinks; requires
USE_PDFLATEX (default NO)
GENERATE_HTML Generate HTML output (default YES)
GENERATE_TREEVIEW For HTML output, produce a hierarchical view of classes
(default NO)
GENERATE_LATEX Generate LaTeX source (default YES)
GENERATE_RTF Generate RTF output (default NO)
GENERATE_MAN Generate manpages (default NO)
HAVE_DOT Use the Graphvis dotprogram to produce collaboration
diagrams (default NO)
UML_LOOK Give collaboration diagrams a UML look (default NO)4.4 Source Code Beautifiers and Browsers 213
4.4.7 Using the Compiler to Analyze Code
The GNU Compiler Collection (GCC) offers a few capabilities to analyze your
source code. First and foremost is the C preprocessor. Most of the preprocessor-related options on the 
gcccommand line are interchangeable with those on the cpp
command line, but in general, it’s a good idea to use gccto interface with the pre-
processor.
4.4.7.1 Dependencies
The compiler can generate dependencies for you via the -Moption. By itself, the 
-Moption includes system headers in the dependency, which produces a great deal of
clutter. Most likely, you will want to show dependencies for only your source files.One way is with the 
-MMoption. Here’s a sample from the strace15source tree:
$ gcc -MM -I ./linux syscall.c
syscall.o: syscall.c defs.h ./linux/syscall.h ./linux/dummy.h \
./linux/syscallent.h ./linux/errnoent.h
Note that the output is intended to be used in a Makefile , which is why each
line ends with a backslash. There aren’t too many options to make this more userfriendly. This output can be used to create 
Makefile s or supplements to
Makefile s but can also give you insight as to what is going on in an unfamiliar
project. The previous example shows you that syscall.c requires a file called
dummy.h , even though this file is not pulled in by syscall.c ; it’s actually pulled
in via syscall.h. The output is also dependent on your include search path, con-
trolled with the Ioption. By default, if a required file is not found, it fails. You can
change that behavior with the -MGoption, which assumes that files that are not
found will be generated at compile time and found in the current directory.
4.4.7.2 Macro Expansions
You can debug preprocessor macros with the -doption, which can be used only
with the preprocessor ( -Eoption). To see a list of predefined macros in no particu-
lar order, you could type the following:
$ echo | gcc -E -dM -214 Chapter 4 • Editing and Maintaining Source Files
15. http://sourceforge.net/projects/strace
Notice that you have to combine the -doption with the -Eoption and that the
-doption must be followed by the letter M, D, N, or I. The usage and meanings of
these letters are described in Table 4-31.
With -dM, what you get is a list of #define statements, cleaned up and printed
verbatim. The white space is trimmed, but the macros printed are equivalent towhat is in the code. The output is in no particular order, so you can’t infer anythingabout where a macro is defined in the code. The 
-dDoption produces the essentially
the same information except that the line order is preserved and the output contains
#line directives to direct you to the correct source line. Consider the following
source file, foo.h :
#define A a+b+c
#define B a                   + b       +c#define C a \
+b              +\c
#define TEXT "Hello             \
World"
The output is cleaned up and presented as follows:
$ gcc -E -dD foo.h...# 1 "foo.h"#define A a+b+c4.4 Source Code Beautifiers and Browsers 215
TABLE 4-31 Flags Used with the -d Option
Option Description
-dM Outputs a list of #define statements from your source as well as
built-in macros. The output is in no particular order.
-dD Essentially the same as -dM. The GNU documentation says this does
not include built-in macros, but in fact, it includes most of them.Macros found in the source are printed in the same order in whichthey are declared.
-dN Produces the same output as -dDexcept that only the macro names
are shown. The macro values are omitted.
-dI In combination with -E, this flag includes the #include statements in
the output; normally, they are omitted from preprocessor output.
#define B a + b +c
#define C a +b + c
#define TEXT "Hello                           World"
Note that although they look very different, the two sets of macros are identical .
The only difference is in the white space that the preprocessor cleans up. This is auseful illustration of how the C preprocessor cleans up white space in your macroexpressions, which has changed over various releases. This output can be very help-ful if you are porting code that compiled in an earlier release of gcc. The newlinebetween 
"Hello" and "World" would have been preserved in gcc 2.9x, for exam-
ple, but gcc 3.x removes it.
The other two flags ( Nand I) don’t improve the output much. Using -dNpro-
duces a list of macro names without their expansions. The -dIleaves the #include
statements intact in the output.
4.5 Summary
This chapter focused on tools to manipulate source code. I listed some of the pro-grammer-centric features you should look for in a text editor. I examined and com-pared the two most popular text editors for Linux: Vim and Emacs. I also looked atsome alternatives, as well as their pros and cons.
I scratched the surface of revision control, introducing the basic concepts and
some of the tools used to support revision control. I showed you how to create andapply patches, which is at the core of many revision control tools.
Finally, I looked at tools that allow you to extract information from your source
code in the form of cross references, browser output, and even typeset documen-tation.
4.5.1 Tools Used in This Chapter
The two main text editors discussed in this chapter are
• Vim—the most widely used clone of the vitext editor, which is the standard
text editor
• Emacs—the flagship GNU text editor216 Chapter 4 • Editing and Maintaining Source Files
I looked at several clones of viand Emacs. Most of these have fewer features but
use less memory:
•viclones—Elvis, nvi, Vile
• Emacs clones—Zile, joe, jed
viand Emacs started out as terminal-based editors and later acquired GUIs. As
a result, they still maintain a terminal-based look and feel. More recent editors areexclusively GUI based, and these may be more intuitive to use for those who are notfamiliar with 
vior Emacs:
• GNOME—Gedit
• KDE—Kate, Kwrite• X (generic)—NEdit, SciTE
I looked at revision control and the tools that support it:• Tools for merging and differencing—
diff , diff3 , patch , xxdiff , vimdiff ,
gvimdiff
• Tools for managing projects—Subversion, cvs, monotone, GNU arch
This chapter looked at several tools for beautifying and browsing code:•
indent
•astyle
•cflow
•ctags
4.5.2 References
•Cameron, D., et al. Learning GNU Emacs. 3d ed. Sebastopol, Calif.: O’Reilly
Media, Inc., 2004.
•Dougherty, D., and A. Robbins. sed and awk. 2d ed. Sebastopol, Calif.:
O’Reilly Media, Inc., 1997.4.5 Summary 217
•Friedl, J.E.F . Mastering Regular Expressions. 3d ed. Sebastopol, Calif.: O’Reilly
Media, Inc., 2006.
•Lamb, L., and A. Robbins. Learning the vi Editor. 6th ed. Sebastopol, Calif.:
O’Reilly Media, Inc., 1998.
4.5.3 Online Resources
Text Editors
• Emacs—www.gnu.org/software/emacs
• Vim—www.vim.org
Text Editor Clones
•bvi—http://bvi.sourceforge.net
• gedit—www.gnome.org/projects/gedit• JED—www.jedsoft.org/jed• joe—http://joe-editor.sourceforge.net• Kate—www.kate-editor.org• nano—www.gnu.org/software/nano• NEdit—www.nedit.org• SciTE—www.scintilla.org/SciTE.html• vile—http://invisible-island.net/vile• WordStar—www.wordstar.org• Zile—http://zile.sourceforge.net
Code Browsers and Beautifiers
• astyle—http://astyle.sourceforge.net
• cflow—www.gnu.org/software/cflow• cscope—http://cscope.sourceforge.net218 Chapter 4 • Editing and Maintaining Source Files
• Doxygen—www.stack.nl/~dimitri/doxygen
• Exuberant Ctags—http://ctags.sourceforge.net, http://xxdiff.sourceforge.net
Revision Control Tools
• arch—www.gnuarch.org/arch• cvs—www.nongnu.org/cvs• monotone—http://venge.net/monotone• Subversion—http://subversion.tigris.org• xxdiff—http://xxdiff.sourceforge.net4.5 Summary 219
This page intentionally left blank 
5.1 Introduction
This chapter assumes you have some experience writing applications for Linux and
some basic understanding of the Linux kernel. I will cover some kernel-related top-ics that are more often covered in books about the kernel itself. Unlike material inthose books, the material in this chapter focuses on applications.
The topics covered in this chapter include a discussion of the Linux scheduler,
which has undergone many changes recently. I cover process priority and preemp-tion, their roles and real-time applications.
In the past, a 32-bit address space was sufficiently large that most applications
never encountered any limits. Today, with 32-bit systems that can have more than4GB of RAM, many programmers are running head first into these limitationswithout a good understanding of what they’re running into. After reading this chap-ter, you should have a much better understanding of these issues and how to workaround them.
5
221What Every Developer
Should Know
about the Kernel
This chapter also looks at system input and output and how it relates to
processes. Perhaps you have been dazzled by the blinding clock speeds of modernprocessors, only to be disappointed by performance that is throttled by slow devices.I’ll look at some of the inefficiencies built into the Linux programming model andhow to work around them. I’ll also look closely at improvements in the Linux 2.6I/O scheduler and how to take advantage of it.
5.2 User Mode versus Kernel Mode
Processes execute in two modes: user mode and kernel mode. The code that youwrite and the libraries you link with execute in user mode. When your processrequires services from the kernel it must execute kernel code, which runs only inkernel mode. That sounds simple, but the devil is in the details. First of all, why dowe need two modes of operation?
One reason is security. When a process executes in user mode, the memory it sees
is unique to it. Linux is a multiuser operating system, so one process should not beallowed to view another process’s memory, which could contain passwords or sensi-tive information. User mode ensures that a process sees only memory that belongsto it. Moreover, if the process corrupts its internal structures, it can crash only itself;it will not take any other processes with it and certainly not the whole system. Thememory that the process sees when in user mode is called user space .
For the system to function as a whole, the kernel needs to be able to maintain
data structures to control every process in the system. To do this, it needs a regionof memory that is common to all processes. Because the kernel is executed by everyprocess in the system, every process needs access to a common memory region. Topreserve security, however, the kernel code and data structures must be strictly iso-lated from user code and data. That is why there is a kernel mode. Only kernel coderuns in kernel mode, where it can see the common kernel data and execute privi-leged instructions. We call the memory that the process sees in kernel mode kernel
space. There is only one kernel space, which is seen by every process when it runs in
kernel mode, unlike user space, which is unique to every process.
Figure 5-1 shows the allocation of virtual addresses among processes and the ker-
nel. In this example, the kernel is allocated the top 1GB of virtual addresses, andthe processes are allocated the rest. This split can be determined when the kernel isbuilt, but this so-called 3G/1G split is common in many stock kernels. In this con-figuration, all addresses above 
0xC0000000 are in the kernel. To use these addresses,
the process must be executing in kernel mode.222 Chapter 5 • What Every Developer Should Know about the Kernel
5.2.1 System Calls
Processes enter and exit kernel mode via system calls. Many common POSIX func-
tions are simply thin wrappers around system calls, such as open , close , read ,
ioctl , and write . Device drivers, for example, run exclusively in kernel mode.
Application code cannot call a device driver function directly. Instead, applicationsuse one of the predefined system calls to enter the driver code indirectly. The call to
read , for example, is equivalent to the following:
#include <syscall.h>
...n = syscall(SYS_read, fd, buffer, length);5.2 User Mode versus Kernel Mode 223
FIGURE 5-1 Virtual Addresses in a Typical 32-Bit Environment0xFFFFFFFF
0xC0000000
0x00000000 0x00000000 0x000000000xBFFFFFFF 0xBFFFFFFF 0xBFFFFFFFLinux Ker nel Kernel Sp ace
User Sp ace Process A Process B Process C
Each system call is assigned a number by the kernel—in this case, defined by the
macro SYS_read . The macros for the system calls are defined in syscall.h . The
list of system calls provided by Linux is determined by the kernel version and haschanged little over time. The mechanism used to make system calls, however, isunique to each processor architecture. The 
syscall function is a wrapper around
the assembly code used to make the system call. You can see an example of thisassembly code for the IA32 architecture in Listing 5-1. Although this example iswritten in IA32 assembly language, this pattern is typical for many other architec-tures as well.
LISTING 5-1 basic.S: A Basic System Call in 80x86 Assembly Language
# Use the C preprocessor for this example
#include "sys/syscall.h".data# Contents of struct timespec {1,0}
sleeptime:.long 1, 0
.text# Linker uses _start as the entry point.
# Equivalent to main() in C.
.global _start
.type _start, @function
_start:
# Execute the nanosleep(2) syscall
# Parameters are stored in registers.# Interrupt 0x80 takes us into kernel space.
movl  $SYS_nanosleep, %eax   # 1st arg, system call number
movl  $sleeptime,     %ebx   # 2nd arg, pointer to struct timespecint   $0x80                  # execute the system call
# Can’t just return. We have to call the exit(2) system call
# with our exit status.
movl  $SYS_exit,%eax    # 1st arg, 1 = exit()
movl  $0,       %ebx    # 2nd arg, exit code
int   $0x80             # execute the system call224 Chapter 5 • What Every Developer Should Know about the Kernel
Building and Running Listing 5-1
The code in Listing 5-1 shows how the system uses interrupts to switch between user
mode and kernel mode. Even exiting the program requires a system call. To build andrun this example, use the following commands:
$ gcc -o basic -nostdlib basic.S
$ strace -t ./basic23:25:27 execve("./basic", ["./basic"], [/* 32 vars */]) = 023:25:27 nanosleep({1, 0}, NULL)        = 023:25:28 _exit(0)                       = ?
We cheat a little by using the C preprocessor in our assembly module. The con-
vention for this is to name the module with the .S extension and pass that to the Ccompiler (not the assembler). The C compiler runs the preprocessor and sends thepreprocessed output to the assembler.
The 
strace command is very useful for tracing system calls and demonstrates
that we are doing exactly what we said we would do. T ry strace on a C program
sometime, and see just how many system calls it takes to print hello world .
Typically, the user code puts arguments on the stack or in predefined registers
and then issues an interrupt that causes a system call handler to be called. The inter-rupt handler switches the process into kernel mode and calls the appropriate systemcall. In kernel mode, the arguments are read from registers or copied from userspace using special functions. If this is unfamiliar to you, that’s because it should be.Portable programs do not use system calls directly but rely on libraries to do the sys-tem calls on their behalf. System calls vary from one operating system to the nextand possibly from one version to the next. Library calls insulate you from thesedifferences.
The technique used by the Linux for the 
syscall is called an Application Binary
Interface (ABI, for short), and it is not unique to Linux. The same technique is used
by other operating systems and even the BIOS. Unlike an Application Programming
Interface (API), which requires you to link with compatible functions, an ABI does
not require you to link your code against the code you want to run. This is one rea-son why your executable program can run on many different kernels withoutrebuilding. Most compatibility issues with different Linux distributions are due tochanges in the library APIs and not the kernel ABI. If you have a statically linked5.2 User Mode versus Kernel Mode 225
executable that runs on a Linux 2.2 kernel, for example, there’s a good chance that
it will still run on a 2.6 kernel, because many of the most common system call inter-faces never change.
5.2.2 Moving Data between User Space and Kernel Space
Memory in kernel space is not visible in user mode, and special care must be takenin kernel mode when accessing memory in user space. As a result, passing data viasystem calls is tricky. Simple arguments can be passed in registers, but large blocksof memory must be copied, which is inefficient.
Listing 5-1 put a pointer to the 
timespec structure in a register, which was
passed to the kernel. What you don’t see is the copy of the struct timespec data
from user space to kernel space. This is some very ugly, architecture-dependent codethat is well hidden inside the kernel. In this case, the copy is trivial—two words.Some system calls (such as 
read and write ) require a large amount of data to be
passed between user space and kernel space. This extra copying is inefficient, but itis necessary to maintain separation between user space and kernel space.
Although copying is a short-term performance hit, most often it helps perfor-
mance in the long run. An example of this is the file-system cache. When you writea data to a file, the data is copied to kernel space before it is written to disk. Becausethe data is copied, the write can complete in the background so that your applica-tion can reuse the user space buffer and continue to execute.
5.3 The Process Scheduler
Back in the days of DOS and CP/M, the typical desktop operating system ran onlyone process at a time. Scheduling was not an issue, because the system did only onething at a time in the order in which it was requested. Those days are history. Today,even the humblest embedded operating system supports multitasking.
The problem that multitasking operating systems share is dividing CPU time
among different tasks. The algorithm that does this is called the scheduler. Each
operating system uses its own scheduling algorithm, maybe even more than one,because no single algorithm is perfect for all applications. A scheduler that workswell for one set of processes may not be suitable for another. The Linux kernel pro-vides several scheduling algorithms and allows the user to select at boot time thetype of scheduler the system will use.226 Chapter 5 • What Every Developer Should Know about the Kernel
5.3.1 A Scheduling Primer
In Linux circles, the scheduler is sometimes discussed as though it were a separate
process. In fact, the scheduler code is executed by every process. Whenever a processgoes to sleep or blocks waiting for a device, it calls the scheduler routines to deter-mine what process to execute next. Calls to the scheduler are often embedded in sys-tem calls and take place when it is necessary for the process to wait for an event.
A process that communicates extensively with devices will call the scheduler often.
Device I/O invariably involves some amount of waiting. When the device is slow,most of the process’s run time will be spent waiting. Such a process does not con-sume much CPU time as a proportion of overall run time. If every process were likethis, the operating system could leave it up to the processes to call the scheduler, andeverything would work out. Such a scheme does exist, and it’s called cooperative
multitasking .
This is illustrated in Figure 5-2. T wo processes, A and B, contend for the CPU,
but only one can run at a time. T ransitions from one process to the next occur onlywhen the running process gives up the CPU, which allows the other to run. In thiscase, Process A waits for disk and gives up the CPU, allowing Process B to run. ThenProcess B waits for a keystroke, which allows Process A to run again. There are alsocalls that allow a process to give up the CPU explicitly and be nice (so to speak). Theproblem with cooperative multitasking occurs when tasks don’t cooperate.5.3 The Process Scheduler 227
FIGURE 5-2 Cooperative Multitasking ExampleProcess B
Process AWait for Disk W ait for keystroke
TimeProcess A
A process that does no I/O, such as a number-crunching application, can con-
sume the CPU and starve other processes of CPU time. Such a process does notprovide any opportunities for the scheduler to execute, so it does not allow anyother processes to run. To deal with this, operating systems use preemptive multi-
tasking. A preemptive multitasking operating system interrupts (preempts)
processes that do not give up the CPU so that another task can be scheduled. AllUNIX variants, including Linux, use a combination of cooperative and preemptivemultitasking. If a process is cooperative and gives up the CPU often, it may neverbe preempted. Preemption is reserved for those processes that do not give up theCPU voluntarily.
Figure 5-3 illustrates an example of preemptive multitasking. Here, Process A is
a niceprocess that gives up the CPU often. The Number Cruncher does not give
up the CPU, so the operating system preempts it via an interrupt. This allows thescheduler to run, which then allows Process A to run again.
5.3.2 Blocking, Preemption, and Yielding
Each Linux process is given a time slice (or quantum ) in which to execute before it
is stopped by the kernel and another process is allowed to run. When the kernelstops a process because its time slice has expired, we say that the process has beenpreempted. The kernel can also preempt a process before its time slice expires if a
higher-priority process is ready to run. When this happens, we say the higher-priority process preempts the lower-priority process.
A process can also give up the CPU voluntarily. When this happens, we say the
process has yielded the CPU. A process can call the 
sched_yield system call to
explicitly yield the CPU from user code. More often, the CPU is yielded for it byother system calls the process makes. When a process calls 
read or write, for
example, chances are that it will have to wait for a device. A well-behaved devicedriver will put the process to sleep and yield the CPU until the device is ready.
When a process waits for an event in kernel mode, we say the process is blocking.
That means that the process will not be ready to run until the event occurs.Therefore, a blocking process does not consume any CPU cycles and does not getscheduled until some event occurs to wake it up.228 Chapter 5 • What Every Developer Should Know about the Kernel
One of the new features in Linux 2.6 is the preemptable kernel. This is available
as a patch on some 2.4 kernels as well. In a nonpreemptable kernel, a process thatis running in kernel mode cannot be preempted until it returns to user mode. So ifa process is in the middle of a system call when a higher-priority process is ready torun, the higher-priority process is forced to wait until the lower-priority process fin-ishes its system call. In a preemptable kernel, the lower-priority process can be pre-empted in the middle of the system call. This allows the higher-priority process tobe scheduled more quickly. This is particularly useful in situations where a defectivedriver is causing a process to take too long in kernel mode. Although a process maybe stuck in the driver, the system can still function by preempting the process. In anonpreemptable kernel, such a process could hang the whole system.
5.3.3 Scheduling Priority and Fairness
All preemptive multitasking operating systems, including Linux, implement a pri-ority scheme for scheduling. In simple terms, priorities resolve scheduling conflictswhen more than one process is ready to run. Whenever this happens, a higher-priority process is allowed to run before a lower-priority process. Priorities can be5.3 The Process Scheduler 229
FIGURE 5-3 Preemptive Multitasking ExampleTimeNumber
CruncherNumber
Cruncher
Process A Process AWait for Disk Wait for DiskPreemption /
RescheduleWait for Disk W ait for DiskPreemptio n /
Reschedule
influenced by the user, but the kernel ultimately determines a process’s priority. To
understand why, consider an example.
Figure 5-4 shows an example of fixed priority with three processes. Process A is
the lowest priority, and perhaps it forks the other two processes: Number CruncherA and Number Cruncher B. These two processes are always running (never wait-ing), so neither process gives up the CPU voluntarily. The CPU spends 100 percentof its time executing one of these two processes, and scheduling occurs only whenthe running process is preempted. Now suppose that the lower-priority process getsan interrupt from the keyboard (perhaps Ctrl+C). Process A will not be scheduledto run until the two number crunchers are done, because it has a lower priority. Theinterrupt is not delivered until the scheduler decides that Process A may run again.Until the number crunchers are done, it would appear that Process A is hung.
To prevent this situation, the Linux kernel continually upgrades or downgrades
a process’s priority as it runs by using dynamic priority, which is illustrated in
Figure 5-5. When a process is identified as interactive, its effective priority is
increased, which allows it to be scheduled even when the system is busy.230 Chapter 5 • What Every Developer Should Know about the Kernel
FIGURE 5-4 Fixed Priority Scheduling Can Allow Noninteractive Processes to Hog the CPUTimePriority
Process ANumber
Cruncher ANumber
Cruncher A
High Priority 
number crunchers
monopolize
the CPU.Number
Cruncher B
Preemption /
ReschedulePreemption /
ReschedulePreemption /
ReschedulePreemptio n /
ReschedulePreemptio n /
ReschedulePreemptio n /
Reschedule
Wait for Keybo ardNumber
Cruncher B
An overriding goal of the Linux scheduler is to see that every task gets a chance to
run—that is, that no task gets starved for CPU time. The scheduler pays attention toeach process’s behavior so that processes that are deemed interactive have a higher pri-ority. A keyboard input process would be a good example of an interactive process.Such a process spends most of its time waiting for input and very little time process-ing. It is easily identified by the scheduler because it is never preempted and consumesvery little CPU time. It always gives up the CPU voluntarily. To give interactiveprocesses a higher priority, the scheduler keeps a bonus value in addition to theprocess’s static priority —the priority assigned when the process is created, which the
scheduler does not change over the life of the process. The effective priority of a process
is the sum of its static priority plus its bonus.
1The bonus value can be positive or neg-
ative, so the effective priority can be higher or lower than the static priority.5.3 The Process Scheduler 231
FIGURE 5-5 Dynamic Priority Allows the Operating System to Promote an Interactive Task
Priority
TimeWait for KeyboardPreemption /
Reschedule
Preemption /
RescheduleCtrl + C
Process
Exit /
Reschedule
Wait for Keybo ard
Process AProcess A
Number
Cruncher ANumber
Cruncher BNumber
Cruncher B
Number
Cruncher A
Number
Cruncher’s 
Priority
DecreasedProcess A
Priority
Increased
Preemptio n /
Reschedule
Preemptio n /
RescheduleCtrl + C
Process
Exit /
Reschedule
1. This ignores the nice value, which I will discuss shortly.
An example will let you see the scheduler in action. First, you’ll need some
processes to run, so you’ll create a couple of scripts. Call one script niceguy ,
because it will spend most of its time sleeping.
#!/bin/sh
# The niceguy - sleeps most of the timewhile true; do
sleep .1
done
You need another process to consume the CPU, but it doesn’t need to do any-
thing important. The scheduler is clever, but it’s not that clever. So create a scriptnamed 
cruncher that just runs the built-in true function forever:
#!/bin/sh
# The cruncher – consumes the CPU with nonsensewhile true; do
true
done
Finally, you need one more script to show you what’s going on, because it’ll prob-
ably happen too fast for you to type. Call it runex . This script will launch both
processes in the background and then run the pscommand periodically to show
those processes’ priorities over time.
#!/bin/sh
./cruncher &./niceguy &
# Trap SIGINT (Ctrl+C) to clean up
trap 'echo stopping; kill %1 %2; break' SIGINT
while true; do
ps -C niceguy -C cruncher -o etime,pid,pri,cmdsleep .5
done
Next, run the example by running the runex script, which launches two
processes and prints out their priorities over time. The output is shown below. Payattention to the 
PRIfield in the output, which is the effective priority:
$ ./runex
ELAPSED   PID PRI CMD
00:00 17076  20 /bin/sh ./cruncher00:00 17077  22 /bin/sh ./niceguy
ELAPSED   PID PRI CMD
00:01 17076  20 /bin/sh ./cruncher00:01 17077  24 /bin/sh ./niceguy232 Chapter 5 • What Every Developer Should Know about the Kernel
ELAPSED   PID PRI CMD
00:01 17076  19 /bin/sh ./cruncher00:01 17077  23 /bin/sh ./niceguy
ELAPSED   PID PRI CMD
00:02 17076  18 /bin/sh ./cruncher00:02 17077  24 /bin/sh ./niceguy
ELAPSED   PID PRI CMD
00:02 17076  17 /bin/sh ./cruncher00:02 17077  24 /bin/sh ./niceguy
ELAPSED   PID PRI CMD
00:03 17076  15 /bin/sh ./cruncher00:03 17077  24 /bin/sh ./niceguy
ELAPSED   PID PRI CMD
00:04 17076  14 /bin/sh ./cruncher00:04 17077  24 /bin/sh ./niceguy
Notice how the priorities of the two processes change over time. The cruncher
process spends all its allotted time running; it never sleeps. This results in the sched-
uler’s giving it negative bonus value, causing it to lower its effective priority. The
niceguy process spends most of its time sleeping, which results in a positive bonus.
Because of this, the scheduler raises niceguy ’s effective priority.
On my machine, the scheduler lowers the cruncher process’s priority from 20 to
a low of 14 after about 4 seconds. Conversely, the scheduler raises the niceguy
process’s priority from 22 to 24. In this controlled example in a controlled envi-
ronment, the priorities settle into a steady state. In a real system, priorities will goup and down accordingly as other processes are activated, created, and destroyed.
A Brief Description of PS Options
This example uses some uncommon options of the pscommand. The -Coption tells
psto show only processes with executable names that match the argument. You spec-
ify -Cmultiple times to tell psto look for more than one command name.
The -ooption allows you to control the output format. It is followed by the
fields you want to see in the output, which are documented in the ps man page.
The fields you are looking at are:
•etime —elapsed clock time since the process started
•pid—process ID
•pri—priority
•cmd—command line used to start the process5.3 The Process Scheduler 233
5.3.4 Priorities and Nice Value
If you ran the example in the previous section, chances are that your system was still
fairly responsive. When you pressed Ctrl+C to kill the program, for example, it
probably terminated immediately. Just having many processes running does notnecessarily mean that your system will be sluggish. In the previous example, thetasks weren’t actually doing anything, so it seems natural that such a process will not
bog down the system.
Such a trivial process canbog down the system if it is given the opportunity, how-
ever. One way to do so is to give it a high priority. The kernel allows users to influ-ence the scheduler’s decisions about priority, using what is called the nice value.
Giving a process a positive nice value causes the scheduler to give it a lower priority;giving a process a negative value causes the scheduler to give it a higher priority.
The nice value is subtracted from the sum of the bonus and the static priority to
create the effective priority. Any unprivileged user can set positive nice values withthe 
nice command, but only the superuser can set a negative nice value. To run a
command with low priority, you would use the nice command as follows:
$ nice -n 1 tar -cvf foo.tar ...# Run tar with a nice value of 1
In this case, the tarcommand runs normally, with no side effects other than its
effective priority. The nice value of a process remains constant over the life of theprocess unless it is changed with the 
renice command. The renice command
works on only one running process, as follows:
$ renice 1 -p 1234
1234: old priority 0, new priority 1
This changes the nice value of process 1234 to 1. Unprivileged users can only
increase the nice value with renice , even if the resulting nice value is still positive.
Note that unlike the nice command, which allows any user to set a positive nice
value, the renice command allows unprivileged users only to raise the nice value,
not to lower it. Only root is allowed to lower a nice value, even if the resulting nicevalue is zero or greater.
The range of nice values is defined by POSIX to be between –20 and 19. Linux
priorities used by the scheduler for normal processes
2are unsigned and fall in the234 Chapter 5 • What Every Developer Should Know about the Kernel
2. A “normal” process is one that is not a real-time process, which I will describe shortly.
range 0 through 39. Looking at it differently, if the nice value is 19, the highest that
the effective priority can ever go is 20. Likewise, with a nice value of –20, the low-est that the effective priority can ever go is 20.
Look at another example, using the 
niceguy script from earlier in the chapter.
This example launches four different processes with four different nice values andthen uses the 
pscommand to see what the scheduler is up to:
$ for nv in 0 1 2 3; do nice -n $nv ./niceguy & done
$ ps -C niceguy -p $$ -o pid,pri,ni,cmd
PID PRI  NI CMD
5694  23   0 bash6661  23   0 /bin/sh ./niceguy
nice –n 0 …
6662  22   1 /bin/sh ./niceguy nice –n 1 …
6663  21   2 /bin/sh ./niceguy nice –n 2 …
6664  20   3 /bin/sh ./niceguy nice –n 3 …
In this example, I ran the pscommand after a few seconds to let things settle
down. New processes inherit their static priority from their parent process, whichin this example is the Bash shell, running with a priority of 23. You can see that thechild with a nice value of zero runs with the same priority as its parent. Notice thatthe processes with nonzero nice values have their priority lowered by that much. Soa process with a nice value of 3 has a priority of 20, which is 3 less than the prior-ity of the parent shell. As you might expect, the effective priority tends to go upwhen you use negative nice values.
This ideal behavior shows up only in an unloaded system with simple processes
that do nothing. In a real system, busy with real processes, priorities shift constantly,and there is no guarantee that two processes with the same nice value will have thesame priority. The only thing the nice value guarantees is that your effective prior-ity will never go higher or lower than a certain level.
5.3.5 Real-Time Priorities
The scheduler provides a different type of scheduling for processes that have strictlatency requirements. Latency refers to the time it takes for software to respond to
external events, such as interrupts. Applications with strict latency requirementsare often called real-time applications. These applications must guarantee that the
software responds to events within a certain interval of time; otherwise, bad thingshappen.5.3 The Process Scheduler 235
A real-time application that you might encounter on your computer is your
media player. When showing a video, the player must update the screen at reason-able intervals; otherwise, you’re going to notice in the form of jerky motion in yourfavorite movie. That’s what we call a soft real-time application, because when the
software is late once in a while, it can always recover. If your media player skips aframe, it’s not the end of the world. A hard real-time application is one that cannot
be late even once. An example of a hard real-time application might be a flight-control computer that has to respond immediately to pilot control movements.Being late in this application could cost lives.
The Linux scheduler provides a real-time scheduling implementation that is very
close to the POSIX 1003.1 standard. This provides an additional 100 priority lev-els, all of which are higher priority than the normal process priorities (0–39). Real-time processes in Linux have priorities in the range 41 to 139. (For some reason,priority 40 is unused.) Like normal priorities, higher values mean higher priority,but what makes real-time priorities different is that they never change over the lifeof the process. Because the priority never changes, real-time processes do not havea nice value, and there is no bonus value. The priority is what it is.
When you designate a process as a real-time process, you also must specify the
scheduling policy. POSIX specifies two scheduling policies for real-time processes:FIFO and round robin.
5.3.5.1 FIFO Scheduling
The term FIFO stands for first in, first out and refers to how the processes are placed
in the run queue. When two FIFO processes of the same priority are ready to run,the first that was ready is the first one to run—always. A FIFO process cannot bepreempted except by another process with a higher priority, which by definition isanother real-time process. If you ran the 
cruncher script as a FIFO process, you
would render your system unusable. T ry this with a safer example that illustrates thissituation nicely. You will need the 
chrt command from the schedutils package:
#!/bin/sh
(sleep 5; kill -ALRM $$) &while true; do
true ;
done236 Chapter 5 • What Every Developer Should Know about the Kernel
This is a variation of the cruncher script that you used earlier, but it runs for
only 5 seconds. You will soon find out why you went to this trouble. Call this script
chewer . Using the chrt program, run this as a real-time/FIFO process, and watch
what happens:
$ sudo chrt --fifo 50 ./chewer &
This launches the script in the background as a real-time process with real-time
priority 50 and FIFO scheduling. You should notice that your system becomesunresponsive for 5 seconds. In fact, it will probably appear to be locked up. A typ-ical Linux system does not have any real-time processes running, so your shell andany daemons that are running are all blocked while this dumb script runs.Fortunately, we launched a background process at the same priority to kill us afterfive seconds. Don’t skip that line or you will need to hit the reset button.
The 
chewer process doesn’t do anything; it simply consumes CPU cycles.
Because it is a real-time process, it can be preempted only by a real-time processwith higher priority. The only time that a lower-priority process gets to run is when
chewer yields the CPU. Because chewer makes no blocking system calls while it
spins in its loop, it never provides any opportunities to yield the CPU.
5.3.5.2 Round-Robin Scheduling
Round-robin scheduling is the second policy for real-time processes and is almost
identical to FIFO scheduling except that round-robin processes are not allowed torun indefinitely; instead, they are given a time slice in which to run. A process run-ning with round-robin scheduling will be preempted only when its time sliceexpires or when a higher-priority process is ready to run. If a round-robin processis preempted by a higher-priority process, the scheduler allows the round-robinprocess to consume the remainder of its time slice before scheduling any otherprocesses at the same priority. Only when a round-robin process yields the CPU arelower-priority processes allowed to run.
Recall that normal processes have a time slice called a quantum .If a normal
process consumes its entire time slice without yielding the CPU, its time slice isshortened the next time it is scheduled. Unlike a quantum, the round-robin timeslice never changes. The process is given the same time slice every time.5.3 The Process Scheduler 237
5.3.6 Creating Real-Time Processes
You saw one way to create a real-time process using the chrt command. On the
inside, chrt uses basic fork and exec calls with an additional POSIX call to set
the priority. To set the real-time priority, an application can use the followingPOSIX functions:
int sched_setscheduler(pid_t pid, int pol, const struct sched_param*p);
int pthread_setschedparam(pthread_t thread, int policy,
const struct sched_param *param);
The sched_setscheduler function is for use by processes and takes a process
ID as its argument. The pthread_setschedparam function is used for threads and
takes a thread ID instead of a process ID. Both functions require a policy and apointer to a 
sched_param structure. The policy is indicated by one of the macros
shown in Table 5-1.
The only value in the sched_param structure that is filled in by the user is the
priority field, which must fall within a specified range of valid values. You can
determine the range of allowable values with the following POSIX functions:
int sched_get_priority_min(int policy);
int sched_get_priority_max(int policy);
POSIX allows each real-time scheduling policy to have a unique range of priori-
ties, although Linux uses the same range for both real-time policies ( SCHED_FIFO
and SCHED_RR ). When setting a nonreal-time policy ( SCHED_OTHER ),
sched_setscheduler does not allow you to set the priority. Any value other than
zero for the priority will produce an error with errno set to EINVAL . Instead, a
process can set the nice value via the nice or the setpriority system calls. Note
that although the name setpriority implies that you are setting priority, it sets
only the nice value.238 Chapter 5 • What Every Developer Should Know about the Kernel
TABLE 5-1 Macros Used for POSIX Scheduling Policy
Macro Meaning
SCHED_FIFO Use FIFO scheduling
SCHED_RR Use round-robin scheduling
SCHED_OTHER Use normal Linux scheduling
Linux uses the range 1 through 99 for POSIX real-time priorities passed to
sched_setscheduler . This is a little confusing, because the Linux scheduler uses
only one continuous range of priorities that includes both normal and real-timeprocesses. The entire range of absolute priorities used by the scheduler extends from0 through 139. When you assign a real-time priority of 1, for example, the sched-uler uses an absolute priority of 41. For example:
$ chrt -f 1 ps -C ps -o pri,ni,rtprio,comm
PRI  NI RTPRIO COMMAND
41   -      1 ps
This runs the pscommand with a SCHED_FIFO policy and a priority value of 1.
The pscommand is instructed to show what it is doing. The PRIcolumn is the
absolute priority used by the scheduler, whereas the RTPRIO column shows the same
priority represented in the real-time range. Notice that the nice value ( NIfield) is
shown with a hyphen because the nice value is not valid for real-time processes.Likewise, if this were a normal process, the 
RTPRIO column would not be valid, and
the nice value would be represented by a decimal number.
5.3.7 Process States
Over the life of your process, it will pass through several states. As a user, you seeonly the states shown via tools like 
psor what you get from the /proc file system
(which is what psuses). The states and their abbreviations are listed in Table 5-2.
TABLE 5-2 Process States As Seen by the User
State Abbrev Meaning
Running R Running or ready to run
Interruptible S Blocked waiting for an event but may be awakened by
a signal
Uninterruptible D Blocked waiting for an event and will not be
awakened by a signal
Stopped T Stopped due to job control or external tracing (for
example, ptrace )
Zombie Z Exited, but its parent has not called wait (not reaped)5.3 The Process Scheduler 239
5.3.7.1 Sleeping versus Running
When a process is in the runnable state, it does not mean that it is running; it means
only that the process is not sleeping or waiting for an event. It is possible to havemultiple processes in the runnable state. A number-crunching process, for example,would always be in the runnable state. You should keep this in mind when usingthe 
pscommand, as the following example illustrates:
$ ./cruncher & ./cruncher & ./cruncher &
$ ps -C cruncher -p $$ -o pid,state,cmd
PID S CMD
2588 S bash2657 R /bin/sh ./cruncher2658 R /bin/sh ./cruncher2659 R /bin/sh ./cruncher
This example launched three cruncher processes as background tasks and then
executed a pscommand to show the process state as well as the state of the parent
shell process. As you can see, the output shows that all three crunchers are in state
R, which means that they are all runnable. This is expected, because they don’t sleep.
Because this is a single CPU system, however, only one of the processes is actuallyrunning. The output also shows that the parent shell (
bash ) is sleeping. This is
expected as well. Because the shell created the process, it probably is sleeping in a
wait system call, waiting for psto exit.
The sleep state that you see in Bash in the previous example is an interruptible
sleep. That means that if the Bash shell receives a signal, it will respond to the sig-nal (that is, run its signal handler). A 
SIGTERM or SIGQUIT signal, for example, will
cause it to terminate. Most of the time, when your code is blocking due to I/O orjust sleeping, it is in an interruptible sleep.
An uninterruptible sleep occurs less frequently and used when the kernel code
(most often, a device driver) decides that the process had better not be interruptedwhile an operation is taking place. Normally, this is a transient state that the driveruses only for short durations to ensure that the process finishes what it starts. Yourdriver might be flipping bits on a particular device and waiting for a response viapolling, for example. The driver wants to ensure that the device is left in a knownstate, which cannot be guaranteed if the process is allowed to terminate during the240 Chapter 5 • What Every Developer Should Know about the Kernel
sleep. To prevent this, the driver puts the process in an uninterruptible sleep until
the hardware is back in a known state.
You can observe uninterruptible sleeps by accessing a slow device, such as a 
CD-ROM. Here, I use the ddcommand to read the entire contents of a CD
(/dev/cdrom ) and dump it to the bit bucket ( /dev/null ):
$ dd if=/dev/cdrom of=/dev/null &
While this runs, you can peek at the process state periodically to see what it is up
to. The CD-ROM is slow enough that you should expect to see it enter an unin-terruptible sleep occasionally. It may take a couple of tries, but the following com-mand will work:
$ ps -C dd -o pid,state,cmd
PID S CMD
4606 D dd if /dev/hdc of /dev/null
A process in an uninterruptible state can be a dangerous thing. Under normal cir-
cumstances, it’s in this state for a very short time, but when hardware or media isfaulty, the uninterruptible state can be a problem. It may never arise until youencounter defective hardware or media. Consider a poorly written driver that usesuninterruptible sleeps with no timeout. When this driver tries to read from a defec-tive device, it may never get the response it requires and can leave a process in anuninterruptible sleep indefinitely. Worse, the user has no idea why. All she knows isthat her process is stuck, and she cannot kill it or wake it up.
If you ever run across a process that you can’t kill, even with 
kill -9 , chances
are that it is stuck in an uninterruptible sleep. There is no remedy for this situationexcept to reboot and fix the device (or perhaps its driver).
5.3.7.2 Zombies and Wait
When a process exits, it does not disappear entirely until its parent calls one of the
wait system calls. Until this happens, the process stays around in a so-called zom-
bie state, waiting for its parent to acknowledge its termination. The name zombie is
a whimsical term for a process that has terminated but stays around neither livingor dead, like its undead namesake. Zombie processes don’t consume memory or5.3 The Process Scheduler 241
processing resources,3but they do show up in the psoutput. If the parent termi-
nates without waiting for its child processes, those processes are “adopted” by the
init process, which calls wait periodically to reap these processes (another dark
metaphor).
In keeping with the undead analogy, create an example script to illustrate zom-
bie processes named romero .4Write this one in Perl so that the Perl programmers
in the audience don’t feel left out.5
#!/usr/bin/perl
use POSIX;$pid = fork();
if ( $pid ){
# Parent stopsprintf("%d is the proud father of %d\n",getpid(),$pid);pause();
}else{
# Child exitsexit(0);
}
The API is virtually identical to the POSIX C API, thanks to Perl’s POSIX pack-
age. You use the fork function to create the child just as you would in a C func-
tion. Perl’s POSIX package has wait functions as well, but you won’t use them for
this example. The parent process simply calls pause , which stops the process until
a signal is received. While the parent is paused, the child simply exits. Although thechild process has exited, it continues to show up in the process tables until the242 Chapter 5 • What Every Developer Should Know about the Kernel
3. They don’t consume human flesh or brains, either.
4. In honor of the king of all zombie movies: George A. Romero.5. Please don’t take this to mean that Perl is a dead language or that Perl programmers are in any way
undead.
parent acknowledges its termination. Now run the romero script in the background
with the trusty pscommand in the foreground to see what is happening:
$ ./romero &
[1] 5039$ 5039 is the proud father of 5040$ ps -o pid,state,cmd
PID S CMD
4545 S bash5039 S /usr/bin/perl ./romero5040 Z [romero] <defunct>5043 R ps -o pid,state,cmd
In this example, process ID 5039 is the parent, and 5040 is the child. The ps
command indicates that the child is in state Zand indicates this further in the CMD
section, where it reads defunct —a slightly more dignified description than zombie.
Rest assured that process 5040 is consuming no processing time or memory.
Why Zombies?
You may ask, “Why bother keeping zombie processes around?” After all, the only use-
ful information they have is their exit status.
But that’s the whole point. You, the application programmer, may not care about
the exit status of a process that you forked (although you should), but the kerneldoes not know that. The kernel assumes that the parent process is interested inknowing the result of the child process that it forked, so it sends the parent a signal(
SIGCHLD ) and keeps the status for the parent to collect. Until the parent retrieves
the return status by calling one of the wait functions, the process continues to exist
in a zombie state.
When a parent process exits before the child process, the child is adopted by init ,
which collects the status immediately, effectively removing the zombie process.
5.3.7.3 Stopped Processes
A process can be stopped for various reasons. You probably have used the shell Ctrl+Z
sequence to stop a process running in the foreground. Terminals traditionally define5.3 The Process Scheduler 243
this character as a so-called SUSP character to be used to stop a process running in
the foreground. In Linux (and UNIX), pressing this key causes the pseudoterminalto send a 
SIGTSTP signal6to the process. You can define this key to be whatever you
like, but the default is Ctrl+Z by convention.7
Several signals will cause a process to enter the stopped state; you can find a list
of them in the signal(7)man page. A process will leave the stopped state and con-
tinue executing when it receives a SIGCONT signal. Otherwise, the only other way
to leave the stopped state is via a termination signal. Normally, a signal received inthe stopped state is recorded by the kernel, and the process does not run its signalhandler until it leaves the stopped state. There are exceptions unique to Linux.
SIGTERM and SIGKILL , for example, are handled immediately, even when the
process is stopped.8
Another way processes can be stopped is by the terminal itself. A terminal man-
ages processes using the convention of background and foreground processes. Each
terminal has one—and only one—foreground process, which is the only processthat receives input from the keyboard. Any other process started from that terminalis considered to be a background process. When a background process tries to readfrom its standard input, the terminal stops it by sending it a 
SIGTTIN signal because
there is only one input device (the keyboard), and that device is connected to theforeground process. A process stopped by 
SIGTTIN does not continue until it is
brought into the foreground by the fgcommand. Note that this concept of fore-
ground and background processes is used only with respect to terminals. The kernel
does not keep track of processes this way, but it provides these signals to facilitatethe terminal’s process management. Here’s an example that demonstrates 
SIGTTIN
in action:
$ read x &
[1] 5851
This example tries to run the bash built-in read command in the background.
Because a background process is not allowed to take standard input from the244 Chapter 5 • What Every Developer Should Know about the Kernel
6. Not to be confused with SIGSTOP . SIGTSTP can be caught; SIGSTOP cannot.
7. Refer to stty(1) .
8.SIGKILL cannot be trapped by a user-defined signal handler, but SIGTERM can.
terminal, the process receives a SIGTTIN signal, which sends it to the stopped state.
The next command shows that indeed, the process is listed in state T(for stopped ):
$ jobs -x ps -p %1 -o pid,state,cmd
PID S CMD
5851 T bash
When stopped by the SIGTTIN , a process can be awakened by SIGCONT but will
block again when it tries to read from the standard input. Only when it is broughtinto the foreground can it complete its input.
The same method can be used to silence background processes and prevent them
from cluttering your terminal display, which always seems to happen at the worstpossible time. The terminal has a 
tostop (terminal output stop ) setting that is off
by default on most systems. When this setting is disabled, background processes areallowed to write to the terminal whenever they need to. When you enable the
tostop flag with the stty command, background processes will be stopped when
they try to write to the standard output. To enable the tostop setting, for example,
use the stty command as follows:
$ stty tostop
$ echo Hello World &$ jobs -l[1]+  2709 Stopped (tty output)    echo Hello World
After enabling the tostop flag, any background process that tries to write to
standard output will receive SIGTTOU , which will put it to sleep. Just as you can
with a process stopped by SIGTTIN , you can wake it up with SIGCONT , but it will
just go back to sleep when it tries to finish the write that was interrupted by the sig-nal. Only when it is brought into the foreground will the process continue.Alternatively, you could disable 
tostop and send the process a SIGCONT .
To disable tostop , use the following command:
$ stty -tostop
In case you weren’t paying attention, the only difference between this stty com-
mand and the earlier one is the dash, which indicates that the subsequent flag is tobe disabled.5.3 The Process Scheduler 245
5.3.8 How Time Is Measured
The kernel keeps track of execution time for each process. The kernel records how
much time each process spends in user mode versus kernel mode separately. The
time command is very useful for illustrating where your process is spending its
time. This feature is built into the Bash shell and some others but also is availableas a command in 
/bin/time . I’ll use the built-in Bash version in this example:
$ time sleep 1
real    0m1.042s
user    0m0.000ssys     0m0.020s
$ time dd if=/dev/urandom of=/dev/null count=1000
1000+0 records in1000+0 records out
real    0m0.527s
user    0m0.000ssys     0m0.500s
The sleep command executes the sleep system call, which causes the process to
block for 1 second. In this case, the process executes for 1.042 seconds total.Because the process blocked the whole time, it consumed no CPU cycles during thesleep. The 
ddcommand, on the other hand, runs diligently for 1,000 blocks, copy-
ing data from /dev/urandom to /dev/null . In this case, the process runs for
527 ms total, with 500 ms of that time being spent in kernel mode. This is mostlikely the 
/dev/urandom driver executing on behalf of the process. Aside from this,
the ddcommand has little to do except copy the data, which likely accounts for the
other 27 ms.
If your process is consuming too much time in user space, you can’t blame it on
the kernel. It might be your code or some library functions you are linking with,but it’s not the kernel. There are several tools at your disposal to improve per-formance in user mode, including optimizing and refactoring. On the other hand,if the process is spending too much time in system calls, it may not be your fault.It could be that you are calling some particular system call more often than you246 Chapter 5 • What Every Developer Should Know about the Kernel
need to, or it could be that the particular system call takes too long. The strace
tool is excellent for tracking down these problems. Look at that ddcommand again
with strace :
$ strace -c dd if=/dev/urandom of=/dev/null count=1000
1000+0 records in1000+0 records out% time     seconds  usecs/call     calls    errors syscall------ ----------- ----------- --------- --------- ----------------
88.77    0.730879         729      1003           read10.44    0.085947          86      1002           write
0.68    0.005611         701         8           close0.04    0.000310         310         1           execve0.03    0.000210          18        12         6 open0.01    0.000079          13         6           old_mmap0.01    0.000064           8         8           rt_sigaction0.01    0.000053          27         2           munmap0.00    0.000039          10         4           fstat640.00    0.000038          19         2           mprotect0.00    0.000036          18         2           mmap20.00    0.000030          10         3           brk0.00    0.000016          16         1         1 access0.00    0.000013          13         1           set_thread_area
------ ----------- ----------- --------- --------- ----------------100.00    0.823325                  2055         7 total
Using the -coption, strace counts the occurrences of each system call as well
as the total amount of time spent executing code in the system call. Notice that
strace causes the process to run more slowly, because it intercepts the system calls.
The entire program took 527 ms before, but now the read calls alone take 729 ms.
The process calls read and write the same number of times, yet roughly 89 per-
cent of the time is spent in the read system call. Because you were reading from
/dev/urandom , this tells you that this device is the culprit for consuming system
time. Anything you can do to minimize the use of this device, therefore, willimprove performance.
This is a contrived example, because 
/dev/urandom spends a nontrivial amount
of time calculating random numbers on your behalf in kernel mode. If you had used
/dev/zero , for example, the numbers would be much shorter.5.3 The Process Scheduler 247
A trickier problem is when your code takes too long because it is blocking. This
is hard to track down, because it can be difficult to find out what is causing you toblock. Look at the same thing again, using a slow device such as a CD-ROM drive:
$ time dd if=/dev/cdrom of=/dev/null count=1000
1000+0 records in1000+0 records out
real    0m0.665s
user    0m0.000ssys     0m0.060s
As expected, most of the time is spent blocking, as indicated by the high real-time
value of 655 ms and the negligible CPU time values. In this case, it’s obvious thatthe culprit is the CD-ROM drive, but the 
strace command is remarkably unhelp-
ful here:
$ strace -c dd if=/dev/cdrom of=/dev/null count=1000
1000+0 records in1000+0 records out% time     seconds  usecs/call     calls    errors syscall------ ----------- ----------- --------- --------- ----------------
51.91    0.131404         131      1003           read44.31    0.112158         112      1002           write
3.01    0.007612         952         8           close0.20    0.000516          43        12         6 open0.13    0.000338         338         1           execve0.11    0.000274          34         8           rt_sigaction0.09    0.000231          39         6           old_mmap0.06    0.000145          36         4           fstat640.04    0.000107          54         2           munmap0.04    0.000104          35         3           brk0.03    0.000088          44         2           mprotect0.03    0.000082          41         2           mmap20.02    0.000040          40         1         1 access0.02    0.000038          38         1           set_thread_area
------ ----------- ----------- --------- --------- ----------------100.00    0.253137                  2055         7 total
Again, you have the same number of calls to both read and write , but the
read s and write s appear to be taking close to the same amount of time. From the
kernel’s perspective, they are consuming about the same number of CPU cycles, butthe 
read s are blocking, whereas the write s are certainly not. The kernel counts
only the CPU cycles used by each system call. Time spent blocking in your processis time the kernel spends doing other things.248 Chapter 5 • What Every Developer Should Know about the Kernel
If your code is running slowly because of a blocking device or system call, there
are few options to speed your code. The most obvious is to avoid doing those calls.If that’s not possible, you can try to parallelize your application with threads orasynchronous I/O.
5.3.8.1 System Time Units
POSIX defines the clock tick (
clock_t ) as one unit for measuring system time in
user space applications. Unfortunately, this particular unit has two definitions. TheANSI definition is used with the ANSI 
clock function, which should not be used
in Linux programs. This function returns the amount of CPU time consumed bythe process, which is roughly equal to the sum of the user time and system time.
clock returns a value measured in CLOCKS_PER_SEC , which is a system-defined
macro that the GNU standard library defines to be 1000000 . On a Linux system,
the return value of the clock function is measured in units of microseconds,
although the actual tick rate will usually be much lower. The frequency of the clocktick used by functions that return a 
clock_t is given by the sysconf function, as
follows:9
sysconf(_SC_CLK_TCK);
sysconf returns a value in ticks per second (or hertz), and any variable of type
clock_t is related to this value. This is important if you are doing performance
measurements, because it determines the precision for functions that return a valueof type 
clock_t .
One problem with the ANSI clock function is that it will overflow within a lit-
tle more than an hour. For processes that run much longer than that, the clock
function is inappropriate. What’s more, the clock function does not take into
account CPU usage by child processes and does not differentiate between user spaceand kernel space. With all these problems, it does not make sense to use the 
clock
function on Linux systems, although it is part of ANSI standard C. Fortunately,Linux provides several alternatives.5.3 The Process Scheduler 249
9. Note that the ANSI CLK_TCK macro is made obsolete by POSIX, although it is still defined as
sysconf(_SC_CLK_TCK ).
The POSIX times function also uses the clock_t type but defines the unit dif-
ferently. Instead of CLOCKS_PER_SEC or microseconds, clock_t values returned by
the times function are measured in system clock ticks. This makes overflow much
less likely.
The prototype for the times function is as follows:
clock_t times(struct tms *buf);
The value returned represents the number of ticks of the wall clock since an arbi-
trary time in the past. Linux defines this point to be the time the system booted.For portability, the return value should be used only as a reference for relative tim-ing, not for absolute times. The important details from 
times are stored in the
struct tms structure, whose address is passed by the calling process. The tms
structure is defined as follows:
struct tms {
clock_t tms_utime;  /* user time */clock_t tms_stime;  /* system time */clock_t tms_cutime; /* user time of children */clock_t tms_cstime; /* system time of children */
};
The tms_utime value is the amount of time the process has spent executing user
code since the process started. The tms_stime field is the time process spent exe-
cuting kernel code since it started. The output of the ANSI clock function is
equivalent to the sum of these two values multiplied by the tick interval. The
tms_cutime and tms_cstime values are the same values except that these are
measured for forked processes that have terminated and been reaped with one of the
wait system calls.
An alternative to times is the getrusage function introduced in BSD:
int getrusage(int who, struct rusage *usage);
Unlike the times function, getrusage fetches specifically the parent or child
information. The rusage structure contains numerous fields in addition to timing
information, but the Linux kernel fills in very few of them. T wo of the fields thatare filled in include the user time and system time, like 
times except these are
stored in a struct timeval :
struct timeval ru_utime;
struct timeval ru_stime;250 Chapter 5 • What Every Developer Should Know about the Kernel
Instead of the ambiguous clock_t type, the timeval structure stores time as
two integers in seconds and microseconds:
struct timeval {
long    tv_sec;         /* seconds */long    tv_usec;        /* microseconds */
};
This gives you higher precision than times , but although the units are in
microseconds, the clock does not tick every microsecond. You might expect the clockto tick at the same rate as 
clock_t , but you would be wrong. In Linux, the fre-
quency of the clock used by getrusage is determined by your running kernel. So
whereas the interval for a clock_t may be 10 ms, the tick interval you get from
getrusage may be 1 ms. It is not uncommon to see Linux 2.6 kernels that run with
an internal tick frequency of 1,000Hz and a user tick frequency of 100Hz. In thiscase, the 
getrusage function will return more-precise values than the times func-
tion. Unfortunately, there is no API to determine the frequency of the kernel tick;therefore, you can never know how accurate the values in the 
timeval structure will
be. It should be safe to assume that it is as precise as clock_t or even more so.
Just when you thought that there were enough clock functions, there is another
one worth talking about. The POSIX real-time extensions defined in POSIX1003.1 added the 
clock_gettime function, which has a couple of advantages. The
clock_gettime prototype looks like the following:
int clock_gettime(clockid_t clk_id, struct timespec *tp);
The first thing to notice is yet another time structure. timespec defines times in
units of nanoseconds as follows:
struct timespec {
time_t      tv_sec;long        tv_nsec;
};
Here again, the nanosecond resolution does not mean that the clock ticks once
every nanosecond. The API allows for multiple clocks, each of which may tick at adifferent interval and have a different reference. You must specify the clock via the
clockid_t parameter. Unlike with getrusage , you can determine the clock period
with the clock_getres function. The prototype for clock_getres looks like this:
int clock_getres( clockid_t clk_id, struct timespec *res);5.3 The Process Scheduler 251
This tells you the clock period with a timespec structure. Although the API
allows multiple clocks, the only clock required by POSIX is CLOCK_REALTIME .
Table 5-3 lists several other useful clocks.
TABLE 5-3 Clocks Used with clock_gettime
ID Description Notes
CLOCK_REALTIME Tick frequency 
typically is the same as 
SC_CLK_TCK .
CLOCK_MONOTONIC Tick frequency typicallyis the same as
SC_CLK_TCK .
CLOCK_PROCESS_CPUTIME_ID This does not have therollover issues that theANSI 
clock function
has. Tick frequency from
clock_getres indicates
1 nanosecond, but Imeasured 1/100th of asecond on my 2.6.14kernel.
CLOCK_THREAD_CPUTIME_ID Tick frequency from
clock_getres indicates
1 nanosecond, butactual ticks are muchlarger. Tick frequencyfrom 
clock_getres
indicates 1 nanosecond,but I measured 1/100thof a second on my
2.6.14 kernel.Indicates CPU timeconsumed by thecurrent thread; same asabove except that inmultithreaded processes,the time is measuredonly for the currentlyrunning thread.Indicates CPU timeconsumed by a process.As with the ANSI
clock function, time
consumed includes userand system time. Formultithreaded processes,this time includes timeconsumed by threads.A simple clock thatrepresents an elapsedtime from an arbitrary(and undefined) time in the past.Required by POSIX;returns seconds inCoordinated UniversalTime (UTC) with ahigher tick frequencythan the ANSI 
time
function.252 Chapter 5 • What Every Developer Should Know about the Kernel
Portable code should use clock_getres to check the clock period as well as the
availability of a particular clock before using it. The clock_gettime function
does not take child processes into account, so the values you get are not affectedby 
wait calls.
The clock_getres(3) man page warns that the clocks using
CLOCK_PROCESS_CPUTIME_ID and CLOCK_THREAD_CPUTIME_ID typically are
implemented using hardware timers in the CPU. This means that the resolution canvary from system to system, which perhaps explains why 
clock_getres says the res-
olution is 1 nanosecond on my system (that is, the kernel doesn’t know the actualresolution). The 
man page goes on to warn that on Symmetric Multiprocessing
(SMP) systems, the hardware timers may not be in sync across CPUs. That meansthat if a process or thread is rescheduled on a different CPU, the values returned bythese timers may vary. This should give you pause (pun intended) before using eitherof these timers in your code. Using these timers in portable code is not advisable.
5.3.8.2 The Kernel Clock Tick
The standard unit of time in the kernel is called the jiffy.One jiffy represents a tick
of an internal clock, which is a hardware timer programmed to generate interruptsat a specific frequency. The frequency is determined when the kernel is built and doesnot change. Most distributions use the default value, which is stored in a macronamed 
HZ. Each architecture defines a unique default value for HZ. Until recently,
this value was not easily configurable in the kernel. Most people were satisfied withthe default value, which for IA32 was 100Hz. As if to keep things simple, this hap-pened to be the same frequency that the GNU standard library uses for 
clock_t .
It used to be that only people concerned with real-time and multimedia per-
formance would tweak the HZvalue; specifically, they would increase it to increase
the frequency. To understand why, consider that the tick interval is the maximumtime it can take to preempt a CPU-intensive process. When a process does not giveup the CPU voluntarily, it will not be preempted until the next clock tick. Eachtime the timer ticks, the scheduler gets an opportunity to run and preempt a run-ning process. At 100Hz, this means that a process can monopolize the CPU for upto 10 ms. This seems like nitpicking, but 10 ms can be an eternity in real-time andmultimedia applications. As you might expect, you cannot arbitrarily increase thefrequency as high as you like. At some point, handling the tick interrupts and con-text switches will consume as much time as executing processes does.
Table 5-4 illustrates some examples of real-world timing requirements compared
with the default 100Hz Linux clock.5.3 The Process Scheduler 253
Starting with the 2.6 kernel, the default tick frequency changed to 1,000Hz to
improve multimedia performance. Linus Torvalds admits that this value was chosenrather arbitrarily. It turns out that the change also has some undesirable side effects.One is that the increased interrupt frequency increases the CPU usage. This is nota problem on a dual Xeon machine plugged into a wall outlet, but it is a problemfor a laptop running on batteries. The increase in CPU usage drains the batteryfaster. SMP systems with many CPUs are also adversely affected by a high systemclock frequency. The overhead of delivering the interrupts to many CPUs at highfrequency can be significant. Finally, embedded systems with slower CPUs can beaffected by both the interrupt overhead and the extra power consumption.
For these reasons, the kernel team decided to make the system clock tick fre-
quency configurable on several of the most popular architectures. The kernel con-figuration tools now give the user three choices for the system clock tick. As of2.6.14, the default value for IA32 is 250Hz, but you can select 100, 250, or 1,000when you build the kernel. Choose a lower frequency for a slow processor or low-power system. Use the higher frequency if you have a desktop system or plan to usemany multimedia applications.
The timer frequency can be set only when the kernel is built. Figure 5-6 shows
what this looks like when you create the kernel using the 
menuconfig target.
Recall that the frequency of the clock used by functions that return type clock_t
is independent of the kernel tick. The macro USER_HZ determines this frequency
and is determined when the kernel is built. This value is the value that is returnedwhen you call 
sysconf(_SC_CLK_TCK) . No matter what you set the HZvalue to in
the kernel, this value will not change. Generally, it is safe to assume that the kerneltick frequency will be equal to or higher than the user tick frequency.254 Chapter 5 • What Every Developer Should Know about the Kernel
TABLE 5-4 Some Example Timing
Application Frequency (Hz) Interval (ms)
Default Linux clock 100 10One NTSC video frame 60 16.67One PAL video frame 50 20
CRT display refresh rate (typical) 80 12.5
5.3.8.3 Timing Your Application
The Bash shell provides built-in commands to monitor the performance of your
application, including time , which allows you to monitor the CPU usage of any
command or script without having to modify the code. The time printed is mea-sured from when the process starts to when it terminates.
Time measurements get tricky when an application forks or uses threads.
Depending on the application and the functions used for timing, the time can bemeasured differently. Specifically, if you time an application that forks a process andthen reaps it by calling any of the 
wait family of system calls, that process’s time
statistics will include the time consumed by its children. If the process neglects toreap any of its children, the time does not reflect their runtime, which could bemisleading.
When you’re timing your application from within, the 
getrusage function has
explicit flags to control what data you get. You tell getrusage which data you want5.3 The Process Scheduler 255
FIGURE 5-6 Changing the Timer Frequency in the Kernel Build

via the first argument, which can be RUSAGE_SELF or RUSAGE_CHILDREN . The
time returned when you use RUSAGE_CHILDREN , however, includes only those chil-
dren that the process has reaped. Until the parent process calls wait , the time
returned for the children will be zero. This is not true for processes that use threads.A thread is not a child process, so time consumed by threads is considered time con-sumed by the process. The timing output from 
getrusage increases as the threads
execute without any additional system calls required.
You can time your application externally from the shell with the time command.
This is implemented as a Bash built-in function and as a general-purpose commandin 
/usr/bin/time . Both accomplish the same thing except that the Bash version
focuses exclusively on timing, whereas the time command also gives you access to
the information from the getrusage system call. To use the built-in time com-
mand, just pass your command line as arguments to time as follows:
$ time sleep 1
real    0m1.007s
user    0m0.000ssys     0m0.004s
You can bypass the built-in command by escaping the command as follows:
$ \time sleep 1
0.00user 0.00system 0:01.00elapsed 0%CPU (0avgtext+0avgdata 0maxresident)k0inputs+0outputs (0major+199minor)pagefaults 0swaps
The output here includes much more information from the rusage structure,
including much that is not filled in by Linux.
Both time commands print out three values: real, user, and system time. I dis-
cussed system and user time earlier, and as you might guess, real time is the time
elapsed from the start of the process to the exit. This is time that you feelwhen you
run an application.
When the real time exceeds the sum of the system time and user time, it means
that the process is either blocking in system calls or not getting a chance to run.When the system is busy, a process will not get to run 100 percent of the time.Blocking can be caused by misuse of system calls or perhaps a slow device in the sys-tem. Running the 
time command is usually the first step in finding out.256 Chapter 5 • What Every Developer Should Know about the Kernel
5.4 Understanding Devices and Device Drivers
Every application communicates with devices at some point. Intentionally or not,
it’s hard not to come in contact with one or more devices on the system. You mightwrite a sophisticated modeling algorithm that runs entirely in memory, but if youwant to save the results, you’ll need to save your data to a file system. Even beforethen, your code might swap to disk due to system load. Any printout you want tosend to the console will likely require the pseudoterminal driver. So try as you mightto avoid them, device drivers will be called from your process.
The Linux device driver API dates back to the early days of UNIX and has been
largely unchanged since then. The POSIX standard formalizes this interface andserves as the basis for Linux.
Many devices are opened just like files on a disk. Communication with these
device drivers starts by opening one of these files. The application uses a file descrip-tor returned by the 
open system call and uses all the system calls that take a file
descriptor for an argument. It’s interesting to consider that this is an object-orientedmodel that uses functional programming, although this was created in the 1970s—long before object-oriented programming was in vogue. Typically, a device isaccessed via its file descriptor (much like an object), and there is a limited set of sys-tem calls that you can use to access the device (think methods).
A device driver may implement only the system calls that it needs, so a system
call that works on one device may not work on another.
10A driver could implement
only the open and close system calls, for example, although such a driver wouldn’t
be very useful. When an application tries to use a system call that is not imple-mented by the driver, the function typically returns an error indication, and 
errno
is set according to the particular system call.
5.4.1 Device Driver Types
Device drivers fall into a few basic categories. The most familiar devices are block
devices or character devices, which are accessible via special files on a disk. These files
are called device nodes, which distinguishes them from plain files and directories.
Other device types include file-system drivers and network drivers, which normally5.4 Understanding Devices and Device Drivers 257
10. You might call that polymorphism, but maybe that’s going too far.
are not accessed directly from an application but work closely with other drivers in
the system.
5.4.1.1 Character Devices
A typical example of a character device is the serial port on your PC or the termi-
nal device you use to type shell commands. Data is received and transmitted 1 byteat a time. A write to the device transmits bytes in the same order in which they werewritten. Imagine the confusion if the letters you typed in the terminal appeared inany arbitrary order. Likewise, a read from the device receives bytes in the same orderin which they were sent.
Not all character devices read and write characters this way, however. Character
devices cover a wide range of hardware and functions. Some character devices canallow random access to data, much like a storage device, so their drivers can sup-port additional system calls, such as 
mmap or lseek .11
The memdriver is a good example. This driver implements several devices that
perform simple, loosely related functions, which are listed in Table 5-5. Some ofthese devices, such as 
/dev/null , don’t involve any hardware at all.
5.4.1.2 Block Devices
A block device is a storage device with a fixed amount of space. As the name sug-
gests, the device manages the storage in fixed-size blocks. The main application forblock devices is to communicate with disk drives, although they are used with othertypes of storage media, such as flash drives. When a disk drive uses logical block
addressing (LBA), there is a one-to-one correlation between blocks on the device and
logical blocks on the disk. A unique feature of block devices is that they can hostfile systems, which requires them to interact closely with a file-system driver.
Block drivers also use system memory as cache to make the most effective use of
the device. Blocks are kept in memory as long as possible to maximize opportuni-ties for reuse. This minimizes the number of times the physical device is read orwritten, which improves performance. When using a file system, your code maynever interact with the underlying block device at all, instead operating entirely outof cache.258 Chapter 5 • What Every Developer Should Know about the Kernel
11. Some drivers don’t allow lseek but still implement the system call. Instead of returning -1, these drivers
typically return 0 to indicate that the position has not changed, so technically, the system call completedwithout error.
5.4.1.3 Network Devices
Unlike block and character devices, a network device does not use a device node.
Network devices are in a class by themselves. Applications rarely need to interactdirectly with network drivers; when they do, they use a specific name such as 
eth0
passed to the ioctl function using an anonymous socket. I will not look at net-
work devices in this book. If you are interested in learning more about networkdevices, the 
netdevice(7) man page is a good place to start.
5.4.1.4 File-System Drivers
Although technically not a device, a file system requires a driver. File-system drivers
require a separate block device. Although applications interact with files all the time,they rarely need to interact with the file-system driver directly. There are some rareexceptions for particular file systems. The XFS file system, for example, allows youto preallocate file extents to improve performance. Such a command is accomplishedvia the 
ioctl function, using the file descriptor of an open file in the file system.
5.4.2 A Word about Kernel Modules
The kernel module is a very popular way to deliver a device driver, but kernel mod-
uleand device driver are not synonymous. Modules may contain any kernel code,
not just device drivers, although that’s what they’re most often used for. Whatmakes modules attractive is that they can be compiled after the kernel is built andthen installed in a running kernel. This enables users to try new drivers withouthaving to rebuild the kernel or even take their system down.
This feature has matured nicely in Linux, and the 2.6 kernel makes building
modules almost child’s play. With a 2.6 kernel, you no longer need the full kernelsource installed—just a bunch of headers that are installed by default in most dis-tributions. A module build line looks like the following:
$ make -C /lib/modules/$(uname -r)/build M=$(pwd) modules
This command line builds against the currently running kernel, assuming that it
uses the standard location for the kernel headers. The resulting module includes asignature so that it cannot be installed on a different kernel. Linux 2.6 forces usersto build modules specifically for their target kernel, but in return, the kernel makesit as easy as possible to do so.5.4 Understanding Devices and Device Drivers 259
Kernel modules are denoted by the .koextension (for kernel object ) and can be
inserted into the kernel directly with the insmod command. Many (but not all)
modules can also be removed from the kernel with the rmmod command. If you
decide to keep a module, you can do a more permanent installation with the fol-lowing command line:
$ make -C /lib/modules/$(uname -r)/build M=$(pwd) modules_install
This places the module in an appropriate place under /lib/modules .
Depending on the module, you may need to run the depmod command to update
the module dependencies.
5.4.3 Device Nodes
Block devices and character devices are accessed as files on disk via device nodes. Anode contains an integer that indicates a major and minor number. T raditionally,the major number identifies the device driver in the kernel, whereas the minornumber is used by the driver to identify specific devices. In Linux 2.4 and earlier,the value used to store the major and minor number was a 16-bit value, with 8 bitsallocated for the major number and 8 bits for the minor number. Linux 2.6increased this value to 32 bits, allocating 12 bits for the major number and 20 bitsfor the minor number.
Nodes can be created on disk with the 
mknod command, which takes the device
type (character or block) as well as the major and minor number as arguments.Because devices nodes provide an interface to device drivers, a security risk isinvolved. After all, you don’t want just anyone to have access to the block devicethat contains your root file system. As a result, device nodes may be created only bythe superuser. The syntax to create 
/dev/mem , which uses major number 1 and
minor number 1, is:
$ mknod /dev/mem c 1 1
By convention, the /dev directory contains all the nodes in the system, but
device nodes can be created on any file system.12Most distributions provide a
comprehensive set of device nodes in /dev through one of a few techniques, so that260 Chapter 5 • What Every Developer Should Know about the Kernel
12. Device nodes on a file system can be rendered useless by mounting it with the nodev option; see mount(8) .
a typical user never has to use the mknod command. You can see a list of the cur-
rently installed devices and their major numbers in /proc/devices :
$ cat /proc/devices
Character devices:
1 mem4 /dev/vc/04 tty4 ttyS5 /dev/tty5 /dev/console5 /dev/ptmx
...Block devices:
1 ramdisk2 fd3 ide09 md
22 ide1
253 device-mapper254 mdp
When a process opens a device node, the kernel locates the appropriate driver
using the major number. The minor number is passed to the driver and is used dif-ferently by each driver. In general, the minor number is used to distinguish betweenfunctions, devices, or both. One straightforward example is the 
memdriver, which
implements several different functions based on the minor number (see Table 5-5).Each function is accessed via separate device nodes. In principle, because all thedevice nodes have a common major number, they all belong to the same driver.
The nodes are defined in 
/dev using the names listed in Table 5-5. When you
open /dev/mem , for example, the kernel calls the memdriver’s open function with a
minor number of 1. This tells the driver that you want to look at the physical mem-ory of the system.
In general, the driver itself does not enforce any access policy. Instead, it relies on
the 
open system call to verify the permissions of the device node against the current
user, the same way that it is done for every other file in the system. Looking at sys-tem memory, for example, is not something you want to let just any user do, becauseany user can use this device to snoop in memory for passwords or to vandalize thesystem. In this case, the convention is to allow only root to open 
/dev/mem and
/dev/kmem , whereas most of the other functions of the memdevice are open to every-
one. You can see this by looking at the file permissions of each device node.5.4 Understanding Devices and Device Drivers 261
It should be apparent that for security reasons, only root is allowed to create
device nodes and change the permissions of a device node. If this were not the case,any user could create a device node to point to 
/dev/mem or some other vulnerable
device and wreak havoc with the system. Likewise, root can prevent device nodesfrom being recognized on user-mountable file systems such as 
/mnt/floppy by
putting the nodev option to mount in /etc/fstab .262 Chapter 5 • What Every Developer Should Know about the Kernel
TABLE 5-5 Character Devices Implemented by the mem Driver
Device Minor
Node Name Number Function
mem 1 Allows access to physical memory.
kmem 2 Allows access to kernel virtual memory.
null 3 A data sink. All data written to this device is
discarded.
port 4 Allows access to I/O ports (found on some
architectures).
zero 5 Reads from this device are filled with zeros.
6 Obsolete /dev/core device replaced by
/proc/kcore .
full 7 A write to this device will always fail with ENOSPC .
random 8 Reads from this device are filled with random bytes;
returns only as many bytes as the driver considersrandom. See 
random(4) for more details.
urandom 9 Like random, except that this returns all the data
requested, regardless of whether it is high-qualityrandom data. See 
random(4) for more details.
10 Not provided by memdriver.
kmsg 11 Allows applications to write to the kernel message log
instead of using the syslog system call.
5.4.3.1 Device Minor Numbers
Minor numbers are used differently depending on the driver, and the convention is
not always straightforward. Block devices are particularly complicated because byconvention, the minor number uniquely identifies a specific drive and partition.
Consider the IDE driver (
/dev/hd ), for example. In Linux 2.6, the convention
is to use the least significant 6 bits to encode the partition and the most signifi-cant 14 bits to encode the drive. That means that the IDE device can map up to16,384 drives (2
14), and each drive can have up to 63 partitions (26-1); partition
zero is used to address the entire drive. The naming convention for disks’ devicenodes is to use a unique letter to identify each drive followed by a decimal num-ber to identify the partition. To address the entire device (partition 0), the parti-tion number is left off. You can see this for yourself with the 
lscommand as
follows:
$ ls -l /dev/hd[ab]*
brw-------  1 john disk 3,  0 Dec 21 10:00 /dev/hdabrw-rw----  1 root disk 3,  1 Dec 21 03:59 /dev/hda1brw-------  1 john disk 3, 64 Dec 21 10:00 /dev/hdbbrw-rw----  1 root disk 3, 65 Dec 21 03:59 /dev/hdb1brw-rw----  1 root disk 3, 66 Dec 21 03:59 /dev/hdb2
When the lscommand encounters a device node, it prints the major and minor
number where it normally would print the file size. Here, you can see that the majornumber for the IDE driver is 3, and the first disk is labeled 
hda. The entire disk is
accessed via a device node named /dev/hda with a major number of 3 and a minor
number of 0. Partition 1 of the first drive has a node with a minor number of 1 andis named 
/dev/hda1 . In this example, hdahas only 1 partition. The second drive’s
device node is named hdb, and its minor numbers start at 64. This is dissected fur-
ther in Table 5-6.
The SCSI driver follows the same convention as the IDE driver, except that the
SCSI driver uses only 4 bits of the minor number to encode the partition, allowingonly 15 partitions. This leaves 16 bits to encode the drive number, which allows theSCSI driver to support up to 65,536 (2
16) drives. For SCSI devices, the formula for
the minor number is 16 * drive + partition.5.4 Understanding Devices and Device Drivers 263
TABLE 5-6 Minor Numbers for IDE Devices Dissected (Linux 2.6 and Later)
Minor No. = 
Drive Partition 64 * Drive + Partition Node Name
0 0 0 /dev/hda
0 1 1 /dev/hda11 0 64 /dev/hdb1 1 65 /dev/hdb1
1 2 66 /dev/hdb2
5.4.3.2 Device Major Numbers
Normally, the major number along with the driver type (character or block) identi-
fies one—and only one—device driver. When a major number is assigned to a char-acter device driver, for example, no other character device driver can use that majornumber. The device driver assigned to a major number owns all the minor num-bers, whether it needs them or not. The 
memdriver, for example, provides several
pseudodevices with the same major number. If it weren’t for that, each device wouldhave to consume a unique major number.
This was a serious issue on kernels before Linux 2.6, which allowed only 256
major numbers to be used in the system at any time. Although no one really needsthat many device drivers in a single system, the problem is that many major num-bers are statically defined so that they don’t change from one system to the next.That limits the total number of possible devices.
Linux 2.6 addresses this in two ways. One, which I’ve already discussed, is the
increase of allowable major numbers to 1,024. Another improvement is that driv-ers can now register for only a range of minor numbers. Having many minor num-bers comes in handy for a disk driver, but many drivers don’t have the potential touse more than a few minor numbers. One example is the NVRAM driver, whichlooks at the battery-backed RAM in your PC. This driver sees only one NVRAM,so it should need only one minor number. This particular driver uses major num-ber 10, which is described as “Miscellaneous.” At this writing, 230 devices aredefined that use major number 10, including the NVRAM driver.264 Chapter 5 • What Every Developer Should Know about the Kernel
A list of permanently assigned major numbers is maintained at
www.lanana.org/docs/devicelist. A snapshot of this list is distributed with eachrelease of the kernel source in 
Documentation/devices.txt . The use of per-
manently assigned major numbers is a headache for custom driver writers. If youare writing a one-of-a kind driver or just experimenting, you don’t want to applyfor a major number just to print “hello world”. But, you cannot borrow a majornumber without the possibility of creating havoc in your system. Even if youdon’t have an IDE drive in your system, for example, you can’t borrow the IDEdriver’s major number (3) for your custom driver. The IDE driver is often com-piled into the kernel, so your driver will fail when it tries to register for majornumber 3.
The reason that drivers, such as the IDE driver, are given fixed major numbers is
consistency. Virtually every x86 motherboard chipset includes an IDE interface.Imagine if every chipset driver used an arbitrarily chosen major number. The valuesin 
/dev would have to be unique for every system configuration. Fortunately, that
is not the case, and distributions can create a default set of nodes in /dev that will
work in all configurations. The permanent assignment of major numbers probablywill be around forever.
One curiosity is an artifact of these permanent major number assignments com-
bined with the 16-bit value used for major/minor identification: The SCSI driverhas not 1 but 16 major numbers assigned to it. This is a workaround to allow theSCSI driver to address more drives in systems with large disk arrays. In a Linux 2.4system, the SCSI driver can address 256 disk drives by consuming 16 major num-bers. In a Linux 2.6 system, a single major number can address 65,536 drives.Because the major numbers are still assigned, the SCSI driver theoretically canaddress up to 1,048,576 drives (2
20).
For custom driver writers, Linux allows a driver to be assigned a major number
from a pool of permanently unused numbers. These are major numbers that arereserved and will never be assigned permanently to any device. Assignment is firstcome, first served, so there is no guarantee that a driver will get the same numberevery time. This creates a new problem: Because the major number is no longerfixed, you have to re-create the device node each time the driver is loaded to accom-modate the fact that the major number can change. This is a nuisance, but it ismanageable.5.4 Understanding Devices and Device Drivers 265
5.4.3.3 Where Device Nodes Come From
Many distributions based on Linux 2.4 and earlier include a single package that
contains hundreds or maybe thousands of device nodes to be extracted to /dev . If
you have a device that is not described by one of these nodes, you have to add thenode yourself. If you should ever look in this directory, you are likely to see hun-dreds of nodes that point to drivers you don’t have and probably never plan to. Thisis an inelegant, brute-force solution that rubbed many Linux users the wrong wayand inspired some alternative solutions. The first one, called 
devfs, was imple-
mented as a file-system driver to create a pseudo file system on /dev . It populates
the /dev directory dynamically with only the nodes that are actually present in the
system. So instead of thousands of nodes in your /dev directory, you see only the
ones that have drivers installed in your system. Nodes are created and deleted asdevices are added to and removed from the system.
devfs was abandoned because of lack of a maintainer as well as some serious
flaws in the design. One major drawback of devfs was that it hard-coded node
names in the kernel (and/or modules). This sort of policy enforcement in the ker-nel is one of the taboos of Linux kernel development. Linux kernel developersbelieve that the kernel has no business telling users what their device nodes shouldbe named or where they should reside.
Although it provided an alternative to the brute-force archiving of thousands of
device nodes, 
devfs was doomed due to philosophical problems. A more palatable
implementation was found in udev , which places the naming and location of device
nodes in user space using helper programs. udev is built on top of another feature
called hotplug , which arose at the same time.
udev and hotplug
The hotplug feature of the kernel is primarily responsible for locating and loading
driver modules for hardware as they come online and go offline. The hotplug
implementation relies on minimal intervention from the kernel; the bulk of thework is done in user space. All the kernel does is recognize when a piece of hard-ware becomes available or unavailable and, in response, spawn a user-space processto handle the event. The user-space process handles the job of recognizing thedevice, finding a driver module for it, and loading the module.266 Chapter 5 • What Every Developer Should Know about the Kernel
By default, the kernel looks for /sbin/hotplug13to execute when a hotplug event
is handled, but it can be replaced by any script or program as required. The programname for the 
hotplug handler process is stored in /proc/sys/kernel/hotplug
and can be overridden by writing a new filename to it. udev , for example, replaces
the default hotplug handler with /sbin/udevsend .
The primary function of udev is to populate the /dev directory with device
nodes that accurately reflect the devices currently available in the system. It was nat-ural to implement this as an extension of the 
hotplug feature.14
To see how this works, look at a module that usually isn’t loaded by default. The
nvram module is used to access the nonvolatile memory in your computer and typ-
ically uses the node /dev/nvram . This module is categorized by www.lanana.org as
a “miscellaneous” device, which means that it uses major number 10. It is assignedto minor number 144. After loading the module on a system with 
udev , the device
node appears immediately after the module is loaded. For example:
$ ls -l /dev/nvram
ls: /dev/nvram: No such file or directory$ modprobe nvram$ ls -l /dev/nvramcrw-rw----  1 root root 10, 144 Aug 13 16:14 /dev/nvram
Similarly, when you remove the module with rmmod , the device node is removed.
How does this happen? The rules for udev are contained in /etc/udev/rules.d .
There, you will find a file named 50-udev.rules ,15which contains default rules
provided by the udev package. In the case of the nvram module, the rule looks
like this:
KERNEL=="nvram", MODE="0660"5.4 Understanding Devices and Device Drivers 267
13. Part of the hotplug package at http://linux-hotplug.sourceforge.net.
14. www.kernel.org/pub/linux/utils/kernel/hotplug/udev.html15. The 
50indicates priority. (Low numbers have higher priority.) Multiple files may reside here with differ-
ent numbers, and rules can be defined more than once. The highest-priority rule is the one that applies.
This KERNEL field tells the udev daemon to match the kernel module name
(nvram ) to apply this rule. The MODE field tells it what permissions to apply to the
device node. The name of the device node defaults to the same name that is used inthe kernel. To illustrate further, change the name of the device node from 
nvram to
cmos . All you need to do is create a file in /etc/udev/rules.d that contains the
new rule.
$ cat <<EOF >/etc/udev/rules.d/25-cmos.rules
> KERNEL="nvram", MODE="0660", NAME="cmos"> EOF
To override the default rule, you need to give it higher priority, so name it 
25-cmos.rules . The leading number and the .rules extension are required, but
everything in between is arbitrary. All you do is copy the existing rule and add a
NAME parameter with the name cmos :
$ modprobe nvram
$ ls -l /dev/cmos /dev/nvramls: /dev/nvram: No such file or directorycrw-rw----  1 root root 10, 144 Aug 13 16:37 /dev/cmos
A complete explanation of the udev rules is contained in the udev(8)man page.
sysfs
sysfs is a new feature key to the hotplug implementation and is worth discussing
a little at this point because it will come up again later. sysfs is a memory-based
file system like procfs that contains text files with system information. It is based
on kernel objects (kobjects), which is new to the 2.6 kernel, so sysfs is not avail-
able on 2.4 or earlier kernels.
By convention, sysfs is mounted on a directory named /sys so that user-
space applications can find it easily. This mount point is a rigid convention thatmany tools depend on, much the way 
/proc is used with procfs . In many ways,
sysfs overlaps procfs , although sysfs has a much more intuitive format and
does not add much (if any) additional code to modules and drivers to support it.
procfs , for example, requires device-driver writers to add callbacks to support
procfs entries, and driver writers must provide the information from scratch.
There are few conventions in procfs as to where files and directories can be268 Chapter 5 • What Every Developer Should Know about the Kernel
located or what the contents of the files should be. Not every driver provides
information in /proc , and when one does, the format is often whatever the
author dreamed up. sysfs makes it very easy for device-driver writers to add
entries into /sys with a trivial amount of code. Many driver entries show up with
no additional code in the driver. The /sys/bus/scsi directory, for example,
describes the SCSI buses in the system based on information already in the ker-nel from kobjects.
Whereas 
procfs typically contains flat files with a great deal of information,
sysfs contains a hierarchy of small files, each containing a minimal amount of
information. In many cases, the directory structure itself conveys information aboutthe system. For example:
$ ls /sys/bus/pci*/devices
/sys/bus/pci/devices:0000:00:00.0  0000:00:07.0  0000:00:07.2  0000:00:0f.0  0000:00:11.00000:00:01.0  0000:00:07.1  0000:00:07.3  0000:00:10.0  0000:00:12.0
/sys/bus/pci_express/devices:
Here, you can see that my system has ten PCI devices and no PCI-Express
devices. Each one of the names listed is actually a directory. The names themselvescontain useful information if you are a device-driver writer.
sysfs tries to create a directory hierarchy that closely mimics the system hard-
ware. Through symbolic links, it is often possible to get the same information fromseveral points of view. Suppose that you want to look at SCSI devices by bus. In thiscase, you will find what you want to know in 
/sys/bus/scsi/devices . Perhaps
you want to know what SCSI device is mapped to block device sda. In that case,
you can look at /sys/block/sda/device . Both of these are links to the same
directory, which contains various information about the device.
The SCSI bus is a good example of how procfs and sysfs differ. Using
procfs , you will find a directory named /proc/scsi that contains a directory for
each host adapter, which usually contains a file for each SCSI bus (named 0, 1, 2,
and so on). Inside this file is whatever the driver writer thought would be useful.Unfortunately, the people who wrote the Adaptec driver didn’t talk to the peoplewho wrote the LSI driver, who never spoke to the people who wrote the BusLogicdriver. As a result, each driver produces similar information in a completely5.4 Understanding Devices and Device Drivers 269
different format. Here’s a small example from the aic79xx SCSI module, which
shows a system attached to an Ultra 320 disk array:
$ cat /proc/scsi/aic79xx/0
Adaptec AIC79xx driver version: 1.3.11Adaptec AIC7902 Ultra320 SCSI adapteraic7902: Ultra320 Wide Channel A, SCSI Id=7, PCI-X 67-100Mhz, 512 SCBsAllocated SCBs: 36, SG List Length: 128
Serial EEPROM:
0x17c8 0x17c8 0x17c8 0x17c8 0x17c8 0x17c8 0x17c8 0x17c80x17c8 0x17c8 0x17c8 0x17c8 0x17c8 0x17c8 0x17c8 0x17c80x09f4 0x0146 0x2807 0x0010 0xffff 0xffff 0xffff 0xffff0xffff 0xffff 0xffff 0xffff 0xffff 0xffff 0x0430 0xb3f7
Target 0 Negotiation Settings
User: 320.000MB/s transfers (160.000MHz DT|IU|QAS, 16bit)
Target 1 Negotiation Settings
User: 320.000MB/s transfers (160.000MHz DT|IU|QAS, 16bit)Goal: 320.000MB/s transfers (160.000MHz DT|IU|QAS, 16bit)Curr: 320.000MB/s transfers (160.000MHz DT|IU|QAS, 16bit)Transmission Errors 0Channel A Target 1 Lun 0 Settings
Commands Queued 1333Commands Active 0Command Openings 32Max Tagged Openings 32Device Queue Frozen Count 0
...
There’s a lot of information here. Other drivers have similar files in a similar
location but formatted completely differently. The equivalent file connected to aBusLogic adapter might show up under 
/proc/scsi/BusLogic/0 but would
look completely different. The only thing these procfs files have in common is
that each one tells you information about the devices on the bus, but each one pro-vides a different amount of detail with a unique format. There’s no guarantee thatthe driver will tell you anything in particular. An important tuning parameter forSCSI drives is the command queue depth (listed by the 
aic79xx driver as
“Command Openings”). This is the length of the queue used for SCSI commands,270 Chapter 5 • What Every Developer Should Know about the Kernel
which is the number of commands that can be active simultaneously. It’s a very
useful tuning parameter, but there’s no guarantee that a different driver will presentthis information, and if it does, you can rest assured that it will be in a differentformat.
The 
sysfs approach is a bit more intuitive and manageable. Under /sys , you
will find /sys/bus/scsi , which lists devices by host adapter number, channel
number, device number, and logical unit number. All that information is encodedin the directory name. Inside each directory, you will find various SCSI parametersin the form of unique files. To get the queue depth for a drive, you can look at thefile named 
queue_depth . For example:
$ ls /sys/bus/scsi/devices/0:0:1:0
block        device_blocked model queue_depth scsi_level timeout  vendordetach_state generic        power rev         state      type
$ cat /sys/bus/scsi/devices/0:0:1:0/queue_depth
32
T ranslating this directory name ( 0:0:1:0 ) into SCSI jargon, you are looking at
host 0, channel 0, device 1, and logical unit 0. The queue depth is 32, which in this
case is in decimal. This structure and format are the same for all drives, regardlessof the driver. This is an improvement over 
procfs , but what if you don’t know the
SCSI device ID of a particular drive? Suppose that you want to verify the queuedepth of the SCSI drive mapped to block device 
/dev/sda . In this case, you don’t
need to know anything about the SCSI device information. All you need to knowis the block device name. Use the following command:
$ cat /sys/block/sda/device/queue_depth
32
The directory /sys/bus/scsi/devices/0:0:1:0 and the directory
/sys/block/sda/device are both symbolic links to a common directory. This
technique is used in many places in the sysfs file system. It allows you to look at
the system from many points of view.
All this is available whether you have an Adaptec SCSI controller, LSI, or what-
ever. The data will be in the same place and in the same format all the time.5.4 Understanding Devices and Device Drivers 271
What Makes sysfs Unique?
Consider at a small example of just how easy it is to use sysfs . For this book, I used
this trivial module to keep track of the internal clocks as I fiddled with various ker-nels, because there is no system call to get this information.
hz.c
#include "linux/module.h"// Store the USER_HZ macro in a variableint user_hz=USER_HZ;
// Store the HZ macro in a variable
int hz=HZ;
// This is all it takes to make it visible in /sys!!!
// I specify the name, the type and file permissions.module_param(user_hz,int,0444);module_param(hz,int,0444);
The Makefile for this module is equally trivial, thanks to the 2.6 build system.
Makefile:
all::
make -C /lib/modules/`uname -r`/build M=`pwd`  modules
obj-m+=hz.o
To build and install this module, I type the following command:
$ make
$ insmod ./hz.ko
Now comes the interesting part. I can look at these variables from user space with
a simple catcommand:
$ cat /sys/module/hz/parameters/hz
1000$ cat /sys/module/hz/parameters/user_hz100
So I have a useful module in about four lines of code. Not bad.
5.4.4 Devices and I/O
Normally, before any data can touch your application buffers, it must pass through
kernel space. A typical read from a device, for example, results in the data being272 Chapter 5 • What Every Developer Should Know about the Kernel
copied at least twice—once to a kernel buffer and then once again to your user
buffer. This is the price we pay for reliability and security. To prevent one processfrom crashing the kernel or crashing other processes, all input and output must behandled by the kernel, which acts as the security checkpoint.
This might come as a surprise to some of you, considering that UNIX and Linux
are viewed as being high-performance operating systems. In fact, as you shall see,the extra copying is to your advantage. Understanding the rules and reasons for thiscan help you use I/O most efficiently.
5.4.4.1 I/O and Character Devices
For a slow serial port, the extra time required for copying is insignificant. For high-
speed devices, these extra copies can be a serious performance issue.
For custom hardware, the character device is often the driver of choice because it
is the most straightforward. Reads and writes are synchronous, which means thattypically when the process calls 
read or write , it blocks and does not return from
the system call until the operation is complete.16When you write to a character
device, the driver may copy the data directly from your user-space buffer to thedevice. This means that the driver cannot allow you to continue until it is finishedwith that memory. Your process blocks, waiting for the write to complete. Timewaiting for the device is time you could spend executing code, so this usually isundesirable. I can illustrate this with the 
/dev/tty device, which is a character
device representing the current terminal, as follows:
$ time dd if=/dev/zero of=/dev/tty count=1000
1000+0 records in1000+0 records out
real    0m0.442s
user    0m0.000ssys     0m0.020s
I just wrote a bunch of NULs to the terminal, which does nothing to the terminal
except consume time. Here, you can see that the command took 442 ms to executebut spent only 20 ms of CPU time. Where did all the time go? It was spent wait-ing for the driver (blocking). What little CPU time the process consumed was usedby the driver (listed here as 
sys).5.4 Understanding Devices and Device Drivers 273
16. It’s a little more subtle than that, but this is the default behavior of most character devices.
The terminal, like the serial port, is a streaming device. Random access on a
streaming device is not possible. So the read and write system calls are the only onesyou can use to interact with this device.
Devices that allow random access often support the 
mmap system call. This allows
an application to see all the data the device has to offer as one big region of mem-ory. A character device driver can support 
mmap exclusively and not allow read and
write calls. When a driver does not support mmap , the system call returns with a
value of MAP_FAILED , and errno is set to ENODEV .
With mmap , reading and writing from the device is almost as simple as allocating
a large block of memory. With a character device, using mmap allows you to access
the data with fewer system calls, because you don’t need to call read or write to
manipulate the data. This is especially important when you’re working with largeamounts of data (see the sidebar “A Simple mmap Example”).
5.4.4.2 Block Devices, File Systems, and I/O
Block devices are the basis for disks and other storage devices that can use a file sys-
tem. For this reason, you can access a block device in two ways: directly or througha file system. Often, the only time you use a block device directly is when you par-tition it or create a file system.
A file system can be created on any block device or partition of a block device.
The floppy driver is unique in that it does not allow partitioning, although thefloppy media can support partitions. Instead of partitions, the minor numbers usedby the floppy driver enumerate the many flavors of floppy drives that have comeand gone over the life of the PC. Most of them don’t exist anymore, but the sup-port is there if you need it.
17
To create a file system, you can use the mkfs command and specify the file-
system type with the -toption. To format a floppy disk that you can use with
Windows, for example, you can type
$ mkfs -t vfat /dev/fd0
mkfs is a wrapper that calls a file system–specific helper program. In this case, it
calls mkfs.vfat , which you can call directly if you want. Most file systems support274 Chapter 5 • What Every Developer Should Know about the Kernel
17. Refer to the fd(4) manpage.
5.4 Understanding Devices and Device Drivers 275
A Simple mmap Example
The following code snippet shows the basic usage of mmap . It helps to think of it as a
memory allocation like malloc , which is how the GNU standard C library imple-
ments many malloc calls.
#include <stdio.h>
#include <stdlib.h>#include <string.h>#include <unistd.h>#include <sys/file.h>#include <sys/mman.h>
#define ERROR(x) do { perror(x);\
exit(EXIT_FAILURE); } while(0)
int main(int argc, char *argv[])
{
const int nbytes = 4096;void *ptr;
int fd = open("/dev/zero", O_RDWR);
if (fd == -1) ERROR("open");
/* /dev/zero allocates memory on our behalf. */
ptr = mmap(0, nbytes, PROT_READ | PROT_WRITE,
MAP_PRIVATE, fd, 0);
if (ptr == MAP_FAILED) ERROR("mmap");
/* We are free to use it just like a malloc call. */
memset(ptr, 1, nbytes);
/* Equivalent of free() */
munmap(ptr,nbytes);
return 0;
}
In this example, I use /dev/zero to do the mmap . You may recall that
/dev/zero is a character device that returns buffers of zeros, but another feature
is that mmap calls to /dev/zero will allocate memory for you. You could get the
same thing by using the MAP_ANONYMOUS flag, which does not use the file descrip-
tor at all.
To support the mmap system call, a device must be able to allow random access.
This rules out streaming character devices. All block devices can support mmap .
multiple options when they are created, and the most common file-system options
are documented in the mkfs(8)man page. More up-to-date details for a specific file
system, such as mkfs.vfat(8) , are available in the helper program’s manpage.
When the block device has a file system, it can be mounted on a directory with
the mount command:
$ mount -t vfat /dev/fd0 /mnt/floppy
Usually, the mount command can figure out what kind of file system is on the
device, so the -toption is optional. After the block device is mounted, you can look
at it in two ways: through the device node (for example, /dev/fd0 ) or through its
mount point (for example, /mnt/floppy ). Reading from the device node will give
you raw data that includes everything in the file system and then some. This mayseem useless, but you can do useful things with it. One idea is to use it for archiv-ing. It’s usually a very inefficient method for archiving a file system, but dumpingthe raw device saves data that an archive utility like 
tarcannot. Although tarcan
create an archive of every file and directory in the file system with all the metadatapreserved, it cannot save the boot block, which is not part of the file system. To getan exact copy of every byte on a floppy, including the boot block, you need to copythe data from the device node. For example:
$ cp /dev/fd0 floppy.img
Note that what gets copied is the data in the device, not the device node. This is
a unique property of device nodes. This technique is used to copy bootable floppyimages that are used more often these days to create bootable CDs than they are forfloppies. If this were a hard disk, copying the entire block device would preserve thepartition tables and master boot record (if any).
As mentioned earlier, this is not an efficient way to archive data. The block device
has no idea how much data is valid and how much is empty space. Only the file sys-tem knows that. As a result, every floppy image will be 1,440K, regardless of howmany files are on the disk. An archive, on the other hand, will contain only the filesin use, so potentially, it can be much smaller.
5.4.4.3 The Role of the Buffer Cache and File-System Cache
One way block devices differ from character devices is that they use system mem-
ory as cache. Linux supports many generic caches through data structures in276 Chapter 5 • What Every Developer Should Know about the Kernel
memory, but the most interesting to application programmers are the buffer cache
and the file-system cache.18
The buffer cache is the storage used for blocks written and read from block devices.
When a process writes to a block device, the data is first copied to a block in thebuffer cache. The block driver is not actually called until the kernel determines thatit is time to write the block to the device, which may be some time later. The kernelsaves a copy of each block read and written in the buffer cache for as long as possi-ble. For physical devices, such as disks, it means that the data may not make it to thedisk for some time after the write occurs. The advantage of this is that the data isavailable for any process that wants to read from that section of the disk later. Thedisadvantage is that if the system crashes or loses power before the data is written tothe device, the data is lost.
Caching improves performance in several ways. One way is that by keeping data
in memory, the system avoids rereading from devices such as disk drives, which aremany orders of magnitude slower than the memory. It also allows the kernel to coa-lesce adjacent blocks of data written to cache into a single large disk write insteadof several small ones, which usually makes more efficient use of the disk. The cachealso cuts down on redundant writes to disk, because if a block is updated before itis written to disk, the kernel needs to perform only one write to disk instead of two.All this comes at the cost of extra copies, which in many applications is insignifi-cant compared with the time that could be lost due to inefficient use of the disk.
The file-system cache works exactly the same way as the buffer cache except that
the data is managed by the file-system driver. Data written to the disk is copied tothe file-system cache before it is written to the disk. Likewise, the kernel will tryto read data from the file-system cache before it reads from disk, and every readfrom disk is copied to the file-system cache.
The beauty of this is that it all takes place without any intervention from the
application programmer or the device-driver writer. Linux uses the same mecha-nisms for all block drivers and file systems, and consumes any unused memory foruse as cache. So if you have gigabytes of memory in your system but few processes,you usually can rest assured that the extra memory is being put to good use.5.4 Understanding Devices and Device Drivers 277
18. Most disk drives include a hardware cache, but this is effectively invisible to the Linux kernel.
You can see the cache in action with the vmstat command:
$ vmstat
procs -----------memory---------- ...
r  b   swpd   free   buffcache
1  0      0  93412   573638096
Write 4MB to the ramdisk.
$ dd if=/dev/zero of=/dev/ram0 bs=1k count=4096
4096+0 records in4096+0 records out
$ vmstat
procs -----------memory---------- ...
r  b   swpd   free   buffcache
0  0      0  89272   983238096 
4MB is added to the buffers.
The vmstat command provides more information, but for now, I’ll focus on the
memory information.19Here, I copied 4MB from /dev/zero (a character device)
into the ramdisk device /dev/ram0 (a block device). Because I am interacting
directly with the block device, it must allocate storage from the buffer cache toaccommodate the writes. You can see that in the 
vmstat output, the size of the
buffer cache went from 5,736K to 9,832K—an increase of exactly 4,096K. Onefeature of the 
ramdisk device is that once it allocates memory, that memory is
never freed, which is why the buffers continue to show up in the buffer cache. Notall block devices do this.
In the following example, you see what happens when you put a file system on
this device and mount it:
$ mkfs -t ext2 /dev/ram0 Create the file system.
mke2fs 1.37 (21-Mar-2005)
Filesystem label=OS type: LinuxBlock size=1024 (log=0)Fragment size=1024 (log=0)4096 inodes, 16384 blocks...278 Chapter 5 • What Every Developer Should Know about the Kernel
19. See also free(1) , which is part of the same procps package. Also see /proc/meminfo .
$ vmstat Get a baseline for cache usage.
procs -----------memory----------
r  b   swpd   free   buff  cache
1  0      0  88792  10164  38272
$ mount /dev/ram0 /mnt/tmp Mount the file system.
$ vmstat There is no significant increase in cache usage.
procs -----------memory----------
r  b   swpd   free   buff  cache
0  0      0  88792  10168  38272
Create a 2MB file in the file system.
$ dd if=/dev/zero of=/mnt/tmp/zero.dat bs=1k count=2048
2048+0 records in2048+0 records out
$ vmstat
File-system cache usage goes up by roughly that amount.
procs -----------memory----------
r  b   swpd   free   buff  cache
1  0      0  86572  10200  40324
Notice that when you create the 2MB file in the file system, the file-system cache
size (listed as cache ) increases by 2,052K, which is just slightly more than the
2,048K you created. The “free” memory decreases by about this amount as well.Notice also that the buffer cache is virtually unchanged—an increase of only 32K.Keep in mind that cache numbers are not static, so the results are not always exact.Another factor is that the file system requires additional space to store file-systeminformation on the disk, which increases the numbers slightly.
The file that you created will sit in the cache until one of the following things
happens:
• It is kicked out by newer data in the cache.
• The file is deleted.• The file system is unmounted.• The kernel flushes it to free memory for processes.• An application explicitly flushes the data with 
sync or fdatasync .
Until one of these events occurs, the data is not written to disk.5.4 Understanding Devices and Device Drivers 279
One thing I haven’t emphasized so far is that the caches contend with processes
for system memory. From the point of view of a process, the buffer cache and file-system cache are free memory, because they can be flushed to make room for moreprocess memory. A system that is doing intensive I/O operations can consume mostof system memory as buffer cache or file-system cache. If processes request morememory than is currently free, the system must free up space somehow. To free upmemory, the kernel can reclaim cache by flushing blocks.
Under normal circumstances, the kernel will take space from the cache by flush-
ing the oldest blocks to disk. Blocks that belong to a file on disk can be written todisk and the memory reclaimed by the kernel. So-called clean blocks can bereclaimed immediately without any disk I/O. A clean block is one that has been read
from disk and not modified, or one that has been written to disk but not reclaimed.Likewise, a dirty block is one that has been modified (or created), and the changes
have not yet been written to disk.
ramdisk versus tmpfs
One feature of the ramdisk device is that it does not allow its memory to be
reclaimed, so ramdisk blocks will always consume free memory until the system is
rebooted. Because the blocks it consumes from the buffer cache are never returnedto the system, a 
ramdisk device cannot be resized or removed. This allows you to
unmount and remount the ramdisk without losing any data.
A disadvantage of this is that when you create a file system on a ramdisk device,
it consumes both buffer cache and file-system cache, so in theory, it can consumetwice as much RAM as a disk file system. Instead of the 
ramdisk device, most appli-
cations that require temporary storage in RAM use the tmpfs file system.
The tmpfs file system is unique in that it does not require a block device for stor-
age. The data in a tmpfs file system exists entirely in the file-system cache. This
memory is also allowed to swap, so you can get the benefits of a high-speed RAM diskand the flexibility of virtual memory at the same time.
tmpfs is the default file system for the shared memory device ( /dev/shm ) in vir-
tually all distributions.
5.4.4.4 How the Kernel Manages the File-System Cache
Normally, the kernel relies on user processes to execute much of the code required
for system maintenance. This is unreliable for things that must occur on a periodic280 Chapter 5 • What Every Developer Should Know about the Kernel
basis, which is why most systems have daemon processes that run in the background
for critical functions. One such daemon is pdflush , which is responsible for mak-
ing sure that data does not sit in the cache too long without being written to disk.
Suppose that you have an idle system, and you write 1MB of data to a file. This
results in 1MB of dirty cache blocks in memory. If the system remains idle, this datacould sit in memory indefinitely. This situation is undesirable, because a power fail-ure or system crash could result in lost data or data corruption of your physicalmedia. To prevent this from happening, 
pdflush executes periodically and writes
all the dirty cache blocks to the block device within a certain amount of time. Thisinterval defaults to 30 seconds in most Linux distributions.
Kernel Threads
One unique aspect of the pdflush daemon is that technically, it is not a process but
a kernel thread. Recall that a process lives in two worlds, so to speak: user space andkernel space. A 
kernel thread is a process that has no user space and runs entirely in
kernel space. Because it has no user space, the code for a kernel thread must resideentirely in the kernel. Unlike a user-space daemon, which typically is executed by the
init process from an executable file on disk, a kernel thread is started directly by the
kernel via functions defined in the kernel and has no executable file.
Kernel threads look and behave like ordinary processes, but their lack of user
space gives them away. One simple way to detect a kernel thread is to look at
/proc/PID/maps , which in a normal process shows its virtual memory map.
Because it has no user space, a kernel thread’s maps file will always be empty.
Blocks may be written to the device earlier by the kernel when it needs to free up
memory. In this case, buffers are reclaimed oldest first—or, more specifically, “leastrecently used.” Because dirty blocks are more likely to be recently used, it’s unlikelythat the kernel will flush these blocks, but in systems that are doing lots of I/O, it ispossible. In this case, 
pdflush may never need to do anything. When this happens,
the job of writing the dirty buffers to disk is done by the currently running process.
Applications can also force the blocks of a particular file to be written early via
the fsync , fdatasync , and sync system calls. The sync command allows users to
call the sync system call from the shell. These system calls allow applications (or
users) to exert more control over the file-system cache by forcing disk I/O to occurat a particular time rather than wait until the 
pdflush daemon runs.5.4 Understanding Devices and Device Drivers 281
5.5 The I/O Scheduler
An important final piece of I/O management is the I/O scheduler. When blocks are
written to or read from a device, the requests are placed in a queue to be completedlater. Each block device has its own queue. The I/O scheduler is responsible forkeeping these queues sorted to make the most efficient use of the media. On a diskdrive, this is very important, because it can cut down on excessive head movement,which is one of the most time-consuming operations in any system. Even on othermedia, such as flash drive, ordering the I/O operations makes the most efficient useof the device.
The default I/O scheduling algorithm in Linux is called an elevator algorithm
because the problem of scheduling reads and writes to a disk is very similar to sched-uling stops on an elevator. Disk drive heads move back and forth across tracks inmuch the same way that an elevator moves up and down in a building. Just as anelevator stops on various floors to pick up or drop off passengers, the drive headstops on cylinders to read or write data. The scheduling problem is the same forboth. If requests are handled simply in the order in which they come in, the resultwill be very inefficient use of the hardware.
Figure 5-7 shows a hypothetical example of a head moving across a disk. In this
example the head starts at track 8, then moves to track 6 and then moves back to track8 again. The total amount of head movement is shown as a dashed line. Figure 5-8shows the same I/O operations after sorting with an elevator algorithm. The dashedline illustrates how this cuts down the amount of head travel dramatically.
For this to work, the kernel must force some I/O requests to wait so as to make
most effective use of the disk. The kernel must decide how many requests it willqueue up before it starts to execute them. There is no ideal solution to this prob-lem, which is why Linux offers several different algorithms for queuing I/O.
5.5.1 The Linus Elevator (aka noop)
Before 2.4, there was only one I/O scheduler, sometimes called the Linus Elevator.This scheduler sorts I/O requests like an elevator, and as new requests come in, itmerges contiguous requests so that it can keep requests from the same part of themedia together. Requests that can’t be merged with one of the existing I/O requestsare placed in the back of the queue.282 Chapter 5 • What Every Developer Should Know about the Kernel
5.5 The I/O Scheduler 283
FIGURE 5-7 Unsorted I/O Operations Cause Extra Head Movement
FIGURE 5-8 Sorting I/O Operations (Elevator Algorithm) Reduces Head Travel DrasticallyDisk He ad Travel with U nsorted  I/O
68
46
36
012345678910CylindersI/O Locations
Total Head Travel
Disk He ad Travel with Sorted I/O
I/O Locations
Total Head Travel3 3567888910
012345678910Cylinders
This is a fairly straightforward algorithm, but it has a problem: Merging a request
with an existing request has the effect of moving it toward the front of the queue.So if new requests continuously come in that happen to be merged with a requestat the front of the queue, a request at the back of the queue can be held off indefi-nitely. When this happens, we say that the request is starved.
The Linus Elevator tends to starve reads in favor of writes, because write requests
stream more easily than read requests do. Thanks to the file-system cache, a processdoes not have to wait for a write to complete before performing the next write. The
write call copies the data to the cache, and the write is scheduled for completion.
Because each request can be merged with the one before it, write requests can pileup quickly in the I/O queue. By contrast, a process that reads from a file has to waituntil each read is complete before it can do another read. There could be severalmilliseconds between read requests, during which time many new write requests cancome in and starve the next read request. In this way, a process that writes largeblocks of data to disk (not an unusual occurrence) can monopolize the device. Largewrites will be merged, which can effectively push any pending read requests to theback of the queue.
It may seem odd, but a process’s priority has no impact on where its requests go in
the I/O queue. A high-priority process gets no preferential treatment from the I/Oscheduler. The reason is that because the data resides in the file-system cache, theprocess that completes the I/O may not be the same process that wrote the data tocache. So the I/O scheduler cannot infer the priority from the currently running task.
Linux 2.6 included a rewrite of the block I/O layer to address these problems.
The Linus Elevator is still available as the 
noop scheduler, but now you can choose
among three other I/O schedulers to find the best fit for your application.
5.5.2 Deadline I/O Scheduler
This sorts and merges requests like the noop scheduler except that requests are also
sorted by age as well as the area of the disk. This scheduler guarantees to servicerequests within a fixed amount of time (a deadline). The deadlines are tunable, andby default, the read deadlines are shorter than write deadlines. This prevents writesfrom starving reads, as the 
noop scheduler tends to do.
5.5.3 Anticipatory I/O Scheduler
This is the new default scheduler, which essentially is the same as the deadline
scheduler except that it waits 6 ms after the last read before continuing with other284 Chapter 5 • What Every Developer Should Know about the Kernel
I/O requests. In this way, it anticipates a new read request coming from the appli-
cation. This improves read performance at the expense of some write performance.
5.5.4 Complete Fair Queuing I/O Scheduler
This is the newest I/O scheduler to be added to the kernel. It gives I/O requests apriority, much the same way that the processes have. The I/O priority of the requestis independent of the process priority, so reads and writes from a high priorityprocess do not automatically inherit high I/O priority.
5.5.5 Selecting an I/O Scheduler
In Linux 2.4 and early versions of 2.6, you could choose only one scheduler for allI/O queues, and this scheduler had to be selected at boot time via the 
elevator
boot parameter. This is passed as a boot parameter to the kernel (typically in
lilo.conf or grub.conf ). Valid values at this writing are listed in Table 5-7 and
can be found in Documentation/kernel-parameters.txt .
In later versions of 2.6, it is no longer necessary to use the elevator option 
at boot time. Now you can choose the scheduler for each block device and changeit on the fly. The current scheduler in use for each block device is listed in
/sys/block/{device}/queue/scheduler . For example:
$ cat /sys/block/hdb/queue/scheduler
noop [anticipatory] deadline cfq5.5 The I/O Scheduler 285
TABLE 5-7 I/O Schedulers Available in Linux 2.6
Parameter Description
noop The Linus Elevator from Linux 2.4 and earlier. Requests are
sorted and new requests are merged to minimize disk seeks.
deadline Similar to noop except that it enforces a deadline for I/O to
complete.
as Anticipatory I/O scheduler; same as deadline except that reads
are followed by a 6 ms pause.
cfq Complete Fair Queuing scheduler; same as deadline except that
I/O requests have a priority, much like a process.
In this case, device hdbis using the anticipatory I/O scheduler. You can change
the scheduler for that device by writing a different value to the file. To change thescheduler to the 
cfqscheduler, for example, you would use the following command:
$ echo cfq > /sys/block/hdb/queue/scheduler
The choice of scheduler is based on the application. Real-time applications work-
ing with disks will want the deadline or cfq scheduler. An embedded system
working with RAM and flash devices, however, might do just fine with the noop
scheduler.
5.6 Memory Management in User Space
One of the nice things about a protected memory operating system like Linux is thefact that programmers don’t need to be concerned about things like where theircode is located in memory or, for that matter where the memory comes from.Everything falls into place with no intervention from the programmer. Most pro-grammers don’t appreciate just how much goes on behind the scenes in the kernel,in the libraries, and in the startup code.
This section focuses on 32-bit processors, which present unique challenges for
applications that work with large data sets. At this writing, 32-bit processors are themost common platforms for running Linux. For 64-bit processors, the problems arethe same, but the boundaries change. The boundaries with 64-bit architectures are large enough that most of the challenges encountered in 32-bit processorsbecome moot for the foreseeable future.
5.6.1 Virtual Memory Explained
The core concept behind virtual memory is that the memory addresses used by your
code have nothing to do with the physical location of the data. The data in yourapplication may not be in physical memory at all but may be saved to disk(swapped) to allow some other process to have memory. What looks like a block of
contiguous data in virtual memory is most likely scattered in pieces in various loca-tions in physical memory or perhaps on the swap disk. This is illustrated inFigure 5-9.286 Chapter 5 • What Every Developer Should Know about the Kernel
Using virtual memory allows Linux to provide each process its own unique data,
protected from other processes. Each process runs as though it were the only processon the machine. In user space, address A of one process points to a different phys-ical memory location than address A of another process. Any time the CPU issuesa load or store to memory, the virtual address used by software must be translatedinto a physical address. The job of translating virtual addresses into physicaladdresses belongs to the Memory Management Unit (MMU).5.6 Memory Management in User Space 287
FIGURE 5-9 Physical Storage as Seen by the ProcessorCPU
Fastest
SlowestPhysical Storage
L1 Cache
L2 Cache
L3 Cache
RAM
Swap / Disk
5.6.1.1 The Role of the Memory Management Unit
The MMU works closely with the caches to move memory between RAM and cache
as required. In general, if your processor has a cache, it has an MMU, and vice versa.All modern desktop processors have some amount of on-chip cache and an MMU.
To make the job of translating addresses manageable, the MMU divides memory
into pages, which are the smallest units of physical memory it manipulates. Totranslate a virtual address into a physical address, the MMU breaks it into twopieces: the page frame number and the offset, as illustrated in Figure 5-10.
The size of a page is determined by each architecture, although 4K is very com-
mon among many architectures, including PowerPC and IA32. In this case, thepage frame number is 20 bits, and the offset is 12 bits.288 Chapter 5 • What Every Developer Should Know about the Kernel
FIGURE 5-10 Logical Address Broken into a Page Frame Number and OffsetMSB Logical Address LSB
Identify the page
Identify the 
Byte inthe PagePage Fr ame Number Offset
Page P age P age P age
Given the page frame number, the MMU can determine the physical address of
the page by using a page table, which is created by the kernel. The offset taken from
the virtual address is added to the physical address of the page to produce a com-plete physical address.
Every time the processor issues a load or store instruction, the virtual address is
translated by the MMU. If the MMU does not find an address in the page table,the result is called a page fault. This can happen when a page is not located in mem-
ory or when the process uses an invalid logical address. If the page fault is caused byan invalid address, the kernel sends the process a segmentation violation signal
(
SIGSEGV ). Page faults also can occur when the requested page has been swapped
to disk. In this case, the kernel must use the disk device to retrieve the page intophysical memory and update the page table to point to the new physical location.
Page faults that involve disk I/O are what Linux calls major page faults. Linux
also keeps track of what it calls minor page faults, which occur when the requested
page is in physical RAM but not in on-chip cache. In this case, the system incurssome small latency caused by the time it takes to move the page from RAM tocache, but because it is handled entirely in hardware, the page fault is considered to be minor.
This discussion is a bit oversimplified, because each architecture adds various
twists to this design, but it is the basic way most processors work. Fortunately, a com-plete understanding of the MMU is not necessary for application programming.
5.6.1.2 The Translation Lookaside Buffer
Because every process on the system has its own virtual addresses, each process must
have a unique page table. An important part of the context switch from one processto another involves changing the page tables to point to the appropriate virtualmemory. It’s actually very sophisticated, but I’ll explain some of the details.
Any time the CPU accesses memory, the MMU must translate the address using
the page tables before it can complete the operation, but page tables are stored inmemory as well. That means that in the worst case, every load or store can requiretwo memory transactions: a read from the page table followed by the actual loador store.
If the page tables were stored exclusively in memory, this would bring the system
to a crawl. Page tables can get quite large, and there is no upper limit on the num-ber of processes an operating system can support, so storing the page tables entirelyon chip is not an option either.5.6 Memory Management in User Space 289
As a compromise, the CPU keeps a cache of page table entries called the
Translation Lookaside Buffer (TLB). The TLB makes it possible for a process to oper-
ate on a large region of memory while keeping the critical address translation infor-mation on chip. The TLB needs to be large enough to cover the entire CPU cache.So a processor with 512K of cache and a page size of 4K needs 128 TLB entries tobe effective. Figure 5-11 shows an example of how the processor translates a virtualaddress using the TLB.
Because the TLB contains a cache of page table entries from the running process,
you might expect that it is flushed when a context switch occurs. Flushing and refill-ing the TLB is expensive, however, so the kernel avoids flushing the TLB at all coststo keep the context switch time low. This is sometimes called lazy TLB flushing. This
is feasible because kernel virtual memory is common to all processes, so it is possibleto reuse the kernel portion of the TLB from one process to the next. This is particularly290 Chapter 5 • What Every Developer Should Know about the Kernel
FIGURE 5-11 TLB Lookup Flow ChartVirtu al Address
Page Fr ame No. Offset
TLBIn the
TLB?TLB
Hit
TLB
MissYes
No
Search
PageTablesFetch from 
Cache (m ay 
need to 
fill cache)
useful in a preemptable kernel, where the kernel can switch from one process in ker-
nel mode to another process in kernel mode. Avoiding the TLB flush until the lastpossible moment gives the kernel opportunities to avoid unnecessary TLB flushes.
5.6.1.3 The CPU Cache
Because the speed of processors has far outstripped the speed of DRAM devices, all
modern processors have some amount of cache memory to allow the processors torun at high clock rates without being slowed by the RAM devices. Creating mem-ory that can run at gigahertz clock frequencies consumes many transistors and agreat deal of power. To compromise, many designs include several levels of cache, asshown in Figure 5-9 earlier in this chapter.
The cache closest to the processor is called the L1 cache, which resides on the chip
and usually is relatively small (8K to 32K is common) but runs with zero latency.That means that a load or store to these memory locations can be completed in onlyone clock cycle. Stated another way, the L1 cache runs as fast as the CPU. On somearchitectures, this may be the only cache that the processor has. Some low-cost ver-sions of the x86 processors, for example, implement only an L1 cache.
To increase the cache size, many architectures include additional levels that are
larger but progressively slower. The L2 cache is the next level and is larger than the
L1 cache but has some latency that will cause a load or store instruction to takemore than one clock cycle. In older designs, the L1 cache resided on chip, whereasthe L2 cache lived outside—on the motherboard or on a daughter card.
20External
cache invariably runs slower than the internal clock of the CPU. As CPUs got faster,it became more difficult to have fast-enough cache outside the CPU, so most high-performance processors include both L1 and L2 cache on the chip. The on-chip L2cache may or may not be slower than the L1 cache, but there surely is a latencypenalty for using it—that is, delays are incurred on certain address boundaries.Some vendors claim that their L2 cache runs at the same frequency as the CPU,which may be true. What they don’t tell you is that the cache cannot run continu-ously at that frequency—only in bursts. Otherwise, it would be an L1 cache.
When the L2 cache moved on chip, chipmakers invented the term L3 cache to
refer to cache memory that resides outside the chip. Recently, Intel has begun tointegrate L3 cache into its Xeon processors. Perhaps this trend will continue, andwe’ll see systems with L4 and L5 caches in the future.5.6 Memory Management in User Space 291
20. The first Pentium II processors came on a daughter card that included L2 cache.
A full discussion of cache is beyond the scope of this book, and fortunately, most
programmers don’t need to know much about cache beyond the basics. The fol-lowing sections discuss the basic concepts that you should know about.
Cache Lines
The CPU never reads or writes bytes or even words from DRAM. Every read or
write from the CPU to DRAM must first go into L1 cache, which reads or writesto the DRAM in units of lines. The cache line is the unit of all cache transactionswith the DRAM. Although a typical virtual-memory page may be 4K, a typicalcache line is on the order of 32 or 64 bytes. Both the page size and the cache linesize are unique to the make and model of the processor in use.
Figure 5-12 shows a simplified flow chart of how this works. To execute a simple
line of code that reads a single byte from memory, the CPU may end up reading anentire cache line (perhaps 64 bytes). If subsequent instructions also read from thesame line of cache, the line fill was worthwhile; otherwise, the extra cycles spent fill-ing the cache line were wasted. An L1 cache miss isn’t always that costly, either. Itis possible that the data is in L2 or L3 cache, in which case the fill is much fasterthan reading from RAM. Usually, the memory on the motherboard is laid out sothat a burst from the DRAM is the same size as the cache line. This way, cache linefills from RAM are as efficient as possible.
Even if the code in Figure 5-12 were writing to memory, the flow would be
exactly the same—that is, to write a single byte of memory you have to fill the entirecache line. When it’s time to write this line of cache back to memory, the CPU willwrite the entire line even if only 1 byte was changed.
This is the safe way to proceed, but it is inefficient. If, for example, the applica-
tion is going to overwrite a large block of data, the CPU will need to fill every cacheline before modifying it. The cycles spent filling the cache lines are a waste of time,because the lines are only going to be overwritten. For this reason, most processorshave assembly-language instructions to instruct the processor to skip the cache linefill because you plan to overwrite it. Unfortunately, there is no portable way toinclude these instructions in your high-level language code.
21This is one justifiable
use of inline assembly in your application.292 Chapter 5 • What Every Developer Should Know about the Kernel
21. POSIX has the madvise function, which can cause the processor to fill in advance, but there is no way
to tell it to skip the fill.
This may seem like nitpicking when you are working with processors that run at
3GHz, but the extra clock cycles add up, particularly if you are using large amountsof data.
Write Back, Write Through, and Prefetching
Caches have different modes of operation, and each CPU architecture has its own
idiosyncrasies. The basic modes that they have in common are
Write Back— This is the highest-performance mode and the most typical. 
In write-back mode, the cache is not written to memory until a newer cacheentry flushes it out or the software explicitly flushes it. This enhances per-formance because the CPU can avoid extra writes to memory when a line ofcache is modified more than once. Also, although cache lines may be writtenin random order, they may be flushed in sequential order, which may improve5.6 Memory Management in User Space 293
FIGURE 5-12 Cache Miss: Reading a Single Byte Can Cause a Cache Line FillRead One Byte
Non-Zero
LatencyZero
Latencychar *x = ...
y = *x;
TLB
Hit?L1
Hit?
TLB
MissNoYesYes
No Fill
Cache
Line
efficiency. This is sometimes called write combining and may not be available
for every architecture.22
Write Through— This is less efficient than write-back because it forces writes
to complete to memory in addition to saving it in cache. As a result, writestake longer, but reads from cache will still be fast. This is used when it’s impor-tant for main memory and the cache to contain the same data at all times.
Prefetching —Some caches allow the processor to prefetch cache lines in
response to a read request so that adjacent blocks of memory are read at thesame time. Reading in a burst of more than one cache line usually is more effi-cient than reading only one cache line. This improves performance if the soft-ware subsequently reads from those addresses. But if access is random,prefetching can slow the CPU. Architectures that allow prefetching usuallyhave special instructions allowing software to initiate a prefetch in the back-ground to gain maximum parallelism.
23
Most caches allow software to set the mode by regions so that one region may be
write-back, another is write-through, and still another is noncacheable. Typically,these operations are privileged, so user programs never modify the write-back orwrite-through modes of the cache directly. This kind of control usually is requiredonly by device drivers.
5.6.1.4 Programming Cache Hints
Prefetching can be controlled by software through so-called cache hints with the
madvise function. This API allows you to tell the operating system how you plan
to use a block of memory. There are no guarantees that the operating system willtake your advice, but when it does, it can improve performance, given the rightcircumstances. To tell the OS that prefetching would be a good idea, you woulduse this pattern:
madvise( pointer, size, MADV_WILLNEED | MADV_SEQUENTIAL);294 Chapter 5 • What Every Developer Should Know about the Kernel
22. Write combining is similar to merging I/O requests in the I/O scheduler discussed earlier in the chapter.
23. Some newer BIOSes allow you to enable or disable cache line prefetching at the system level.
These two flags tell the OS that you will be using the memory shortly and that
you will be doing sequential access. Prefetching can be a liability if you are access-ing data in a random fashion, so the same API allows you to tell the OS thatprefetching is a bad idea. For example:
madvise( pointer, size, MADV_RANDOM );
The madvise function has other flags to suggest that flushing or syncing would
be a good idea, but the msync function usually is more appropriate for this purpose.
5.6.1.5 Memory Coherency
Memory coherency refers to the unique problem that multiprocessor systems have in
keeping their caches up to date. When one processor modifies a memory locationin cache, the second processor will not see it until that cache is written back tomemory. In theory, if the second processor reads that location, it will get the incor-rect value. In reality, modern processors have elaborate mechanisms in hardware toensure that this doesn’t happen. Under normal circumstances, this is transparent tosoftware, particularly in user space. In a Symmetric Multiprocessing System (SMP),
the hardware is responsible for keeping the cache coherent between CPUs.
Even in a single-processor system, memory coherency can be an issue because
some peripheral hardware can take the place of other processors. Any hardware thatcan access system memory via Direct Memory Access (DMA) can read or write mem-
ory without the processor’s knowledge. Most PCI cards, for example, have DMAcontrollers. When a controller writes to system memory via DMA, there is a chancethat some of those locations are sitting in the CPU cache. If so, the data in cachewill be invalid. Likewise, if the necessary data is sitting in cache when a device readsfrom memory via DMA, the device will get the wrong data. It is the job of the oper-ating system (typically, a device driver) to manage the DMA transfers and the cacheto prevent this. If the device driver allows 
mmap , it may be up to the application to
manage the memory coherency.
When the data in cache is older than the data in memory, we say that it is stale.
If the software initiates a DMA transfer from a device to RAM, the software musttell the CPU that the cached entries must be discarded. On some systems, this iscalled invalidating the cache entries.5.6 Memory Management in User Space 295
When the data in cache is newer than the data in RAM, we say that it is dirty.
Before a device driver can allow a device to read from memory via DMA, it mustmake sure that all dirty entries are written to memory. This is called flushing or syn-
chronizing the cache.
Fortunately, most application programmers are shielded from cache-coherency
problems by the hardware and the operating system. Only specific drivers may pre-sent this problem to the application when it uses the 
mmap system call. One exam-
ple is a memory-mapped file. If a process makes a shared mapping of a file, changesto that file are not reflected immediately to other processes. The process must syn-chronize the memory explicitly with the file before other processes can see itschanges.
For this reason, POSIX provides the 
msync function, which allows the applica-
tion to do the equivalent of a flush or invalidate. To update the file with the changesin memory (that is, flush), use the following pattern:
msync( ptr, size, MS_SYNC );
The MS_SYNC flag indicates that the msync operation should complete before the
msync function returns. Without this flag, the operation will be scheduled by the
operating system but may not be complete when the function returns.
This synchronization is between the currently running process and the file on
disk. Other processes may have copies of the data in memory, which will be out ofsync. To make sure that other processes invalidate these copies, 
msync provides the
MS_INVALIDATE flag. This flag tells the kernel to make sure that any other process
that has mapped this particular file will invalidate its pages so that the next accessto the data will read from the file and update the data in memory.
5.6.1.6 The Role of Swap
Adding swap space has the effect of adding more memory to your system. The idea
is that much of the memory allocated by processes is not needed most of the time.With this in mind, it makes sense to remove these blocks of memory from DRAMand store them temporarily on disk so that you can free up the DRAM for otheruses. When the memory is needed again, the data can be read from disk and placedback in memory, while perhaps another unused block of memory is removed frommemory and put on disk. The two blocks of memory swap places, which is where296 Chapter 5 • What Every Developer Should Know about the Kernel
the name swap comes from. Programmers often use the word swap as both a noun
and a verb. We call the region of the disk used to store these pages the swap space,
but swap is also the word we use to describe the operation of moving data to and
from the swap partition. In operating system circles, swap is never used as a verb.
The action of moving data from memory to the swap partition is simply calledpaging .
Paging occurs in the background with no intervention from the application. The
application experiences only the side effect of increased latency, which is the tech-nical way to say that everything slows down. Determining the appropriate swap sizefor your system is more art than science. The rule of thumb used to be to allocatetwice as much swap as DRAM. Depending on the application and the amount ofRAM you have, you may not need that much swap. Most systems should have aswap partition, but some systems can function without a swap partition. Mostembedded Linux devices have no swap partition at all, for example.
One problem that occurs with swap is called thrashing, which occurs when sev-
eral running processes are simultaneously accessing more memory than is physicallyavailable. The system must swap pages in and out with each context switch, whichmeans that it spends more time moving pages in and out than it does running code.This brings your system to a crawl, as the CPU is consumed with the task of mov-ing data on and off the swap disk. The alternative, however, is to kill off processesvia the out-of-memory killer (also called OOM; more on this subject later in the
chapter).
Another issue can occur when the system is under heavy I/O load. In this case,
the file-system cache may be consuming the majority of memory while runningprocesses are trying to request more memory. If a process requests a large block ofmemory, and the request can’t be filled immediately, the system has to decidebetween swapping and reclaiming file-system cache buffers. The kernel doesn’t fac-tor in device speed when deciding to free up cache or page to disk. This small deci-sion can have big consequences if you have a very fast disk array and a relatively slowswap disk. The kernel thinks both transactions are equal, but in this example, pag-ing to disk would be much more time consuming than freeing cache blocks. This isone example in which turning off the swap partitions may be a good idea.
You can disable swap at any time by using the 
swapon and swapoff commands.
Linux allows you to have more than one swap partition, so these commands allow5.6 Memory Management in User Space 297
you to enable or disable specific partitions. You also can disable them all by using
the -aoption, which applies the command to all partitions in /etc/fstab listed
as swap partitions.
Swap devices need not be disk partitions. The mkswap command is used to for-
mat a swap partition but will format a plain file as well. To create a 4MB swap file,for example, you can use the following commands:
$ dd if=/dev/zero of=/tmp/swap.dat bs=1k count=4096
4096+0 records in4096+0 records out
$ mkswap /tmp/swap.dat
Setting up swapspace version 1, size = 4190 kB...$ swapon /tmp/swap.dat
$ swapon -s
Filename                           Type            Size    Used    Priority
/dev/mapper/VolGroup00-LogVol01    partition       327672  0       -1
/tmp/swap.dat                      file            4088    0       -3
Just-in-time swap files like this can be useful if it becomes necessary to increase
swap space temporarily. You can do so without repartitioning your drives.
5.6.1.7 Processes and Virtual Memory
From a programmer’s point of view, each process in Linux has its own virtual mem-
ory. The kernel space is common to all processes so that when processes run in ker-nel mode, they all see the same memory. This is necessary because it allows thekernel to delegate tasks to the currently running process. There is a trade-off here,because there is a finite amount of address space that must be divided into kernelspace and user space.
User-space addresses start at zero and extend up to a fixed upper limit. The upper
limit marks the maximum theoretical size of the memory seen by a user-spaceprocess. All kernel virtual addresses start at this address and cannot be seen in usermode. The most common default for 32-bit architectures is to reserve 3GB for userspace and 1GB for kernel space. This boundary is configured when the kernel isbuilt and cannot be changed without rebuilding the kernel.298 Chapter 5 • What Every Developer Should Know about the Kernel
In theory, a 32-bit process can allocate up to 3GB of memory. In reality, a good
deal of memory used by a simple C program is consumed by the standard libraryand any other libraries you include, as well as dynamic memory. Listing 5-2 showsan example.
LISTING 5-2 pause.c: A Trivial Program to Illustrate Memory Usage
int main()
{
return pause();
}
The program in Listing 5-2 does nothing but stop so that you can examine it.
You can run it in the shell in the background and then look at its memory maps.You can view each process’s (user space) memory map by looking at the file
/proc/PID/maps , but you can see more user-friendly output with the pmap com-
mand, which is part of the procps package:
$ ./pause &
[1] 6321$ pmap 63216321:   ./pause004d0000    104K r-x--  /lib/ld-2.3.5.so004ea000      4K r----  /lib/ld-2.3.5.so004eb000      4K rw---  /lib/ld-2.3.5.so004ee000   1168K r-x--  /lib/libc-2.3.5.so00612000      8K r----  /lib/libc-2.3.5.so00614000      8K rw---  /lib/libc-2.3.5.so00616000      8K rw---    [ anon ]08048000      4K r-x--  /home/john/examples/mm/pause08049000      4K rw---  /home/john/examples/mm/pauseb7f08000      4K rw---    [ anon ]b7f1a000      4K rw---    [ anon ]bfb05000     88K rw---    [ stack ]ffffe000      4K -----    [ anon ]
total     1412K
The pmap command lists the virtual addresses and sizes of various segments of
virtual memory. As you can see, each memory region has a set of permissions like a5.6 Memory Management in User Space 299
file. Next to each region, pmap lists the file associated with the mapping, if any. You
can see from this output that the process that does nothing consumes about 1.4MBof virtual memory, most of which is consumed by the C standard library(
/lib/libc-2.3.5.so ). Another big culprit is the dynamic linker ( ld-2.3.5.so ),
which consumes 112K. My trivial code occupies only 4K, which is a single page ofmemory—the smallest possible size. I should point out that although 
libc con-
sumes 1.1MB of virtual memory, the read-only sections are shared among allprocesses in the system that use it—that is, the library consumes only 1.1MB ofphysical storage in the entire system. This is one of the main advantages of usingshared libraries.
Another thing to notice about the map is that there can be big gaps in the vir-
tual-memory addresses, which means that the amount of contiguous virtual mem-ory you can allocate in your process is less than it would be if those regions werecontiguous. One such gap occurs between the region located at address 616000 andthe executable segment located at 8048000 (approximately 122MB). In most appli-cations, this is not a problem, but if your application needs to keep a large amountof data in memory, these gaps can be an issue.
Now look at the same example using assembly language. For simplicity, I’ll use
an 80x86 assembly, but the results should be similar on any platform. Listing 5-3is the same program as Listing 5-2 written in 80x86 assembly language. The dif-ference is that this uses hand-coded system calls and does not use the standard Clibrary.
LISTING 5-3 pause.s: Trivial 80x86 Assembly-Language Program
.text
# Linker uses _start as the entry point.
.global _start
.type _start, @function
# Signal handler. Does nothingsighdlr:
ret
_start:300 Chapter 5 • What Every Developer Should Know about the Kernel
# Use the BSD signal() syscall; same as : signal(SIGCONT,sighdlr)
movl $sighdlr, %ecx     # 3rd arg, sighdlr
movl $18, %ebx          # 2nd arg, 18 = SIGCONTmovl $48, %eax          # 1st arg, 48 = BSD signal() system callint  $0x80              # execute the system call
# Execute the pause() syscall
movl  $29, %eax         # 1st arg, 29 = pause() system call
int   $0x80             # execute the system call
# Exit system call
# We only get here if you send SIGCONT.
movl  $0,%ebx           # 2nd arg, exit code
movl  $1,%eax           # 1st arg, 1 = exit()
int   $0x80             # execute the system call
You can build this program with the following command:
$ gcc -nostdlib -o pause pause.s
Now when you run this, you’ll see a much smaller memory map:
$ ./pause &[1] 6992$ pmap 69926992:   ./pause08048000      4K r-x--  /home/john/examples/mm/pause08049000      4K rwx--  /home/john/examples/mm/pausebf8f5000     88K rwx--    [ stack ]ffffe000      4K -----    [ anon ]
total      100K
What you see here was only what the linker and the Linux exec system call cre-
ated. The linker added a writable data section because you did not specify one. exec
mapped the code into a single read-only page at address 8049000. The permissionbits are very similar to the file permission bits and show that code in this page maybe read and executed. Next to the permission bits, 
pmap lists the executable name
so that you know where this page came from. exec also allocated a single writable
page for your data segment. Finally, exec created a stack, which is the largest piece5.6 Memory Management in User Space 301
of the map at 88K. The anonymous mapping at ffffe000 is used in Linux 2.6 as part
of a new, more efficient mechanism for system calls on IA32. This example uses theold method.
A Look at Intel’s Physical Address Extension (PAE)
The amount of RAM you can install on your computer is not limited just by the
number of DIMM slots on your motherboard. It’s also limited to the amount of phys-ical memory that your processor is capable of addressing. At one time, this limit wasdetermined by the word size of the CPU. A 32-bit machine, for example, could storeonly 32-bit pointers; therefore, the physical address limit was 2
32bytes, or 4GB.
When the first 32-bit processors came out, the idea that anyone could need, muchless afford, 4GB of RAM seemed improbable.
Time went on; DRAMs got denser; and soon it became possible to produce sys-
tems with 4GB of RAM for a reasonable cost. It wasn’t hard for software to figure outways to consume all this memory, and soon, users were demanding more. One obvi-ous solution would have been to switch to a 64-bit architecture. But at that time,switching to a 64-bit processor meant porting all your applications to a new platform.This was a costly solution, especially considering that what most customers wantedwas more processes, not bigger processes. This led Intel to implement a technique toexpand the physical memory without requiring a costly transition to a 64-bit proces-sor architecture.
Intel’s 
Physical Address Extensions (PAE) allow the processor to address up to
64GB (236 bytes) of RAM by enlarging the page address from 20 bits to 24 bits. The
page size does not change, so the offset still requires 12 bits. That means that theeffective physical address is 36 bits. Because the logical address must fit in a 32-bitregister, individual processes still can address only 4GB of virtual memory.
The MMU and the operating system use page addresses exclusively for manipu-
lating pages, so the operating system is free to use the 24-bit page address when allo-cating pages to cache or processes. Therefore, cumulative virtual memory available tothe system is effectively 64 GB (2
36).
This is occasionally a source of misunderstanding among programmers who intu-
itively assume that a process can see as much virtual memory as the whole system canaddress physically. Indeed, until recently this assumption was still baked into parts ofthe Linux kernel long after support for PAE was implemented. Luckily, it affected
only certain device drivers, and only then in a system with more than 4GB of RAM.302 Chapter 5 • What Every Developer Should Know about the Kernel
5.6.2 Running out of Memory
Any system is constantly in flux, allocating and deallocating memory at all times.
Many processes allocate small chunks of memory for short periods; other processesallocate memory once and never free it. A process can run out of memory eventhough the system has plenty, and the system can run out of memory while someprocesses continue to run without error. Everything depends on the circumstances.
The standard library and the swap partition conspire to confuse the average pro-
grammer when he tries to get a handle on just how much memory is available. Theswap disk makes your system look as though it has more physical memory than itdoes. So when you want to know how much memory is available, the answer usu-ally is fuzzy. Meanwhile, the standard library employs some tricks that can make itlook as though your process has just allocated far more memory than the system canallow it to have. To make matters more confusing, the process may not even crash.
5.6.2.1 When a Process Runs out of Memory
Processes can run out of memory in one of two ways: They can run out of virtual
addresses, or they can run out of physical storage. Running out of virtual addressesmay seem to be improbable. After all, if you have only 1GB of DRAM and no swapdisk, wouldn’t 
malloc fail long before you ran out of virtual addresses? The pro-
gram in Listing 5-4 illustrates that this is not the case. This program allocates mem-ory in 1MB chunks until 
malloc fails.
LISTING 5-4 crazy-malloc.c: Allocate As Much Memory As Possible
#include <stdio.h>
#include <string.h>#include <stdlib.h>
int main(int argc, char *argv[])
{
void *ptr;int n = 0;
while (1) {
// Allocate in 1 MB chunksptr = malloc(0x100000);5.6 Memory Management in User Space 303
continues
// Stop when we can't allocate any more
if (ptr == NULL)
break;
n++;
}// How much did we get?printf("malloced %d MB\n", n);
// Stop so we can look at the damage.
pause();
}
I ran the program in Listing 5-4 on a 32-bit machine with 160MB of RAM and
swap disabled. Care to guess what happened? malloc did notfail until the process
allocated almost 3GB of RAM! I included the pause call, so that you can look at
the memory map:
$ ./crazy-malloc &
[1] 2817malloced 3056 MB$ jobs -x pmap %12823:   ./crazy-malloc000cc000   4112K rw---    [ anon ]004d0000    104K r-x--  /lib/ld-2.3.5.so004ea000      4K r----  /lib/ld-2.3.5.so004eb000      4K rw---  /lib/ld-2.3.5.so004ee000   1168K r-x--  /lib/libc-2.3.5.so00612000      8K r----  /lib/libc-2.3.5.so00614000      8K rw---  /lib/libc-2.3.5.so00616000      8K rw---    [ anon ]006cf000 124388K rw---    [ anon ]08048000      4K r-x--  /home/john/examples/mm/crazy-malloc08049000      4K rw---  /home/john/examples/mm/crazy-malloc08051000 2882516K rw---    [ anon ]b7f56000 125424K rw---    [ anon ]bfa43000     84K rw---    [ stack ]bfa58000   5140K rw---    [ anon ]ffffe000      4K -----    [ anon ]
total  3142980K
Recall that the typical kernel split between user space and process space is 3GB,
which is true for this particular kernel as well. As expected, the total memory304 Chapter 5 • What Every Developer Should Know about the Kernel
allocated by this process, as reported by pmap , cannot exceed this limit and is very
close to it.24The discrepancy is due to holes in the memory map that were not big
enough to fit the 1MB allocations, so they remain unused. You can see these in the
pmap output if you look for them. One such hole is at virtual address 618000 and
is 732 KB—too small for a 1MB block but still useful for smaller blocks. Although
pmap does not highlight this, it starts immediately after the 8K block at 616000 and
stops at the next block, which is at 6CF000 .
If you’ve never seen this behavior before, you may wonder how this is possible.
There are two culprits at work here. The first is the GNU C standard library’simplementation of the heap; the other is the Linux virtual-memory subsystem.
The GNU St andard C Libr ary and the He ap
The heap is the term used to describe the pool of memory used by C and C++ pro-
grams for dynamic memory allocations. There are several ways to implement aheap, and the GNU standard library seems to use all of them. The classic method,described in The C Programming Language, by Kernighan and Ritchie (Prentice
Hall PTR), involves allocating a large pool of memory and keeping track of freeblocks with a linked list in that pool. This has the drawback that your process mayconsume memory that it doesn’t need. For efficiency, most heap implementationswill allocate heap only as necessary via the 
brksystem call. This allows the applica-
tion to start with a small heap that can grow in response to additional requests fordynamic memory. Once allocated, this memory is seldom returned to the system.
Another drawback is that a monolithic pool of memory will tend to get frag-
mented over time. This occurs when small blocks are allocated and not immediately
freed, as illustrated in Figure 5-13. When small blocks are allocated and not freed,such as blocks 2 and 4 in the illustration, they have the effect of splitting largerblocks. When we first allocate block 1, the size of the allocation is limited only bythe size of the memory pool. After four allocations and two frees, the maximum sizeof the next allocation is much less than the total memory available. The small blocksallocated in blocks 2 and 4 have fragmented the memory pool. In a system withoutvirtual memory, this can continue indefinitely until allocations start to fail.Fortunately, the standard library takes many steps to prevent fragmentation.5.6 Memory Management in User Space 305
24. 3GB is exactly 3,145,728K.
A complete discussion of the heap is beyond the scope of this book, but I’ll
describe one trick the GNU standard library uses to avoid fragmentation, which isat work in the program in Listing 5-5 later in this chapter.
The GNU standard C library uses a conventional pool of memory for small allo-
cations but uses the 
mmap system call to allocate large blocks of memory. This tends
to prevent the kind of fragmentation illustrated in Figure 5-13 because it separatesthe small and large blocks into different pools.
25For most applications, the virtual-
memory pool is much larger than you would ever want your heap to be, so there306 Chapter 5 • What Every Developer Should Know about the Kernel
FIGURE 5-13 Memory Fragmentation IllustratedFree
MemoryFree
MemoryFree
MemoryFree
MemoryFree
Memory
4   Blockth4   Blockth
3   Block
Timerd
2    Blocknd
1   Blockst1   Blockst1   Blockst1   Blockst2    Blocknd2    Blocknd3   Blockrd
2    Blocknd
25. This also confounds some heap-checking tools, which are unaware of this trick.
are always enough virtual addresses to go around. For many applications, this is
enough. Under the right circumstances, however, you can fragment the virtualaddress space just like a traditional heap. So the library allows applications somecontrol over how 
mmap memory is used via the mallopt function. There is no man
page for mallopt , but you can find out more about it in the GNU info page for
libc as follows:
$ info libc mallopt
The mallopt function is part of the SVR4 standard, although the values that it
takes can vary from system to system. A couple of useful ones defined by GNU areshown in Table 5-8.
You can use 
mallopt , for example, to disable the use of mmap entirely, or you can
just tweak the threshold. To disable the use of mmap , use the following code:
#include <malloc.h>
r = mallopt(M_MMAP_MAX,0);if ( r == 0 ) // error...
Unlike POSIX functions, mallopt returns zero for error and nonzero for suc-
cess. Unfortunately, there is no way to determine the current value of a parametersuch as 
M_MMAP_THRESHOLD , for example.265.6 Memory Management in User Space 307
TABLE 5-8 Tunable Parameters Defined by GNU for Use with mallopt()
Parameter Usage
M_MMAP_THRESHOLD Set to a threshold size in bytes. Any allocation larger than
this threshold will use mmap instead of the heap.
M_MMAP_MAX Maximum number of mmap ped blocks to use at any time.
When this threshold is exceeded, all allocated blocks willuse the heap. Set this threshold to zero to disable the use
of mmap .
26. There is a function named mallinfo , but it provides only statistical data on how the heap is currently
being used.
Virtual Memory and the He ap
In the crazy-malloc example, the code allocated almost all the user space as
dynamic memory on a system with only 160MB and no swap partition. The mapillustrated how the standard library created anonymous mappings in user space via
the 
mmap call. Using strace , you can see that each malloc call results in a call to
mmap as follows:
mmap2(NULL,1052672,PROT_READ|PROT_WRITE,MAP_PRIVATE|MAP_ANONYMOUS,-1,0)
The mmap2 system call allows a process to allocate memory by setting the
MAP_ANONYMOUS flag. This does not require a device driver for storage, which is why
the file descriptor argument is –1. For efficiency, the kernel defers finding any phys-
ical space for these pages until they are used, so mmap2 returns a pointer to virtual
memory that does not exist yet. Not until you try to use this virtual address will thephysical memory be allocated. When this happens, it will cause a page fault, whichwill cause the kernel to find physical RAM for the page. This is a very effective tech-nique that increases the efficiency of many operations by preventing unnecessarymemory access.
If you modify the program in Listing 5-4 to modify the data that it allocates, it
will force page faults to occur, and then you’ll see a very different behavior, shownin Listing 5-5.
LISTING 5-5 crazy-malloc2.c: Allocate Memory and Touch It
#include <stdio.h>
#include <string.h>#include <stdlib.h>
int main(int argc, char *argv[])
{
void *ptr;int n = 0;
while (1) {
// Allocate in 1MB chunksptr = malloc(0x100000);
// Stop when we can't allocate any more
if (ptr == NULL)308 Chapter 5 • What Every Developer Should Know about the Kernel
break;
// Modify the data.
memset(ptr, 1, 0x100000);printf("malloced %d MB\n", ++n);
}// Stop so we can look at the damage.pause();
}
When the program in Listing 5-5 runs, the results may not be what you expect.
Instead of pausing so you can inspect the damage, the program is killed before itgets there:
$ ./crazy-malloc2
malloced 1 MBmalloced 2 MBmalloced 3 MB...malloced 74 MBKilled$
The program was killed by the dreaded out-of-memory killer (often abbreviated as
OOM ). By modifying the data, you forced the system to run out of memory. From
the process’s point of view, there was plenty of memory in the form of virtualaddresses. When the system runs out of storage in the form of RAM and swap, thekernel responds by killing the processes. The kernel gets a bit chatty to try to helpyou debug what was going on (perhaps out of guilt). Among many esoteric itemsyou will find in 
/var/log/messages is the following:
Out of Memory: Killed process 2995 (crazy-malloc2).
If you’re keeping score, the only time malloc failed was when it ran out of vir-
tual memory. malloc continued to return pointers to virtual memory far beyond
what the system was able to provide. You might say that malloc was writing checks
it couldn’t cash. Linux calls this overcommit, which refers to the fact that the kernel
allows a process to allocate more memory than is currently available. The kernel iseffectively speculating that the memory will be available when needed. You can alterthis behavior if necessary.5.6 Memory Management in User Space 309
You can force the kernel to disable overcommit by running the following com-
mand as root:
$ echo 2 > /proc/sys/vm/overcommit_memory
This forces the kernel to allow allocations based only on the physical storage
that is currently available. T ry it and rerun the examples to see how it changes thebehavior.
These examples fly in the face of a common excuse people use for not checking
pointers returned from 
malloc . Perhaps you have heard the excuse that goes some-
thing like this: “If malloc fails, the whole system is screwed anyway, so what’s the
point of trying to recover?” What we have seen is that the system takes care of itselfand actively avoids getting screwed up by your process. So when 
malloc fails, it’s
your problem. And yes, you can recover.
5.6.2.2 When the System Runs out of Memory
You have seen how the kernel uses the out-of-memory killer to deal with processes
when the system runs out of memory. Before that happens, the kernel will flush thefile-system cache to free up space along with any other cache that can be flushed.After that, it will resort to swapping pages to disk if it can.
These are time-consuming operations that usually are charged to the process that
is causing the problem (the one requesting all the memory). When memory gets low,however, virtually every process can cause swapping as a result of a simple contextswitch. This is the thrashing that I described earlier. It’s only when the system runs
out of swap that you run into the out-of-memory killer, as you did in an earlierexample. Before that happens, the system can waste a great deal of time thrashing.
5.6.2.3 Locking Down Memory
Both kernel and user-space pages can page to disk, but some memory cannot be
paged to disk. These pages are said to be locked. Memory allocated by the 
ramdisk
device cannot be swapped to disk, for example. The kernel allows user-space
processes to lock memory by using the mlock and munlock system calls. Locking
memory consumes RAM and reduces the amount of pageable memory. Doing thiscan lead to thrashing, as unlocked pages have fewer physical pages to use. For thisreason, only processes with superuser privileges can use the 
mlock and munlock
system calls.310 Chapter 5 • What Every Developer Should Know about the Kernel
To prevent a region of memory from being swapped to disk, use the following call:
r = mlock( ptr , size );
Like all POSIX functions, mlock returns zero for success and -1 for error. When
it returns, you are guaranteed that the pages are resident in RAM so there will beno significant latency when accessing this memory. More specifically, a page faultwill never occur as a result of accessing this memory.
This is one way critical processes keep running even when the system is out of
memory. Pages that are locked are above the fray when the system is thrashing. Acontext switch to a process that has locked most or all of its pages will not be ascostly as switching to another process. That is why there is another useful functionfor locking pages: 
mlockall . This function takes only a flags argument, which can
be a combination of MCL_CURRENT to lock all pages that are currently allocated or
MCL_FUTURE to lock all future pages that are allocated by this process. An obnox-
ious daemon might insist on locking all its pages in memory at all times. This wouldbe done by setting both flags:
r = mlockall( MCL_CURRENT | MCL_FUTURE );
After this call, all memory in use by the process will remain in RAM until it is
unlocked. Any new pages that are created as a result of a call to brk(usually the
result of a malloc ) or any other that allocates new pages will also remain in RAM
indefinitely. You had better hope that you don’t have a memory leak.
Unlike the mlock function, the mlockall function can be called by a process
without superuser privileges. The restriction is simply that the process cannot lockpages unless it has superuser privileges. An unprivileged process can call 
mlockall
(MCL_FUTURE) , which does not lock any of the pages currently allocated but tells
the kernel to lock any new pages that are allocated by this process. If the processdoes not have superuser privilege, when this happens, this allocation will fail. If a
malloc results in a brksystem call, for example, the brkcall will fail, which in turn
will result in malloc returning a NULL pointer. This is one way to test your error
handling for out-of-memory conditions.
As you might expect, mlock and mlockall have counterparts to unlock pages,
intuitively named munlock and munlockall . It is important to lock pages only
when necessary and to unlock pages if you can to free physical memory for use byother processes.5.6 Memory Management in User Space 311
When your system has much less memory than the virtual address space of your
processor, your process isn’t likely to run out of memory until the system runs outof memory. This is unfortunate, because you might be able to include some errorhanding in your application to deal with this situation. You can’t deal with anythingwhen the out-of-memory killer has killed your process. One workaround is avail-able from the GNU C library, which is part of the 
sysconf library function. You
can query the number of available physical pages from the system with the follow-ing call:
num_pages = sysconf( _SC_AVPHYS_PAGES );
This tells you the number of pages the system can allocate without having to
flush cache or page to disk. It is roughly equal to the MemFree value you see in
/proc/meminfo . Because this value does not take into account memory that could
be freed by flushing pages from the file-system cache, it is a very conservative value.
Beware—the value you get by multiplying the number of available pages by the
page size can overflow. This is due to the fact that both IA32 and PowerPC havememory extensions to allow the processor to see more than 4GB of RAM (see thesidebar on Intel’s PAE earlier in this chapter). In case you don’t already know, Cdoes not inform you when an integer overflows; it provides an invalid result instead.The best advice is to do any math in units of pages, not bytes.
There is another line of defense when dealing with process memory usage: the
setrlimit system call, which allows the administrator or even a user to impose
limits on the amount of resources a single process can use. Listing 5-6 is the
crazy-malloc program reworked to include a call to setrlimit based on the
available memory in the system.
LISTING 5-6 crazy-malloc3.c: Allocate Memory with Resource Limits Set
1 #include <stdio.h>
2 #include <string.h>3 #include <stdlib.h>4 #include <limits.h>5 #include <signal.h>6 #include <unistd.h>7 #include <sys/types.h>8 #include <sys/stat.h>312 Chapter 5 • What Every Developer Should Know about the Kernel
9 #include <sys/resource.h>
1011 int main(int argc, char *argv[])12 {13     void *ptr;14     int n = 0;15     int r = 0;16     struct rlimit rl;17     u_long pages, max_pages, max_bytes;1819     pages = sysconf(_SC_AVPHYS_PAGES);2021     /* Calculate max_bytes, but look out for overflow */22     max_pages = ULONG_MAX / sysconf(_SC_PAGE_SIZE);23     if (pages > max_pages)24         pages = max_pages;25     max_bytes = pages * sysconf(_SC_PAGE_SIZE);2627     r = getrlimit(RLIMIT_AS, &rl);2829     printf("current hard limit is %ld MB\n",30            (u_long) rl.rlim_max / 0x100000);3132     /* Modify the soft limit and don't change the hard limit. */33     rl.rlim_cur = max_bytes;3435     r = setrlimit(RLIMIT_AS, &rl);36     if (r) {37         perror("setrlimit");38         exit(1);39     }4041     printf("limit set to %ld MB\n", max_bytes / 0x100000);4243     while (1) {44         // Allocate in 1 MB chunks45         ptr = malloc(0x100000);4647         // Stop when we can't allocate any more48         if (ptr == NULL) {49             perror("malloc");50             break;51         }525.6 Memory Management in User Space 313
continues
53         memset(ptr, 1, 0x100000);
54         printf("malloced %d MB\n", ++n);55     }56     // Stop so we can look at the damage.57     printf("paused\n");58     raise(SIGSTOP);59     return 0;
60 }
When you run crazy-malloc3 , instead of getting killed by the OOM killer, it
fails. On my system, I got the following output:
$ ./crazy-malloc3
current hard limit is 4095 MBlimit set to 53 MBmalloced 1 MBmalloced 2 MBmalloced 3 MB...malloced 50 MBmalloced 51 MBmalloc: Cannot allocate memorypaused
The rlimit structure consists of a soft and a hard limit. The hard limit typically
is set at system startup; otherwise, there are no default limits. An unprivileged usercan set the soft limit to any value up to, but not greater than, the hard limit. Theuser can also lower the hard limit for the current process and its children, but whenthat happens, the limit can’t be raised again by this process. That’s why before youcall 
setrlimit , you use getrlimit (on line 27) so that you don’t inadvertently
lower the hard limit. Unprivileged processes should modify only the soft limit.
Now instead of just dying, the process can take some corrective action, attempt
to recover, or just fail-safe. In this instance, the process found that 53MB was avail-able, but it could 
malloc only 51 blocks of 1MB due to overhead from the system
libraries. This is expected, based on what you’ve already seen.
Keep in mind that the number of available pages you get from sysconf is only
a snapshot. On any system, this value will go up and down with demand from otherprocesses. On a busy system, this value may be totally unreliable.
If you’re tuning system memory usage at this level, chances are that you are work-
ing on an application-specific system, such as an embedded device. In this case, youhave probably accounted for most of the memory usage in the system anyway, and314 Chapter 5 • What Every Developer Should Know about the Kernel
if you haven’t, you should. Only then will you know what are good values to use for
setrlimit .
Bash allows users access to getrlimit and setrlimit via the built in ulimit
function, which takes its name from the deprecated library function that used toserve this purpose. These limits apply only to the current shell and any children. Ifyou want to apply limits systemwide, you should set this in 
/etc/profile , which
applies to all Bash shells.
5.7 Summary
In this chapter, I took an in-depth look at how processes function in Linux. Idescribed the concepts of user mode and kernel mode. I explored the basics of sys-tem calls and explained how many of the library functions you take for granted areactually thin wrappers around system calls.
I also looked at the Linux scheduler and how it affects your code. I described
some of the user commands you can use to influence the scheduler behavior. Inaddition to describing the scheduler, I described how the kernel keeps track of time.I showed some of the different clocks in the system that tick at various rates. Ideally,you know which ones are most appropriate for your needs.
I described the basics of device drivers and device nodes, as well as the basics of
system input and output using device drivers. I introduced the I/O schedulers anddemonstrated how you can adjust and tune them at runtime.
I finished this chapter with a discussion of virtual memory and what it means to
your process. Along the way, I demonstrated the various out-of-memory conditionsthat processes can run into and introduced the dreaded OOM killer.
5.7.1 Tools Used in This Chapter
•mkswap , swapon , swapoff —tools for manipulating swap partitions
•nice , renice , chrt —tools to influence the scheduler’s behavior
•pmap —shows you a map of a process’s virtual memory
•ps, time , times —used to show how much time your process spends in user
space and kernel space
•strace —an excellent tool for analyzing the system call behavior of your
program5.7 Summary 315
5.7.2 APIs Discussed in This Chapter
•clock_getres , clock_gettime —high-resolution POSIX clocks
•getrusage , times —library functions to look at resource usage
•mallopt —a GNU API to allow you to influence how malloc behaves
•mlock , mlockall —allow you to lock pages in RAM
•mmap , msync , madvise —allow you to influence how memory is stored in
RAM and on disk
•pthread_setschedparam —chooses a scheduling policy for a thread
•sched_get_priority_min/max —determines at runtime the minimum and
maximum priorities for a given scheduling policy
•sched_setscheduler —chooses a scheduling policy for a process
•sysconf —tells you details about system configuration constants
5.7.3 Online References
• www.kernel.org/pub/linux/utils/kernel/hotplug/udev.html—numerous
resources for learning about udev
• http://linux-hotplug.sourceforge.net—resources and documentation for the
Linux hotplug features
5.7.4 References
• Cesati, M., and D.P . Bovet. Understanding the Linux Kernel. 3d ed. Sebastopol,
Calif.: O’Reilly Media, Inc., 2005.
• Kernighan, B.W., and D. Ritchie. The C Programming Language. Englewood
Cliffs, N.J.: Prentice Hall, 1988.
• Kroah-Hartman, G., J. Corbet, and A. Rubini. Linux Device Drivers.
Sebastopol, Calif.: O’Reilly Media, Inc., 2005.
• Love, R. Linux Kernel Development. 3d ed. Indianapolis: Novell Press, 2005.
• Rodriguez, C.S., G. Fischer, and S. Smolski. The Linux Kernel Primer: A Top-
Down Approach for x86 and PowerPC Architectures. Englewood Cliffs, N.J.:
Prentice Hall, 2006.316 Chapter 5 • What Every Developer Should Know about the Kernel
6.1 Introduction
I introduced the Linux process model in Chapter 5. Most of that discussion focused
on process interaction with the kernel. In this chapter, I focus on processes in userspace. I look at the life cycle of a process from 
exec to exit and everything in
between. This chapter looks closely at the process footprint and shows you severaltools and APIs that you can use to examine the resources a process consumes.
6.2 Where Processes Come From
Linux processes have a parent–child relationship. A process has one—and onlyone—parent, but it can have (almost) any number of children. All processes have asingle common ancestor: the 
init process. init is the first process to run when
you boot the system and remains alive until you shut it down. init is responsible
for preserving sanity on your system by enforcing graceful startup and shutdown.
6
317Understanding Processes
You cannot terminate the init process via a signal, even as superuser. You must
politely ask it to terminate in one of several ways. When you do, it shuts down thesystem—gracefully, you hope.
Linux creates processes with one of three system calls. T wo of these are tradi-
tional system calls provided by other UNIX variants: 
fork and vfork . The third
is Linux specific and can create threads as well as processes. This is the clone sys-
tem call.
6.2.1 fork and vfork
The fork system call is the preferred way to create a new process. When fork
returns, there will be two processes: a parent and child, identical clones of eachother. 
fork returns a process ID ( pid_t ) that will be either zero or nonzero. From
the programmer’s perspective, the only difference between parent and child is thevalue returned by the 
fork function. The parent sees a nonzero return value, which
is the process ID of its child process (or –1 if there’s an error). The child sees a zeroreturn value, which indicates that it is the child. What happens next is up to theapplication. The most common pattern is to call one of the 
exec system calls
(which I will discuss shortly), although that is by no means required.
The vfork system call is something of an artifact. It is virtually identical to fork
except that vfork guarantees that the user-space memory will not be copied. In the
bad old days, a fork call would cause all the process’s user-space memory to be
copied into new pages. This is especially wasteful if the only thing the child processis going to do is call 
exec . In that case, all that copying is done for nothing. This
happens to be exactly what the init process does, for example. The children of
init have no use for a copy of init ’s user space, so copying it is a waste of time.
The idea behind vfork was to eliminate this copying step to make processes like
init more efficient.
The problem with vfork is that it requires the child process to call exec imme-
diately, without modifying any memory. This is harder than it sounds, especially ifyou consider that the 
exec call could fail. The vfork(2) man page has an interest-
ing editorial on this topic for the interested reader.
All modern UNIX variants use a technique called copy on write, which makes a
normal fork behave very much like a vfork , thereby making vfork not just unde-
sirable, but also unnecessary.318 Chapter 6 • Understanding Processes
6.2.2 Copy on Write
The purpose of copy on write is to improve efficiency by eliminating unnecessary
copying. The idea is relatively simple. When a process forks, both processes sharethe same physical memory for as long as possible—that is, the kernel copies onlythe page table entries and marks all the pages copy on write. This causes a page faultwhen either process modifies the memory. When a page fault occurs due to copy onwrite, the kernel allocates a new page of physical storage and copies the page beforeallowing it to be modified. This is illustrated in Figure 6-1.
If a process forks and the child modifies only a tiny fraction of memory, this is a
big win, because you save the time of copying all that data. It also conserves physi-cal memory, because the unmodified pages reside in memory that is shared by twoprocesses. Without copy on write, the system would need twice as much physicalstorage for parent and child.6.2 Where Processes Come From 319
FIGURE 6-1 Copy-on-Write Flag Triggers a Page Fault When Data Is ModifiedAfter a Fork:
both parent and 
child use the same 
physical storage.After a Modify:
the kernel copies the
physical memory into a
new physical page.
Parent
Virtu al PageParent
Virtu al PageChild
Virtu al PageParent’s
Physic al PageParent’s
Physic al PageCopyNew
Physic al Page
Child
Virtu al Page
Think of how long startup would take if init had to copy all its pages each time
it started a process. The basic job of the init process is to fork and exec . The child
has no use for any of init ’s memory. Likewise, there are many system daemons
whose job it is to fork and exec just like init . (These processes benefit as well.)
Such daemons include xinetd , sshd , and ftpd .
6.2.3 clone
The clone system call is unique to Linux and can be used to create processes or
threads. I mention it here for completeness only. Portable code should never use the
clone system call. The POSIX APIs should be sufficient enough to provide what
you need, be it a thread or a process.
clone is a complicated system call implemented as kind of a general-purpose
fork . It gives the application full control over which parts of the child process will
be shared with the parent. This makes it suitable for creating processes or threads.You can think of a thread as being a special-case process that shares its user spacewith its parent.
If you look at the Linux source, you will find separate system calls for 
fork ,
vfork , and clone . As you might expect, these are just wrappers around the same
kernel code. To implement the library calls for fork , exec , and pthread_create
in Linux, GLIBC seems to use the clone system call almost exclusively.
6.3 The exec Functions
The exec functions allow you to transfer control of your process from one exe-
cutable program to another. There is no function named exec in Linux, but I use
the term here to refer to a family of library calls. The calls are documented in the
exec(3) man page.1Although there are many library functions to implement an
exec , there is only one system call: execve . All the functions provided by the
library are just wrappers around this one system call. The execve system call itself
is accessed via the following function:
int execve(const char *filename, char *const  argv [], char *const envp[]);320 Chapter 6 • Understanding Processes
1. Note that the manpage is in section 3 (libraries) and not section 2 (system calls).
The execve system call looks for the file you specify; determines whether it is
executable; and, if so, tries to load it and execute it. It is unusual for a system callto do so much work, but 
execve is unique. For the purpose of this chapter, I’ll use
the term execve to refer to the specific system call. The term exec will refer to any
of the exec functions listed in exec(3) .
The first step for the kernel is to look at the permissions on the file. The process
owner must have permission to execute the file before the kernel will attempt toread it. If that test fails, 
execve returns an error ( -1) and sets errno to EPERM .
Having passed the permission test, it’s time for the kernel to look at the contents
of the file and determine whether it really is an executable. In general, executablefiles fall into three categories: executable scripts, executable object files, and miscella-
neous binaries . 
6.3.1 Executable Scripts
Executable scripts are text files that direct the kernel to an interpreter, which must
be an executable object file. If not, execve fails with errno set to ENOEXEC (exec
format error). The interpreter may not be another script.
The kernel recognizes an executable script by looking at the first two characters
of the file. If it sees the characters #!, it parses this first line into one or two addi-
tional tokens separated by white space. A typical example is a shell script, whichstarts with this line:
#!/bin/sh
The kernel interprets the token following #!as the path to an executable object
file. If this file does not exist or is not an executable object file, execve returns -1
to indicate an error. Otherwise, the kernel breaks line 1 of the script into threetokens and creates an 
argv vector for the interpreter as follows:
•argv[0] —the pathname of the interpreter executable
•argv[1] —all text following the name of the interpreter (the argument)
•argv[2] —the filename of the script
argv[1] consists of everything following the interpreter (white space and all) 
on the first line of the script, packed into a single string. This is unlike a normal6.3 The exec Functions 321
command line, where each element of argv has no white space. This can lead to
odd behavior if you’re not aware of it. This script works, for example:
#!/bin/sh -xv argv[1] = "-xv"
echo Hello World
This script, on the other hand, does not work:
#!/bin/sh -x -v argv[1] = "-x -v"
echo Hello World
Both are legal syntax in a regular command line, but when execve processes the
script, the latter example is equivalent to:
$ sh '-x -v'
sh: - : invalid optionUsage:  sh [GNU long option] [option] ...
The shell expects the arguments to be stripped of white space from the command
line. When that is not the case, it gets confused. 
The interpreter can be any program, but this technique is intended only for script
interpreters. Some common choices are Perl, Python, Awk, and Sed. An awkscript
can be written as follows:
#!/bin/awk -f
BEGIN { print "Hello World" }END { print "Goodbye World" }
Notice that the -foption is required so that when awkis called, the argv vector
is equivalent to the following command:
/bin/awk -f scriptname
Here, you can see why the options from the first line are sandwiched into the sec-
ond element of the argv vector. Without the -foption to awk, you can’t write an
awkscript that can be executed directly. Perl, Python, and most other script inter-
preters don’t require any additional arguments to function this way.
Linux limits the first line of a script to 128 characters,2including white space,
after which the line is truncated and used as is. Any arguments that exist past the128th character are silently discarded. Other systems may have larger limits.
2. This number is determined by BINFMT_BUF_SIZE in the kernel.322 Chapter 6 • Understanding Processes
Typos and Scripts
I have made my share of typos in scripts and have seen some bizarre behavior. The
shell works to hide some of these issues from you without your knowing it. Here aresome antipatterns that work from the shell but not from 
execve :
# !/bin/sh Note the space between #and !.
#!/bin/sh Note the space before #.
When you try to execute one of these scripts with an execve system call, it will
fail with the error ENOEXEC . If you happen to start one of these scripts from the shell,
however, it will work.
When the shell starts one of these scripts, it calls execve just like you would from
an application. Just like your application, the execve call fails. But unlike your appli-
cation, the shell’s child process is a perfectly functional command interpreter, so itdetermines that the file happens to be a text file and then proceeds to interpret thetext as commands. The first line, which you thought was a parameter to 
execve , is
now ignored as a shell comment, and the rest of the statements are interpreted with-out errors. Voilà! Your shell script works—by accident.
Bash uses a simple algorithm to determine whether a file rejected by 
ENOEXEC will
be passed to the interpreter. Version 3.00.17 reads the first 80 characters or the firstline (whichever is shorter), looking for non-ASCII characters. If it sees only ASCIIcharacters, the file is passed to the shell interpreter; otherwise, it throws an error.
Another antipattern occurs when you use Windows text editors that excrete car-
riage returns in your file. 
execve sees the carriage return on the first line as part of
the interpreter filename. As expected, it fails with ENOENT (no such file or directory).
The shell takes this one at face value and quits. For example:
$ unix2dos ./busted-script
unix2dos: converting file ./busted-script to DOS format ...$ ./busted-script: bad interpreter: No such file or directory...
Thanks to your helpful text editors, it may be hard to figure out what’s wrong
when this happens. Both Vim and Emacs do their best to hide carriage returns fromview. For Vim, you can use the 
-boption to force it to show these Windows waste
products as ^Msequences, for example:
#!/bin/sh^M
The ^M is an abbreviation for Ctrl+M , which is the control key that emits an
ASCII carriage return.6.3 The exec Functions 323
6.3.2 Executable Object Files
Executable object files are object files that have been linked with no unresolved ref-
erences other than dynamic library references. The kernel recognizes only a limitednumber of formats that are allowed to be used with the 
execve system call. There
are some variations by processor architecture, but the ELF3format is common.
Before ELF , the common format for Linux systems was a.out (short for assembly
output ), which is still available as an option today. 
Other formats might be recognized depending on your kernel and architecture.
Processors that don’t have a Memory Management Unit (MMU), for example, usea so-called flat format that the kernel supports. When compiled for the MIPS archi-tecture, the kernel also allows the ECOFF format, which is a variation of theCommon Object File Format that is the predecessor of ELF .
To identify an object file, the kernel looks for a signature in the file, typically
called a magic number. ELF files, for example, have a signature in the first 4 bytes
of the file—specifically, the byte 
0x7f followed by the string 'ELF' . Although all
ELF files have this signature, not all ELF files are executable. Compiled modules(
.ofiles), for example, are not executable, although they are ELF binary object files.
When the kernel encounters an ELF file, it also checks the ELF header in additionto the magic number to verify that it is an executable file before loading and exe-cuting it. Compilers generate object files without execute permission, so 
execve
should never see such a file by accident.
6.3.3 Miscellaneous Binaries
The kernel allows you to extend the way execve handles executables with the
BINFMT_MISC option to the kernel. This option is specified in the kernel build and
allows the superuser to define helper applications that execve can call on to run
programs. This is useful for running Windows applications with wine ,4Java exe-
cutables, or jarfiles.
Obviously, the kernel can’t load and run a Windows executable, but wine exe-
cutes Windows programs much like an interpreter executes a script. Likewise, Javabinaries are executed in a similar fashion by the Java interpreter.
With a kernel built with the 
BINFMT_MISC option, you can tell the kernel how
to recognize a non-native Linux file and what helper program to execute with it. All324 Chapter 6 • Understanding Processes
3.ELF is short for Executable and Linkable Format.
4. www.winehq.org
this is done within the execve system call, so the application calling execve does
not need to know that the program it is about to execute is not a native Linux file.
To start, you need to mount a special procfs entry, as follows:
$ mount binfmt_misc -t binfmt_misc /proc/sys/fs/binfmt_misc
This mounts a directory with two entries:
$ ls -l /proc/sys/fs/binfmt_misc/
total 0--w-------  1 root root 0 Feb 12 15:19 register-rw-r--r--  1 root root 0 Feb 11 20:06 status
The register pseudofile is for writing new rules to the kernel, and the status
entry allows you to enable and disable the kernel’s handling of miscellaneous bina-
ries. You also can query the status by reading this file.
New rules can be added by writing a specially formatted string to the register
pseudofile. The format consists of several tokens separated by colons:
:name:type:offset:magic:mask:interpreter:flags
The name is any name you like, which will show up under the binfmt_misc
directory for later reference. The type field tells the kernel how to use this rule to
recognize the file type. This field can be Mfor magic number or Efor extension. When
you’re using a magic number ( M), the rest of the rule will include a string of bytes
to look for, as well as its location in the file. When you’re using an extension ( E),
the rest of the rule tells the kernel what file extension to look for. This is most oftenused with DOS and Windows executables.
The 
offset , magic , and mask fields are for handling the so-called magic num-
ber. The offset is optional and indicates the first byte of the file where the magic
number resides. The magic field indicates the value that kernel should look for as
a magic number. The mask is optional as well. This is a bit mask that the kernel
applies to the magic number (via a bitwise AND) before testing the value. This allows
a single rule to specify a family of magic numbers. The number and mask may beindicated by raw ASCII characters. If necessary, binary bytes can be used, providedthat they use hexadecimal escape sequences.
You can enable 
wine to handle Windows executables automatically by using the
following rule:
$ echo ':Windows:M::MZ::/usr/bin/wine:' >  /proc/sys/fs/binfmt_misc/register6.3 The exec Functions 325
This uses a magic number of 2 bytes ( M) followed by Z. Because no offset is pro-
vided, the kernel reads the magic number from the beginning of the file. There isno mask, either, which means that the magic number appears as is in the file.
5
This works, provided that your Windows program has executable file permission
and that you use the complete filename; this should be sufficient to recognize thefile and execute 
wine when it is passed to execve . Naturally, you can also double-
click its icon on the desktop to run it as well.
When the rule has been enabled, you see a new file in the binfmt_misc
directory:
$ ls -l /proc/sys/fs/binfmt_misc/
total 0-rw-r--r--  1 root root 0 Feb 12 15:19 Windows--w-------  1 root root 0 Feb 12 15:19 register-rw-r--r--  1 root root 0 Feb 11 20:06 status
The Windows pseudofile tells you that a rule named Windows has been installed.
You can see the details of the rule by reading the file:
$ cat /proc/sys/fs/binfmt_misc//Windows
enabledinterpreter /usr/bin/wineflags:offset 0magic 4d5a
Notice that the magic number you specified as MZis now represented in hexa-
decimal. To disable this rule, you can delete it by writing -1to the file. For example:
$ echo -1 > /proc/sys/fs/binfmt_misc/Windows
The BINFMT_MISC driver also accepts certain flags in the rules, which are docu-
mented in the kernel source.6Be aware that the rules are applied in the reverse order
from the order in which they were set. If you have a file that matches two rules (per-haps one by extension and one by magic number), the rule added more recently isthe one that applies.326 Chapter 6 • Understanding Processes
5. By the way, the wine RPM from Fedora comes with a startup service that handles these settings for you.
6. See Documentation/binfmt_misc.txt in the kernel source.
6.4 Process Synchronization with wait
The underlying assumption when you create a process is that you want to wait
around to find out how things turned out. When a process exits, it sends the par-ent process a 
SIGCHLD signal. The default behavior for the SIGCHLD signal is to
ignore the signal, although the information is not lost. The process status remainsin memory until the parent collects it with one of the 
wait functions listed
below:
pid_t wait(int *status);
pid_t waitpid(pid_t pid, int *status, int options);pid_t wait3(int *status, int options, struct rusage *rusage);pid_t wait4(pid_t pid, int *status, int options, struct rusage *rusage);
As I discussed in Chapter 5, the act of waiting for a child process to terminate is
called reaping the process. When a parent process neglects to wait for a child process
that has terminated, the child process goes into a so-called zombie state, where thekernel keeps around just enough information to inform the parent of the child’s exitstatus. 
In Linux (and UNIX), it does not matter whether the child process has termi-
nated before or after the parent calls 
wait . The wait function behaves the same
way in both cases except that it can block if the child has not terminated whenthe 
wait function is called. If a parent terminates before the child process, the
child process continues normally except that it is adopted by the init process
(pid 1 ). When the child process terminates, init will reap the status. Likewise,
any zombie children left over by the parent when it exits are adopted and reapedby 
init .
Table 6-1 summarizes the features of wait functions. What these functions have
in common is that they all map to the same one or two Linux system calls. Eachone takes a pointer to an 
intvariable to hold the child process’s status, and they all
return the process ID of the process that terminated. This is the basic function ofthe 
wait call. The waitpid and wait4 functions add a process ID to the input so
that the caller can wait explicitly for one of many children to terminate. The wait
and wait3 functions do not take a pidargument. These functions will return as
soon as any child process terminates.6.4 Process Synchronization with wait 327
TABLE 6-1 Summary of wait Functions
Function pid options rusage
wait No No No Returns as soon as a child
process exits or immediately ifno child processes are running.
waitpid Yes Yes No Same as wait except that the
caller can return immediatelywithout blocking, if desired.Can also return when a childprocess is stopped.
wait3 No Yes Yes Supports the same options as
waitpid but takes no pidas an
argument. Returns when anychild process exits or stops asdetermined by the options. Also returns an 
rusage struct
to indicate resource usage bythe child.
wait4 Yes Yes Yes Same as wait3 except that it
takes a pidas an argument.
The waitpid , wait3 , and wait4 functions take an options argument that can
have one of two flags:
•WNOHANG —When set, the function does not block and returns immediately.
The return status is -1if no process was reaped.
•WUNTRACED —When set, the function returns for processes that are in the
stopped state and are not being traced (by a debugger, for example).
Recall when I discussed the getrusage function that the kernel does not provide
data for child processes until they have been reaped. This is where the WUNTRACED
option comes in handy. You can stop a child process explicitly to check on resourceusage, as follows:328 Chapter 6 • Understanding Processes
struct rusage ru;
kill(pid,SIGSTOP); Stop the process.
r = wait4(pid,&status,WUNTRACED,&ru); Wait for it to get status.
if ( r == pid )kill(pid,SIGCONT); Start it again.
Another thing all wait functions have in common is that they can all return -1
to indicate that no process was reaped. The exact reason for the -1value can be
determined by checking the value of errno . The value ECHILD indicates that there
was no unreaped child process to wait for. This can occur if your process has notforked any children successfully. It also can happen when you use the 
WNOHANG
option, which tells the wait function to return immediately, whether or not it has
reaped a process. 
6.5 The Process Footprint
As I discussed in Chapter 5, each process has its own unique virtual-memory space(user space). In addition, processes have several other properties, and they consumeresources other than virtual memory.
When a process is created by the kernel, it is given some initial values for these
properties as well as a virtual-memory space to work in. Part of this is determinedby the kernel, and part is determined by the compiler and libraries. An example isshown in Figure 6-2 for an IA32 with a typical kernel.
In Figure 6-2, the kernel is compiled with a 3G/1G split, which means that the
lower 3GB of virtual addresses belong to user space, while the top 1GB is kernelspace shared by all processes. This division is the same for every process runningunder this kernel.
By now, you know that the process ID uniquely identifies each process on the
system. This is the key that the kernel uses to search its internal tables to find infor-mation about a particular process.
The maps shown in Figure 6-2 are of virtual memory. Not all this memory is
consumed by the process; the diagram shows its intended uses. Memory is not con-
sumed until it is allocated. The stack, for example, can grow to some predefinedmaximum (typically, 1MB or more), but initially, the kernel allocates only a fewpages. As the stack grows, more pages will be allocated. I will look at the stack inmore detail later in this chapter.6.5 The Process Footprint 329
All processes except init start life as a fork of another process—that is, they
don’t start with a clean slate. The memory map in Figure 6-2 is initially populatedby the parent’s mappings. The data is identical to the parent’s until the child processmodifies the memory or calls 
exec . When the child process calls exec , the slate is
cleaned (so to speak), and the map is populated by only program text, data, and astack. If it is a C program, the process typically populates the map with someamount of shared libraries and dynamic storage.330 Chapter 6 • Understanding Processes
FIGURE 6-2 Typical Memory Map on IA32Arrows Indicate Variable Sized 
Buffers That can Grow in the Direction of the ArrowVirtu al Address
Kernel Sp ace
Stack
Heap (large blocks)
Heap (sm all blocks)
Progr am Data
Progr am Text
Shared Libr aries /
Free Memory1GB
Stack SizeStack Bottom
(approx)
Stack Top (m ax)0xffffffff
0xc0000000
Brk
0x08048000
The kernel space story is a little different. When a child process forks, it gets a
copy of the page tables in kernel space. The memory required for this is unique tothe process, so although the process is an identical clone in user space, in kernelspace, it is unique. The child process gets its own set of file descriptors, which ini-tially are clones of the parent’s (more on that later in this chapter).
In general, the process’s footprint includes
• Page tables
• Stack (includes environment variables)• Resident memory• Locked memory
In addition, each process has properties, which coincidentally may be the same
as those of other processes, but they are unique to each process. These include
• Root directory
• Current working directory• File descriptors• Terminal• umask• Signal mask
6.5.1 File Descriptors
File descriptors are plain integers returned by the open system call. Several system
calls take file descriptors as arguments, which they use as indexes into importantkernel structures. In general, the file descriptor is a simple index into a table thatthe kernel manages for each process.
Each process has its own set of file descriptors. When created, a process typi-
cally has three open file descriptors: 0, 1, and 2. These are, respectively, standardinput, standard output, and standard error, known collectively as standard I/O.These are initially inherited from the parent process. One job of a process such as
sshd is to make sure that these three file descriptors are associated with the6.5 The Process Footprint 331
proper pseudoterminal or socket. Before the sshd child process calls exec , it
must close these file descriptors and open new ones for standard I/O.
Every file descriptor has unique properties, such as read or write permission.
These are specified in the open call with the flags argument. For example:
fd = open("foo",O_RDONLY); Open as read-only
fd = open("foo",O_WRONLY); Open as write-only
fd = open("foo",O_RDWR); Open for reading and writing
The flags must agree with the file permissions; otherwise, the open call fails. You
cannot open a file for writing if the current user does not have permission to writeto it, for example. When this happens, the 
open call indicates the failure by return-
ing -1and setting errno to EACCESS (permission denied).
When the open call succeeds, the read/write attributes are enforced exclusively
by file descriptor. If the file permissions are changed during the course of the pro-gram’s execution, it doesn’t matter. The file’s permissions are enforced only duringthe 
open call.
Each file descriptor has unique properties, even when multiple file descriptors
point to the same file. Suppose that a process has two file descriptors open thatpoint to the same file. One was opened with 
O_RDONLY ; the other, with O_WRONLY .
Any attempt to write to the read-only file descriptor will fail with EBADF (bad file
descriptor). Likewise, an attempt to read from the write-only file descriptor will failwith the same error.
Just as file descriptors within a process are unique, file descriptors cannot be
shared between processes. The only exception to this rule is between parent andchild. When a process calls 
fork , all the files that were open when fork was called
are still open in both the parent and the child. Moreover, writes to a file descriptorin the child affect the same file descriptor in the parent, and vice versa. This is illus-trated in Listing 6-1.
LISTING 6-1 fork-file.c: An Example of File Descriptor Usage Following a Fork
1 #include <stdio.h>
2 #include <string.h>3 #include <stdlib.h>4 #include <assert.h>5 #include <unistd.h>6 #include <sys/file.h>7 #include <sys/times.h>332 Chapter 6 • Understanding Processes
8 #include <sys/stat.h>
9 #include <sys/wait.h>
10 11 // Write a NUL terminated string to an fd.12 void writestr(int fd, char *buf)13 {14     int r = write(fd, buf, strlen(buf));15     if (r == -1)16         perror(buf);17 }18 19 // Simple busy-wait loop to throw off our timing.20 void busywait(void)21 {22     clock_t t1 = times(NULL);23     while (times(NULL) - t1 < 2);24 }25 26 int main(int argc, char *argv[])27 {28     int fd = open("thefile.txt",29                   O_CREAT | O_TRUNC | O_RDWR,30                   S_IRWXU | S_IRWXG | S_IRWXO);31     assert(fd != -1);32 33     writestr(fd, "This is the parent.\n");34 35     pid_t pid = fork();36 37     // Both parent and child do a busywait,38     // which should throw off our timing.39     busywait();40 41     if (pid == 0) {42         // Child process43         writestr(fd, "Child write\n");44     }45     else {46         // parent process writes one line and47         // waits for the child48         writestr(fd, "Hi it's me. I'm back.\n");49 50         int status;51         waitpid(pid, &status, 0);52     }53     close(fd);54 55     return 0;
56 }6.5 The Process Footprint 333
This example is a textbook pattern of a race condition because there is no syn-
chronization between parent and child except for the waitpid call. As a result, the
output will vary from one run to the next:
$ cc -o fork-file fork-file.c
$ ./fork-file && cat thefile.txt
This is the parent.Hi it's me. I'm back.
Parent writes before child
Child write
$ ./fork-file && cat thefile.txt
This is the parent.Child write
Child writes before parent
Hi it's me. I'm back.
As you can see, the order of the lines of text varies from one run to the next. This
is illustrated further in Figure 6-3. I’ll show you more about race conditions inChapter 7. The point of this example is to illustrate how parent and child affecteach other’s file descriptors. Notice that both the parent’s and the child’s outputappears in the file, and one does not overwrite the other . This indicates that the child’s
writes caused the parent’s file descriptor to move forward, and vice versa.
Most of the time, this behavior is what you want, but you may not realize it.
Consider any program you have written that uses the 
system library call to fork
and exec a shell command for you. Because your standard input and output file
descriptors are inherited by your child process, it is able to print to the same termi-nal as the parent process, so your program looks like a single coherent applicationto the user and not like Frankenstein’s monster.
All the parent’s file descriptors are inherited in this way across an 
exec . Many times,
the child process has no use for the open files other than the standard I/O. Thinkabout how many programs you have written. How many times have you stopped tothink about how many file descriptors your process has open when it starts?
This is important for a few reasons. One is the fact that a process has a finite
number of file descriptors. This number is fixed when the kernel is built and can-not be increased. Leaving file descriptors open is a bit like leaking memory.Eventually, you will run out. Depending on the nature of your application, you maynever run into a problem.
Another problem is that open file descriptors can cause child processes to hold
on to resources that you want to free up. A device, for example, may allow only oneopen file descriptor at a time. If you 
fork and exec with this device open, it will334 Chapter 6 • Understanding Processes
remain open until the child terminates, which means that simply closing it in the
parent process is not enough to free the resource.
So what’s a programmer to do? You could be paranoid and close all open file
descriptors after you fork. This is called for sometimes but can be tricky. A moreproactive approach is to set the 
FD_CLOEXEC flag on open file descriptors. When
this flag is set, the file descriptor will be closed when exec is called. (By default, file
descriptors are not closed automatically.) You can set this flag only by using the
fcntl call, as follows:
fcntl( fd, FD_SETFD, FD_CLOEXEC );
File descriptors can point to open files, devices, or sockets. Each is copied during
the fork and remains open after exec .7Leaving file descriptors open, particularly6.5 The Process Footprint 335
FIGURE 6-3 Timing Diagram for Listing 6-1First Ru n Seco nd Ru n
No
Synchronization!Parent
Child Child
exit exitthefile.txtParent
open(...,O_CREAT)
forkWrite (“This is the Parent”)
Write (Hi it’s me. I’m back”)
write (”Child  write”)
waitpidthefile.txtopen(...,O_CREAT)
forkWrite (“This is the Parent”)
Write (Hi it’s me. I’m back”)write (”Child  write”)
waitpid
7. One  notable exception is the file descriptor returned by shm_open , which is specified to have
FD_CLOEXEC set.
unused ones, is a problem that can lead to bizarre side effects. There are tools to
help. The /proc file entry for each process contains a subdirectory named fd, which
shows currently open files as symbolic links. For example:
$ ls /proc/self/fd > foo.txt
$ cat foo.txttotal 4lrwx------  1 john john 64 Feb 16 23:08 0 -> /dev/pts/2l-wx------  1 john john 64 Feb 16 23:08 1 -> /home/john/foo.txtlrwx------  1 john john 64 Feb 16 23:08 2 -> /dev/pts/2lr-x------  1 john john 64 Feb 16 23:08 3 -> /proc/26186/fd
Note that the subdirectory self is a symbolic link to the process ID of the cur-
rently running process. The fddirectory under here applies to the lscommand
that is currently running. There is one symbolic link for each open file descriptorinside the process. The names are just the decimal values of the file descriptor num-bers inside the process. Each symbolic link points to an open file or device. 
In this example, you will notice that I redirected the output to a file. The result-
ing output shows that file descriptor 1 (the standard output) for the current processpoints to the file I am redirecting to (
foo.txt ). Also notice that standard input and
standard error file descriptors point to the current pseudoterminal ( /dev/pts/2 ).
Finally, a fourth file descriptor is required by the lscommand to read the directory
you are looking at.
6.5.1.1 The lsof Command
The lsof8command allows you to look at all the open files of all processes in the
system. You need superuser permission to see everything; otherwise, you get to seeonly processes that you own. 
lsof allows you to look at much more than file descrip-
tors. Compare the output between lsof and simply looking at /proc/pid/fd :
$ ls -l /proc/26231/fd
total 4lrwx------  1 root root 64 Feb 17 19:40 0 -> /dev/pts/0lrwx------  1 root root 64 Feb 17 19:40 1 -> /dev/pts/0lrwx------  1 root root 64 Feb 17 19:40 2 -> /dev/pts/0lrwx------  1 root root 64 Feb 17 19:40 255 -> /dev/pts/0336 Chapter 6 • Understanding Processes
8. ftp://lsof.itap.purdue.edu/pub/tools/unix/lsof
$ lsof -p 26231
COMMAND   PID USER   FD   TYPE DEVICE     SIZE   NODE NAMEbash    26231 root  cwd    DIR  253,0     4096 542913 /rootbash    26231 root  rtd    DIR  253,0     4096      2 /bash    26231 root  txt    REG  253,0   686520 415365 /bin/bashbash    26231 root  mem    REG  253,0   126648 608855 /lib/ld-2.3.5.sobash    26231 root  mem    REG  253,0  1489572 608856 /lib/libc-2.3.5.sobash    26231 root  mem    REG  253,0    16244 608859 /lib/libdl...bash    26231 root  mem    REG  253,0    12924 606897 /lib/libtermcap...bash    26231 root  mem    REG    0,0               0 [heap] (stat: ...bash    26231 root  mem    REG  253,0 48501472 801788 /usr/lib/locale...bash    26231 root  mem    REG  253,0    46552 606837 /lib/libnss_fil...bash    26231 root  mem    REG  253,0    22294 862494 /usr/lib/gconv/...bash    26231 root    0u   CHR  136,0               2 /dev/pts/0bash    26231 root    1u   CHR  136,0               2 /dev/pts/0bash    26231 root    2u   CHR  136,0               2 /dev/pts/0bash    26231 root  255u   CHR  136,0               2 /dev/pts/0
Here, you can see that lsof shows much more than just file descriptors. Notice
that in the FDcolumn of the lsof output, several files are listed in addition to those
that have a file descriptor. These files have the abbreviations memand txt, which
indicate that these files have been mmap ped into the process’s space. These files don’t
consume file descriptors, even though they are mapped into memory. Like otherfiles opened with file descriptors, you can delete these files, but they will continueto take up space on the file system until no process has them open. Finally, there arethe abbreviations 
cwdfor the current working directory and rtdfor the root direc-
tory. As I mentioned earlier, each process has its own unique root and current work-ing directory (more on that later in this chapter).
lsof is a rather complicated tool with many options. In addition to the man
page, a QUICKSTART file that comes with it should be installed with the lsof pack-
age. This file contains some very good tutorial information.
6.5.1.2 Limits to File Descriptor Usage
The actual number of open files allowed to each process is determined by the ker-
nel, but you can find out at runtime with the sysconf function as follows:
sysconf(_SC_OPEN_MAX);
You have used the sysconf call before, to determine the system page size and
clock tick. When called with the _SC_OPEN_MAX argument, sysconf returns the
maximum number of files a single process may have open at one time. When the6.5 The Process Footprint 337
process reaches its limit, a subsequent call to open will fail with the error EMFILE
(too many open files). It doesn’t matter whether it’s a file, a device, or a socket; the
limit is on the number of file descriptors.
6.5.2 Stack
The stack is a region of memory in user space used by the process for temporarystorage. The stack gets its name because the behavior is analogous to a stack ofitems. As with a real stack of objects, the last item to be placed on the stack is thefirst one that is taken off. This is sometimes called a LIFO (last in, first out) buffer.Placing data on the stack is called pushing, and removing data from the stack is
called popping.
For programmers, the stack is where local variables are stored inside functions. A
portable C/C++ program never allocates memory from stack directly, instead rely-ing on the compiler to allocate local variables for it. This works nicely with func-tional programming languages, because variables can be pushed on the stack duringthe life of the function, and all the compiler needs to ensure is that the stack pointeris restored to its original location before the function exits. In other words, mem-ory is allocated and freed automatically. C/C++ refers to local variables that arestored on the stack as automatic storage and uses the 
auto keyword to indicate this.
This happens to be the default storage class for local variables, so almost no one everuses the 
auto keyword. The alternative to automatic storage is static storage, which
is identified with the static keyword. Local variables listed as static do not use
the stack for storage but rely on permanent storage allocated by the linker and/orloader.
9
As shown in Figure 6-2 earlier in this chapter, the base (bottom) of the stack is
placed near the highest user-space virtual address, and the stack grows down fromthere. The maximum size of the stack is fixed at process start time. The maximumcan be adjusted for new processes, but when a process starts, the maximum size ofthe stack cannot be changed. If the process consumes too much stack space, theresult is called a stack overflow. Linux responds to a stack overflow with a simple
SIGSEGV to the process.
You cannot know for sure where the stack base will be because of a couple of fea-
tures in the kernel that randomize the location of the stack base (see the “Stack338 Chapter 6 • Understanding Processes
9. The static keyword is notorious for being overloaded (having different meanings in different contexts).
Just remember that in every context, static means permanent storage and limited scope.
Coloring” sidebar). So although you don’t know exactly where the base of the stack
is, you know it will be somewhere near the maximum user-space virtual address.
Stack Coloring
The base address of the stack (the bottom) is not the same in every process because
of a technique called stack coloring. When the stack base is placed at the same virtualaddress every time, processes running the same executable tend to get the same vir-tual addresses for stack variables every time they run.
This creates performance issues on Intel processors with Hyperthreading technol-
ogy. This is an Intel feature that allows a single CPU to behave like two independentCPUs on a single chip. Unlike a true dual-core CPU, which contains two indepen-dent CPUs, a CPU with Hyperthreading shares most of the processor resourcesbetween the two logical cores, including the cache.
When two threads or processes use the same virtual address for the stack, they con-
tend for the same cache lines, causing contention and degrading performance. By ran-domizing the base address of the stack, multiple processes are more likely to usedifferent cache lines and avoid thrashing.
Although stack coloring was not intended to be a security feature, it does provide
a modest security enhancement. Some buffer overflow attacks rely on the fact that vir-tual addresses will be the same from one run to the next. Randomizing the stack base
makes it less likely (but not impossible) that such an attack will succeed.
Stack size limits are determined by the setrlimit system call, which also can be
accessed via the Bash built-in ulimit command. The new limit is enforced for all
children of the current process.
6.5.3 Resident and Locked Memory
The use of virtual memory means that parts of a process may not be stored in RAM.These parts may be stored on the swap disk or not stored at all. Memory that hasnot been initialized or accessed, for example, does not need to be allocated physi-cally. Such pages will not be allocated until a memory access causes a page fault,which causes the kernel to allocate them.
Part of a process’s footprint includes the amount of RAM that it consumes,
which is characterized by the amount of resident memory, which refers specifically
to the parts of a process’s memory that are stored in RAM. It does not include partsof the process that are in swap or are not stored.6.5 The Process Footprint 339
A subset of the resident memory is locked memory, which refers to any virtual
memory that has been explicitly locked into RAM by the process. A locked pagecannot be swapped and is always resident in RAM. A process locks a page to pre-vent the latency that can occur due to swapping. Locked pages mean that less RAMis available to other processes. For this reason, only processes with root privileges areallowed to lock pages. 
6.6 Setting Process Limits
The setrlimit function can be used to enforce limits on the resources a process
can consume. You can examine the current limits by using the getrlimit function
call. I introduced these functions in Chapter 5. These are defined as follows:
int setrlimit(int resource, const struct rlimit *rlim);
int getrlimit(int resource, struct rlimit *rlim);
Recall that the rlimit structure has a softand hard limit. The hard limit typically
is set at system startup and usually is not modified. When the hard limit is reduced,it cannot be raised for that process. The soft limit can be raised and lowered asdesired but cannot exceed the hard limit. The 
rlimit structure is defined as follows:
struct rlimit {
rlim_t rlim_cur;  /* Soft limit */rlim_t rlim_max;  /* Hard limit (ceiling for rlim_cur) */
};
Note that a process can set limits only for itself; there is no API to change the
limits of a different process. A typical pattern for setting resource usage is to do sofollowing a 
fork in the child process before an execve . For example:
pid_t pid = fork();
if ( pid == 0 ) {
struct rlimit limits = {...};getrlimit( RLIMIT_..., &limits);
Modify soft limit.
setrlimit( RLIMIT_..., &limits );exec( ... );
}
The caller indicates the resource to be limited in the first argument. The
resources that can be controlled include many that apply to the current user, notjust the current process. A complete list is provided in Table 6-2.340 Chapter 6 • Understanding Processes
TABLE 6-2 Resource Flags Used by setrlimit and getrlimit
Resource Description
RLIMIT_AS Limits the amount of virtual memory (address space) a process
may consume. This applies to both stack and heap. When the size exceeds the soft limit, dynamic allocations (includinganonymous 
mmap s) will fail with ENOMEM . If a stack allocation
causes the limit to be exceeded, the process will be killed with
SIGSEGV .
RLIMIT_CORE Limits the size of a core file. Setting this to zero disables thegeneration of core files, which may be desirable for securityreasons but undesirable during software development.
RLIMIT_CPU Limits the amount of CPU time a process may consume.Input is in seconds. When the soft limit expires, the processreceives 
SIGXCPU once per second until the hard limit, when
it receives SIGKILL .
RLIMIT_DATA Maximum size of the data segment. This affects calls to brk
and sbrk , which (in theory) means that dynamic memory
allocations will fail when the soft limit is reached. The errno
when this occurs is ENOMEM . glibc uses mmap when brkfails,
effectively neutering this feature.
RLIMIT_FSIZE Maximum size of an individual file a process may create.When the process exceeds the soft limit, it receives 
SIGXFSZ ,
and the write and truncate system calls fail with EFBIG .
RLIMIT_LOCKS Limits the number of locks a process may have at one time;not used in Linux 2.6.
RLIMIT_MEMLOCK Sets the maximum number of bytes a process may havelocked at one time; can be overridden by privileged users.
RLIMIT_NOFILE Limits the number of file descriptors a process may haveopen at one time. 
RLIMIT_NPROC Maximum number of processes that may be created by thereal user ID of the calling process. When the soft limit isreached, the 
fork call fails with errno set to EAGAIN .
RLIMIT_RSS Limits the resident set size of a process, which is the size of all
resident memory of the process; allegedly enforced in Linuxbut not observed in 2.6.14.6.6 Setting Process Limits 341
continues
TABLE 6-2 Continued
Resource Description
RLIMIT_SIGPENDING Limits the number of signals that may be queued (pending)
for the given process. See Chapter 7.
RLIMIT_STACK Sets the maximum size of the stack allowed for the process.
The behavior of the process when it attempts to exceed one of these resources
depends on the resource being limited. If you are limiting stack, for example, theprocess will abort with a 
SIGSEGV when you try to allocate too much automatic
storage. On the other hand, if you are limiting the number of file descriptors, theprocess will most likely fail in an 
open call by returning -1.
Checking the limits via getrlimit is one way to ensure robust operation. In
theory, this would work very well with the getrusage system call. Unfortunately,
this system call in Linux provides very little information about resource usage to theapplication. What little is provided comes after an exit call, making it largely unus-able at runtime.
You may be wondering why anyone would want to impose limits on processes.
There are several reasons. One might be to prevent users from crippling your sys-tem by allocating and using too much memory. A malicious (or poorly written)process can bring a system to a crawl simply by allocating lots of memory. Excessivepage faulting caused by the process can cause other processes to experience excessivelatency and slow everything. Another reason for limits is to disable core files, whichmay contain passwords or other sensitive information. Think that’s a stretch?Listing 6-2 demonstrates a trivial example of how even an encrypted password canbe exposed in a core file.
LISTING 6-2 insecure.c: Password Exposed in a Core File
#include <stdio.h>
#include <stdlib.h>
// Encrypted message, might take years to crack.
unsigned char secret_message[] = {
0x8f, 0x9e, 0x8c, 0x8c, 0x88, 0x90, 0x8d, 0x9b, 0xc2, 0x8b, 0x90,0x8f, 0xdf, 0x8c, 0x9a, 0x9c, 0x8d, 0x9a, 0x8b
};
int main(int argc, char *argv[])342 Chapter 6 • Understanding Processes
{
int i;for (i = 0; i < sizeof(secret_message); i++) {
// Okay maybe not years, but you get the idea.secret_message[i] ^= 0xff;
}abort();
}
The password might be protected by encryption, but if the program decrypts the
password into memory, it can be visible when the process dumps a core file. The datais only as secure as the core image. If the core gets dumped to disk, it can be read byanyone with permission to read it. For this reason, the kernel creates core files withrestricted permissions so that only the owner can read them. This is only a modestdefense against unauthorized users gaining access to sensitive information.Sometimes, the owner of the executable file may be unauthorized to see the core file.
Consider Listing 6-2, which contains a secret message. It’s simply an ASCII string
with all the bits toggled. This is a pitiful form of encryption, but it is enough to hidethe message from a casual observer. The first thing this program does is decrypt thesecret message so that it resides in memory. The subsequent abort causes the core tobe written to disk, now containing the unencrypted message.
$ ulimit -c unlimited Many distributions disable core files for your protection.
$ ./insecure
Aborted (core dumped)$ strings ./core | grep password
Just looking . . .
password=top secret Look what I found!
This is a fairly contrived example, but it illustrates how easy it can be to steal pri-
vate information from insecure code. Most distributions disable core file generationby default for this reason.
6.7 Processes and procfs
The procfs file system is a pseudo file system that presents information to the user
about the system and individual processes. Some operating systems are notoriousfor having numerous arcane system calls to provide the information in 
procfs .
With procfs in Linux, however, the only system calls you need are open , close ,
read , and write . Beware—no standards cover the contents of procfs . In theory,
a program that reads from procfs may work fine on one kernel and fail on another.6.7 Processes and procfs 343
By convention, procfs is mounted on the /proc directory. In this file system,
there is a tree of system and process information. Much of what you find here is notavailable via a system call, which is often why it is here.
procfs was introduced in UNIX to make debugging easier. It is vital to support
process monitoring commands in user space, such as ps. Since Linux adopted it, it
has suffered from quite a bit of feature creep.
procfs has two basic missions in Linux today. One is the same as in UNIX,
which is to provide information about each process running in the system; the otheris to provide information about the system as a whole. 
For the most part, the information in 
procfs is in ASCII text.
Each process has a subdirectory under /proc named after its process ID, so
process 123 has a directory named /proc/123 , which exists for the lifetime of the
process. In addition, there is a directory named /proc/self , which is a link to the
directory of the currently running process.
Inside each subdirectory, you will find much of the information I have discussed
about the process footprint, and then some. Some of this information is quite use-ful, and some is esoteric. It never hurts to explore, though.
The 
/proc directory is an excellent debugging tool, allowing you to get infor-
mation about system behavior with no further tools. The actual contents of thedirectory can vary depending on the options compiled into the kernel, as well asfrom one release to the next. Table 6-3 lists some of the most common entries andtheir uses.
TABLE 6-3 Sample Files in the /proc/PID Directory
File Description Format
auxv Binary
cmdline ASCII
cwd N/A
environ ASCII The process’s environment packed into an ASCII string
delimited by ASCII NULs. Each token is represented as it
is in the envp vector passed to C programs (for example,
PATH=xyz:abc ).A symbolic link to the current working directory of theprocess.An ASCII string delimited with ASCII NULs (0) repre-
senting the argv vector that a C program would see.A vector of values used by tools like gdb, containing
information about the system.344 Chapter 6 • Understanding Processes
File Description Format
exe N/A
fd N/A
maps ASCII text
mem Binary
mounts Text
oom_adj Allows the user to adjust the oom_score (see below). Text
oom_score Text
root N/A
smaps ASCII text
stat ASCII text
statm ASCII text
status ASCII text
wchan Text Indicates the kernel function in which the process is
blocking (if applicable).Same information as stat in a more human-readable form.A summary of process memory usage, most of which isin 
stat .A one-line, scanf -friendly representation of the process
status, used by the pscommand.Detailed list of mappings of the shared libraries used by this process. Unlike maps, this includes more detailsabout the mappings, including the amount of clean anddirty pages.A symbolic link to the root file system of the process;normally points to 
/but will be different if the process
has called chroot .The process’s “badness” as determined by the OOM (outof memory) killer. When the system runs out of memory,processes with high scores are killed first.A list of mounted file systems, like /etc/mtab . This is
the same for all processes.A file that allows other processes to access this process’suser space; used by programs like 
gdb.A textual representation of the user space memorymapped by the process. For kernel threads, this file isempty, because kernel threads have no user space.A directory containing a symbolic link for each filedescriptor open by the process. These links includeanything open with a file descriptor, including plain files,sockets, and pipes.A symbolic link to the file containing the code for thisprocess. 6.7 Processes and procfs 345
Technically, procfs is not required to run a Linux system, although many tools
depend on the contents of /proc . You might be surprised by what doesn’t work if
you try to run a system without it.
6.8 Tools for Managing Processes
The procps project10contains many tools for mining the procfs file system and
is included with most distributions. Some of the tools included with the package,such as the 
pscommand, are part of the POSIX standard. Other tools are non-
standard yet very useful. Often, it is more convenient to use these commands thanto plod through the 
/proc directories yourself. Before you write a script to go
through the /proc directories, check here first.
6.8.1 Displaying Process Information with ps
The pscommand from the procps package implements the features specified by
POSIX standard, as well as several other standards. While these standards are con-verging, they each have their own argument conventions, which cannot bechanged easily without affecting a large body of client code (scripts). As a result,you will find that 
psusually has at least two ways to do the same thing, two names
for each field, and so on. Not surprisingly, the manpage for the pscommand is a
bit dense.
Typing pswith no arguments shows you only processes that are owned by the
current user and attached to the current terminal. This generally includes all theprocesses that were started in the currently running shell, although it has nothingto do with the shell’s concept of background and foreground processes. Recall thatthe shell keeps track of processes as jobsrunning in the foreground or background.
T wo different shells can use the same terminal, but the jobs listed include only thecurrently running shell, whereas the default 
psoutput includes processes started
from both shells. For example:
$ tty What is the name of our terminal?
/dev/pts/1
$ ps346 Chapter 6 • Understanding Processes
10. http://procps.sourceforge.net
PID TTY          TIME CMD
21563 pts/1    00:00:00 bash21589 pts/1    00:00:00 ps
Only two processes are running on this terminal.
$ sleep 1000 &[1] 21590
Background job 1, process ID 21590
$ jobs -l[1]+ 21590 Running  sleep 1000 &$ ps
PID TTY          TIME CMD
21563 pts/1    00:00:00 bash21590 pts/1    00:00:00 sleep21591 pts/1    00:00:00 ps
New process is listed by the ps and jobs commands.
$ bash Start a new shell in the same terminal.
$ jobs The list of jobs is now empty because it’s a new shell.
$ ps ps shows processes from both shells.
PID TTY          TIME CMD
21563 pts/1    00:00:00 bash21590 pts/1    00:00:00 sleep21592 pts/1    00:00:00 bash21609 pts/1    00:00:00 ps
Most often, you want to see more information than the default output from ps.
Either you are interested in what’s happening outside your terminal, or you want tosee more information about the state of your process. The 
-loption is a good place
to start. This provides a longer list of process properties. For example:
$ ps -l
F S   UID   PID  PPID  C PRI  NI ADDR SZ WCHAN  TTY        TIME CMD0 S   500 21563 21562  0  75   0 -  1128 wait   pts/1  00:00:00 bash0 S   500 21590 21563  0  76   0 -   974 -      pts/1  00:00:00 sleep0 R   500 21623 21563 96  85   0 -  1082 -      pts/1  00:00:26 cruncher0 R   500 21626 21563  0  75   0 -  1109 -      pts/1  00:00:00 ps
The field headings provide a brief, sometimes cryptic description of the columns
in the output. Reading from left to right, the column descriptions from the defaultlong output are listed in Table 6-4.
Chances are that this output has more than you’re interested in or is missing
something you want. Fortunately, 
psallows you to customize your output to show
you exactly what you want to know.6.8 Tools for Managing Processes 347
TABLE 6-4 Output Columns of the ps Long Format
Column Header Description
F Flags (see sched.h )
S The process state:
R—running
S—sleeping (interruptible)
T—stopped
D—sleeping (uninterruptible)
Z—terminated but not reaped
UID Effective user ID of the process
PID Process ID
PPID Parent process ID
C CPU utilization percentage
PRI Process’s priority
NI Process’s nice value
ADDR Unused in Linux
SZ Approximate virtual-memory size of the process, in pages
WCHAN System call or kernel function that is causing the process to
sleep (if any)
TTY Controlling terminal
TIME Amount of CPU time consumed by the process
CMD Command name as listed in /proc/stat (truncated to 
15 characters)348 Chapter 6 • Understanding Processes
6.8.2 Advanced Process Information Using Formats
The procps package is intended for use in operating systems besides Linux, and
nowhere is this more apparent than the pscommand. In particular, the formatting
fields used by the -ooption are a motley bunch of mnemonics derived from various
flavors of UNIX over the years. Many of these are synonyms, due to the fact thateach vendor happened to choose a slightly different name. SGI came up with onemnemonic, Sun came up with another, and Hewlett-Packard with yet another. Asof 
procps version 3.2.6, the pscommand recognizes 236 different formatting
options.11Only a few are documented in the manpage.
I have used this feature in some earlier examples; now I’ll show it in detail. To
illustrate the formatting options at work, you can use the following command to seehow much time your process has been running and how much CPU time it hasconsumed:
$ sleep 10000 &
[1] 23849$ ps -o etime,time -p 23849
ELAPSED     TIME
00:06 00:00:00
The etime format option shows the elapsed time since the process began, 
and the time format option shows the CPU time consumed by the process in
seconds. 
Table 6-5 shows a listing of the most useful formats. In many instances, there
are several formats to tell you the same information. Sometimes, the output for-mat is slightly different; at other times, there are multiple aliases for the identicalformat.
If you are using this feature in a script that will run in multiple operating sys-
tems, beware: Not all these formats are supported in all (non-
procps ) versions of
ps. The procps source code complains about many ambiguities in the standards
that apply.6.8 Tools for Managing Processes 349
11. Of these, 86 options produce no useful data!
TABLE 6-5 Format Options Supported by ps
Format Description
Time related The time and date when the process
started. Each format producesslightly different output. Someformats includes the date; someinclude the seconds; all include thehour and minutes.
etime Elapsed time from the start of theprocess.
Cumulative CPU time consumed by
the process in hours, minutes, andseconds. 
bsdtime is minutes and
seconds only.
Memory related size Approximate total swappable process
memory; includes stack and heap.
m_size Virtual-memory size of the process asreported by the kernel; not exactlythe same as 
size .
pmem, %mem The process’s resident memory,expressed as a percentage of totalphysical memory in the system.
The number of major page faults as
defined by the kernel.
minflt, min_flt The number of minor page faults as
defined by the kernel.
sz, vsz, vsize Total virtual memory used by theprocess. 
szis reported in pages; vsz
and vsize are reported in K.
rss, rssize, rsz Total process memory resident inRAM, expressed in K.
lim Process limit on rssset by setrlimit .
stackp The lowest address of allocated stack;fixed until more stack is allocated.majflt, maj_flt,
pageintime, cputime,atime, bsdtimestart, start_time,lstart, bsdstart350 Chapter 6 • Understanding Processes
Format Description
Scheduler related cpu On SMP systems, identifies the CPU
that the processes is executing on; prints 
-for uniprocessor systems.
Indicate the scheduler class of the
process as a number or a mnemonic,defined as follows:
TS (0) —Normal, time sliced
FF (1) —Real time, FIFO
RR (2) —Real time, round robin
cp, %cpu, c, util CPU utilization of the process,expressed as a percentage. 
pri Priority listed as a positive integer, withhigher values indicating higher priority(0–39 normal, 41–99 real time; priority40 is unused by Linux). These are thesame values you would see in the kernel.
priority Priority using lower numbers to indicatehigher priority (39–0 normal, -1– -99real time). These are the same valuesfound in 
/proc/PID/stat /.
opri, intpri Inverted version of priority format 
(-39– -0 normal, 1–99 real time).Positive numbers are used for real-timeprocesses, and negative numbers arenormal processes.
s, state, stat Process state. state is D, R, S, T, orZ.
stat adds a character for more infor-
mation.
tid, spid, lwp For multithreaded processes, indicates thethread ID of multiple threads. Threadsare shown only with the 
-Toption.
wchan, wname Name of system call or kernel functioncausing the process to block; 
-if process
is not sleeping.policy, class,
cls, sched6.8 Tools for Managing Processes 351
6.8.3 Finding Processes by Name with ps and pgrep
Occasionally, you need to find what’s going on with a command you launched from
another terminal or perhaps during startup. You might know the command name,but you don’t know the process ID. The typical pattern is
$ ps -ef | grep myprogram
It’s so common that it was added to the pscommand with the -Coption—a
feature I have used repeatedly in the examples in this book. When you use thisoption, the command name you provide must match exactly. Then the 
pscommand
will apply to all processes that match. When you don’t know the exact command (ordon’t want to type it), the 
pgrep command is a nice alternative. As you might expect,
the argument to pgrep is a regular expression that matches anywhere in the string,
just like grep . Unlike the pscommand, however, the default output consists of
unadorned pids, which makes it suitable for generating a list of pids that can be
used by other programs. For example:
$ ./myproc &
[1] 5357$ cat /proc/$(pgrep myproc)/stat # 
Embed pidof myproc in the /proc filename.
5357 (myproc) T 3681 5357 3681 34817 ...
pgrep has some peculiar behavior for processes with names longer than 16 char-
acters due to the fact that it allocates only 16 characters to store the commandname. This can be dangerous when used in combination with commands like 
kill
(something that is not advisable anyway). For more examples, see the discussion of
skill and pkill later in this chapter. These commands have the same issue, but
for a different reason.
Other useful options for pgrep include the -xoption, which forces an exact
match, and the -loption, which provides some additional information similar to
the default output from ps. This saves you the trouble of having to send the out-
put of pgrep to the pscommand.
Finally, pgrep has several options that allow you to filter output based on termi-
nal name, user ID, group ID, and so on. One very useful option is the -noption,
which shows you only the most recently executed command that matches. If you352 Chapter 6 • Understanding Processes
want to know what the most recently spawned telnetd process is, you could use
the following:
$ pgrep -n telnetd
The telnetd process forks a new copy for each terminal that logs in. This tells
you which process is the most recent.
A similar but less flexible program is pidof , which also takes a command name
as an argument. This is not part of procps but is part of the SysVinit package,
which is used for startup scripts. This command lives in /sbin and is intended for
use by startup scripts. Not every distribution uses the SysVinit package, so it’s
probably wise to avoid it if portability is a concern.
6.8.4 Watching Process Memory Usage with pmap
I have used the pmap command before to look at processes. This information is con-
tained in /proc/PID/maps , which shows a map of a process’s virtual memory. For
example:
$ cat &
[1] 3989$ cat /proc/3989/maps009db000-009f0000 r-xp 00000000 fd:00 773010     /lib/ld-2.3.3.so009f0000-009f1000 r-xp 00014000 fd:00 773010     /lib/ld-2.3.3.so009f1000-009f2000 rwxp 00015000 fd:00 773010     /lib/ld-2.3.3.so009f4000-00b15000 r-xp 00000000 fd:00 773011     /lib/tls/libc-2.3.3.so00b15000-00b17000 r-xp 00120000 fd:00 773011     /lib/tls/libc-2.3.3.so00b17000-00b19000 rwxp 00122000 fd:00 773011     /lib/tls/libc-2.3.3.so00b19000-00b1b000 rwxp 00b19000 00:00 008048000-0804c000 r-xp 00000000 fd:00 4702239    /bin/cat0804c000-0804d000 rwxp 00003000 fd:00 4702239    /bin/cat0804d000-0806e000 rwxp 0804d000 00:00 0          [heap]b7d1e000-b7f1e000 r-xp 00000000 fd:00 1232413    /usr/../locale-archiveb7f1e000-b7f20000 rwxp b7f1e000 00:00 0bfd1b000-bfd31000 rw-p bfd1b000 00:00 0          [stack]ffffe000-fffff000 ---p 00000000 00:00 0          [vdso]
This is a little hard to read, but the format is described fully in the proc(5) man
page. Each line is a range of virtual memory. The range of addresses is shown on the
left in the first column. The second column shows the permissions and an sor pto6.8 Tools for Managing Processes 353
indicate shared and private mappings. The next column indicates the device offset,
such as would be used for an equivalent mmap call. For anonymous memory maps,
this is the same as the virtual address. If the virtual memory is mapped to a file ordevice, the subsequent fields indicate the device identifier in major/minor format,followed by the inode
12and finally the file/device name.
The equivalent map produced by the pmap command is a bit more user friendly:
$ pmap 3989
3989:   cat009db000     84K r-x--  /lib/ld-2.3.3.so009f0000      4K r-x--  /lib/ld-2.3.3.so009f1000      4K rwx--  /lib/ld-2.3.3.so009f4000   1156K r-x--  /lib/tls/libc-2.3.3.so00b15000      8K r-x--  /lib/tls/libc-2.3.3.so00b17000      8K rwx--  /lib/tls/libc-2.3.3.so00b19000      8K rwx--    [ anon ]08048000     16K r-x--  /bin/cat0804c000      4K rwx--  /bin/cat0804d000    132K rwx--    [ anon ]b7d1e000   2048K r-x--  /usr/lib/locale/locale-archiveb7f1e000      8K rwx--    [ anon ]bfd1b000     88K rw---    [ stack ]ffffe000      4K -----    [ anon ]
total     3572K
This tells you more of what you probably want to know, such as the total amount
of memory mapped. Each region is presented only with the base address and size,along with the permissions and device name. For nondevice mappings, an appro-priate substitute is provided. If you want to see more device information from
/proc/PID/maps , use the -doption.
6.8.5 Sending Signals to Processes by Name
The skill and pkill commands function like the kill command, except that
they try to match process names instead of a process ID. T reat these commands likeloaded weapons. Use with caution.
skill takes a process name as an argument and looks only for exact matches. It
uses the contents of /proc/PID/stat to match. Linux stores only the first 15 char-
acters of the command name in /proc/PID/stat ,13so if you are looking for a
process with an unusually long command name, you won’t find it with skill .354 Chapter 6 • Understanding Processes
12. I discuss inodes in more detail in Chapter 7.
13. Defined by TASK_COMM_LEN in the kernel.
Things can get weird if you have commands that are exactly 15 characters and com-
mands that are longer with the same first 15 characters. For example:
$ ./image_generator & Exactly 15 characters
[1] ...
$ ./image_generator1 & Exactly 16 characters
[2] ...$ skill image_generator
Kills both!
[1] Terminated ./image_generator[2] Terminated ./image_generator1
pkill works on a similar principle except that it uses a regular expression for
the process name. This is even more dangerous, because it will match anywhere in the command string. Unlike 
skill , pkill uses /proc/cmdline , which stores
the entire argv vector as -is. If not careful, the unwary user is likely to kill unin-
tended processes. For example:
$ ./proc_abc &
[1] ...$ ./abc_proc &[2] ...
$ pkill abc
Kills both!
$ pkill ^abc Kills only abc_proc.
$ pkill abc\$ Kills only proc_abc. ($ is escaped with a backslash.)
Because the argument is a regular expression, the first command in the previous
example kills both processes because the term abcis found in both commands. To
be more specific, you could specify the entire command, or you could use the reg-ular-expression syntax to be more precise. That is what the two subsequent com-mands do. The regular expression 
^abc indicates that the command must begin
with the letters abc, which prevents it from killing proc_abc . Similarly, the regu-
lar expression abc$ indicates that the command must end with the letters abc,
which prevents it from killing abc_proc .
6.9 Summary
This chapter focused on the user-space aspects of processes. I took a detailed lookat how 
exec occurs and some of the tricks that Linux uses to execute various kinds
of code. I also illustrated a few pitfalls associated with exec .
I looked in detail at the various resources that processes consume and how to look
for them. Finally, I looked at some of the tools you can use to manage processesfrom the shell.6.9 Summary 355
6.9.1 System Calls and APIs Used in This Chapter
•execve —Linux system call to initialize a process’s user space and execute pro-
gram code. POSIX defines several functions with similar signatures, but they alluse this system call. This is usually called after a call to 
fork . Refer to exec(3) .
•fcntl —used to set flags on file descriptors. I used this function to set the
FD_CLOEXEC flag.
•fork —system call to create a clone of the currently running process. This is
the first step in creating a new process.
•kill —system call to send a signal to a running process.
•setrlimit , getrlimit —functions to test and set process resource limits.
•sysconf —returns system constants that can be used at runtime.
•wait , waitpid , wait3 , wait4 —allow a parent to synchronize with a child
process.
6.9.2 Tools Used in This Chapter
•pgrep —finds processes that match a regular expression
•pmap —prints a process’s memory map
•ps—the well-known process status command
•ulimit —Bash built-in function to test and set process resource limits
6.9.3 Online Resources
• http://procps.sourceforge.net—the home page for the procps project, which
provides many useful tools for tracking process and system resources
• www.unix.org—publishes the Single UNIX Specification• www.unix.org and www.opengroup.org—publish the POSIX standard (IEEE
Standard 1003.2) and many others (registration required)356 Chapter 6 • Understanding Processes
7.1 Introduction
Because each process has its own separate address space, communication between
processes is not always easy. There are several techniques for interprocess commu-nication (IPC), each with benefits and drawbacks.
A central problem that arises in applications with multiple processes or threads is
the race condition. A race condition describes any situation in which multiple
processes (or threads) attempt to modify the same data at the same time. Withoutsynchronization, there is no guarantee that one process isn’t going to clobber theoutput of another. Perhaps more important, race conditions make the outputunpredictable. 
In general, race conditions are caused by a lack of synchronization, which can
result in output that changes based on system load or other factors. You saw somesimple examples in Chapter 6, where the text from parent and child processes var-ied from one run to the next. Race conditions are almost never this obvious. Manytimes, a race condition may not exhibit itself until after the code is released.
7
357Communication 
between Processes
IPC is vital to preventing race conditions. When you use it improperly, however,
you may introduce race conditions rather than prevent them. This chapter will helpyou understand how to use IPC properly and will show you some tools you can useto debug processes using IPC.
7.2 IPC Using Plain Files
Plain files are a primitive but effective way to communicate between processes.When two processes that don’t execute simultaneously must communicate, a file isperhaps your only choice for IPC. An example of this is the C compiler. When youcompile a program with gcc, for example, it generates an assembly-language file,which is passed to the assembler. The intermediate file is deleted after assembly, soyou normally don’t see it, but you can see it for yourself with the 
-voption to gcc:
$ gcc -v -c hello.c
....../cc1 ... hello.c ... -o /tmp/ccPrPSPE.s
Compiler generates a temporary file.
...as -V -Qy -o hello.o /tmp/ccPrPSPE.s
Assembler uses temporary file for input.
This works for the C compiler because it must work in a serial fashion—that is,
the compiler must finish before the assembler can start. So although these are dif-ferent processes, they don’t run simultaneously.
You can use files for IPC between processes that are running simultaneously, but
the opportunity for race conditions looms. When two processes communicate viafile, there is no guarantee that one isn’t writing while the other is reading, or viceversa. That means you can read a message that is half written or read an old mes-sage when you were expecting a new one. One such naïve—and seriously flawed—implementation is shown in Listing 7-1.
LISTING 7-1 file-ipc-naive.c: Naïve IPC Using a File
1 #include <stdio.h>
2 #include <string.h>3 #include <stdlib.h>4 #include <unistd.h>5 #include <sched.h>6 #include <sys/wait.h>7 8 // This is the file parent and child will use for IPC.9 const char *filename = “messagebuf.dat";
10 11 void error_out(const char *msg)12 {13     perror(msg);358 Chapter 7 • Communication between Processes
14     exit(EXIT_FAILURE);
15 }16 17 void child(void)18 {19     // Child reads from the file.20     FILE *fp = fopen(filename, “r");21     if (fp == NULL)22         error_out(“child:fopen");23 24     // Read from the file25     char buf[32];26     fread(buf, sizeof(buf), 1, fp);27 28     printf(“child read %s\n", buf);29     fclose(fp);30 }31 32 void parent(void)33 {34     // Parent creates the file35     FILE *fp = fopen(filename, “w");36     if (fp == NULL)37         error_out(“parent:fopen");38 39     // Write a message to the file.40     fprintf(fp, “Hello World\n");41     fclose(fp);42 }43 44 int main(int argc, char *argv[])45 {46     pid_t pid = fork();47 48     if (pid == 0) {49         child();50     }51     else {52         parent();53 54         // Wait for the child to finish.55         int status = 0;56         int r = wait(&status);57         if (r == -1)58             error_out(“parent:wait");59 60         // Child returns non-zero status on failure.61         printf(“child status=%d\n", WEXITSTATUS(status));62         unlink(filename);63     }64     exit(0);
65 }7.2 IPC Using Plain Files 359
Listing 7-1 runs with no synchronization whatsoever, so the output is unpre-
dictable for the most part, but on my machine it fails almost every time:
$ ./file-ipc-naive
child:fopen: No such file or directorychild status=1
Can you spot the race condition? You may be inclined to use strace to find it,
but you could be in for a surprise. Again on my machine, I observed the following:
$ strace -o strace.out -f ./file-ipc-naive
child read Hello World The #$!% thing works now!
child status=0
Monitoring with strace interfered with the timing enough to cause the pro-
gram to produce the expected result. This is where a less experienced programmeris likely to put a 
sleep call with a comment like “Don’t remove this!” That’s a sure
sign that the programmer encountered a race condition and didn’t know how todeal with it. Also, it’s usually very inefficient.
So how do you fix this code? Well, one thing I won’t show you is where to put
the 
sleep calls. There are several elegant solutions to the problem, but basically,
you need to synchronize access to the file between the parent and the childprocesses. One simple way to do this is by using the 
lockf function. An example
of this is shown in Listing 7-2.
LISTING 7-2 file-ipc-better.c: IPC Using Files and Synchronization with lockf
1 #include <stdio.h>
2 #include <string.h>3 #include <stdlib.h>4 #include <unistd.h>5 #include <sched.h>6 #include <sys/wait.h>7 #include <sys/file.h>8 #include <sys/stat.h>9 
10 const char *filename = "messagebuf.dat";1112 void error_out(const char *msg)13 {14     perror(msg);15     exit(EXIT_FAILURE);16 }17 18 void child(void)19 {360 Chapter 7 • Communication between Processes
20     // With mandatory locks we block here until the parent unlocks the file.  
21     FILE *fp = fopen(filename, "r+");22     if (fp == NULL)23         error_out("child:fopen");24 25     // With advisory locks we block here until the parent unlocks the file.26     int r = lockf(fileno(fp), F_LOCK, 0);27     if (r == -1) 28         error_out("parent:lockf");2930     // Now we know the data is valid.31     char buf[32];32     fread(buf, sizeof(buf), 1, fp);33     if (ferror(fp))34         error_out("fread");35 36     printf("child read '%s'\n", buf);37 }38 39 void parent(FILE * fp)40 {41     // Write our PID to the file.42     fprintf(fp, "%#x", getpid());43 44     // Flush the user-space buffers to the45     // filesystem before unlocking.46     fflush(fp);47 48     // As soon as the data on the filesystem is up-to-date49     // we can unlock the file and let the child read it.50     int r = lockf(fileno(fp), F_ULOCK, 0);51     if (r == -1)52         error_out("lockf:F_ULOCK");53 54     fclose(fp);55 }56 57 int main(int argc, char *argv[])58 {59     int r;60 61     // Create the file before the fork62     int fd = 63         open(filename, O_CREAT | O_TRUNC | O_RDWR,64              0666 /*|S_ISGID */ );65        66     FILE *fp = fdopen(fd, "r+");67     if (fp == NULL)68         error_out("parent:fopen");69 7.2 IPC Using Plain Files 361
continues
70     // Put an exclusive lock on the file.
71     r = lockf(fileno(fp), F_LOCK, 0);72     if (r == -1)73         error_out("parent:lockf");74 75     // Now we fork with the file locked.76     pid_t pid = fork();77 78     if (pid == 0) {79         // Run the child-only code.80         child();81         exit(0);82     }83     else {84         // Run the parent-only code and wait for the child to finish.85         int status = 0;86         parent(fp);87         wait(&status);88 89         // Child returns non-zero status on failure.90         printf("child status=%d\n", WEXITSTATUS(status));91     }92     unlink(filename);93     exit(0);
94 }
A Note about the Examples
Notice that these examples use fopen instead of open , but the fopen call is imple-
mented on top of open . So underneath, there is still a file descriptor. See fileno(3)
for more information.
The new, improved version of the program creates the file before forking and
then locks it using lockf . When the child opens the file, it locks the file before
reading from it, which causes it to block until the parent unlocks the file. By keep-ing the file locked, the parent can ensure that the contents are valid before the childreads from it. The parent unlocks the file after it ensures that the file has been writ-ten. Now you have a robust implementation that is free of race conditions.
7.2.1 File Locking
There are two kinds of locks: advisory and mandatory. Advisory locks work when
every process calls lockf to lock the file before reading or writing. If a process362 Chapter 7 • Communication between Processes
neglects to call lockf , the lock will be ignored. Mandatory locks address this prob-
lem by causing any process that accesses a locked file to block in the read or writecall. The locking is enforced by the kernel, so you don’t need to worry about unco-operative processes ignoring your advisory lock. To use mandatory locking,GNU/Linux requires that the file system be mounted with the 
mand flag and that
the file be created with the group execute bit off and the setgid bit set. If any of
these conditions is not met, mandatory locking is not enforced.
7.2.2 Drawbacks of Using Files for IPC
There are several drawbacks to using files for IPC. Using a file means that you arelikely to encounter latency caused by the underlying media. A large file-systemcache can insulate you from this to some extent, but you are likely to encounter itjust when you least expect it.
Another problem with using files for IPC is security. Placing unencrypted data in
a file makes it vulnerable to prying eyes. If your data contains sensitive information,storing it unencrypted in a file is not a good idea. For noncritical user tasks, how-ever, a file can be a very simple means of IPC.
7.3 Shared Memory
As you know, processes cannot simply expose their memory to other processes forreading and writing, thanks to memory-protection mechanisms in Linux. A pointerto a memory location in a process is a virtual address, so it does not necessarily referto a physical location in memory. Passing this address to another process accom-plishes nothing except maybe crashing the other process. A virtual address hasmeaning only in the process that created it.
Linux and all UNIX operating systems allow memory to be shared between
processes via the shared memory facilities. There are two basic APIs for sharing mem-
ory between processes: System V and POSIX. Both use the same principles, with dif-ferent functions. The core idea is that any memory to be shared must be explicitlyallocated as such. That means that you cannot simply take a variable from the stackor the heap and share it with another process. To share memory between processes,you must allocate it as shared memory, using the special functions provided.
Both APIs use keys or names to create or attach to shared memory regions.
Processes that want to share memory must agree on a naming convention so thatthey can map the correct shared regions into memory. The System V API uses keys,which are application-defined integers. The POSIX API uses symbolic names thatfollow the same rules as filenames.7.3 Shared Memory 363
7.3.1 Shared Memory with the POSIX API
The POSIX shared memory API is arguably the more intuitive of the two. Table 7-1
shows an overview of the API. The functions shm_open and shm_unlink behave
much like the open and unlink system calls provided for regular files. These even
return file descriptors that work with the regular system calls like read and write .
In fact, shm_open and shm_unlink aren’t strictly required in Linux, but if you are
writing portable applications, you should use them instead of some other shortcut.
Because the API is based on file descriptors, there is no need to reinvent new APIs
to support additional operations. Any system call that use file descriptors can beused for this purpose. Listing 7-3 shows a complete example of how to create ashared memory region that can be seen by other processes.
TABLE 7-1 POSIX Shared-Memory API
Function Usage
shm_open Create a shared memory region or attach to an existing shared
memory region. Regions are specified by name, and the functionreturns a file descriptor, just like the 
open system call.
shm_unlink Delete a shared memory region using the file descriptor returned by
shm_open . As with the unlink system call used for files, the region
is not removed until all processes unlink from it. No new processescan attach to this region after 
shm_unlink has been called, however.
mmap Map a file into the process’s memory. The input includes a filedescriptor provided by 
shm_open . The function returns a pointer 
to the newly mapped memory. mmap can also use file descriptors that
belong to plain files and some other devices.
munmap Unmap a region of memory that was mapped by a mmap call. The
amount of memory unmapped can be less than or equal to theamount of memory mapped with the 
mmap call, provided that the
region to be unmapped satisfies all the alignment and size require-ments of the operating system.
msync Synchronize access to a region of memory mapped with mmap and
writes any cached data to the physical memory (or other device) so
that other processes can see the changes.364 Chapter 7 • Communication between Processes
LISTING 7-3 posix-shm.c: POSIX Shared Memory Example
1 /* posix-shm.c : gcc -o posix posix.c -lrt */
2 #include <stdio.h>3 #include <string.h>4 #include <stdlib.h>5 #include <unistd.h>             // POSIX6 #include <sys/file.h>           // Pulls in open(2) and friends.7 #include <sys/mman.h>           // Pulls in mmap(2) and friends.8 #include <sys/wait.h>9 
10 void error_out(const char *msg)11 {12     perror(msg);13     exit(EXIT_FAILURE);14 }15 16 int main(int argc, char *argv[])17 {18     int r;19 20     // shm_open recommends using a leading '/' in21     // the region name for portability, but Linux22     // doesn't require it.23     const char *memname = "/mymem";24 25     // Use one page for this example26     const size_t region_size = sysconf(_SC_PAGE_SIZE);27 28     // Create the shared memory region.29     // Notice the args are identical to open(2).30     int fd = shm_open(memname, O_CREAT | O_TRUNC | O_RDWR, 0666);31     if (fd == -1)32         error_out("shm_open");33 34     // Allocate some memory in the region. We use ftruncate, but35     // write(2) would work just as well.36     r = ftruncate(fd, region_size);37     if (r != 0)38         error_out("ftruncate");39 40     // Map the region into memory.41     void *ptr =42         mmap(0, region_size, PROT_READ | PROT_WRITE, MAP_SHARED, fd,43              0);44     if (ptr == MAP_FAILED)45         error_out("mmap");46 47     // Don't need the fd after the mmmap call.7.3 Shared Memory 365
continues
48     close(fd);
49 50     pid_t pid = fork();51 52     if (pid == 0) {53         // Child process inherits the shared memory mapping.54         u_long *d = (u_long *) ptr;55         *d = 0xdeadbeef;56         exit(0);57     }58     else {59         // Synchronize with the child process.60         int status;61         waitpid(pid, &status, 0);62 63         // Parent process sees the same memory.64         printf("child wrote %#lx\n", *(u_long *) ptr);65     }66 67     // Done with the memory, umap it.68     r = munmap(ptr, region_size);69     if (r != 0)70         error_out("munmap");71 72     // Remove the shared memory region.73     r = shm_unlink(memname);74     if (r != 0)75         error_out("shm_unlink");76 77     return 0;
78 }
shm_open creates a shared memory region exactly as you would create a file. Just
like a file, the region is empty when you create it. Allocating space in a newly cre-ated shared memory region is exactly the same as filling a file with data. You canwrite to it with the 
write system call, but with shared memory, it’s often more con-
venient to use the ftruncate system call. Finally, shared memory regions persist
just like files—that is, they don’t disappear when a process terminates but must beexplicitly removed.
Listing 7-3 creates a peer process via a 
fork call. Although it may seem like
cheating, this does illustrate an important point. Shared memory mappings areinherited across forks. So unlike any stack or dynamic memory mappings, which arecloned using copy on write semantics, a shared memory mapping will point to thesame physical storage in both parent and child.366 Chapter 7 • Communication between Processes
$ gcc -o posix-shm posix-shm.c -lrt
$ ./posix-shm$ child wrote 0xdeadbeef
You can share memory between processes that do not have a parent–child rela-
tionship, such as peer processes. A peer process that needs to connect to this sharedmemory region would use virtually the same code as Listing 7-3. The only differ-ence is that the peer does not need to create or truncate the region, so you wouldremove the 
O_CREAT flag in the shm_open call and the call to ftruncate .
Processes must take measures to ensure proper synchronization to avoid race con-
ditions. I noted that the wait call in Listing 7-3 synchronizes parent and child.
Without this synchronization, there would be a race condition. The value printedout by the parent would depend on which process executed first: parent or child.By inserting the 
waitpid call, you force the child to finish first, which ensures that
your value is valid. This is a simple method that works for this example, but youwill see later how to use more sophisticated synchronization.
Inside POSIX Shared Memory in Linux
The Linux implementation of shared memory is dependent on the shared memory
file system, which by convention is mounted on /dev/shm . If this mount point does
not exist, shm_open will fail. Any file system will work, but most distributions
mount the tmpfs file system by default. Even if tmpfs is not mounted on
/dev/shm , shm_open will use the underlying disk file system. You might not notice
if this happens, because depending on the size of the memory regions, the memorycould spend most of its time in the file-system cache. Recall that 
tmpfs is basically a
file system with a cache and no media.
Because shm_open creates files in /dev/shm , each shared memory region is visi-
ble as a file in the directory. The filename is the same that was used by the processthat created the region. This is a very useful debugging feature, because all the tools
available for debugging files are also available for debugging shared memory.
7.3.2 Shared Memory with the System V API
The System V API is still widely used by X Window System, and by extension, many
X applications use it. For most other applications, the POSIX shared memory inter-face is preferred. Table 7-2 shows the API at a glance. This same API is also used forsemaphores and message queues, both of which I discuss later in this chapter.7.3 Shared Memory 367
TABLE 7-2 System V Shared Memory API at a Glance
Function Usage
shmget Create a shared memory region or attach to an existing one (like
shm_open )
shmat Get a pointer to a shared memory region (like mmap )
shmdt Unmap a region of shared memory mapped with shmat (like munmap )
shmctl Many uses, including unlinking a shared memory region created with
shmget (like shm_unlink )
A complete example using the System V API equivalent of Listing 7-3 is shown
in Listing 7-4. The steps are essentially the same except that the shmget function
both creates and allocates the shared memory region, so no truncation step isrequired.
LISTING 7-4 sysv-shm.c: Shared Memory Example Using System V
1 #include <stdio.h>
2 #include <string.h>3 #include <stdlib.h>4 #include <unistd.h>5 #include <sys/ipc.h>6 #include <sys/shm.h>7 #include <sys/wait.h>8 9 void error_out(const char *msg)
10 {11     perror(msg);12     exit(EXIT_FAILURE);13 }14 15 int main(int argc, char *argv[])16 {17 18     // Application-defined key, like the filename in shm_open()19     key_t mykey = 12345678;20 21     // Use one page for this example22     const size_t region_size = sysconf(_SC_PAGE_SIZE);23 24     // Create the shared memory region.25     int smid = shmget(mykey, region_size, IPC_CREAT | 0666);26     if (smid == -1)368 Chapter 7 • Communication between Processes
27         error_out("shmget");
28 29     // Map the region into memory.30     void *ptr;31     ptr = shmat(smid, NULL, 0);32     if (ptr == (void *) -1)33         error_out("shmat");34 35     pid_t pid = fork();36 37     if (pid == 0) {38         // Child process inherits the shared memory mapping.39         u_long *d = (u_long *) ptr;40         *d = 0xdeadbeef;41         exit(0);42     }43     else {44         // Synchronize with the child process.45         int status;46         waitpid(pid, &status, 0);47 48         // Parent process sees the same memory.49         printf("child wrote %#lx\n", *(u_long *) ptr);50     }51 52     // Done with the memory, umap it.53     int r = shmdt(ptr);54     if (r == -1)55         error_out("shmdt");56 57     // Remove the shared memory region.58     r = shmctl(smid, IPC_RMID, NULL);59     if (r == -1)60         error_out("shmdt");61 62     return 0;
63 }
The key used by shmget is functionally equivalent to the filename used by
shm_open . The shmid returned by shmget is functionally equivalent to the file
descriptor returned by shm_open . In each case, one is defined by the application,
and the other is defined by the operating system. Listing 7-4 is almost identical toListing 7-3, so as you might expect, the output is the same:
$ gcc -o sysv-shm sysv-shm.c
$ ./sysv-shmchild wrote 0xdeadbeef7.3 Shared Memory 369
Unlike memory created with the POSIX API, memory created with the System
V API is not visible in any file system. The ipcs command is designed specifically
for manipulating System V shared memory objects. I discuss this tool later in thechapter.
7.4 Signals
Arnold Robbins goes to great lengths in his book1to decry the use of signals for
IPC, so I won’t belabor the point. One problem with signals is that signal handlersare not free to call every standard library function that is available. You cannot pre-dict when a signal will arrive, so when it does, the operating system interrupts therunning process to handle the signal with no regard to what the process is doing atthe time. That means that the libraries are in an unknown state when your signalhandler runs. Your process might have been in the middle of a 
malloc or printf
call when it was interrupted by the signal handler, for example. When that happens,any global or static variables may be in an inconsistent state. If the signal handlerwere to call the library function, it might use these values incorrectly and cause yourprocess to crash. Such functions are unsafe to call from a signal handler. The POSIXstandard specifies which functions must be safe to call from a signal handler. Theseare listed in the 
signal(2) man page.
Making matters worse, functions called from a signal handler don’t know that
they’re in a signal handler. So there’s no way for a function that is not signal safe to
fail safe—that is return an error status when called from a signal handler. Instead,the function behaves unpredictably. Your program may crash, produce garbage, orseem to work fine. This places several constraints on the things a signal handler cansafely do.
Typically, signals are used to handle exception conditions (not IPC), so that even
if the signal handler calls an unsafe function, it happens infrequently enough thatthe likelihood of finding the bug is low. That’s no excuse for poor programming,but the signal API is complicated and full of pitfalls to trap even experienced pro-grammers. 
Suppose that you decide to use signals as an IPC framework. Presumably, the fre-
quency of signals in your system will be much higher than if you used them only370 Chapter 7 • Communication between Processes
1.Linux Programming by Example: The Fundamentals
for exceptions. This provides many more opportunities to discover a signal handler
that is using an unsafe function. You’ll find out only when it crashes. Unless youknow what you are doing and are prepared to take a risk, it probably is best to avoidusing signals for IPC. 
7.4.1 Sending Signals to a Process
Signals have been around since UNIX was created, and although the API hasevolved, the signal mechanism is still fairly simple. When a process receives a signal,an internal flag is set to indicate that the signal has been received. When the kernelgets around to scheduling the process, instead of resuming the process where it leftoff, it calls the signal handler. When the signal handler is called, the flag is cleared.
A process sends a signal to another process using the 
kill system call, which
takes a process ID and signal number as its arguments:
int kill( pid_t pid, int signal );
Each signal is represented by a unique integer, so the kill system call can send
only one signal at a time. The signal value of 0means no signal, which is a useful
trick to test for a process’s existence, as in this example:
int r = kill(pid,0); Using a signal of 0has no effect on the process.
if ( r == 0 )
/* process exists! */
else if ( errno == ESRCH ) Must check errno before drawing any conclusions.
/* process does not exist */
Like all POSIX functions, kill returns -1when it fails and sets errno . In this
case, when it returns 0, it simply means that the process existed at the time the sig-
nal was sent. Because the signal was 0, no signal was sent, so all it tells you is that
the process existed when you checked. When kill returns -1and the value of
errno is ESRCH , it means that there is no process with the given process ID.
7.4.2 Handling a Signal
There are two basic APIs for handling signals: POSIX and System V. The POSIXAPI was inherited from BSD and is the preferred technique because it is more flex-ible and more immune to race conditions. The System V API is the one chosen byANSI for the standard C library. It’s simpler but less flexible than the POSIX API.7.4 Signals 371
Both POSIX and System V allow you to define one of the following behaviors
for each signal:
• Call a user-defined function
• Ignore the signal• Return to the default signal behavior
The POSIX API also allows you to block and unblock individual signals without
changing the underlying signal handler. This is key to preventing race conditions.Note that the signals 
SIGKILL and SIGSTOP cannot be handled, blocked, or
ignored, because these are vital for process control. 
The process must tell the operating system about user-defined signal handlers
using either API. The ANSI function for setting signal handlers is the signal sys-
tem call, which is defined as follows:
typedef void (*sighandler_t)(int);
sighandler_t signal(int signum, sighandler_t handler);
The signal function takes as an argument a pointer to the function to be used
as a handler. GNU provides a type definition ( typedef ) for this argument called
sighander_t . I included the definition of sighandler_t above because this type
is not defined by any standard,2and it makes the prototype easier to read. 
The POSIX system call is sigaction and is defined as follows:
int sigaction(int signo, struct sigaction *new, struct sigaction *old);
The sigaction function requires a pointer to a sigaction structure (which 
I will look at shortly) to define the new signal handler. Both functions return thecurrent signal handler, which your application can use to restore the signal handlerat a later time. Another use for the old signal handler is to allow the new signal han-dler to call it before returning.
Table 7-3 shows the patterns you can use to determine the signal-handling behav-
ior discussed earlier.372 Chapter 7 • Communication between Processes
2. GNU defines this type for you only when you specify -D_GNU_SOURCE on the command line.
TABLE 7-3 Setting Signal Behavior
Action System V Pattern POSIX Pattern
sigaction(signo,new,old)
Ignore the signal sigaction(signo,new,old)
sigaction(signo,new,old)
7.4.3 The Signal Mask and Signal Handling
The signal mask is what the kernel uses to determine how to deliver signals to a
process. Conceptually, this is just a very large word with 1 bit per signal. If a processsets the mask for a particular signal, that signal is not delivered to the process. Whena signal is masked, we also say that it is blocked.
POSIX defines 
sigset_t to manage the signal mask. For portability, you should
never modify a sigset_t directly but use these functions:
int sigemptyset(sigset_t *set);             Clears all signals in the mask
int sigfillset(sigset_t *set);              Sets all signals in the mask
int sigaddset(sigset_t *set, int signum);   Sets one signal in the mask
int sigdelset(sigset_t *set, int signum);   Clears one signal in the mask
int sigismember(sigset_t *set, int signum); Returns 1if signum is set in the mask and 0otherwise
sigaddset , sigdelset , and sigismember take a single signal number as an
argument. Thus, each function affects only one signal in the mask. A process must callone of these functions once for each signal it wants to modify. Note that these func-tions operate only on the 
sigset_t argument; they do not affect signal handling. 
When a process finishes modifying the mask, it passes the mask to the
sigprocmask function to change the process’s signal mask. This allows an appli-
cation to affect all signals simultaneously, using a single system call to avoid raceconditions. The prototype for 
sigprocmask is as follows:
int sigprocmask(int how, const sigset_t *set, sigset_t *oldset);
The function takes two pointers to a sigset_t . The first is the new mask to be
applied, and the second is the old mask, which can be used to restore the signalold = signal(signo,SIG_DFL) (Re)set to
default handlerold = signal(signo,SIG_IGN)old = signal(signo,handler) Call a user-defined function7.4 Signals 373
mask to its original state at some later point. The howargument indicates how to
apply the input signal mask to the process’s signal mask. This value can be one ofthe following: 
•
SIG_BLOCK —Add any set signals from the input signal mask to the process’s
signal mask. The signals indicated in the input signal mask will be blocked inaddition to the currently blocked signals.
•
SIG_UNBLOCK —Remove any set signals in the input signal mask from the
process’s signal mask. The signals indicated in the input signal mask will beunblocked, but the rest of the process’s signal mask will remain unchanged.
•
SIG_SET —Overwrite the current signal mask with the value of the input sig-
nal mask. Only the signals listed in the input mask will be blocked. Any othersignals will be unblocked, and the current signal mask will be discarded.
You should notice that the 
sigprocmask function does not take a signal handler
as an argument. Blocking a signal only delays the delivery of the signal; it does notdiscard the signals that have been sent to the process. The example in Listing 7-5should illustrate.
LISTING 7-5 sigprocmask.c: Using sigprocmask to Delay Delivery of a Signal
1 #include <stdio.h>
2 #include <string.h>3 #include <stdlib.h>4 #include <signal.h>5 #include <unistd.h>6 7 volatile int done = 0;8 9 // Signal handler
10 void handler(int sig)11 {12     // Ref signal(2) - write() is safe, printf() is not.13     const char *str = "handled...\n";14     write(1, str, strlen(str));15     done = 1;16 }17 18 void child(void)19 {20     // Child process exits immediately, and generates SIGCHLD to the parent374 Chapter 7 • Communication between Processes
21     printf("child exiting\n");
22     exit(0);23 }24 25 int main(int argc, char *argv[])26 {27     // Handle SIGCHLD when child process exits.28     signal(SIGCHLD, handler);29     sigset_t newset, oldset;30 31     // Set all signals in the set32     sigfillset(&newset);33 34     // Block all signals and save the old signal mask35     // so we can restore it later36     sigprocmask(SIG_BLOCK, &newset, &oldset);37 38     // Fork a child process39     pid_t pid = fork();40     if (pid == 0)41         child();42 43     printf("parent sleeping\n");44 45     // Sleep with all signals blocked.46     int r = sleep(3);47 48     // r == 0 indicates that we slept the full duration.49     printf("woke up! r=%d\n", r);50 51     // Restore the old signal mask,52     // which will result in our handler being called.53     sigprocmask(SIG_SETMASK, &oldset, NULL);54 55     // Wait for signal handler to run.56     while (!done) {57     };58 59     printf("exiting\n");60     exit(0);
61 }
The program in Listing 7-5 forks a child process that exits immediately. This
results in the parent process’s receiving a SIGCHLD signal. Before you fork, however,
you install a signal handler for SIGCHLD and block the SIGCHLD signal for 3 sec-
onds. Immediately after you return from the sleep, you restore the original signal7.4 Signals 375
handler, which unblocks SIGCHLD . Then the signal handler executes, demonstrat-
ing that the signal was delivered:
$ ./sigprocmask
child exiting This is when SIGCHLD is sent to the parent.
parent sleeping SIGCHLD is blocked, so the parent goes to sleep.
woke up! r=0 The return of 0indicates we slept for 3 seconds; still no signal.
handled... Signal is delivered after the signal mask is restored.
exiting
The POSIX API includes a couple of additional functions for dealing with sig-
nal masks while signals are blocked. The prototypes are
int sigpending(sigset_t *set); 
int sigsuspend(const sigset_t *mask);
sigpending allows you to examine signals that have been sent but not delivered.
In Listing 7-5, you could have tested the signal mask before going to sleep and possi-bly would have seen the signal. There is no guarantee, however, because 
sigpending
does not wait for signals; it simply takes a snapshot of the signals and presents themto the caller. This technique is also called polling. If you want to wait for a specific set
of signals and no other signals, the 
sigsuspend function is what you want. This
function temporarily overwrites the signal mask with the input mask until the desiredsignal is delivered. Before returning, it restores the signal mask to its original state. 
7.4.4 Real-Time Signals
Astute readers may notice a few issues that arise as a result of blocking signals. Whathappens, for example, if more than one signal is sent while a signal is blocked?When a single signal arrives while signals are blocked, the value remains set in thekernel, so the signal is not lost; it is delivered immediately when the processunmasks the signal. If the same signal is sent twice while it is blocked, the secondsignal normally is discarded. When the signal is unmasked, the signal handler iscalled only once.
POSIX introduced real-time signals to provide applications the ability to receive
a signal multiple times, even when that signal is blocked. When a real-time signalis blocked, the kernel will keep track of the number of times the signal is receivedand call the signal handler that many times when the signal is unblocked. 
Real-time signals are identified by a range of signal numbers. This range is iden-
tified by the macros 
SIGRTMIN and SIGRTMAX . If you specify a signal number
between these two values, the signal will be queued. Listing 7-6 shows an example.376 Chapter 7 • Communication between Processes
LISTING 7-6 rt-sig.c: Blocking and Real-Time Signals
1 #include <stdio.h>
2 #include <string.h>3 #include <stdlib.h>4 #include <signal.h>5 #include <unistd.h>6 7 volatile int done = 0;8 9 // Signal handler
10 void handler(int sig)11 {12     // Ref signal(2) - write() is safe, printf() is not.13     const char *str = "handled...\n";14     write(1, str, strlen(str));15     done = 1;16 }17 18 void child(void)19 {20     int i;21     for (i = 0; i < 3; i++) {22         // Send rapid fire signals to parent.23         kill(getppid(), SIGRTMIN);24         printf("child - BANG!\n");25     }26     exit(0);27 }28 29 int main(int argc, char *argv[])30 {31     // Handle SIGRTMIN from child32     signal(SIGRTMIN, handler);33     sigset_t newset, oldset;34 35     // Block all signals and save the old signal mask36     // so we can restore it later37     sigfillset(&newset);38     sigprocmask(SIG_BLOCK, &newset, &oldset);39 40     // Fork a child process41     pid_t pid = fork();42     if (pid == 0)43         child();44 45     printf("parent sleeping\n");46 47     // Sleep with all signals blocked.48     int r = sleep(3);7.4 Signals 377
continues
49 
50     // r == 0 indicates that we slept the full duration.51     printf("woke up! r=%d\n", r);52 53     // Restore the old signal mask,54     // which will result in our handler being called.55     sigprocmask(SIG_SETMASK, &oldset, NULL);56 57     // Wait for signal handler to run.58     while (!done) {59     };60 61     printf("exiting\n");62     exit(0);
63 }
Listing 7-6 is very similar to Listing 7-5 except that it uses SIGRTMIN as the sig-
nal instead of SIGCHLD , and the child now sends this signal to the parent three
times before exiting. There are no special flags required to use queued signals. Thesignal number tells the kernel to use queued signals. Running this example showshow it works:
$ ./rt-sig
child - BANG!child - BANG!child - BANG!parent sleepingwoke up! r=0handled...handled...handled...exiting
Here again, you see that the parent is not interrupted by the signals until after
the signal mask is restored. When the signal mask is restored, the handler is calledthree times—once for each time the child called 
kill .
7.4.5 Advanced Signals with sigqueue and sigaction
Finally, we come to an alternative to the kill system call named sigqueue . The
prototype is shown below and looks much like the kill system call except that it
takes an additional argument:
int sigqueue(pid_t pid, int sig, const union sigval value);378 Chapter 7 • Communication between Processes
The pidand sigarguments are identical to the kill system call, but the value
argument actually lets you include data with the signal. Recall that non–real time
signals are not queued, so only when you use a signal number between RTSIGMIN
and RTSIGMAX will the signals be queued with their associated information.
The additional information provided by the sigqueue function can be retrieved
only by a handler installed with the sigaction function, which I introduced ear-
lier as an alternative to the signal system call. Installing a handler with sigaction
is a bit more involved. Using Listing 7-6 as an example, the signal function could
be replaced by sigaction as follows:
struct sigaction sa = { Using C99 structure initializer syntax . . .
.sa_handler = handler, Same handler as before
.sa_flags = SA_RESTART Rearm the signal handler after it’s called . . .
};
sigemptyset(&sa.sa_mask); Create an empty signal mask
sigaction(SIGRTMIN,&sa,NULL); Discard the old action via NULL
Filling in the sigaction structure is an extra step required to use the sigaction
function. The sa_mask field is a signal mask that is used while the signal handler
runs. Normally, when the signal handler is called, the signal being handled isblocked. The mask specified here indicates additional signals to be blocked whilethe handler runs. The handler in the 
sigaction structure is a pointer to a func-
tion. This is actually a union containing pointers to two different handler types.
sa_handler points to a System V style signal handler, used above. sa_sigaction
points to a new-style handler that has the following prototype:
void handler(int sig, struct siginfo *si, void *ptr)
To take full advantage of sigqueue , you should define a new-style handler. To
indicate that you are using a new-style handler, you set the SA_SIGINFO flag in the
sa_flags field as follows:
struct sigaction sa = { Using C99 structure initializer . . .
.sa_sigaction = handler, Use new-style handler
.sa_flags = SA_RESTART |SA_SIGINFO DON’T FORGET THIS!
};
sigemptyset(&sa.sa_mask);
When the kernel sees the SA_SIGINFO flag, it puts different arguments on the
stack for the signal handler. Without this, your signal handler will be called with thewrong arguments and probably will cause your application to crash with a 
SIGSEGV .7.4 Signals 379
Now that you have a new-style handler, you can replace the kill function with
sigqueue as follows:
union sigval sv = {
.sival_int = 42 Any user-defined number will do.
};sigqueue(getppid(), SIGRTMIN, sv);
You can use any signal, but only RT signals are queued.
Looking at the handler again, the siginfo structure contains various things
defined by different standards. Following are the set of values used in Linux andtheir POSIX definitions:
int           si_signo  Signal number – SIGINT and so on
int           si_code Signal code (see text)
int           si_errno  If nonzero, an errno value associated with this signal
pid_t         si_pid    Sending process ID
uid_t         si_uid    Real user ID of sending process
void         *si_addr   (see text)
int           si_status Exit value or signal
long          si_band   Band event for SIGPOLL
union sigval  si_value  Signal value
As the annotation above suggests, several signal-specific fields in the siginfo
structure are not defined under all circumstances. Of all the fields defined, only the
si_signo , si_errno , and si_code fields contain valid data all the time, accord-
ing to POSIX. If the signal is sent by another process, si_pid and si_uid indicate
the process ID and user ID of the process that sent the signal.
The si_code takes one of the values defined in Table 7-4 and indicates some
details about the source of the signal and the reason for it. A signal that was theresult of another process or a call to the 
raise function results in an si_code
value of SI_USER . A signal that is sent with the sigqueue function has an
si_code value of SI_QUEUE .
An example of a context-sensitive field is the si_value field, which is defined
only if the caller used sigqueue to send the signal. When this is true, si_value
contains a copy of the sigval that was sent by the function. Most of the other
fields deal with various exception conditions that have nothing to do with IPC andcome from the current process. 
Another context-sensitive field, 
si_errno may be nonzero if an error is associ-
ated with this signal. si_band is defined only for SIGPOLL , which can be useful in
certain IPC applications.380 Chapter 7 • Communication between Processes
TABLE 7-4 Values Defined si_code
si_code Meaning
SI_USER Signal sent via kill() or raise()
SI_KERNEL Signal sent from the kernel
SI_QUEUE Signal sent via sigqueue()
SI_TIMER POSIX timer expired
SI_MESGQ POSIX message queue state changed
SI_ASYNCIO Asynchronous I/O (AIO) completed
SI_SIGIO Queued SIGIO (not used in Linux)
SI_TKILL Signal sent via tkill() or tgkill() ; Linux only
7.5 Pipes
Pipes are simple to create and use. They come in two varieties: an unnamed pipe for
use between a parent and a child process, and a named pipe for use between peerprocesses on the same computer. You can create an unnamed pipe in a process withthe 
pipe system call, which returns a pair of file descriptors. The prototype for the
pipe function looks like the following:
int pipe(int filedes[2]);
The caller passes an array of two integers, which will hold two file descriptors
upon successful return. The first file descriptor in the array is read-only, and the sec-ond is write-only. Unnamed pipes are useful only for communication between aparent and a child process. Because the child inherits the same file descriptors as theparent, the parent can create the pipe before forking to create a communicationchannel between parent and child. Following is a typical pattern:
int fd[2];
r = pipe(fd);if ( r == -1 ) 
Check for errors.
pid_t pid = fork();if ( pid == 0 )
Child writes to fd[1] inherited from parent.
{
write(fd[1], "Hello World", 11);...
}
int n = read(fd[0],buf,11);
Parent reads from fd[0] .7.5 Pipes 381
Linux and UNIX allow named pipes, which are identified by special files on disk
created with the mkfifo or mknod function:
int mkfifo(const char *pathname, mode_t mode);
Both mkfifo and mknod are also available as shell commands. A named pipe can
be opened, read, and written just like a regular file with the familiar open , read ,
and write system calls. The file on disk is used only for naming; no data is ever
written to the file system.
Normally, when a process opens a named pipe for reading, the process blocks
until another process opens the named pipe for writing, or vice versa. This makesnamed pipes useful for synchronizing with other processes as well. The followingdemonstrates a named pipe with some simple shell commands:
$ mkfifo myfifo Create the fifo.
$ cat < myfifo & catcommand blocks until we write to the pipe.
[1] 29668
$ echo Hello World > myfifo Write to the pipe.
Hello World catcommand completes.
[1]+  Done ...
$ echo Hello World > myfifo & echo command blocks until we read from the pipe.
[1] 29670
$ cat myfifo Read from pipe.
Hello World[1]+  Done ...
echo command completes.
If you are opening for reading, you can avoid the blocking by using the
O_NONBLOCK flag to open it in nonblocking mode. Linux does not allow you to
open a named pipe for writing in nonblocking mode.
7.6 Sockets
Sockets are general-purpose tools for communication between processes; they canbe used locally or across a network. Sockets behave very much like pipes except thatunlike pipes, sockets are bidirectional. When you need to distribute processes acrossprocessors in a network, the socket allows you to use the same API to communicateamong all processes, whether they are local or not.
A full tutorial on sockets is beyond the scope of this book,
3but I’ll discuss some
basic examples.382 Chapter 7 • Communication between Processes
3. A good tutorial is available on the glibc info page: info libc sockets .
7.6.1 Creating Sockets
The system call for creating general-purpose sockets is the socket function, which
creates sockets that can be used for local or network connections. There is also the
socketpair function, which creates a local connection exclusively. Following are
the prototypes for both functions:
int socket(int domain, int type, int protocol);
int socketpair(int domain, int type, int protocol, int fd[2]);
Both these functions require that you specify a domain, type, and protocol for the
socket, which I will discuss shortly. The socketpair function returns an array of
two file descriptors, just like the pipe system call.
7.6.1.1 Socket Domains
The socket domain parameter helps determine the interface the socket can use—
that is, it determines whether the socket will use the network interface, a local inter-face, or some other interface. Domain is the term used by POSIX, but GNU refers
to it as the namespace to avoid overloading the term domain, which has many other
meanings. The POSIX constants defined for this parameter use the prefix 
PF(for
protocol family ). Table 7-5 provides a partial list of these constants. The ones you are
likely to encounter are PF_UNIX and PF_INET . PF_UNIX is used for communica-
tion between processes on the same computer, and PF_INET is used for communi-
cation across an IP network.4
TABLE 7-5 Some Domains Used for Socket Functions
Domain Protocol/Usage
PF_UNSPEC Unspecified. OS decides. Defined as zero.
PF_UNIX, PF_LOCAL Communication between processes on the same computer.
PF_INET Use IPv4 Internet protocols.
PF_INET6 Use IPv6 Internet protocols.
PF_NETLINK Kernel user interface device.
PF_PACKET Low-level packet interface to allow direct access to a device.7.6 Sockets 383
4. More precisely, an IPv4 network. IPv4 is the original IP protocol, with 32 bits per address, whereas IPv6
uses 128 bits for addressing.
7.6.1.2 Socket Types
The type of socket is the second argument required by the socket functions, indi-
cating the type of service the application is looking for. Table 7-6 provides a moredetailed explanation. Perhaps the most common and easiest to use is the
SOCK_STREAM type. This type requires a connection, which essentially means that a
running process must have each end of the socket open to function; otherwise, it’sconsidered to be an error. The reliability of the connection is determined by thethird argument: the protocol .
7.6.1.3 Socket Protocols
Each protocol has different capabilities, and not all protocols support all socket
types. POSIX defines a set of protocols that all socket implementations must sup-port (Table 7-7). The macros for these protocols are defined in 
<netinet/in.h> .
Other protocols may be supported, but these may be nonstandard. Either way, a listof known protocols is maintained in 
/etc/protocols . To specify a protocol by the
name listed in /etc/protocols , you would use one of the getprotoent(3) fam-
ily of library calls and use the value returned to designate the protocol.384 Chapter 7 • Communication between Processes
TABLE 7-6 Socket Types
Socket Type Description
SOCK_STREAM “Reliable” connection-based data transfer. The under-
lying protocol guarantees that data is read in the sameorder it is transmitted. The protocol may support “out of band” data.
SOCK_DGRAM “Unreliable” connectionless transfer, with no guaranteesabout delivery or delivery order.
SOCK_SEQPACKET Similar to SOCK_STREAM , but reader is required to read
entire packets at a time.
SOCK_RAW Allows access to raw network packets.
SOCK_RDM Similar to SOCK_STREAM except that there are no
guarantees with respect to delivery ordering.
TABLE 7-7 Socket Protocols
Protocol 
Macro Name Description
IPPROTO_IP ip Internet protocol, technically not a protocol. This
macro is used for local sockets of any type and can beused with connectionless network and local sockets.
IPPROTO_ICMP icmp Internet Control Message Protocol, used by applica-tions such as the 
ping command.
IPPROTO_TCP tcp T ransmission Control Protocol, a connection-based,reliable protocol for use with 
SOCK_STREAM sockets.
IPPROTO_UDP udp User Datagram Protocol, a connectionless, unreliableprotocol used for IPC that provides low latency atthe expense of reliability and is most often used withsockets of type 
SOCK_DGRAM .
IPPROTO_IPV6 ipv6 Like IP , technically not a protocol but does specifythat packets should use IPv6 addressing.
IPPROTO_RAW raw Allows an application to receive raw packets; usually
not available to unprivileged users.
The pseudoprotocol iphas a value of 0, which is the value used for all local sock-
ets (PF_LOCAL ). As a result, many programmers just use 0for the protocol when
they want a local socket.
7.6.2 Local Socket Example Using socketpair
The simplest way to create a local socket is to use the socketpair function, which
is illustrated in Listing 7-7.
LISTING 7-7 socketpair.c: Sockets Example Using socketpair
1 #include <stdio.h>
2 #include <string.h>3 #include <stdlib.h>4 #include <errno.h>5 #include <unistd.h>6 #include <sys/types.h>7 #include <sys/socket.h>7.6 Sockets 385
continues
8 #include <sys/wait.h>
9 #include <netdb.h>
10 11 int main(int argc, char *argv[])12 {13     int fd[2];14 15     // ref. socketpair(2)16     // domain (aka Protocol Family) = PF_LOCAL (same as PF_UNIX)17     // type = SOCK_STREAM (see table)18     // protocol = zero (use default protocol)19     int r = socketpair(PF_LOCAL, SOCK_STREAM, 0, fd);20     if (r == -1) {21         perror("socketpair");22     }23 24     pid_t pid = fork();25 26     if (pid == 0) {27         // Child process reads.28         char buf[32];29         int n = read(fd[1], buf, sizeof(buf));30         if (n == -1) {31             perror("read");32         }33         printf("read %d bytes '%s'\n", n, buf);34     }35     else {36         // Parent process writes.37         char msg[] = "Hello World";38         int n = write(fd[0], msg, sizeof(msg));39         if (n == -1) {40             perror("write");41         }42 43         // Wait for child to finish.44         int status;45         wait(&status);46     }47 48     exit(0);49     return 0;
50 }
Listing 7-7 illustrates the basics of creating a socket via the socketpair function.
As mentioned earlier, this is much like a pipe except that the socket is a bidirectionallink, unlike a pipe. Recall that a pipe provides two file descriptors—one for writingand one for reading. If you want bidirectional communication between twoprocesses via pipes, you must create one pipe for each direction. Because the socket386 Chapter 7 • Communication between Processes
is bidirectional by design, only one socket is required to provide a full-duplex com-
munication path between two processes.
7.6.3 Client/Server Example Using Local Sockets
Sockets are most often used in client/server applications, which require the use ofthe more general-purpose 
socket system call. Unlike socketpair , which returns
a pair of file descriptors, socket returns a single file descriptor. socketpair shields
the programmer from many of the gory details of sockets, but it can be used onlybetween a parent and a child process.
Before you can use the 
socket function, you need to introduce some additional
functions. Figure 7-1 shows a flow chart for a basic client and server. Notice thatthe 
socketpair function provides a convenient shortcut for all the additional API
calls required for a general-purpose client and server.7.6 Sockets 387
FIGURE 7-1 Client/Server Sockets API and Associated Socket StatesClient Socket St ates Socket Socket St ates
Unconnected
Bound
Listening
Client “connects,” 
server “accepts.”
Connectedsocket
bind
listen
acceptsocketpair
closesocket
Unconnected
Connectedconnect
closesocketpair
All these functions have comprehensive manpages in Linux, so I won’t go into
detail here; instead, I will provide some working examples. First, however, I’ll intro-duce one more function not listed in Figure 7-1: 
select . select is used to wait
on multiple file descriptors with an optional timeout.5It’s used with many file
descriptors other than sockets, but writing a sockets program without it is hard.
select is defined as follows:
int select(int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds,
struct timeval *timeout);
The fd_set type is how the application tells select which file descriptors to
monitor. Each fd_set can be manipulated with a group of macros defined as
follows:
FD_SET(int fd, fd_set *set) Add fdto the fd_set .
FD_CLR(int fd, fd_set *set) Remove fdfrom the fd_set .
FD_ISSET(int fd, fd_set *set) T est for the presence of fdin the fd_set .
FD_ZERO(fd_set *set) Remove all fds from the fd_set .
A complete example will illustrate how all these functions work together.
Listing 7-8 is a simple server using sockets. Notice that the socket file descriptorreturned by 
socket is passed to listen , which transforms it into a so-called listen
socket. Listen sockets are used by servers to accept connections. The accept func-
tion takes a listen socket as input and waits for a client to connect. At this point, itreturns a new file descriptor, which is a socket connected to the client.
LISTING 7-8 server_un.c: Socket Server Using a Local Socket
1 #include <stdio.h>
2 #include <string.h>3 #include <signal.h>4 #include <stdlib.h>5 #include <unistd.h>6 #include <sys/types.h>7 #include <sys/socket.h>8 #include <sys/un.h>9 
10 // Call perror and exit if return value from system call is -111 #define ASSERTNOERR(x,msg) do {\12 if ((x) == -1) { perror(msg); exit(1); }} while(0)13 388 Chapter 7 • Communication between Processes
5. A function related to select is poll .
14 // Local sockets require a named file, this must be unlinked later
15 #define SOCKNAME "localsock"16 17 int main(int argc, char *argv[])18 {19     // ref. socket(2)20     // Create a local stream socket21     // domain (aka Protocol Family) = PF_LOCAL (same as PF_UNIX)22     // type = SOCK_STREAM (see table)23     // protocol = zero (use default protocol)24     int s = socket(PF_LOCAL, SOCK_STREAM, 0);25 26     ASSERTNOERR(s, "socket");27 28     // Local sockets are given a name that resides on a filesystem.29     struct sockaddr_un sa = {30         .sun_family = AF_LOCAL,31         .sun_path = SOCKNAME32     };33 34     // This creates the file.35     // If file exists it fails with EADDRINUSE,36     // so don't forget to unlink when done!37     int r = bind(s, (struct sockaddr *) &sa, sizeof(sa));38 39     ASSERTNOERR(r, "bind");40 41     // Allow clients to connect. Allow a backlog of one.42     // This call does not block. That occurs during accept.43     r = listen(s, 0);44 45     ASSERTNOERR(r, "listen");46 47     // We use struct sockaddr_un for the Unix socket address.48     // This requires a path to a file in a mounted filesystem.49     struct sockaddr_un asa;50     size_t addrlen = sizeof(asa);51 52     // Block until a client connects. Returns the file descriptor53     // of the new connection as well as its address.54     int fd = accept(s, (struct sockaddr *) &asa, &addrlen);55 56     ASSERTNOERR(fd, "accept");57 58     while (1) {59         char buf[32];60         fd_set fds;61 62         // Use select to wait for data from the client.63         FD_ZERO(&fds);7.6 Sockets 389
continues
64         FD_SET(fd, &fds);
65         int r = select(fd + 1, &fds, NULL, NULL, NULL);66 67         ASSERTNOERR(r, "select");68 69         // Read the data70         int n = read(fd, buf, sizeof(buf));71         printf("server read %d bytes\n", n);72 73         // Zero length read means client closed the socket.74         if (n == 0)75             break;76     }77 78     // Plain unlink system call is sufficient.79     unlink(SOCKNAME);80 81     return 0;
82 }
Listing 7-9 is a simple client for use with the server in Listing 7-8. Here, the file
descriptor used by socket is passed to connect , which returns after the socket is
connected to the server.
LISTING 7-9 client_un.c: Simple Client Using Local Sockets
1 #include <stdio.h>
2 #include <string.h>3 #include <stdlib.h>4 #include <signal.h>5 #include <unistd.h>6 #include <sys/types.h>7 #include <sys/socket.h>8 #include <sys/un.h>9 
10 #define ASSERTNOERR(x,msg) do {\11     if ((x) == -1) { perror(msg); exit(1); }} while(0)12 13 // Name must match the server we want to connect to.14 #define SOCKNAME "localsock"15 16 int main(int argc, char *argv[])17 {18     // Options ust match server_un.c19     int s = socket(PF_LOCAL, SOCK_STREAM, 0);20 21     ASSERTNOERR(s, "socket");22 390 Chapter 7 • Communication between Processes
23     // Socket address takes AF_ macros
24     struct sockaddr_un sa = {25         .sun_family = AF_LOCAL,26         .sun_path = SOCKNAME27     };28 29     // Returns -1 if failed.30     int r = connect(s, (struct sockaddr *) &sa, sizeof(sa));31 32     ASSERTNOERR(r, "connect");33 34     const char data[] = "Hello World";35     r = write(s, data, sizeof(data));36 37     printf("client wrote %d bytes\n", r);38 39     return 0;
40 }
When the server runs, it creates a file in the local directory named localsock .
This is required for a UNIX domain socket and is specified in the sockaddr_un
structure. Both the client and server specify the socket name, but only the servercreates the socket. A local socket is visible via the 
lscommand and can be identi-
fied by the sin first column of the permissions, as in this example:
$ ./server_un &
$ ls -l localsocksrwxrwxr-x  1 john john 0 Apr 29 22:28 localsock
A client using local sockets is fairly simple, as shown in Listing 7-9 earlier in the
chapter. Running the client (after the server) produces the following output:
$ ./client_un
client wrote 12 bytesserver read 12 bytesserver read 0 bytes[1]+  Done                    ./server_un
I deliberately glossed over a few details in these examples. The listen system
call, for example, requires a backlog argument, which determines how many con-
nections the operating system will accept on your behalf. Each pending connectionrequires the server to call 
accept , but a pending connection causes the client to
block until the server code calls accept . I specified a backlog of zero, which means
that only one connection can be active at a time. As long as this connection is open,any clients that try to connect to the server will be refused. 7.6 Sockets 391
As soon as you close this connection, the operating system may initiate a new
connection before you have a chance to close the listen socket. The shutdown sys-
tem call prevents the kernel from accepting any new connections while the serverterminates gracefully.
7.6.4 Client Server Using Network Sockets
Luckily, a network client and server are not much different from the local client andserver created in Listing 7-9 and Listing 7-8. I will highlight the differences here.Essentially, the difference is in the protocol family and address used. Other thanthat, the code is virtually identical.
• Replace the include file 
sys/un.h with netinet/in.h .
• The protocol family passed to the socket function changes from PF_LOCAL
to a network protocol family. ( PF_INET is typical.)
• The socket address structure changes from sockaddr_un to sockaddr_in . It
has a network address family ( AF_INET is typical) and is initialized with a net-
work address and a port.
• You do not unlink the socket when you are done. A network socket does not
persist after the server exits.
The socket address is slightly different for client and server and is worth a closer
look. Because it is a network address, it must be specified using an address and anumeric port. The server can allow connections on any interface using the specialaddress macro 
INADDR_ANY . When initializing the structure, you need to be aware
of network byte order as well. The code to initialize a server address port address of5000 looks like the following:
struct sockaddr_in sa = { 
.sin_family = AF_INET, .sin_port=htons(5000), .sin_addr = {INADDR_ANY} };
The macro htons stands for host to network short and converts the byte order of
the port ID to network byte order.6Instead of INADDR_ANY , you can specify a392 Chapter 7 • Communication between Processes
6. Refer to inet(3) .
specific interface address in the sin_addr field and use a struct in_addr , which
can be initialized using the following pattern:
struct in_addr ifaddr;
int r = inet_aton("192.168.163.128",&ifaddr);
Then you can use the value of ifaddr to initialize sin_addr in the struct
sockaddr . The same changes apply to the client except that a client typically uses
a specific address instead of INADDR_ANY . 
7.7 Message Queues
Message queues are yet another way to transfer data between two processes. Asyou might expect, there are two ways to create a message queue: the System Vmethod and the POSIX method. Each method is a little different, but the prin-ciple is the same.
Both types of message queues emphasize fixed-size, priority-based messages. A
receiver must read exactly the number of bytes transmitted; otherwise, the readfrom the message queue will fail. This provides an application some degree of assur-ance that a process it is communicating with uses the same version of a messagestructure. 
Each API implements priority slightly differently. The System V API allows the
receiver a bit more flexibility in prioritizing incoming messages, whereas the POSIXAPI enforces strict prioritization.
7.7.1 The System V Message Queue
The function for creating or attaching to a message queue is msgget , which takes
an integer value as a user-defined key for the message queue, just like the System VAPI for shared memory. The function returns a system-defined key for the queue,which is used for subsequent reads and writes to the queue. Following is the proto-type for 
msgget :
int msgget(key_t key, int msgflg);
As I mentioned earlier in the chapter, the keyargument can be an application-
defined key that never changes. The value returned by msgget is the system-defined
message queue identifier that is used for all reads and writes by this process. The keycan be replaced by the macro 
IPC_PRIVATE . Using this value for the key will always7.7 Message Queues 393
create a new message queue. The message queue IDs returned by msgget are valid
across processes, unlike file descriptors. So the message queue ID can be sharedbetween parent and child as well as between peer processes.
The 
msgflag argument to msgget is very similar to the flags passed to the open
system call. In fact, the lower 9 bits are the same permission bits used by the open
call. Other flags allowed are IPC_CREAT and IPC_EXCL . IPC_CREAT is used to cre-
ate a new message queue; if the message queue exists, it will return a message queueID for the existing queue. When 
IPC_CREAT is specified with IPC_EXCL , it causes
the msgget to fail if the message queue exists. This behavior is the same as the
O_CREAT and O_EXCL flags used with the open(2) system call.
There is no equivalent to a close function for the message queue, because queue
IDs do not consume file descriptors. The IDs are visible to all processes, and theoperating system does not keep track of message queues by process.
A process can remove a message queue with the 
msgctl function, which, as the
name suggests, does many things besides removing message queues:
int msgctl(int msqid, int cmd, struct msqid_ds *buf);
The cmdargument to remove a message queue is IPC_RMID , in which case the
bufargument may be NULL . Refer to msgctl(2) for more details.
Reading and writing to a message queue are accomplished with the following two
functions:
int msgsnd(int qid, void *msg, size_t msgsz, int msgflg);
ssize_t msgrcv(int qid, void *msg, size_t msgsz, long typ, int msgflg);
System V message queues allow messages to have a variable length, provided
that the sender and receiver agree on the size. The message typedoubles as the
message priority. So depending on the application, you can choose to prioritizemessages in the queue, use the priority to identify the message type, or some com-bination of both.
You can see a demonstration of all these techniques in Listing 7-10.
LISTING 7-10 sysv-msgq-example.c: Message Queue Example Using System V API
1 #include <stdio.h>
2 #include <string.h>3 #include <stdlib.h>4 #include <unistd.h>5 #include <sys/wait.h>6 #include <sys/stat.h>7 #include <sys/file.h>394 Chapter 7 • Communication between Processes
8 #include <sys/msg.h>
9 #include <sys/ipc.h>
10 11 // Fixed-size message to keep things simple.12 struct message {13     long int mtype;14     char mtext[128];15 };16 17 // Stuff text into a message and send it.18 int send_msg(int qid, int mtype, const char text[])19 {20     struct message msg = {21         .mtype = mtype22     };23     strncpy(msg.mtext, text, sizeof(msg.mtext));24 25     int r = msgsnd(qid, &msg, sizeof(msg), 0);26     if (r == -1) {27         perror("msgsnd");28     }29     return r;30 }31 32 // Read message from queue into a message struct.33 int recv_msg(int qid, int mtype, struct message *msg)34 {35     int r = msgrcv(qid, msg, sizeof(struct message), mtype, 0);36     switch (r) {37     case sizeof(struct message):38         /* okay */39         break;40     case -1:41         perror("msgrcv");42         break;43     default:44         printf("only received %d bytes\n", r);45     }46     return r;47 }48 49 void producer(int mqid)50 {51     // Pay attention to the order we are sending these messages.52     send_msg(mqid, 1, "type 1 - first");53     send_msg(mqid, 2, "type 2 - second");54     send_msg(mqid, 1, "type 1 - third");55 }56 57 void consumer(int qid)7.7 Message Queues 395
continues
58 {
59     struct message msg;60     int r;61     int i;62     for (i = 0; i < 3; i++) {63         // -2 accepts messages of type 2 or less.64         r = msgrcv(qid, &msg, sizeof(struct message), -2, 0);65         printf("'%s'\n", msg.mtext);66     }67 }68 69 int main(int argc, char *argv[])70 {71     // Create a private (unnamed) message queue.72     int mqid;73     mqid = msgget(IPC_PRIVATE, S_IREAD | S_IWRITE);74     if (mqid == -1) {75         perror("msgget");76         exit(1);77     }78 79     pid_t pid = fork();80     if (pid == 0) {81         consumer(mqid);82         exit(0);83     }84     else {85         int status;86         producer(mqid);87         wait(&status);88     }89     // Remove the message queue.90     int r = msgctl(mqid, IPC_RMID, 0);91     if (r)92         perror("msgctl");93     return 0;
94 }
Some things to point out in this example are that it uses an unnamed message
queue and fixed-length messages. The receiver uses a feature of msgrcv that I have
not discussed. By specifying the acceptable message type as -2, it indicates that any
message of type 2 or lower may be accepted. This produces some interesting outputwhen you run the program:
$ cc -o sysv-msgq-example sysv-msgq-example.c
$ ./sysv-msgq-example'type 1 - first''type 1 - third''type 2 – second'396 Chapter 7 • Communication between Processes
Notice that the type 1 messages were received first. Lower numbers are higher
priority and received first. As a result, you received your messages in a differentorder from the order in which they were sent. To receive the messages in the sameorder in which they were sent (FIFO), use a zero for the 
type argument in msgrcv .
When the type argument of msgrcv has a nonzero value, the messages are
received by priority, as follows:
• Positive—Only messages of that type are accepted. 
• Negative—Only messages of the absolute value of the type specified or lower
values are accepted.
Linux also adds a MSG_EXCEPT flag, which is not part of any standard. When the
type is positive and MSG_EXCEPT is set in the flags argument of msgrcv , this has the
effect of inverting the selection—that is, instead of message type N, msgrcv will
accept anything but message type N.
7.7.2 The POSIX Message Queue
The POSIX API is functionally very similar to the System V API except that mes-sage queues are modeled closely after the POSIX file model. As in the file model,message queues have names, which must obey the same rules as filenames. Messagequeues can be opened, closed, created, and unlinked, just like files. In Linux, thesemessage queues consume file descriptors, just like open files. The POSIX messagequeue API should be very intuitive for a programmer who is comfortable with thePOSIX API for files.
7.7.2.1 Creating, Opening, Closing, and Removing POSIX Message Queues
POSIX does not provide a 
creat function for creating message queues. Instead,
message queues are created by using the mq_open function with the O_CREAT flag.
mq_open is defined as follows:
mqd_t mq_open(const char *name, int oflag, ...);
The function signature looks a lot like the open system call. The POSIX stan-
dard allows (but does not require) the return value to be a file descriptor. In Linux,it is a file descriptor. It should come as no surprise, then, that the 
oflag argument
takes the same arguments as the open system call, including O_CREAT , O_READ ,
O_WRITE , and O_RDWR . When the caller sets the O_CREAT flag, however, mq_open7.7 Message Queues 397
requires two additional arguments to create the message queue. The third argument
is the mode, just like the open call, and determines the read/write permissions of
the message queue. This takes the same values as the permission flags (such as
S_IREAD ) and is enforced by subsequent calls to mq_open on that message queue.
The fourth argument to mq_open , required by O_CREAT , is a pointer to a mq_attr
structure, which is defined as follows:
struct mq_attr
{
long int mq_flags; Implementation-defined flags, including O_NONBLOCK
long int mq_maxmsg; Maximum number of messages pending in the queue
long int mq_msgsize; Maximum size of each message in the queue
long int mq_curmsgs; Number of messages currently in the queue
long int __pad[4];
};
The mq_attr argument is optional and can be omitted, in which case the system
will use implementation-defined default values for the attributes. You can’t use amessage queue with unknown attributes, so POSIX provides the function
mq_getattr to retrieve this structure for a given message queue. There is also a
function named mq_setattr that allows you to adjust the flags of the queue. Both
these functions are defined as follows:
int mq_setattr(mqd_t mqdes, const struct mq_attr *iattr, struct mq_attr *oattr);
int mq_getattr(mqd_t mqdes, struct mq_attr *oattr);
The mq_maxmsg and mq_msgsize fields are used only by the mq_open call when
the message queue is created. These fields determine the maximum number of mes-sages the queue will hold and the size of each message, respectively. These values arefixed for the lifetime of the queue, so they are ignored by 
mq_setattr . Similarly, the
mq_curmsgs field has no meaning when passed as input to mq_open or mq_setattr
but is filled in only by the mq_getattr call and the output of the mq_getattr call.
Because the message queue ID returned by the Linux version of mq_open is a file
descriptor, you need a close function to free up the file descriptor and any associ-
ated resources. POSIX defines the mq_close function for this purpose:
int mq_close(mqd_t mqdes);
As you might guess, the Linux version of mq_close is just an alias for the close
system call. To delete a message queue permanently, use the mq_unlink function,
which is patterned after the unlink system call, as follows:398 Chapter 7 • Communication between Processes
int mq_unlink(const char *name);
As with a regular file, the message queue is not removed until the reference count
goes to zero. Any processes that have the message queue open when it is unlinkedwill continue to be able to use it.
7.7.2.2 Reading and Writing to a POSIX Message Queue
The functions for reading and writing the message look much like the System V
equivalents. Following are the prototypes for 
mq_send and mq_receive , slightly
abbreviated to conserve space:
int mq_send(mqd_t mqdes, char *ptr, size_t len, unsigned prio);
ssize_t mq_receive(mqd_t mqdes, char *ptr, size_t len, unsigned *prio);
Like the System V calls, these functions include a priority, but unlike System V,
POSIX places an upper limit on the size of messages. The mq_send function takes
arguments much like a write system call, with the addition of a priority argument.
mq_send will write messages that are smaller than the maximum allowed for the
message queue. mq_receive requires the receiver to provide enough space for the
maximum message size and returns the actual size of the message received.
By default, reading from an empty message queue blocks your process until a
message is available. Likewise, writing to a full message queue causes your processto block as well. You can specify the 
O_NONBLOCK flag when you open the message
queue to change this behavior. You also can change this flag dynamically with the
mq_setattr function.
7.7.2.3 A Complete Example Using POSIX Message Queues
Now let’s look at a complete example of a program that uses a POSIX message
queue. Listing 7-11 shows a basic usage of POSIX message queues. Here again, youhave a producer and a consumer process.
LISTING 7-11 posix-msgq-ex.c: Example of POSIX Message Queue
1 #include <stdio.h>
2 #include <string.h>3 #include <stdlib.h>4 #include <unistd.h>5 #include <mqueue.h>6 #include <sys/stat.h>7 #include <sys/wait.h>8 7.7 Message Queues 399
continues
9 // Simple message wrapper.
10 struct message {11     char mtext[128];12 };13 14 int send_msg(int qid, int pri, const char text[])15 {16     int r = mq_send(qid, text, strlen(text) + 1, pri);17     if (r == -1) {18         perror("mq_send");19     }20     return r;21 }22 23 void producer(mqd_t qid)24 {25     // Low priority messages26     send_msg(qid, 1, "This is my first message.");27     send_msg(qid, 1, "This is my second message.");28 29     // High priority message...30     send_msg(qid, 3, "No more messages.");31 }32 33 void consumer(mqd_t qid)34 {35     struct mq_attr mattr;36 37     // We assume the producer is finished at this point.38     do {39         u_int pri;40         struct message msg;41         ssize_t len;42 43         len = mq_receive(qid, (char *) &msg, sizeof(msg), &pri);44         if (len == -1) {45             perror("mq_receive");46             break;47         }48         printf("got pri %d '%s' len=%d\n", pri, msg.mtext, len);49 50         // Check for more messages in the queue.51         int r = mq_getattr(qid, &mattr);52         if (r == -1) {53             perror("mq_getattr");54             break;55         }56     } while (mattr.mq_curmsgs); // Stop when no more messages57 }58 400 Chapter 7 • Communication between Processes
59 int main(int argc, char *argv[])
60 {61     // Allow up to 10 messages before blocking.62     // Message size is 128 bytes (see above).63     struct mq_attr mattr = {64         .mq_maxmsg = 10,65         .mq_msgsize = sizeof(struct message)66     };67 68     mqd_t mqid = mq_open("/myq",69                          O_CREAT | O_RDWR,70                          S_IREAD | S_IWRITE,71                          &mattr);72 73     if (mqid == (mqd_t) -1) {74         perror("mq_open");75         exit(1);76     }77 78     // Fork a producer process, we'll be the consumer.79     pid_t pid = fork();80     if (pid == 0) {81         producer(mqid);82         mq_close(mqid);83         exit(0);84     }85     else {86         // Wait for the producer to send all messages so87         // we can illustrate priority.88         int status;89         wait(&status);90 91         consumer(mqid);92         mq_close(mqid);93     }94 95     mq_unlink("/myq");96     return 0;
97 }
When the program runs, it forks a producer process, which sends three messages
in sequence. The last message is given a higher priority than the first two for thepurpose of illustrating priority. The consumer process (the parent) waits for the pro-ducer to finish—not because it has to, but for illustration. This demonstration showsthat the message queue can hold messages until they can be delivered, even if the7.7 Message Queues 401
sender has exited. By allowing the messages to sit in the queue, you receive the mes-
sages in priority order, not in sequence:
$ ./posix-msgq-ex
got pri 3 'No more messages.' len=18got pri 1 'This is my first message.' len=26got pri 1 'This is my second message.' len=27
Note that you did not need to synchronize with the wait function. You could
have just as easily synchronized with mq_receive , but then the order of the mes-
sages would be undefined. They could come in the same order or in sequence. Theactual results would depend on the implementation and the OS scheduler.
7.7.3 Difference between POSIX Message Queues and System V
Message Queues
There are some significant differences in behavior between System V and POSIX
messages queues. System V allows the size of a message to vary as long as the valueread matches the value written. POSIX message queues, however, allow the senderto write variable-length messages, although the reader must provide enough roomfor the fixed message size—that is, a call to 
msg_receive fails if the size given is
not large enough to hold a full message.
Another difference is that whereas System V allows the reader to do some rudi-
mentary filtering of messages based on a particular priority, POSIX messages aredelivered in strict priority order—that is, the reader cannot pick which priority toread or block until a message of a particular priority is available. A read from aPOSIX message queue will always retrieve the highest-priority message available. Ifmore than one message of a given priority is available in the queue, the first mes-sage queued is the first message read (like a FIFO).
7.8 Semaphores
When two processes share resources, it is important that they maintain orderlyaccess to the shared resources; otherwise, chaos can erupt in the form of garbledoutput and program crashes. The word semaphore in computer terms refers to a spe-
cial type of flag that is used to synchronize concurrent processes. Semaphores arelike traffic lights for concurrent processes.
Here again, you have two APIs for using semaphores: the System V API and the
POSIX API. The basic semaphore is a counter. Conceptually, the counter keeps402 Chapter 7 • Communication between Processes
track of some finite resource. A very common pattern is to use one semaphore per
resource so that the counter never increments more than 1. This sometimes is calleda binary semaphore, because the value of the semaphore count is always 1 or 0.
A complete explanation of semaphores and concurrency is beyond the scope of
this book, but consider an example using the POSIX API. Listing 7-12 is a con-trived example of two processes that are trying to send two halves of the same mes-sage to the standard output. In this case, the standard output is the shared resourcethat must be protected with a semaphore.
LISTING 7-12 hello-unsync.c: Two Unsynchronized Processes Trying to Write to Standard Output
1 #include <stdio.h>
2 #include <string.h>3 #include <stdlib.h>4 #include <unistd.h>5 #include <sys/file.h>6 #include <sys/times.h>7 #include <sys/stat.h>8 #include <semaphore.h>9 #include <assert.h>
10 11 // Simple busy-wait loop to throw off our timing.12 void busywait(void)13 {14     clock_t t1 = times(NULL);15     while (times(NULL) - t1 < 2);16 }17 18 /*19 ** Simple message. 1st half printed by one process20 ** 2nd half printed by the other. No synchronization21 ** so the output is designed to be garbage.22 */23 int main(int argc, char *argv[])24 {25     const char *message = "Hello World\n";26     int n = strlen(message) / 2;27 28     pid_t pid = fork();29     int i0 = (pid == 0) ? 0 : n;30     int i;31 32     for (i = 0; i < n; i++) {33         write(1, message + i0 + i, 1);34         busywait();35     }
36 }7.8 Semaphores 403
When you run the program in Listing 7-12, you invariably will see garbage.
Note that I included a busywait routine to randomize the runtime just a little.
Otherwise, the scheduler can unintentionally allow this program to run in the cor-rect order:
$ cc -o hello-unsync hello-unsync.c
$ $ ./hello-unsyncHWelolo rld
Garbage caused by out-of-sync access to the standard output
The timing of Listing 7-12 is illustrated in Figure 7-2. The basic problem is that
both write calls can occur in any order. I deliberately made things worse by writ-
ing 1 byte at a time. To clean this up, you need a traffic cop to prevent more thanone process from accessing the standard output at a time. Listing 7-13 shows howto use a semaphore to do this using the POSIX API.404 Chapter 7 • Communication between Processes
FIGURE 7-2 Timing of Listing 7-12: Unsynchronized Code Produces GarbageParent
Child
write() write()fork()
exit() exit()No
Sychro nization
LISTING 7-13 hello-sync.c: Orderly Output with Two Processes
1 #include <stdio.h>
2 #include <string.h>3 #include <stdlib.h>4 #include <unistd.h>5 #include <sys/file.h>6 #include <sys/times.h>7 #include <sys/stat.h>8 #include <semaphore.h>9 #include <assert.h>
10 11 // Simple busy-wait loop to throw off our timing.12 void busywait(void)13 {14     clock_t t1 = times(NULL);15     while (times(NULL) - t1 < 2);16 }17 18 /*19 ** Simple message. 1st half printed by one process20 ** 2nd half printed by the other. Synchronized21 ** with a semaphore.22 */23 int main(int argc, char *argv[])24 {25     const char *message = "Hello World\n";26     int n = strlen(message) / 2;27 28     // Create the semaphore.29     sem_t *sem = sem_open("/thesem", O_CREAT, S_IRUSR | S_IWUSR);30     assert(sem != NULL);31 32     // Initialize the semaphore count to zero.33     int r = sem_init(sem, 1, 0);34     assert(r == 0);35 36     pid_t pid = fork();37     int i0 = (pid == 0) ? 0 : n;38     int i;39 40     // Parent waits for semaphore to increment.41     if (pid)42         sem_wait(sem);43 44     for (i = 0; i < n; i++) {45         write(1, message + i0 + i, 1);46         busywait();47     }48 49     // Child increments the semaphore when done.50     if (pid == 0)51         sem_post(sem);
52 }7.8 Semaphores 405
The fixed-up timing is shown in Figure 7-3. I’ll look at the POSIX API in more
detail shortly, but the example is about as simple as it gets with semaphores. You ini-tialize the semaphore count with zero so that any process that wants to wait for thesemaphore (to go nonzero) will block. I chose to allow the parent to block for thisexample and let the child print the first half of the message. So the first thing theparent does is wait for the semaphore with the 
sem_wait function, which causes it
to block.406 Chapter 7 • Communication between Processes
FIGURE 7-3 Timing of Listing 7-13: Semaphore Acts as Traffic CopParent
Child
write ()
write()fork()
sem_wait()sem_init()
exit()sem_post()
exit()sem_wait() blocks until 
child calls sem_post()
When the child is done, it increments the semaphore using the POSIX
sem_post function. This has the effect of unblocking the parent process, which
allows it to print the second half of the message. Now the message comes out cor-rectly every time:
$ cc -o hello-sync hello-sync.c
$ ./hello-syncHello World
Astute readers may have noticed that I could have used a wait system call to get
the same effect. In this trivial example, that’s true, but semaphores are much moreuseful. Notice that the POSIX API uses the terms wait and post to refer to decre-
menting and incrementing the semaphore, respectively. This may suggest that nocounting is going on, but these are counting semaphores.
7.8.1 Semaphores with the POSIX API
You saw one complete example of a semaphore between parent and child inListing 7-13. POSIX semaphores have names that are visible throughout the sys-tem. A semaphore exists from the time it is created until the time it is unlinked orthe system reboots, as the program in Listing 7-14 illustrates.
LISTING 7-14 posix_sem.c: Simple POSIX Semaphore Program
1 #include <stdio.h>
2 #include <string.h>3 #include <stdlib.h>4 #include <assert.h>5 #include <errno.h>6 #include <unistd.h>7 #include <sys/file.h>8 #include <sys/stat.h>9 #include <semaphore.h>
10 11 int main(int argc, char *argv[])12 {13     const char *semname = "/mysem";14 15     // Create the semaphore, and initialize count to zero.16     // Since we use O_EXCL, this will Fail if the17     // semaphore exists with EEXIST.18     sem_t *sem = sem_open(semname,7.8 Semaphores 407
continues
19                           O_CREAT | O_EXCL,
20                           S_IRUSR | S_IWUSR,21                           0);22     if (sem != SEM_FAILED) {23         printf("created new semaphore\n");24     }25     else if (errno == EEXIST) {26         // Semaphore exists, so open it without O_EXCL27         printf("semaphore exists\n");28         sem = sem_open(semname, 0);29     }30 31     assert(sem != SEM_FAILED);32 33     int op = 0;34 35     // User argument : zero, positive or negative36     if (argc > 1)37         op = atoi(argv[1]);38 39     if (op > 0) {40         printf("incrementing semaphore\n");41         sem_post(sem);42     }43     else if (op < 0) {44         printf("decrementing semaphore\n");45         sem_wait(sem);46     }47     else {48         printf("not modifying semaphore\n");49     }50     int val;51     sem_getvalue(sem, &val);52     printf("semaphore value is %d\n", val);53 54     return 0;
55 }
The semaphore is created the first time the program is run using the sem_open
function. The first three arguments to sem_open are identical to the open(2) sys-
tem call. The fourth argument is used only when the semaphore is created, and thatcontains the semaphore count. The prototype is as follows:
sem_t *sem_open( const char *name, int oflag, ...);408 Chapter 7 • Communication between Processes
I used the O_EXCL flag to force sem_open to fail when the semaphore exists. This
is not necessary, and I could have left it out. The flag is there so that you can printa different message when the semaphore is created.
This program does a single semaphore operation based on the user argument.
The user can increment the semaphore by using a positive integer for an argumentor decrement the semaphore by using a negative argument. For example:
$ cc -o posix_sem posix_sem.c -lrt
$ ./posix_sem 1 Create and increment the semaphore.
created new semaphoreincrementing semaphoresemaphore value is 1
$ ./posix_sem 1
Increment the semaphore again.
semaphore exists
incrementing semaphoresemaphore value is 2
Semaphore value reflects the number of sem_post calls.
$ ./posix_sem -1 Now let’s decrement.
semaphore existsdecrementing semaphoresemaphore value is 1
$ ./posix_sem 0
No operation
semaphore exists
not modifying semaphoresemaphore value is 1 
Value = 2 x sem_post – 1 x sem_wait
$ ./posix_sem -1semaphore existsdecrementing semaphoresemaphore value is 0
$ ./posix_sem -1
semaphore existsdecrementing semaphore
Semaphore blocks!
Listing 7-14 uses a named semaphore. I neglected to call sem_close in this
example, which, as you would expect, frees up the user-space resources consumedby the semaphore. Likewise, there is a 
sem_unlink function that removes a sem-
aphore from the system and frees up any system resources that the semaphoreconsumed.
The POSIX API also allows unnamed semaphores, but beware: Unnamed sema-
phores in Linux work only with threads. An unnamed semaphore is defined without7.8 Semaphores 409
using sem_open , which requires a name. Instead, the application calls sem_init ,
which has the following prototype:
int sem_init( sem_t *sem, int pshared, int value );
POSIX states that a nonzero pshared argument indicates that the semaphore
may be shared between processes. Linux does not implement this, which means thatunnamed semaphores may be used only between threads in a process. One patternto initialize an unnamed semaphore looks like the following:
sem_t mysem; User-defined storage
int r = sem_init( &mysem, 0, 0 ); Initialize to zero pshared=0
...
sem_destroy(&mysem); Do this to reclaim storage.
Note that the sem_destroy call is required before the storage can be reclaimed;
otherwise, memory corruption may result. If you use unnamed semaphores, it’ssafest to allocate them for the life of the application instead of putting them on thestack or heap.
7.8.2 Semaphores with the System V API
The System V API for semaphores is consistent with the APIs used for shared mem-ory and message queues—that is, the application defines a key and the system assignsan ID when the semaphore is created. An equivalent example to Listing 7-14 isshown in Listing 7-15.
LISTING 7-15 sysv_sem.c: Example Using System V Semaphores
1 #include <stdio.h>
2 #include <string.h>3 #include <stdlib.h>4 #include <assert.h>5 #include <errno.h>6 #include <unistd.h>7 #include <sys/stat.h>8 #include <sys/sem.h>9 
10 int main(int argc, char *argv[])11 {410 Chapter 7 • Communication between Processes
12     // Make a key using ftok
13     key_t semkey = ftok("/tmp", 'a');14 15     // Create the semaphore - an "array" of length 1.16     // Since we use IPC_EXCL, this will fail if the17     // semaphore exists with EEXIST.18     int semid =19         semget(semkey, 1, IPC_CREAT | IPC_EXCL | S_IRUSR | S_IWUSR);20     if (semid != -1) {21         printf("created new semaphore\n");22     }23     else if (errno == EEXIST) {24         // Semaphore exists, so open it without IPC_EXCL25         printf("semaphore exists\n");26         semid = semget(semkey, 1, 0);27     }28 29     assert(semid != -1);30 31     // Note: zero is a legitimate Sys V semaphore operation32     // So we only do an operation if we have an argument33     if (argc == 2) {34         int op = atoi(argv[1]);35 36         // Initialize the operations structure,37         // which applies to an array of semaphores.38         // but in this case we are using only one.39         struct sembuf sb = {40             .sem_num = 0,       // index into the array.41             .sem_op = op,       // value summed with the count42             .sem_flg = 0        // flags (e.g. IPC_NOWAIT)43         };44 45         // One call does it all!46         int r = semop(semid, &sb, 1);47 48         assert(r != -1);49         printf("operation %d done\n", op);50     }51     else {52         printf("no operation\n");53     }54 55     printf("semid %d value %d\n", semid, semctl(semid, 0, GETVAL));56 57     return 0;
58 }7.8 Semaphores 411
The functions map almost one for one to the POSIX API, with a few important
differences. For one thing, the System V API uses only the semop function for both
the equivalent wait and post operations. Also, the System V API includes a “wait
for zero” operation, which does not modify the semaphore value but blocks thecaller until the semaphore count goes to zero. Let’s look at this:
$ cc -Wall  -lrt  sysv_sem.c   -o sysv_sem
$ ./sysv_sem 0 Create semaphore
created new semaphore
operation 0 donesemid 360448 value 0
$ ./sysv_sem 1
Increment the semaphore
semaphore exists
operation 1 donesemid 360448 value 1
$ ./sysv_sem 0 &
Launch a background task to wait for zero
[1] 32475
semaphore exists Process blocks
$ ./sysv_sem -1semaphore existsoperation -1 donesemid 360448 value 0operation 0 done
Background job wakes up
semid 360448 value 0
The wait-for-zero operation is unique to System V semaphores. Just like POSIX
semaphores, decrementing a semaphore will also block if the semaphore value is zero.
7.9 Summary
This chapter introduced the basics of interprocess communication. I introducedseveral APIs and basic examples of each. In most cases, there are at least two APIsfor doing the same thing. I discussed the history and rationale behind these, whichideally should give you enough background to make an intelligent choice of API.
7.9.1 System Calls and APIs Used in This Chapter
I covered many APIs in this chapter in several categories.412 Chapter 7 • Communication between Processes
7.9.1.1 Miscellaneous
•flock —places an advisory lock on a file
•ftok —creates unique keys for use with System V IPC
•lockf —places a mandatory lock on a file
•select —function for blocking on or polling multiple file descriptors
7.9.1.2 Shared Memory
•shm_open , shm_unlink , mmap —POSIX shared memory routines
•shmget , shmat , shmdt , shmctl —System V shared memory routines
7.9.1.3 Signals
•kill , sigqueue —functions for sending signals
•sigaction , signal —functions for defining signal handlers
•sigpending , sigsuspend —functions for waiting on signals
•sigprocmask , sigemtpyset , sigfillset , sigaddset , sigdelset ,
sigismember —functions for manipulating signal masks
7.9.1.4 Pipes
•mkfifo —create a named pipe
•pipe —create an unnamed pipe
7.9.1.5 Sockets
•bind , listen , accept , close —vital functions for creating connection-
oriented servers
•connect —client function for connecting to a server socket
•socket —main function for creating sockets7.9 Summary 413
7.9.1.6 Message Queues
•mq_open , mq_close , mq_unlink , mq_send , mq_receive , mq_setattr ,
mq_getattr —POSIX message queue functions
•msgget , msgsend , msgrcv , msgctl —System V message queue functions
7.9.1.7 Semaphores
•sem_open , sem_close , sem_post , sem_wait —POSIX semaphore functions
•semget , semop —System V semaphore functions
7.9.2 References
• Gallmeister, B. POSIX 4 Programmers Guide. Sebastopol, Calif.: O’Reilly
Media, Inc., 1995.
• Robbins, A. Linux Programming by Example, The Fundamentals. Englewood
Cliffs, N.J.: Prentice Hall, 2004.
• Stevens, W. R., et al. UNIX Network Programming. Boston, Mass.: Addison-
Wesley, 2004.
7.9.3 Online Resources
• www.opengroup.org—publishes the POSIX standard (IEEE Standard 1003.2)
and many others (registration required)
• www.unix.org—publishes the Single UNIX Specification414 Chapter 7 • Communication between Processes
8.1 Introduction
In this chapter, I look at techniques and commands you can use from the shell for
debugging interprocess communication (IPC). When you are debugging commu-nication between processes, it’s always nice to have a neutral third party to intervenewhen things go wrong.
8.2 Tools for Working with Open Files
Processes that leave files open can cause problems. File descriptors can be “leaked”like memory, for example, consuming resources unnecessarily. Each process has afinite number of file descriptors it may keep open, so if some broken code contin-ues to open file descriptors without closing them, eventually it will fail with an
errno value of EMFILE . If you have some thoughtful error handling in your code,
it will be obvious what has happened. But then what?
8
415Debugging IPC with 
Shell Commands
The procfs file system is very useful for debugging such problems. You can see
all the open files of a particular process in the directory /proc/PID/fd . Each open
file here shows up as a symbolic link. The name of the link is the file descriptornumber, and the link points to the open file. Following is an example:
$ stty tostop Force background task to stop on output.
$ echo hello | cat ~/.bashrc 2>/dev/null & Run catin the background.
[1] 16894 It’s stopped.
$ ls -l /proc/16894/fd Let’s see what files it has open.
total 4
lr-x------  1 john john 64 Apr  9 12:15 0 -> pipe:[176626]lrwx------  1 john john 64 Apr  9 12:15 1 -> /dev/pts/2l-wx------  1 john john 64 Apr  9 12:15 2 -> /dev/nulllr-x------  1 john john 64 Apr  9 12:15 3 -> /home/john/.bashrc
Here, I piped the output of echo to the catcommand, which shows up as a pipe
for file descriptor zero (standard input). The standard output points to the currentterminal, and I redirected the standard error (file descriptor 2) to 
/dev/null .
Finally, the file I am trying to print shows up in file descriptor 3. All this showsfairly clearly in the output.
8.2.1 lsof 
You can see a more comprehensive listing by using the lsof command. With no
arguments, lsof will show all open files in the system, which can be overwhelm-
ing. Even then, it will show you only what you have permission to see. You canrestrict output to a single process with the 
-poption, as follows:
$ lsof -p 16894
COMMAND   PID USER   FD   TYPE DEVICE     SIZE   NODE NAMEcat     16894 john  cwd    DIR  253,0     4096 575355 /home/johncat     16894 john  rtd    DIR  253,0     4096      2 /cat     16894 john  txt    REG  253,0    21104 159711 /bin/catcat     16894 john  mem    REG  253,0   126648 608855 /lib/ld-2.3.5.socat     16894 john  mem    REG  253,0  1489572 608856 /lib/libc-2.3.5.socat     16894 john  mem    REG    0,0               0 [heap] cat     16894 john  mem    REG  253,0 48501472 801788 .../locale-archivecat     16894 john    0r  FIFO    0,5          176626 pipecat     16894 john    1u   CHR  136,2               4 /dev/pts/2cat     16894 john    2w   CHR    1,3            1510 /dev/nullcat     16894 john    3r   REG  253,0      167 575649 /home/john/.bashrc
This output shows not only file descriptors, but memory-mapped files as well.
The FDheading tells you whether the output is a file descriptor or a mapping. A
mapping does not require a file descriptor after mmap has been called, so the FD416 Chapter 8 • Debugging IPC with Shell Commands
column includes some text for each mapping to indicate the type of mapping. File
descriptors are shown by number as well as the type of access, as summarized inTable 8-1.
You also can use 
lsof to discover which process has a particular file open by
providing the filename as an argument. There are many more options to the lsof
command; see lsof(8) for details.
8.2.2 fuser
Another utility for tracking down open files is the fuser command. Suppose that
you need to track down a process that is writing a huge file that is filling up yourfile system. You could use 
fuser as follows:
$ fuser some-huge-file.txt What process has this file open?
some-huge-file.txt:  17005
If that’s all you care about, you could go ahead and kill the process. fuser allows
you to do this with the -koption as follows:
]$ fuser -k -KILL some-huge-file.txt
some-huge-file.txt:  17005[1]+  Killed                  cat some-huge-file.txt8.2 Tools for Working with Open Files 417
TABLE 8-1 Text Used in the FD Column of lsof Output
Identifier Meaning
cwd Current working directory
ltx Shared library text (code and data)
mem Memory-mapped file
mmap Memory-mapped device
pd Parent directory
rtd Root directory
txt Program text (code and data)
{digit}r File descriptor opened read-only
{digit}w File descriptor opened write-only
{digit}u File descriptor opened read/write.
This sends the SIGKILL signal to any and all processes that have this file open.
Another time fuser comes in handy is when you are trying to unmount a file sys-
tem but can’t because a process has a file open. In this case, the -moption is very
helpful:
$ fuser -m /mnt/flash What process has files open on this file system?
/mnt/flash:          17118
Now you can decide whether you want to kill the process or let it finish what it
needs to do. fuser has more options that are documented in the fuser(1)man page.
8.2.3 ls
You will be interested in the longlisting available with the -loption. No doubt you
are aware that this gives you the filename, permissions, and size of the file. The out-put also tells you what kind of file you are looking at. For example:
$ ls -l /dev/log /dev/initctl /dev/sda /dev/zero
prw-------  1 root root    0 Oct  8 09:13 /dev/initctl A pipe (p)
srw-rw-rw-  1 root root    0 Oct  8 09:10 /dev/log A socket (s)
brw-r-----  1 root disk 8, 0 Oct  8 04:09 /dev/sda A block device (b)
crw-rw-rw-  1 root root 1, 5 Oct  8 04:09 /dev/zero A char device (c)
For files other than plain files, the first column indicates the type of file you are
looking at. You can also use the -Foption for a more concise listing that uses unique
suffixes for special files:
$ ls -F /dev/log /dev/initctl /dev/zero /dev/sda
/dev/initctl|  /dev/log=  /dev/sda  /dev/zero
A pipe is indicated by adding a |to the filename, and a socket is indicated by
adding a =to the filename. The -Foption does not use any unique character to
identify block or character devices, however.
8.2.4 file
This simple utility can tell you in a very user-friendly way the type of file you arelooking at. For example:
file /dev/log /dev/initctl /dev/sda /dev/zero
/dev/log:     socket/dev/initctl: fifo (named pipe)/dev/sda:     block special (8/0)
Includes major/minor numbers
/dev/zero:    character special (1/5) Includes major/minor numbers418 Chapter 8 • Debugging IPC with Shell Commands
Each file is listed with a simple, human-readable description of its type. The
file command can also recognize many plain file types, such as ELF files and
image files. It maintains an extensive database of magic numbers to recognize filetypes. This database can be extended by the user as well. See 
file(1) for more
information.
8.2.5 stat
The stat command is a wrapper for the stat system that can be used from the
shell. The output consists of all the data you would get from the stat system call
in human-readable format. For example:
stat /dev/sda
File: `/dev/sda'Size: 0               Blocks: 0          IO Block: 4096   block special file
Device: eh/14d  Inode: 1137        Links: 1     Device type: 8,0Access: (0640/brw-r-----)  Uid: (    0/    root)   Gid: (    6/    disk)Access: 2006-10-08 04:09:34.750000000 -0500Modify: 2006-10-08 04:09:34.750000000 -0500Change: 2006-10-08 04:09:50.000000000 -0500
stat also allows formatting like the printf function, using specially defined
format characters defined in the stat(1) man page. To see only the name of each
file followed by its access rights in human-readable form and octal, you could usethe following command:
stat --format="%-15n %A,%a" /dev/log /dev/initctl /dev/sda /dev/zero
/dev/log        srw-rw-rw-,666/dev/initctl    prw-------,600/dev/sda        brw-r-----,640/dev/zero       crw-rw-rw-,666
stat can be very useful in scripts to monitor particular files on disk. During
debugging, such scripts can act like watchdogs. You can watch a UNIX socket tolook for periods of inactivity as follows:
while [ true ]; do
ta=$(stat -c %X $filename) # Time of most recent activitytnow=$(date +%s) # Current time
if [ $(($tnow - $ta)) -gt 5 ]; then
echo No activity on $filename in the last 5 seconds.
fisleep 1
done8.2 Tools for Working with Open Files 419
In this example, the script checks a file every second for the most recent access to
the file, which is given with the %Xformat option to stat . Whenever a process
writes to the socket, the time is updated, so the difference between the current timeand the time from the 
stat command is the amount of elapsed time (in seconds)
since the last write or read from the socket.
8.3 Dumping Data from a File
You probably are familiar with a few tools for this purpose, including your favoritetext editor for looking at text files. All the regular text processing tools are at yourdisposal for working with ASCII text files. Some of these tools have the ability towork with additional encodings—if not through a command-line option, maybevia the locale setting. For example:
$ wc -w konnichiwa.txt Contains the Japanese phrase “konnichiwa” (one word).
0 konnichiwa.txt wc reports 0words based on current locale.
$ LANG=ja_JP.UTF-8 wc -w konnichiwa.txt
1 konnichiwa.txt Forced Japanese locale gives us the correct answer.
Several tools can help with looking at binary data, but not all of them help inter-
pret the data. To appreciate the differences among tools, you’ll need an example(Listing 8-1).
LISTING 8-1 filedat.c: A Program That Creates a Data File with Mixed Formats
1 #include <stdio.h>
2 #include <string.h>3 #include <stdlib.h>4 const char message[] = { // UTF-8 message5     0xbf, 0xe3, 0x81, 0x93, 0xe3, 0x82, 0x93, 0xe3, 0x81, 0xab,6     0xe3, 0x81, 0xa1, 0xe3, 0x81, 0xaf, '\r', 0x20, 0x20, 0x20,7     0x20, 0x20, 0x20, 0x20, 0x20, 0x20, 0x20, 0x0a, 0x0a, 08 };9 
10 int main(int argc, char *argv[])11 {12     const char *filename = "floats-ints.dat";13     FILE *fp = fopen(filename, "wb");14 15     /* error checking omitted. */16 17     fprintf(fp, "Hello World\r%12s\n", "");18     fwrite(message, sizeof(message), 1, fp);19 20     /* write 250 zeros to the file. */420 Chapter 8 • Debugging IPC with Shell Commands
21     char *zeros = calloc(250, 1);
22     fwrite(zeros, 250, 1, fp);23 24     int i;25 26     /* Write four ints to the file 90000, 90001, ... */27     for (i = 0; i < 4; i++) {28         int idatum = i + 90000;29         fwrite((char *) &idatum, sizeof(idatum), 1, fp);30     }31 32     /* Write four floats to the file 90000, 90001, ... */33     for (i = 0; i < 4; i++) {34         float fdatum = (float) i + 90000.0;35         fwrite((char *) &fdatum, sizeof(fdatum), 1, fp);36     }37     printf("wrote %s\n", filename);38     fclose(fp);
39 }
Listing 8-1 creates a file that contains a mix of ASCII, UTF-8, and binary data.
The binary data is in native integer format (32 bits on my machine) and IEEE float(also 32 bits). A simple 
catcommand produces nothing but garbage:
$ ./filedat
wrote floats-ints.dat$ cat floats-ints.dat
floats-ints.dat____È¯GÈ¯GÉ¯GÉ¯G$
Not even “Hello World” is printed!
The problem, of course, is that catjust streams bytes out to the terminal, which
then interprets those bytes as whatever encoding the locale is using. In the “ Hello
World ” string on line 17 of Listing 8-1, I included a carriage return followed by 12
spaces. This has the effect of writing “ Hello World ” but then overwriting it with
12 spaces, which effectively makes the string invisible on the terminal.
You could use a text editor on this file, but the results may vary based on your
text editor. Earlier, I looked at the bvieditor, which is a Vi clone for files with
binary data.
Figure 8-1 shows that bvidoes a good job of representing raw bytes and ASCII
strings, and even lets you modify the data, but it is not able to represent dataencoded in UTF-8, IEEE floats, or native integers. For that, you’ll need other tools.8.3 Dumping Data from a File 421
8.3.1 The strings Command
Often, the text strings in a data file can give you a clue as to its contents. Sometimes,
the text can tell you all you need to know. When the text is embedded in a bunchof binary data, however, you need something better than a simple 
catcommand. 
Looking back at the output of Listing 8-1, you can use the strings command
to look at the text strings in this data:
$ strings floats-ints.dat
Hello World
Invisible characters? Newlines? Who knows?
$ 
Now you can see Hello World and the spaces, but something is still missing.
Remember that message array on line 18? It’s actually UTF-8 text I encoded in
binary. strings can look for 8-bit encodings (that is, non-ASCII) when you use
the -eoption as follows:
$ strings -eS floats-ints.dat T ell strings to look for 8-bit encodings (-eS)
Hello World
Japanese “konnichiwa,” “good day” in UTF-8
Our floats and ints produce this gobbledygook.
422 Chapter 8 • Debugging IPC with Shell Commands
FIGURE 8-1 The Output from Listing 8-1 As Seen in bvi

The example above shows that the UTF-8 output is in Japanese, but I glossed
over one detail: To show this on your screen, your terminal must support UTF-8characters. Technically, you also need the correct font to go with it, but it seems thatmost UTF-8 font sets have the Hiragana
1characters required for the message above.
With gnome-terminal , you can get the required support by setting the character
encoding to UTF-8. This is visible below Terminal on the menu bar. Not every ter-minal supports UTF-8; check your documentation.
By default, 
strings limits the output to strings of four characters or more; any-
thing smaller is ignored. You can override this with the -noption, which indicates the
smallest string to look for. To see the binary data in your file, you will need other tools.
8.3.2 The xxd Command
xxdis part of Vim and produces output very similar to bvi. The difference is that
xxdis not a text editor. Like bvi, xxdshows data in hexadecimal and shows only
ASCII characters:
$ xxd floats-ints.dat
0000000: 4865 6c6c 6f20 576f 726c 640d 2020 2020  Hello World.0000010: 2020 2020 2020 2020 0abf e381 93e3 8293          ........0000020: e381 abe3 81a1 e381 af0d 2020 2020 2020  ..........0000030: 2020 2020 0a0a 0000 0000 0000 0000 0000      ............0000040: 0000 0000 0000 0000 0000 0000 0000 0000  ................0000050: 0000 0000 0000 0000 0000 0000 0000 0000  ................0000060: 0000 0000 0000 0000 0000 0000 0000 0000  ................0000070: 0000 0000 0000 0000 0000 0000 0000 0000  ................0000080: 0000 0000 0000 0000 0000 0000 0000 0000  ................0000090: 0000 0000 0000 0000 0000 0000 0000 0000  ................00000a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................00000b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................00000c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................00000d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................00000e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................00000f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................0000100: 0000 0000 0000 0000 0000 0000 0000 0000  ................0000110: 0000 0000 0000 0000 0000 0000 0000 0000  ................0000120: 0000 0000 0000 0000 0000 0000 0000 0000  ................0000130: 0090 5f01 0091 5f01 0092 5f01 0093 5f01  .._..._..._..._.0000140: 0000 c8af 4780 c8af 4700 c9af 4780 c9af  ....G...G...G...0000150: 47                                       G8.3 Dumping Data from a File 423
1. Hiragana is one of three sets of characters required to render Japanese text.
xxddefaults to 16-bit words, but you can adjust this with the -goption. To see
the data in groups of 4 bytes, for example, use -g4. Make sure, however, that the
groups preserve the byte order in the file. This means that 32-bit words printed onan IA32 will be incorrect. IA32 stores words with the least significant byte first,which is the reverse of the byte order in memory. This is sometimes called Little
Endian byte order. To display the correct words, you must reverse the order of the
bytes, which 
xxddoes not do.
This can come in handy on some occasions. If you need to look at Big Endian
data on a Little Endian machine, for example, you do not want to rearrange thebytes. Network protocols use the so-called network byte order for data transfer,which happens to be the same as Big Endian. So if you happen to be looking at afile that contains protocol headers from a socket, you would want a tool like 
xxd
that does not swap the bytes.
8.3.3 The hexdump Command
As the name suggests, hexdump allows you to dump a file’s contents in hexadeci-
mal. As with xxd, the default format from hexdump is 16-bit hexadecimal, how-
ever, the byte order is adjusted on Little Endian architectures, so the output candiffer between 
xxdand hexdump .
hexdump is better suited for terminal output than xxdbecause hexdump elimi-
nates duplicate lines of data skipped to avoid cluttering the screen. hexdump can
produce many other output formats besides 16-bit hexadecimal, but using themcan difficult. Because the 
hexdump(1) man page does such a rotten job of explain-
ing this feature, here’s an example using 32-bit hexadecimal output:
$ hexdump -e '6/4 "%8X "' -e '"\n"'  floats-ints.dat
6C6C6548 6F57206F  D646C72 20202020 20202020 2020202081E3BF0A 9382E393 E3AB81E3 81E3A181 20200DAF 2020202020202020      A0A        0        0        0        0
0        0        0        0        0        0
*
0        0        0        0  15F9000  15F9100
15F9200  15F9300 AFC80000 AFC88047 AFC90047 AFC98047
47
Notice that I included two -eoptions. The first tells hexdump that I want 6 val-
ues per line, each with a width of 4 bytes (32 bits). Then I included a space, fol-lowed by the 
printf -like format in double quotes. hexdump looks for the double424 Chapter 8 • Debugging IPC with Shell Commands
quotes and spaces in the format arguments, and will complain if it does not find
them. That is why I needed to enclose the entire expression in single quotes.
Still looking at this first argument, I had to include a space following the %8Xto
separate the values. I could have used a comma or semicolon or whatever, but
hexdump interprets this format verbatim. If you neglect to include a separator, all
the digits will appear as one long string.
Finally, I told hexdump how to separate each line of output (every six words) by
including a second -eoption, which for some reason must be enclosed in double
quotes. If you can’t tell, I find hexdump to be a nuisance to use, but many pro-
grammers use it. The alternatives to hexdump are xxdand od.
8.3.4 The od Command
odis the traditional UNIX octal dump command. Despite the name, odis capa-
ble of representing data in many other formats and word sizes. The -toption is
the general-purpose switch for changing the output data type and element size(although there are aliases based on legacy options). You can see the earlier textfile as follows:
$ od -tc floats-ints.dat Output data as ASCII characters
0000000   H   e   l   l   o       W   o   r   l   d  \r
0000020                                  \n 277 343 201 223 343 202 2230000040 343 201 253 343 201 241 343 201 257  \r0000060                  \n  \n  \0  \0  \0  \0  \0  \0  \0  \0  \0  \00000100  \0  \0  \0  \0  \0  \0  \0  \0  \0  \0  \0  \0  \0  \0  \0  \0
*
Duplicate lines are skipped (indicated with “*”).
0000460  \0 220   _ 001  \0 221   _ 001  \0 222   _ 001  \0 223   _ 001
0000500  \0  \0 310 257   G 200 310 257   G  \0 311 257   G 200 311 2570000520   G0000521
This output is comparable to what you’ve already seen with other tools. By
default, the offsets on the left are printed in octal (in keeping with the name). Youcan change the base of the offsets with the 
-Aoption. -Ax, for example, prints the
offsets in hexadecimal.
od’s treatment of strings is similar to that of xxdand bvi. It recognizes ASCII
for display on the terminal but treats everything else as raw binary. What odcan do
that the others can’t is rearrange the bytes when necessary to represent data in native
format. Recall that the data from Listing 8-1 has IEEE floats and integers in the8.3 Dumping Data from a File 425
data. To see the integers in decimal, you can use the -tdoption, but you must tell
odwhere the data starts. In this case, the float data starts at offset 0x131 in the file,
so use the -joption as follows:
$ od -td4 -j0x131 floats-ints.dat Show me 4-byte words in decimal.
0000461       90000       90001       90002       90003
0000501  1202702336  1202702464  1202702592  1202702720 Float data (gibberish)
0000521
Now you can see the four consecutive decimal numbers we stored, starting with
90000 . If you do not specify the offset to the data, the output will be incorrect. The
float and integer data in this case starts on an odd boundary. The float data starts atoffset 
0x141 , so you must use the -joption again to see your floats:
$ od -tf4 -j0x141 floats-ints.dat
0000501   9.000000e+04   9.000100e+04   9.000200e+04   9.000300e+040000521
I stored four consecutive float values starting with 90000 . Notice that in this case,
I qualified the type as -tf4 . I used IEEE floats in the program, which are 4 bytes each.
The default for the -tfoption is to display IEEE doubles, which are 8 bytes each. If
you do not specify IEEE floats, you would see garbage. 
Note that odadjusts the byte order only when necessary. As long as your data is
in native byte order, odwill produce correct results. If you are looking at data that
you know is in network byte order (that is, Big Endian), odwill show you incorrect
answers on a Little Endian machine such as IA32.
8.4 Shell Tools for System V IPC
The preferred tools for working with System V IPC objects are the ipcs and
ipcrm commands. ipcs is a generic tool for all the System V IPC objects I’ve dis-
cussed. ipcrm is used to remove IPC objects that may be left behind after a process
exits or crashes.
8.4.1 System V Shared Memory
For shared memory objects, the ipcs command will show you the application-
defined key (if any), as well as the system-defined ID for each key. It will also showyou whether any processes are attached to the shared memory. The X Window sys-tem uses System V IPC shared memory extensively, so a spot check on your systemis likely to reveal many shared memory objects in use. For example:426 Chapter 8 • Debugging IPC with Shell Commands
$ ipcs -m -m indicates that only shared memory objects should be shown.
------ Shared Memory Segments --------
key        shmid      owner      perms      bytes      nattch     status0x00000000 163840     john      600        196608     2          dest0x66c8f395 32769      john      600        1          00x237378db 65538      john      600        1          00x5190ec46 98307      john      600        1          00x31c16fd1 131076     john      600        1          00x00000000 196613     john      600        393216     2          dest0x00000000 229382     john      600        393216     2          dest0x00000000 262151     john      600        196608     2          dest0x00000000 294920     john      600        393216     2          dest0x00000000 327689     john      600        393216     2          dest0x00000000 360458     john      600        196608     2          dest0x00000000 393227     john      600        393216     2          dest0x00000000 425996     john      600        196608     2          dest0x00000000 884749     john      600        12288      2          dest0x00000000 2031630    john      600        393216     2          dest0x00000000 2064399    john      600        196608     2          dest0x00000000 2097168    john      600        16384      2          dest
Here, you see a mix of private and public shared memory objects. Private objects
have a keyof zero, although every object has a unique shmid . The nattch column
tells you how many processes currently are attached to the shared memory object.The 
-poption of ipcs shows you the process ID of the object’s creator and the
process ID of the process that most recently attached to or detached from eachshared object. For example:
$ ipcs -m -p
------ Shared Memory Creator/Last-op --------
shmid      owner      cpid       lpid163840     john       2790       290632769      john       2788       065538      john       2788       098307      john       2788       0131076     john       2788       0196613     john       2897       2754229382     john       2899       2921262151     john       2899       2921294920     john       2907       2754327689     john       2921       2923360458     john       2893       2754393227     john       2893       2754425996     john       2921       2754884749     john       2893       27542031630    john       8961       93922064399    john       8961       93922097168    john       8961       93928.4 Shell Tools for System V IPC 427
The creator’s PID is listed as cpid and the last PID to attach or detach is listed
as lpid . You may think that as long as nattch is 2, these are the only processes.
Don’t forget that there is no guarantee that the object creator is still attached (orstill running). Likewise, the last process to attach or detach to the object doesn’ttell you much.
If 
nattch is zero, and neither process listed by ipcs is running, it may be safe to
delete the object with the icprm command. What ipcs does not answer is “Who
has this memory mapped now?” You can answer this question with a brute-force
search using the lsof command. Consider the following example:
$ ipcs -m
------ Shared Memory Segments --------
key        shmid      owner      perms      bytes      nattch     status...0xdeadbeef 2752529    john      666        1048576    3
There are three processes attached to this object, but what are they?
$ ipcs -m -p
------ Shared Memory Creator/Last-op --------
shmid      owner      cpid       lpid2752529    john       10155      10160
Process 10155 and 10160 are suspects. lsof to the rescue.
$ lsof | head -1 ; lsof | grep 2752529COMMAND     PID    USER   FD   TYPE   DEVICE  SIZE    NODE NAMEsysv-shm  10155    john  DEL    REG      0,7       2752529 /SYSVdeadbeefsysv-clie 10158    john  DEL    REG      0,7       2752529 /SYSVdeadbeefsysv-clie 10160    john  DEL    REG      0,7       2752529 /SYSVdeadbeef
The lsof command produces a great deal of output, but you can grep for the
shmid to see which processes are still using this object. Notice that lsof indicates
the key in the NAME column in hexadecimal. You could have used this as the search
key as well.
If no running process is attached to the shared memory, you probably can assume
that this object is just code droppings and can be removed.428 Chapter 8 • Debugging IPC with Shell Commands
You can get more information about a shared memory object by using the -i
option to ipcs . When you’ve decided that it’s safe to remove a System V shared
memory object, you need to use the shmid of the object (not the key). For example:
$ ipcs -m -i 32769 T ell me about this shmid .
Shared memory Segment shmid=32769
uid=500 gid=500 cuid=500 cgid=500mode=0600 access_perms=0600bytes=1 lpid=0 cpid=2406 nattch=0
Created by process 2406 ...
att_time=Not set                   det_time=Not set                   change_time=Sat Apr  8 15:48:24 2006  
$ kill -0 2406
Is this process still running?
bash: kill: (2406) – No such process Not running
$ ipcrm -m 32769 Let’s delete the shared memory.
Notice that you must indicate that you are deleting a shared memory object with
-m. ipcs is used for all types of IPC objects, not just shared memory. The shmid
alone does not tell the system about the type of object; nothing prevents a message
queue and a shared memory object from using the same identifier.
8.4.2 System V Message Queues
You can use the ipcs command to list all the System V message queues by using
the -qoption as follows:
$ ipcs -q
------ Message Queues --------key        msqid      owner      perms      used-bytes   messages0x00000000 131072     john       600        0            00x00000000 163841     john       600        0            00x00000000 196610     john       600        0            00x00000000 229379     john       600        132          1
The values listed in the keycolumn are the application-defined keys, whereas the
values listed under msqid are the system-defined keys. As you might expect, the sys-
tem-defined keys are unique. The application-defined keys in this case are all 0,
which means these message queues were created with the IPC_PRIVATE key.
One of the queues listed above ( msgqid 229379 ) has data in it, which you can
see below the headings used-bytes and messages . This could be a symptom of a8.4 Shell Tools for System V IPC 429
problem, because most applications don’t let messages sit in queues for very long.
Again, the -ioption of ipcs is helpful:
$ ipcs -q -i 229379
Message Queue msqid=229379
uid=500 gid=500 cuid=500        cgid=500        mode=0600cbytes=132      qbytes=16384    qnum=1  lspid=12641     lrpid=0send_time=Sun Oct 22 15:25:53 2006rcv_time=Not setchange_time=Sun Oct 22 15:25:53 2006
Notice that the lspid and lrpid fields contain the last sender PID and the last
receiver PID, respectively. If you can determine that this queue is no longer needed,
you can delete it by using the message queue ID as follows:
$ ipcrm -q 229379
Again, the ipcrm command applies to more than just message queues, so you
indicate the system ID of the object as well as the fact that it is a message queuewith the 
-qoption.
8.4.3 System V Semaphores
Just as with message queues and shared memory, the ipcs command can be used
to list all the semaphores in the system with the -soption, as follows:
$ ipcs -s
------ Semaphore Arrays --------
key        semid      owner      perms      nsems0x6100f981 360448     john      600        1
Recall that System V semaphores are declared as arrays. The length of the array
is shown in the nsems column. The output is very similar to the output for mes-
sage queues. Likewise, you can remove the semaphore with the ipcrm command as
follows:
$ ipcrm -s 360448
Here again, you specify the system semaphore ID (not the key) to remove the
semaphore. Additional information can be retrieved with the -ioption:430 Chapter 8 • Debugging IPC with Shell Commands
$ ipcs -s -i 393216
Semaphore Array semid=393216
uid=500  gid=500         cuid=500        cgid=500mode=0600, access_perms=0600nsems = 1otime = Tue May  9 22:23:30 2006ctime = Tue May  9 22:22:23 2006semnum     value      ncount     zcount     pid0          3          0          1          32578
The output is similar to the stat command for files except that there is addi-
tional information specific to the semaphore. The ncount is the number of
processes blocking on the semaphore, waiting for it to increment. The zcount is
the number of processes blocking on the semaphore, waiting for it to go to zero.The 
pidcolumn identifies the most recent process to complete a semaphore opera-
tion; it does not identify processes waiting on the semaphore.
The pscommand can help identify processes waiting on a semaphore. The
wchan format option shows what system function is blocking a process. For a
process blocking on a semaphore, it looks as follows:
$ ps -o wchan -p 32746
WCHANsemtimedop
The semtimedop is the system call that is used for the semaphore operation.
Unfortunately, there is no way to identify which process is waiting on which sema-phore. The process maps and file descriptors do not give away the semaphore IDs.
8.5 Tools for Working with POSIX IPC
POSIX IPC uses file descriptors for every object. The POSIX pattern is that everyfile descriptor has a file or device associated with it, and Linux extends this with spe-cial file systems for IPC. Because each IPC object can be traced to a plain file, thetools we use for working with plain files are often sufficient for working withPOSIX IPC objects.
8.5.1 POSIX Shared Memory
There are no tools specifically for POSIX shared memory. In Linux, POSIX sharedmemory objects reside on the 
tmpfs pseudo file system, which typically is mounted
on /dev/shm . That means that you can use all the normal file-handling tools at8.5 Tools for Working with POSIX IPC 431
your disposal to debug these objects. Everything that I mentioned in the section on
working with open files applies here. The only difference is that all the files you willneed to look at are on a single file system.
As a result of the Linux implementation, it is possible to create and use shared
memory with only standard system calls: 
open , close , mmap , unlink , and so on.
Just keep in mind that this is all Linux specific. The POSIX standard seems toencourage this particular implementation, but it does not require it, so portablecode should stick to the POSIX shared memory system calls.
Just to illustrate this point, let’s walk through an example of some shell com-
mands mixed with a little pseudocode. I’ll create a shared memory segment fromthe shell that a POSIX program can map:
$ dd if=/dev/zero of=/dev/shm/foo.shm count=100 Create /foo.shm
100+0 records in
100+0 records out$ ls -lh /dev/shm/foo.shm-rw-rw-r--  1 john john 50K Apr  9 21:01 /dev/shm/foo.shm
Now a POSIX shared memory program can attach to this shared memory, using
the name /foo.shm :2
int fd = shm_open("/foo.shm",O_RDWR,0);
Creating a shared memory segment this way is not portable but can be very use-
ful for unit testing and debugging. One idea for a unit test environment is to cre-ate a wrapper script that creates required shared memory segments to simulate otherrunning processes while running the process under test.
8.5.2 POSIX Message Queues
Linux shows POSIX message queues via the mqueue pseudo file system.
Unfortunately, there is no standard mount point for this file system. If you need todebug POSIX message queues from the shell, you will have to mount the file sys-tem manually. To mount this on a directory named 
/mnt/mqs , for example, you
can use the following command:
$ mkdir /mnt/mqs
$ mount -t mqueue none /mnt/mqs Must be the root user to use mount432 Chapter 8 • Debugging IPC with Shell Commands
2. The leading slash is not strictly required, but it is recommended.
When the file system is mounted, you can see an entry for each POSIX message
queue in the system. These are not regular files, however. If you catthe file, you
will see not messages, but a summary of the queue properties. For example:
$ ls -l /mnt/mqs
total 0-rw-------  1 john john 80 Apr  9 00:20 myq
$ cat /mnt/mqs/myq
QSIZE:6          NOTIFY:0     SIGNO:0     NOTIFY_PID:0
The QSIZE field tells you how many bytes are in the queue. A nonzero value here
may be indication of a deadlock or some other problem. The fields NOTIFY , SIGNO ,
and NOTIFY_PID are used with the mq_notify function, which I do not cover in
this book.
To remove a POSIX message queue from the system using the shell, simply use
the rmcommand from the shell and remove it from the mqueue file system by name.
8.5.3 POSIX Semaphores
Named POSIX semaphores in Linux are implemented as files in tmpfs , just like
shared memory. Unlike in the System V API, there is no system call in Linux to cre-ate a POSIX semaphore. Semaphores are implemented mostly in user space, usingexisting system calls. That means that the implementation is determined largely bythe GNU real-time library (
librt ) that comes with the glibc package.
Fortunately, the real-time library makes some fairly predictable choices that are
easy to follow. In glibc 2.3.5, named semaphores are created as files in /dev/shm .
A semaphore named mysem shows up as /dev/shm/sem.mysem . Because the
POSIX API uses file descriptors, you can see semaphores in use as open files in
procfs ; therefore, tools such as lsof and fuser can see them as well.
You can’t see the count of a POSIX semaphore directly. The sem_t type that
GNU exposes to the application contains no useful data elements—just an array of
ints. It’s reasonable to assume, however, that the semaphore count is embedded in
this data. Using the posix_sem.c program from Listing 7-14 in Chapter 7, for
example:
$ ./posix_sem 1 Create and increment the semaphore.
created new semaphore
incrementing semaphoresemaphore value is 18.5 Tools for Working with POSIX IPC 433
$ ./posix_sem 1 Increment the semaphore again.
semaphore exists
incrementing semaphoresemaphore value is 2$ od -tx4 /dev/shm/sem.mysem
Dump the file to dump the count.
0000000 00000002 ...
Although you can use tools like lsof to find processes using a semaphore,
remember that just because a process is using a semaphore doesn’t mean that it’s
blocking on it. One way to determine whether a process is blocking on a particularsemaphore is to use 
ltrace . For example:
$ lsof /dev/shm/sem.mysem Identify the process using a named semaphore...
COMMAND PID USER  FD   TYPE DEVICE SIZE    NODE NAME
pdecr   661 john mem    REG   0,16   16 1138124 /dev/shm/sem.mysem
$ ltrace -p 661 Find out what it is doing...
__errno_location()                               = 0xb7f95b60
sem_wait(0xb7fa1000, 0x503268, 0xbffa3968, 0x804852f, 0x613ff4 <unfinished ...>
Process is blocking in a sem_wait call on a semaphore located at 0xb7a1000 ...
$ pmap -d 661 | grep mysemb7fa1000       4 rw-s- 0000000000000000 000:00010 sem.mysem
This address is mapped to a file named sem.mysem ! This process is blocking on our semaphore.
This is a bit of work, but you get your answer. Note that for this to work, your
program must handle interrupted system calls. I did not do that in the examples,but the pattern looks like this:
do {
r = sem_wait(mysem); Returns -1 with errno == EINTR if interrupted
} while ( r == -1 && errno == EINTR );
This is required because tools like ltrace and strace stop your process with
SIGSTOP . This results in a semaphore function returning with -1and errno set to
EINTR .
8.6 Tools for Working with Signals
One useful command for debugging signals from the shell is the pscommand,
which allows you to examine a process’s signal mask as well as any pending(unhandled) signals. You can also see which signals have user-defined handlersand which don’t.434 Chapter 8 • Debugging IPC with Shell Commands
By now, you may have guessed that the -ooption can be used to view the signal
masks as follows:
$ ps -o pending,blocked,ignored,caught
PENDING          BLOCKED          IGNORED           CAUGHT
0000000000000000 0000000000010000 0000000000384004 000000004b813efb0000000000000000 0000000000000000 0000000000000000 0000000073d3fef9
A more concise equivalent uses the BSD syntax, which is a little unconventional
because it does not use a dash to denote arguments. Nevertheless, it’s easy to use andprovides more output for you:
$ ps s Notice there’s no dash before the s.
UID   PID   PENDING   BLOCKED   IGNORED    CAUGHT ...
500  6487  00000000  00000000  00384004  4b813efb ...
500  6549  00000000  00000000  00384004  4b813efb ...500 12121  00000000  00010000  00384004  4b813efb ...500 17027  00000000  00000000  00000000  08080002 ...500 17814  00000000  00010000  00384004  4b813efb ...500 17851  00000000  00000000  00000000  73d3fef9 ...
The four values shown for each process are referred to as masks, although the ker-
nel stores only one mask, which is listed here under the BLOCKED signals. The other
masks are, in fact, derived from other data in the system. Each mask contains 1or
0for each signal Nin bit position N-1, as follows:
• Caught—Signals that have a nondefault handler
• Ignored—Signals that are explicitly ignored via signal(N,SIG_IGN)
• Blocked—Signals that are explicitly blocked via sigprocmask
• Pending—Signals that were sent to the process but have not yet been handled
Let’s spawn a shell that ignores SIGILL (4) and look at the results:
$ bash -c 'trap "" SIGILL; read '&
[1] 4697$ jobs -x ps s %1
UID   PID   PENDING   BLOCKED   IGNORED    CAUGHT STAT ...500  4692  00000000  00000000  0000000c  00010000 T    ...
You ignore SIGILL by using the built-in trap command in Bash. The value for
SIGILL is 4, so you expect to see bit 3 set under the IGNORED heading. There,8.6 Tools for Working with Signals 435
indeed, you see a value of 0xc—bits 2 and 3. Now this job is stopped, and you know
that if you send a SIGINT to a stopped job, it won’t wake up, so see what happens:
$ kill -INT %1
[1]+  Stopped                 bash -c 'trap "" SIGILL; read '$ jobs -x ps s %1
UID   PID   PENDING   BLOCKED   IGNORED    CAUGHT STAT ...500  5084  00000002  00000000  0000000c  00010000 T    ...
Now you can see a value of 2(bit 1) under the PENDING heading. This is the
SIGINT (2) you just sent. The handler will not be called until the process is
restarted.
Another useful tool for working with signals is the strace command. strace
shows transitions from user mode to kernel mode in a running process while listingthe system call or signal that caused the transition. 
strace is a very flexible tool,
but it is a bit limited in what it can tell you about signals.
For one thing, strace can only inform you when the user/kernel transition takes
place. Therefore, it can only tell you when a signal is delivered, not when it was sent.Also, queued signals look exactly like regular signals; none of the sender’s informa-tion is available from 
strace . To get a taste of what strace is capable of, look at
the rt-sig program from Listing 76 in Chapter 7 when you run it with strace .
$ strace -f -e trace=signal ./rt-sig > /dev/null
rt_sigaction(SIGRT_2, {0x8048628, [RT_2], SA_RESTART}, {SIG_DFL}, 8) = 0rt_sigprocmask(SIG_BLOCK, ~[RTMIN RT_1], [], 8) = 0Process 18460 attached[pid 18459] rt_sigprocmask(SIG_BLOCK, [CHLD], ~[KILL STOP RTMIN RT_1], 8) = 0[pid 18460] kill(18459, SIGRT_2)        = 0[pid 18460] kill(18459, SIGRT_2)        = 0[pid 18460] kill(18459, SIGRT_2)        = 0Process 18460 detachedrt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0--- SIGCHLD (Child exited) @ 0 (0) ------ SIGRT_2 (Real-time signal 0) @ 0 (0) ---sigreturn()                             = ? (mask now [])--- SIGRT_2 (Real-time signal 0) @ 0 (0) ---sigreturn()                             = ? (mask now [])--- SIGRT_2 (Real-time signal 0) @ 0 (0) ---sigreturn()                             = ? (mask now [])
I cheated a little here. Because rt-sig forks, I can trace both processes with the
-foption, which follows forks. This allows me to see the sender and receiver in
one trace.436 Chapter 8 • Debugging IPC with Shell Commands
strace normally produces a great deal of output that has little to do with what
you are interested in. It is common to use a filter, specified with the -eoption, to
limit the output to what you are interested in. In this case, you would use the
trace=signal filter to limit the output to the results of signals and signal-related
system calls.
8.7 Tools for Working with Pipes and Sockets
The preferred user-space tool for debugging sockets is netstat , which relies heav-
ily on the information in the /proc/net directory. Pipes and FIFOs are trickier,
because there is no single location you can look at to track down their existence.The only indication of a pipe’s or FIFO’s existence is given by the 
/proc/pid/fd
directory of the process using the pipe or FIFO.
8.7.1 Pipes and FIFOs
The /proc/pid/fd directory lists pipes and FIFOs by inode number. Here is a
running program that has called pipe to create a pair of file descriptors (one write-
only and one read-only):
$ ls -l !$
ls -l /proc/19991/fdtotal 5lrwx------  1 john john 64 Apr 12 23:33 0 -> /dev/pts/4lrwx------  1 john john 64 Apr 12 23:33 1 -> /dev/pts/4lrwx------  1 john john 64 Apr 12 23:33 2 -> /dev/pts/4lr-x------  1 john john 64 Apr 12 23:33 3 -> pipe:[318960]l-wx------  1 john john 64 Apr 12 23:33 4 -> pipe:[318960]
The name of the “file” in this case is pipe:[318960] , where 318960 is the inode
number of the pipe. Notice that although two file descriptors are returned by the
pipe function, there is only one inode number, which identifies the pipe. I discuss
inodes in more detail later in this chapter.
The lsof function can be helpful for tracking down processes with pipes. In this
case, if you want to know what other process has this pipe open, you can search forthe inode number:
$ lsof | head -1 && lsof | grep 318960
COMMAND  PID    USER   FD      TYPE     DEVICE     SIZE       NODE NAMEppipe  19991    john    3r     FIFO        0,5              318960 pipeppipe  19991    john    4w     FIFO        0,5              318960 pipeppipe  19992    john    3r     FIFO        0,5              318960 pipeppipe  19992    john    4w     FIFO        0,5              318960 pipe8.7 Tools for Working with Pipes and Sockets 437
As of lsof version 4.76, there is no command-line option to search for pipes and
FIFOs, so you resort to grep . Notice that in the TYPE column, lsof does not dis-
tinguish between pipes and FIFOs; both are listed as FIFO . Likewise, in the NAME
column, both are listed as pipe .
8.7.2 Sockets
T wo of the most useful user tools for debugging sockets are netstat and lsof .
netstat is most useful for the big-picture view of the system use of sockets. To get
a view of all TCP connections in the system, for example:
$ netstat --tcp -n
Active Internet connections (w/o servers)Proto Recv-Q Send-Q Local Address               Foreign Address             Statetcp        0     48 ::ffff:192.168.163.128:22   ::ffff:192.168.163.1:1344   ESTABLISHED
Following is the same command using lsof :
$ lsof -n -i tcp
COMMAND     PID    USER   FD   TYPE DEVICE SIZE NODE NAMEportmap    1853     rpc    4u  IPv4   4847       TCP *:sunrpc (LISTEN)rpc.statd  1871 rpcuser    6u  IPv4   4881       TCP *:32769 (LISTEN)smbd       2120    root   20u  IPv4   5410       TCP *:microsoft-ds (LISTEN)smbd       2120    root   21u  IPv4   5411       TCP *:netbios-ssn (LISTEN)X          2371    root    1u  IPv6   6310       TCP *:x11 (LISTEN)X          2371    root    3u  IPv4   6311       TCP *:x11 (LISTEN)xinetd    20338    root    5u  IPv4 341172       TCP *:telnet (LISTEN)sshd      23444    root    3u  IPv6 487790       TCP *:ssh (LISTEN)sshd      23555    root    3u  IPv6 502673       ...
TCP 192.168.163.128:ssh->192.168.163.1:1344 (ESTABLISHED)
sshd      23557    john    3u  IPv6 502673       ...
TCP 192.168.163.128:ssh->192.168.163.1:1344 (ESTABLISHED)
The lsof output contains PIDs for each socket listed. It shows the same socket
twice, because two sshd processes are sharing a file descriptor. Notice that the
default output of lsof includes listening sockets, whereas by default, netstat
does not.
lsof does not show sockets that don’t belong to any process. These are TCP
sockets that are in one of the so-called wait states that occur when sockets are closed.
When a process dies, for example, its connections may enter the TIME_WAIT state.
In this case, lsof will not show this socket because it no longer belongs to a process.
netstat on the other hand, will show it. To see all TCP sockets, use the --tcp
option to netstat as follows:438 Chapter 8 • Debugging IPC with Shell Commands
$ netstat -n --tcp
Active Internet connections (w/o servers)Proto Recv-Q Send-Q Local Address     Foreign Address       Statetcp        0      0 127.0.0.1:60526   127.0.0.1:5000        TIME_WAIT
When using these tools to look at sockets, note that every socket has an inode
number, just like a file. This is true for both network sockets and local sockets, butit is more important for local sockets, because the inode often is the only uniqueidentifier for the socket. Consider this output from 
netstat for local sockets:
Active UNIX domain sockets (w/o servers)
Proto RefCnt Flags       Type       State         I-Node Pathunix  2      [ ]         DGRAM                    3478   @udevdunix  2      [ ]         DGRAM                    5448   @/var/run/...unix  8      [ ]         DGRAM                    4819   /dev/logunix  3      [ ]         STREAM     CONNECTED     642738unix  3      [ ]         STREAM     CONNECTED     642737unix  2      [ ]         DGRAM                    487450unix  2      [ ]         DGRAM                    341168unix  3      [ ]         STREAM     CONNECTED     7633unix  3      [ ]         STREAM     CONNECTED     7632
This is just a small piece of the output. I’ll zoom in on something specific that I
can talk about in more detail. The GNOME session manager, for example, createsa listen socket in the 
/tmp/.ICE-unix directory. The name of the socket is the
process ID of the gnome-session process. A look at this file with lsof shows that
this file is open by several processes:
lsof /tmp/.ICE-unix/*
COMMAND    PID USER   FD   TYPE     DEVICE SIZE NODE NAMEgnome-ses 2408 john   15u  unix 0xc3562540      6830 /tmp/.ICE-unix/2408gnome-ses 2408 john   19u  unix 0xc2709cc0      7036 /tmp/.ICE-unix/2408gnome-ses 2408 john   20u  unix 0xc27094c0      7054 /tmp/.ICE-unix/2408gnome-ses 2408 john   22u  unix 0xc2193100      7072 /tmp/.ICE-unix/2408gnome-ses 2408 john   23u  unix 0xc1d3ddc0      7103 /tmp/.ICE-unix/2408gnome-ses 2408 john   24u  unix 0xc1831840      7138 /tmp/.ICE-unix/2408gnome-ses 2408 john   25u  unix 0xc069b1c0      7437 /tmp/.ICE-unix/2408gnome-ses 2408 john   26u  unix 0xc3567880      7600 /tmp/.ICE-unix/2408bonobo-ac 2471 john   15u  unix 0xc3562540      6830 /tmp/.ICE-unix/2408gnome-set 2473 john   15u  unix 0xc3562540      6830 /tmp/.ICE-unix/2408wnck-appl 2528 john   15u  unix 0xc3562540      6830 /tmp/.ICE-unix/2408gnome-vfs 2531 john   15u  unix 0xc3562540      6830 /tmp/.ICE-unix/2408notificat 2537 john   15u  unix 0xc3562540      6830 /tmp/.ICE-unix/2408clock-app 2541 john   15u  unix 0xc3562540      6830 /tmp/.ICE-unix/2408mixer_app 2543 john   15u  unix 0xc3562540      6830 /tmp/.ICE-unix/24088.7 Tools for Working with Pipes and Sockets 439
The first thing to notice is that most of these have unique inodes, although they
all point to the same file on disk. Each time the server accepts a connection, a newfile descriptor is allocated. This file descriptor continues to point to the same file(the listen socket), although it has a unique inode number.
A little intuition and some corroborating evidence tell you that the server is the
gnome-session process—PID2408 . In this case, the filename of the socket is a
dead giveaway as well. The server is listening on file descriptor 15(inode number
6830 ). Several other processes are using file descriptor 15and inode number 6830 .
Based on what you know about fork , these processes appear to be children or
grandchildren of gnome-session . Most likely, they inherited the file descriptor
and neglected to close it.
To locate the server using netstat , try using -lto restrict the output to listen
sockets and -pto print the process identification, as follows:
$ netstat --unix -lp | grep /tmp/.ICE-unix/
unix 2 [ACC] STREAM LISTENING 7600 2408/gnome-session /tmp/.ICE-unix/2408
Notice that the duplicate file descriptors are omitted, and only one server is
shown. To see the accepted connections, omit the -loption (by default, netstat
omits listen sockets):
netstat -n --unix -p | grep /tmp/.ICE-unix/2408
Proto RefCnt/Flags/Type/State/I-Node/PID/Program name Pathunix  3 [ ] STREAM CONNECTED 7600 2408/gnome-session  /tmp/.ICEunix/2408unix  3 [ ] STREAM CONNECTED 7437 2408/gnome-session  /tmp/.ICEunix/2408unix  3 [ ] STREAM CONNECTED 7138 2408/gnome-session  /tmp/.ICEunix/2408unix  3 [ ] STREAM CONNECTED 7103 2408/gnome-session  /tmp/.ICEunix/2408unix  3 [ ] STREAM CONNECTED 7072 2408/gnome-session  /tmp/.ICEunix/2408unix  3 [ ] STREAM CONNECTED 7054 2408/gnome-session  /tmp/.ICEunix/2408unix  3 [ ] STREAM CONNECTED 7036 2408/gnome-session  /tmp/.ICEunix/2408
Unlike lsof , the netstat command does not show the inherited file descrip-
tors that are unused.
8.8 Using Inodes to Identify Files and IPC Objects
Linux provides a virtual file system ( vfs) that is common to all file systems. It
enables file systems that are not associated with a physical device (such as tmpfs and
procfs ) and at the same time provides an API for physical disks. As a result, vir-
tual files are indistinguishable from files that reside on a disk.440 Chapter 8 • Debugging IPC with Shell Commands
The term inode comes from UNIX file-system terminology. It refers to the struc-
ture saved on disk that contains a file’s accounting data—the file-size permissionsand so on. Each object in a file system has a unique inode, which you see in userspace as a unique integer. In general, you can assume that anything in Linux thathas a file descriptor has an inode. 
Inode numbers can be useful for objects that don’t have filenames, including net-
work sockets and pipes. Inode numbers are unique within a file system but are notguaranteed to be unique across different file systems. Although network sockets canbe identified uniquely by their port numbers and IP addresses, pipes cannot. Toidentify two processes that are using the same pipe, you need to match the inodenumber.
lsof prints the inode number for all the file descriptors it reports. For most files
and other objects, this is reported in the NODE column. netstat also prints inode
numbers for UNIX domain sockets only. This is natural, because UNIX-domainlisten sockets are represented by files on disk.
Network sockets are treated differently, however. In Linux, network sockets have
inodes, although 
lsof and netstat (which run under operating systems in addi-
tion to Linux) pretend that they don’t. Although netstat will not show you an
inode number for a network socket, lsof does show the inode number in the
DEVICE column. Look at the TCP sockets open by the xinetd daemon (you must
be the root user to do this):
$ lsof -i tcp -a -p $(pgrep xinetd)
COMMAND  PID USER   FD   TYPE DEVICE SIZE NODE NAMExinetd  2838 root    5u  IPv4  28178       TCP *:telnet (LISTEN)
Here, you can see that xinetd is listening on the telnet socket (port 23).
Although the NODE column contains only the word TCP, the DEVICE column con-
tains the inode number. You also can find the inode for network sockets listed invarious places in 
procfs . For example:
$ ls -l /proc/$(pgrep xinetd)/fd
total 7lr-x------  1 root root 64 Oct 22 22:24 0 -> /dev/nulllr-x------  1 root root 64 Oct 22 22:24 1 -> /dev/nulllr-x------  1 root root 64 Oct 22 22:24 2 -> /dev/nulllr-x------  1 root root 64 Oct 22 22:24 3 -> pipe:[28172]l-wx------  1 root root 64 Oct 22 22:24 4 -> pipe:[28172]lrwx------  1 root root 64 Oct 22 22:24 5 -> socket:[28178]lrwx------  1 root root 64 Oct 22 22:24 7 -> socket:[28175]8.8 Using Inodes to Identify Files and IPC Objects 441
Now procfs uses the same number for file descriptor 5as lsof , although it
appears inside the filename between brackets. It’s still not obvious that this is theinode, however, because both 
lsof and procfs are pretty cryptic about reporting
it. To prove that this is really the inode, use the stat command, which is a wrap-
per for the stat system call:
$ stat -L /proc/$(pgrep xinetd)/fd/5
File: `/proc/2838/fd/5'Size: 0               Blocks: 0          IO Block: 1024   socket
Device: 4h/4d   Inode: 28178       Links: 1Access: (0777/srwxrwxrwx)  Uid: (    0/    root)   Gid: (    0/    root)Access: 1969-12-31 18:00:00.000000000 -0600Modify: 1969-12-31 18:00:00.000000000 -0600Change: 1969-12-31 18:00:00.000000000 -0600
Finally, the inode is unambiguously indicated in the output.3Notice that I used
the -Loption to the stat command, because the file-descriptor files in procfs are
symbolic links. This tells stat to use the lstat system call instead of stat .
8.9 Summary
This chapter introduced several tools and techniques for debugging various IPCmechanisms, including plain files. Although System V IPC requires special tools,POSIX IPC lends itself to debugging with the same tools used for plain files.
8.9.1 Tools Used in This Chapter
•ipcs , ipcrm —command-line utilities for System V IPC
•lsof , fuser —tools for looking for open files and file descriptor usage
•ltrace —traces a process’s calls to functions in shared objects
•pmap —user-friendly look at a process’s memory map
•strace —traces the system call usage of a process442 Chapter 8 • Debugging IPC with Shell Commands
3. Another place to look is /proc/net/tcp . The inode is unambiguous, but the rest of the output is not
very user friendly.
8.9.2 Online Resources
• http://procps.sourceforge.net—the procps home page, source of the pmap
command
• http://sourceforge.net/projects/strace—the strace home page8.9 Summary 443
This page intentionally left blank 
9.1 Introduction
In this chapter, I look at performance issues from both a system perspective and an
application perspective. Sometimes, your slow application will not be helped muchby faster CPUs, faster memory, or faster disk drives. After reading this chapter, youshould be able to figure out the difference between an application that is slowbecause it is inefficient and one that is bogged down by slow hardware.
9.2 System Performance
When system performance is not optimal, it affects all processes. The system usersfeel it, whether it’s a slow window update or slow connections to a server. Manytools can show you system performance. Unfortunately, some of these tools are bur-dened with so much detail that they’re often unused because of it. With a basicunderstanding of system performance issues, these details won’t be so unfamiliar.
9
445Performance Tuning
9.2.1 Memory Issues
It’s a shopworn tech-support tip that has lost all meaning: “You need more mem-
ory.” Why should adding more memory fix anything? As far as performance goes,it doesn’t increase your CPU’s clock frequency; it doesn’t increase your computer’sbus speed. So why should you expect that adding more memory is going to makeyour computer run faster?
Adding more memory sometimes can address performance issues, but no one
wants to waste money by throwing RAM at a problem only to find that it didn’tsolve anything. Besides, there are only so many DIMM slots in a motherboard, sothrowing RAM at a problem may not be an option anyway. Being able to predictthat more RAM will fix a problem takes more than guesswork. You need to under-stand how the system uses memory to understand whether memory is your per-formance problem.
9.2.1.1 Page Faults
Counting the number of page faults, therefore, can be a good measure of how effi-
ciently your system is using memory. Recall that a page fault occurs when the CPUrequests a page of memory that doesn’t reside in RAM. Normally, this happenseither because the page hasn’t been initialized yet or because it has been kicked outof RAM and stored on the swap device.
When the system is generating many page faults, it affects every process in the
system. A couple of simple programs can illustrate this situation. First is a programthat will allocate a chunk of memory just for the sake of consuming it. This pro-gram will touch the memory once and not use it. This program, appropriatelynamed 
hog, is illustrated in Listing 9-1; it allocates the memory via malloc , mod-
ifies 1 byte in each page, and then goes to sleep.
It’s worth noting here that you need to read or write to at least 1 byte in each
page of an allocated region to consume the page from memory. Linux is smartenough not to commit any physical storage to a page that has not been touched. It’sinteresting that all it takes is 1 byte!
$ free -m Report memory usage in megabytes (MB).
total       used       free     shared    buffers     cached
Mem:           250        118        131          0          0          9-/+ buffers/cache:        108        141Swap:          511          0        511
Note 131MB free and no swap space in use.446 Chapter 9 • Performance T uning
$ ./hog 127 & Consume 127MB of memory.
[1] 4513
$ allocated 127 mb
$ free -m
total       used       free     shared    buffers     cached
Mem:           250        245          4          0          0          9-/+ buffers/cache:        235         14Swap:          511          0        511
Note that free memory decreased by exactly 127MB and still no swap in use.
I deliberately used less than the total available free space in this example to avoid
paging. Linux will not allow the free space to go to zero, so it will use the swap par-tition to page out the least recently used pages when it needs to increase the freememory. If you were to launch a second 
hogprocess, you would observe paging tak-
ing place, because there is not enough free memory to accommodate the allocations.I will look at that topic in more detail later in this chapter.
You can see evidence of the page faults by using the GNU 
time command. Recall
that this is not the same as the Bash built-in time command. Use a backslash to get
the GNU version, as follows:
$ \time ./hog 127
allocated 127 mbCommand terminated by signal 20.01user 0.32system 0:01.17elapsed 28%CPU (0avgtext+0avgdata 0maxresident)k0inputs+0outputs (0major+ 32626minor )pagefaults 0swaps
Here, I have highlighted the useful information, which is the number of minor
page faults. A major page fault is a page fault that requires input and/or output to
disk, and a minor page fault is any other page fault.
In more precise terms, when the hogprogram requests memory with malloc ,
the kernel creates page table mappings for the process’s user space. At this point, nostorage has been allocated; only the mapping has been created. It is not until theprocess tries to read or write the memory for the first time that a page fault occurs,
requiring the kernel to find storage for the page. This mechanism is the method thatthe kernel uses to allocate new storage for processes.
The example above allocates 127MB, which requires 32,512 pages on an IA32
with 4K pages. The 
time command shows that hog caused 32,626 minor page
faults, which agrees nicely with my prediction. The difference is caused by addi-tional pages required to load the program code and data.9.2 System Performance 447
LISTING 9-1 hog.c: A Program That Allocates Memory but Doesn’t Use It
1 #include <stdio.h>
2 #include <string.h>3 #include <stdlib.h>4 #include <unistd.h>56 int main(int argc, char *argv[])7 {8     if (argc != 2)9         exit(0);
1011     size_t mb = strtoul(argv[1], NULL, 0);1213     // Allocate the memory14     size_t nbytes = mb * 0x100000;1516     char *ptr = (char *) malloc(nbytes);17     if (ptr == NULL) {18         perror("malloc");19         exit(EXIT_FAILURE);20     }2122     // Touch the memory (could also use calloc())23     size_t i;24     const size_t stride = sysconf(_SC_PAGE_SIZE);25     for (i = 0; i < nbytes; i += stride) {26         ptr[i] = 0;27     }2829     printf("allocated %d mb\n", mb);30     pause();31     return 0;
32 }
9.2.1.2 Swapping
Until now, plenty of free memory was available, so the new pages were taken from
free memory, and the latency was low. If you run this process again with inadequatefree memory to meet your needs, the kernel will be forced to store pages to the swappartition with each page fault. Specifically, it stores the least recently used pages(sometimes abbreviated LRU) to the swap partition.
Paging and swapping are among the worst things that can happen when per-
formance is critical. When the system is forced to page, a load or store to memory448 Chapter 9 • Performance T uning
that normally takes a few nanoseconds to complete now takes tens of milliseconds
or more. When the system needs to page in or out a few pages, the effect is not sosevere; if it has to move many pages, it can slow your system to a crawl.
To illustrate, I’ll revisit the 
hogprogram, this time allocating enough memory to
cause paging:
$ free -m
total       used       free     shared    buffers     cached
Mem:           250        109        140          0          0         11-/+ buffers/cache:         97        152Swap:          511          0        511
Note 250MB total RAM, 140MB free, 0MB swap when we start
$ \time ./hog 200allocated 200 mbCommand terminated by signal 20.03user 0.61system 0:03.94elapsed 16%CPU (0avgtext+0avgdata 0maxresident)k0inputs+0outputs (0major+51313minor)pagefaults 0swaps
Note 51,313 minor page faults, but no major page faults!
$ free -m
total       used       free     shared    buffers     cached
Mem:           250         48        201          0          0          6-/+ buffers/cache:         42        208Swap:          511         68        443
Yet we paged out 68MB to disk!
Here, you see that the hogprogram caused 68MB of memory to get paged to
disk, yet the time command reports that it saw no major page faults. This is mis-
leading, but it’s not an error. A major page fault occurs when a process requests apage that resides on disk. In this case, the pages did not exist; therefore, they didnot reside on disk and thus do not count as major page faults. Although the 
hog
process caused the system to write pages to disk, it did not actually write those pagesto disk. The actual writes were done by 
kswapd .
The kswapd kernel thread takes care of the dirty work of moving the data from
memory to disk. Only when the process that owns those pages tries to use themagain will a major page fault occur. That page fault will be charged to the processthat requested the data as a major page fault. This may seem like a bit of Enron-style9.2 System Performance 449
accounting1going on here, but usually, things aren’t so unbalanced. Before I show
you another example, I’ll introduce a new tool.
The top Command
Yet another useful tool from the procps package is the topcommand. This com-
mand uses the ncurses library, which makes the most of a text terminal.2The out-
put is very much like the formats you can get from the pscommand except that
top includes many fields that psdoes not support. The output from top is
refreshed periodically, so you usually set aside one window for topdisplay and do
your thing in another. Following is a typical topwindow:
top - 20:27:24 up  3:06,  4 users,  load average: 0.17, 0.27, 0.41
Tasks:  64 total,   3 running,  61 sleeping,   0 stopped,   0 zombieCpu(s):  6.8% us,  5.1% sy,  0.8% ni, 80.8% id,  6.2% wa,  0.3% hi,  0.0% siMem:    158600k total,    30736k used,   127864k free,     1796k buffersSwap:   327672k total,    10616k used,   317056k free,    16252k cached
PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
1 root      16   0  1744   96   72 R  0.0  0.1   0:00.89 init2 root      34  19     0    0    0 S  0.0  0.0   0:00.00 ksoftirqd/03 root      RT   0     0    0    0 S  0.0  0.0   0:00.00 watchdog/04 root      10  -5     0    0    0 S  0.0  0.0   0:00.10 events/05 root      13  -5     0    0    0 S  0.0  0.0   0:00.03 khelper6 root      10  -5     0    0    0 S  0.0  0.0   0:00.00 kthread8 root      20  -5     0    0    0 S  0.0  0.0   0:00.00 kacpid
61 root      10  -5     0    0    0 S  0.0  0.0   0:01.37 kblockd/064 root      10  -5     0    0    0 S  0.0  0.0   0:00.00 khubd
...
Because there is so much information to display, topbreaks it into four screens
full of information called field groups. Pressing Shift+G in the main screen prompts
you with the following:
Choose field group (1 – 4):
As the prompt indicates, you can choose among four screens of information. This
is necessary because it’s not possible to fit all the possible columns onto one textterminal screen. You can still see all four screens at the same time by breaking across450 Chapter 9 • Performance T uning
1. Enron was the notorious American company that defrauded investors by (among other things) hiding
losses in subsidiaries that existed solely for the purpose of hiding losses.
2. There is also GNOME gtop , which in Fedora is gnome-system-monitor . You don’t get as many
options with the GUI.
rows. You do this by pressing Shift+A , which sacrifices some rows to show more
screens. For example:
1:Def - 22:26:14 up  1:41,  3 users,  load average: 0.02, 0.01, 0.10
Tasks:  69 total,   1 running,  68 sleeping,   0 stopped,   0 zombieCpu(s):  0.7% us,  0.0% sy,  0.0% ni, 99.3% id,  0.0% wa,  0.0% hi,  0.0% siMem:    256292k total,   174760k used,    81532k free,     8860k buffersSwap:   524280k total,    28120k used,   496160k free,   113608k cached
1  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
4026 john      15   0 42232  11m 6504 S  0.7  4.7   0:05.13 gnome-terminal
30696 john      16   0  1936  956  764 R  0.3  0.4   0:00.01 top
3583 john      15   0  7056  660  396 S  0.0  0.3   0:04.98 sshd
2  PID  PPID    TIME+  %CPU %MEM  PR  NI S  VIRT SWAP  RES  UID COMMAND
30696  4034   0:00.01  0.3  0.4  16   0 R  1936  980  956  500 top
4054  4026   0:00.03  0.0  0.3  16   0 S  4276 3600  676  500 bash4034  4026   0:00.09  0.0  0.3  15   0 S  4276 3484  792  500 bash4033  4026   0:00.00  0.0  0.1  16   0 S  2068 1920  148  500 gnome-pty-helpe
3  PID %MEM  VIRT SWAP  RES CODE DATA  SHR nFLT nDRT S  PR  NI %CPU COMMAND
4026  4.7 42232  29m  11m  256  17m 6504  542    0 S  15   0  0.7 gnome-termin 4030  0.5  4576 3248 1328   48 1604  896   47    0 S  16   0  0.0 gconfd-2
30696  0.4  1936  980  956   48  268  764    0    0 R  16   0  0.3 top
3588  0.3  4276 3460  816  580  260  624   10    0 S  16   0  0.0 bash
4  PID  PPID  UID USER     RUSER    TTY         TIME+  %CPU %MEM S COMMAND
4026  3588  500 john     john     pts/1      0:05.13  0.7  4.7 S gnome-termina 4030     1  500 john     john     pts/1      0:00.32  0.0  0.5 S gconfd-2
30696  4034  500 john     john     pts/2      0:00.01  0.3  0.4 R top
3588  3583  500 john     john     pts/1      0:00.29  0.0  0.3 S bash
topis very permissive about terminal dimensions and will gladly truncate the
output to fit the terminal window. For those of you who have limited terminalspace, you can eliminate or change columns to make the output fit whatever win-dow you are working in. The options are a bit overwhelming at first, but when youget used to them, they’re easy to remember.
You can change fields interactively by typing 
fin the main screen while topis
running. This brings up a new screen full of options for you to choose among. Afteryou select the fields you want to see, you can return to the main screen by pressing
Enter . If you want to keep the changes as your defaults, you can save the settings
by typing Win the main screen.
Using top to Track Down Hogs
I need another program similar to hog.c in Listing 9-1 to show some timing 
information and gain more control of its behavior. This program, which I’ll call 
son-of-hog.c , is shown in Listing 9-2. I’m going to do a couple of tricks with9.2 System Performance 451
this program so that you can better see the action in top. To build this example,
do the following:
$ cc -O2 -o son-of-hog son-of-hog.c -lrt librt required for clock_gettime
$ ln -s son-of-hog hog-a Give it two different names that we can see in top.
$ ln -s son-of-hog hog-b
Now for the tricky part. Empty the swap partition so you can get a better pic-
ture of what is going on. You do this with the swapon and swapoff commands,
as follows:
$ free -m
total       used       free     shared    buffers     cached
Mem:           250        246          3          0         14        165-/+ buffers/cache:         66        183Swap:          511          4        507
4MB swap in use
$ sudo swapoff -a Turn off all swap partitions (must be root); pull all swapped pages into RAM.
$ sudo swapon -a Turn on all swap partitions again.
$ free -m
total       used       free     shared    buffers     cached
Mem:           250        246          3          0         14        160-/+ buffers/cache:         71        178Swap:          511          0        511
On my system, about 178MB RAM can be used without paging. This shows up
in the output from the free command in the +/- buffers/cache row. Buffers
and cache represent storage that can be reclaimed without sending it to the swappartition.
3Next, tell hog-a to use 150MB, which should be safe to prevent resort-
ing to paging:
$ ./hog-a 150 &
[1] 30825$ touched 150 mb; in 0.361459 sec
$ free -m
total       used       free     shared    buffers     cached
Mem:           250        246          3          0         11         17-/+ buffers/cache:        217         32Swap:          511          0        511452 Chapter 9 • Performance T uning
3. This is an ideal number. In reality, there are several complicating factors, but in rough numbers, this is OK.
Notice that you were able to touch 150MB of pages in a reasonable amount of
time (about 361 ms). No paging was required, but a great deal of data in cacheneeded to be reclaimed. Give the process a 
SIGUSR1 , which will cause it to wake up
and touch its memory again:
$ kill -USR1 %1
$ touched 150 mb; in 0.009929 sec
Notice that the same job took only 9 ms! The second time you touched the
buffer, all the pages should have been in RAM, so there were no page faults to han-dle. Now launch another 
hogand see what happens:
$ ./hog-b 150 &
[2] 30830$ touched 150 mb; in 5.013068 sec$ free -m
total       used       free     shared    buffers     cached
Mem:           250        246          3          0          0         10-/+ buffers/cache:        235         14Swap:          511        136        375
What a difference paging makes! What took only 361 ms before now takes more
than 5 seconds. This is as expected, because you knew that hog-b would have to
kick out many of the pages from hog-a to free up space. More precisely, the free
command tells you that hog-b forced 136MB to disk. No wonder it was so slow!
A second pass here should run much faster:
$ pkill -USR1 hog-b
touched 150 mb; in 0.019061 sec
Not surprisingly, it’s very close to the value you saw for hog-a ’s second run. Now
see what tophas to say about all this. Launch topusing the -poption to show only
the hogprocesses, as follows:
$ top -p $(pgrep hog-a) -p $(pgrep hog-b)
Then you can show all four windows with Shift+A . The output looks like the
following:
1:Def - 23:21:16 up  2:36,  2 users,  load average: 0.00, 0.02, 0.01
Tasks:   2 total,   0 running,   2 sleeping,   0 stopped,   0 zombieCpu(s):  0.0% us,  0.0% sy,  0.0% ni, 100.0% id,  0.0% wa,  0.0% hi,  0.0% siMem:    256292k total,   252048k used,     4244k free,      692k buffersSwap:   524280k total,   139760k used,   384520k free,    11908k cached9.2 System Performance 453
1  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
30825 john      16   0  151m  63m  172 S  0.0 25.3   0:00.36 hog-a30830 john      16   0  151m 150m  296 S  0.0 60.1   0:00.57 hog-b
2  PID  PPID    TIME+  %CPU %MEM  PR  NI S  VIRT SWAP  RES  UID COMMAND
30830  4054   0:00.57  0.0 60.1  16   0 S  151m 1160 150m  500 hog-b30825  4054   0:00.36  0.0 25.3  16   0 S  151m  88m  63m  500 hog-a
3  PID %MEM  VIRT SWAP  RES CODE DATA  SHR nFLT nDRT S  PR  NI %CPU COMMAND
30830 60.1  151m 1160 150m    4 150m  296    9    0 S  16   0  0.0 hog-b30825 25.3  151m  88m  63m    4 150m  172    2    0 S  16   0  0.0 hog-a
4  PID  PPID  UID USER     RUSER    TTY         TIME+  %CPU %MEM S COMMAND
30830  4054  500 john     john     pts/3      0:00.57  0.0 60.1 S hog-b30825  4054  500 john     john     pts/3      0:00.36  0.0 25.3 S hog-a
Notice that both processes have a virtual-memory footprint of 151MB, as indi-
cated by the VIRT column in screen 1. After swapping 130MB to disk, both hogs
show fewer than ten major faults, as indicated in the nFLT column in screen 3. The
REScolumn (screens 1 and 2) indicates how much of that is present in RAM (that
is, resident). There is also a SWAP column, which indicates how much of that process
resides on disk.
LISTING 9-2 son-of-hog.c: A Modified hog.c
1 #include <stdio.h>
2 #include <string.h>3 #include <stdlib.h>4 #include <signal.h>5 #include <time.h>6 #include <unistd.h>7 #include <sys/time.h>89 void handler(int sig)
10 {11     // Does nothing12 }1314 // convert a struct timespec to double for easier use.15 #define TIMESPEC2FLOAT(tv) ((double) (tv).tv_sec + (double) (tv).tv_nsec *1e-9)1617 int main(int argc, char *argv[])18 {19     if (argc != 2)20         exit(0);2122     // Dummy signal handler.23     signal(SIGUSR1, handler);2425     size_t mb = strtoul(argv[1], NULL, 0);26454 Chapter 9 • Performance T uning
27     // Allocate the memory
28     size_t nbytes = mb * 0x100000;29     char *ptr = (char *) malloc(nbytes);30     if (ptr == NULL) {31         perror("malloc");32         exit(EXIT_FAILURE);33     }3435     int val = 0;36     const size_t stride = sysconf(_SC_PAGE_SIZE);3738     // Each loop touches memory, then stops and wait for a SIGUSR139     while (1) {40         int i;41         struct timespec t1, t2;4243         // t1 - when we started to touch memory44         clock_gettime(CLOCK_REALTIME, &t1);4546         // All it takes is one byte per page!47         for (i = 0; i < nbytes; i += stride) {48             ptr[i] = val;49         }50         val++;5152         // t2 - when we finished touching memory53         clock_gettime(CLOCK_REALTIME, &t2);5455         printf("touched %d mb; in %.6f sec\n", mb,56                TIMESPEC2FLOAT(t2) - TIMESPEC2FLOAT(t1));5758         // Wait for a signal.59         pause();60     }61     return 0;
62 }
As a final illustration of how processes can cause one another to run slow, alter-
nate signals between hog-a and hog-b to deliberately cause the system to write to
disk. Whenever the system is spending more time paging to disk than it is execut-ing user code, we say that it’s thrashing. If you’re running this example, you can
watch the action in your 
topwindow, but I won’t show that here:
$ pkill -USR1 hog-a hog-b was the last one we signaled.
$ touched 150 mb; in 14.286939 sec
$ pkill -USR1 hog-b hog-b is now mostly on disk.
$ touched 150 mb; in 16.731990 secpkill -USR1 hog-a$ touched 150 mb; in 16.944799 sec9.2 System Performance 455
I’ll wrap up this section on swapping by putting things in perspective. Each hog
is using 150MB of memory but touching 1 byte per page. This example ran on a
Pentium 4 with a page size of 4K, which means that there are 38,400 pages total.Stated another way, it took as much as 17 seconds to modify 37K of memory. Thespeed of the memory in this example is largely irrelevant; the time for each pass isdominated entirely by the speed of the swap device.
When you can point to paging as a cause for performance issues, it is likely that
more RAM will alleviate the problem. If you are writing an application that is caus-ing excessive paging, it may be possible to rework your code to use memory moreefficiently rather than to add more RAM. Using the tools in this section, you shouldbe able to determine the right course of action.
9.2.2 CPU Utilization and Bus Contention
In the previous section, I looked at paging, which is caused when processes contendfor a limited amount of RAM. Likewise, there are other scarce resources in the sys-tem that processes contend for. One of these resources is bus bandwidth.
Figure 9-1 shows a simplified bus layout of a typical PC using PCI Express
(PCIe). The frontside bus (FSB) is the point of entry for all data going into and
out of the CPU. The DRAM may have one or multiple paths because it may beaccessed via the CPU or peripherals, or (in some systems) the video controller.More often, the video controller has a decent amount of memory, as well as itsown high-speed bus (PCIe or AGP) so that it doesn’t have to contend for theDRAM bus.
9.2.2.1 Multiprocessing and the Frontside Bus
The speed of the FSB is always a major factor in computer performance because in
personal computers today, the FSB is significantly slower than the CPU clock. Thespeed of the FSB determines the upper limit of I/O in the system.
With the rise of multiprocessor systems, the FSB is becoming a significant bot-
tleneck. A typical multiprocessor system looks exactly like Figure 9-1, except thatthe block labeled CPU contains not just one but two or more processors, all shar-ing a single FSB. This means that instead of one fast CPU waiting for a slower FSB,you now have two. So the problem of FSB contention gets worse with more CPUs.456 Chapter 9 • Performance T uning
This type of multiprocessing computer is called a Symmetric Multiprocessing
(SMP) computer. These computers have been around for some time in high-endservers and workstations. Linux has had support for SMP since Linux 2.0. Recently,multicore CPUs have become available for desktop computers, making SMP avail-able to more users. A computer with a single multicore processor is functionallyidentical to an SMP computer except that the processors reside in a single chipinstead of multiple chips.
So now FSB contention is a problem on the desktop as well as in servers and
workstations. FSB contention exhibits itself as increased instruction latency. Codethat relies heavily on the FSB runs slower when it has to contend for the FSB withanother processor—that is, the same instructions take longer to execute because ofFSB contention.9.2 System Performance 457
FIGURE 9-1 Typical PC Architecture Using Intel Architecture CPU and PCI ExpressCPU
North
BridgeDRAM VideoPCIe DDR2FSB
PCIe
Peripher als
and/or 
South Bridge
You can get an idea of how much of the FSB a process uses by counting the num-
ber of page faults. This is not the whole picture, however. A process uses the mem-ory bus when it tries to read or write a memory location that is not in cache, whichdoes not always result in a page fault but, rather, a cache miss.
9.2.2.2 CPU Utilization versus Efficiency
In principle, CPU utilization refers to percentage of the time the CPU spends run-
ning code. A CPU that is 100 percent utilized is running code all the time. Whilethe system is up, of course, the CPU is always running code. Each architecture hasits own 
cpu_idle function, which Linux calls when the scheduler cannot find any
other process to run. Utilization can be expressed as any time the CPU runs codethat is not part of the 
cpu_idle function.
Many tools can show CPU utilization, as you have seen already. What you have
looked at mostly was utilization by process—the percentage of time the CPU spendsexecuting one particular process. Utilization does not tell the whole story, however.What utilization doesn’t tell you is how efficiently the CPU is being utilized.
I have already demonstrated how something as simple as modifying a few kilobytes
of RAM can run at dramatically different speeds. In each case, the same instructionsrun at different speeds. In every case, the CPU utilization is 100 percent. CPU uti-lization is not enough to characterize the efficiency of a process or the system.
The issue of processor efficiency is not hard to understand. You encounter the
same problems in real life. Think of the last time you went to buy groceries. Evenif you are the most efficient person in the world, you could still be held up by a longline at the checkout counter. Either way, you are devoting 100 percent of your timeto your errands, but your efficiency is largely out of your control. In a computer,the long lines come in the form of increased instruction latency. Increased latencycan be caused by contention for a resource (such as another processor or device), orby a slow resource (such as DRAM).
The kernel scheduler is somewhat handicapped in its ability to detect inefficient
processes. Just like in real life, it’s not necessarily the fault of the process; the processis a victim of circumstances. The scheduler relies primarily on utilization to adjustprocess priorities. Processor hogs have their effective priority lowered,
4whereas so-
called interactive processes (ones that spend most of their time waiting) are given ahigher priority.458 Chapter 9 • Performance T uning
4. If they are not real-time processes.
Many processors include additional performance monitoring registers to moni-
tor code efficiency. But these registers are not available on all processors that runLinux, and to date, they are not used by the scheduler. This is somewhat subjective,after all. Just because a task is inefficient doesn’t mean it’s not important.
This is where we users have some value to add. In the following sections, I look
at tools that can help determine code efficiency. The Intel architecture has a richset of performance-monitoring registers for this purpose, and a few tools are avail-able to make use of them. As a result, many tools are available only on the Intelarchitecture.
9.2.3 Devices and Interrupts
When you think of devices and performance, you probably think the devices areindependent of the rest of the system—that is, a device doesn’t affect processes thataren’t using it. Very often, that’s true, but devices have a way of creating side effectsyou may not be aware of.
9.2.3.1 Bus Contention
Most conventional computer designs today, regardless of the CPU architecture, rely
on the PCI bus for peripherals. I’ll use PCI as the example, although the same issuesapply whether the bus is SBUS, ISA, VME, or whatever. What all these buses havein common is that they are parallel buses, which means that the devices share thesame wires. Devices that want to talk to the CPU must negotiate time on the busto do so. Bus bandwidth is fixed and must be shared among the different devices onthe bus. Just like multiple CPUs sharing a common FSB, devices on a peripheralbus contend with one another for time on the bus.
The PCI bus allows computers to break the bus into segments in a treelike fash-
ion. As illustrated in Figure 9-2, these segments form a hierarchy, with the northbridge at the top. T wo devices on different segments don’t contend with each otherfor bandwidth on their own bus segments. If these devices need to access the CPUor memory, they contend with each other for bandwidth at the north bridge. It’sunlikely, but the most efficient use of such a bus scheme occurs when two devicescommunicate with each other without involving the CPU. In this case, dependingon the bus layout, there may be no contention for bandwidth, because each seg-ment is separate.9.2 System Performance 459
Recently, manufacturers have been moving away from parallel buses to high-
speed serial connections that are point to point. T wo examples are Intel’s PCIExpress (PCIe) and AMD’s Hypertransport. These are point-to-point connections,so there is no contention for the link. In principle, however, this is similar to thebus segments I just discussed. You can think of each device as having a dedicatedbus segment that it does not have to share. These so-called switched fabric architec-
tures are similar in concept to a conventional Ethernet network. In such a configu-ration, each bridge (including the north bridge) functions more like a switch than460 Chapter 9 • Performance T uning
FIGURE 9-2 A Hypothetical Bus HierarchyNorth
Bridge
Bus
BridgeSome
Device
Some
DeviceSome
DeviceSome
DeviceSome
DeviceBus 0
Bus 1 Bus 2Bus
Bridge
a bridge. PCIe devices contend with other devices at each bridge they must cross.
For traffic that must cross the north bridge (such as a memory access), a PCIe devicemust contend with every other PCIe device in the system.
9.2.3.2 Interrupts
In the bad old days before USB and FireWire, peripherals like scanners and frame
grabbers required special adapters to be installed on the ISA bus. What all adaptercards have in common is that they require an interrupt line to the CPU. In a legacyPC, there are only 15 usable interrupts total, and many of those are already dedicatedto system functions, leaving them unavailable for use by additional devices. To workaround this, the hardware can share an interrupt with another card, provided that thedriver is written to allow this. This has an adverse effect on performance because itrequires the operating system to call each driver in turn until one handles the inter-rupt. Worse, some poorly written drivers don’t work with shared interrupts, so if youare out of interrupts, you are out of luck. The lack of interrupts in a typical PC archi-tecture has been made moot by the development of buses like USB and FireWire,which make it possible to add peripherals without adding adapter cards.Nevertheless, some systems require additional adapter cards—maybe additional net-work cards. In this case, finding interrupts for these cards may be an issue.
You can see the status of interrupts and drivers in the pseudofile
/proc/interrupts . Here, you can see exactly which device is assigned to which
interrupt. The pseudofile also displays a count next to each interrupt, which is thenumber of times that interrupt had been handled since the system started. Forexample:
$ cat /proc/interrupts
CPU0
0:      27037          XT-PIC  timer1:         10          XT-PIC  i80422:          0          XT-PIC  cascade7:          1          XT-PIC  parport08:          1          XT-PIC  rtc9:          0          XT-PIC  acpi
11:       1184          XT-PIC  uhci_hcd:usb1, uhci_hcd:usb2, uhci_hcd:usb3, eth0
12:          0          XT-PIC  VIA823314:       6534          XT-PIC  ide015:        269          XT-PIC  ide1
NMI:          0LOC:      27008ERR:          0MIS:          09.2 System Performance 461
Almost always, you will see the timer interrupt with the most counts, incre-
mented for every system tick. Recall that the tick frequency is determined when youconfigure the kernel. If you configured the kernel with a tick frequency of 250, thiscounter will increment 250 times per second.
In this example, you can see that my USB peripherals share a common interrupt,
which is assigned to the root hub. Also, the Ethernet adapter shares the same inter-rupt. Because these are onboard peripherals, I can’t do anything about this.Otherwise, it might be possible to move the adapter to a different slot to use a dif-ferent interrupt.
9.2.3.3 PIC versus APIC
The interrupt architecture is one of the few remaining bits of ISA legacy left in the
desktop PC of today. Most PC chipsets contain an implementation of the old 8259Programmable Interrupt Controller (PIC) embedded in the south bridge (refer to
Figure 9-1). Interrupts that are delivered by the 8259 must travel across two bridgesto reach the CPU. These two bridges must be crossed again to acknowledge theinterrupt. The extra “hops” across the bridges increase the latency of the interrupts,which can affect system performance.
Pentium processors come with a built-in interrupt controller called an Advanced
Programmable Interrupt Controller (APIC) , which allows system designers to provide
a low-latency path for interrupts to the processor. Intel also defines an interface toan external APIC that is required for multiprocessor systems. This will be presentonly if you have a motherboard that can support two or more CPUs. The onboardAPIC usually is referred to as the Local APIC (or LAPIC) , whereas the external
APIC generally is referred to as the I/O APIC.
Neither of these APICs is required in a single-processor system, and each can be
disabled in software. Many BIOSes disable the APIC for compatibility and fallback on the old-fashioned 8259 PIC in the south bridge. If this is the case, Linuxwill run with the old PIC. You might see a message in 
/var/log/messages like
the following:
localhost kernel: Local APIC disabled by BIOS -- you can enable it with "lapic"
As the message says, to enable the local APIC that has been disabled by the BIOS,
you must specify it on the boot line with the lapic parameter. The resulting entry
in /etc/grub.conf might look like this:462 Chapter 9 • Performance T uning
Fedora Core (2.6.16np)
root (hd1,0)kernel /vmlinuz-2.6.16np ro root=/dev/VolGroup00/LogVol00 rhgb quiet lapic
initrd /initrd-2.6.16np.img
Enabling the APIC will improve interrupt latency, which can be an issue in real-
time applications. It also provides significantly more interrupts than the old-fashioned PIC. So enabling the APIC means that cards don’t have to share aninterrupt, which I will discuss shortly.
The APIC has been blamed for breaking some drivers. If your BIOS has enabled
the APIC, and you want to disable it, you can do so with the 
nolapic option. An
SMP system requires an I/O APIC, so by default, this is enabled when you run an SMP kernel—and ideally by the BIOS as well.
9.2.3.4 Devices and Slots
If you have adapters installed in slots on the motherboard, you should be aware of
a few things. As I mentioned earlier, the slot often determines which interrupt acard will use. If your card is sharing an interrupt with another device, moving to a different slot may prevent the sharing. Sharing may not be a problem, but it is notoptimal for performance.
Parallel buses like PCI and PCI-X must divide the available bus cycles between
installed cards. A slow card mixed with a fast card can slow both cards. PCI-X allowsboth 66MHz cards and 133MHz-capable cards to reside on the same PCI-X bus,for example, but the bus runs only as fast as the slowest card. Likewise, mother-boards that can support 133 MHz PCI-X cards will tune the clock frequency basedon the number of cards installed. The motherboard may provide two PCI-X slotscapable of 133MHz, but due to signal-quality issues, you may slow the clock if youpopulate both slots. This is entirely dependent on the motherboard design and theBIOS; every one will be different.
PCIe is unique in that it uses a point-to-point connection that eliminates many
of the signal-quality issues associated with parallel buses. PCIe bandwidth isexpressed in lanes of fixed bandwidth (2.5GB/s per lane). The physical slot dimen-
sion determines the maximum number of lanes an installed card can have. So an x8slot can support cards with up to 8 lanes (20 GB/s). The PCIe spec allows manu-facturers to provide slots that are physically wider than the motherboard can sup-
port. The slot may be x8 physically, for example, but the motherboard provides onlyfour lanes. In this case, the card still works, but at half the speed.9.2 System Performance 463
Just remember that all slots are not created equal. The specific configurations are
unique for every motherboard, and for quality motherboards, these are documentedfor the concerned user. It always pays to read the manual.
9.2.3.5 Tools for Dealing with Slots and Devices
A very useful tool for determining slot configuration is the 
lspci utility, which can
show common bus segments that are shared by more than one device. Sometimes,you find that a particular slot shares a common bus segment with a device solderedto the motherboard. In this case, you may be slowing your bandwidth without real-izing it.
With no options, 
lspci lists the devices on the PCI bus. Each PCI device has 
a vendor ID and a device ID. The vendor ID is a 16-bit number that identifies the manufacturer. This ID is assigned by the PCI-SIG,
5which maintains the PCI
specifications. Each manufacturer assigns its own device IDs to the parts it ships,which, together with the vendor ID, uniquely identify devices. 
lspci includes a
table of vendors and devices so that it can report accurate device information inhuman-readable format. For example:
$ lspci
00:00.0 Host bridge: Intel Corporation E7501 Memory Controller Hub (rev 01)
Subsystem: Intel Corporation Unknown device 341aFlags: bus master, fast devsel, latency 0Capabilities: <access denied>
00:00.1 Class ff00: Intel Corporation E7500/E7501 Host RASUM Controller (rev 01)
Subsystem: Intel Corporation Unknown device 341aFlags: fast devsel
00:03.0 PCI bridge: Intel Corporation E7500/E7501 Hub Interface C PCI-to-PCI
Bridge (rev 01) (prog-if 00 [Normal decode])
Flags: bus master, 66MHz, fast devsel, latency 64Bus: primary=00, secondary=02, subordinate=04, sec-latency=0I/O behind bridge: 00002000-00005fffMemory behind bridge: eff00000-feafffffPrefetchable memory behind bridge: eda00000-edcfffff
00:03.1 Class ff00: Intel Corporation E7500/E7501 Hub Interface C RASUM
Controller (rev 01)
Subsystem: Intel Corporation Unknown device 341aFlags: fast devsel
...464 Chapter 9 • Performance T uning
5. www.pcisig.com
Each device in this listing includes a number that identifies the device’s logical
location on the bus. The default format, listed above, is
bus : slot . function
PCI is a parallel bus, which means that cards must share a common set of signals.
The busfield indicates the bus number. PCI allows multiple buses with the use of
bridges. Bus 0 is closest to the processor (the north bridge). Each bridge creates anew bus segment with a higher number. You can see this graphically with the 
-t
option to lspci:
-[0000:00]-+-00.0
+-00.1+-03.0-[0000:02-04]--+-1c.0|                    +-1d.0-[0000:04]--+-07.0|                    |                 +-07.1|                    |                 +-09.0|                    |                 +-09.1|                    |                 +-0a.0|                    |                 \-0a.1|                    +-1e.0|                    \-1f.0-[0000:03]--+-07.0|                                      +-07.1|                                      \-0a.0+-03.1+-1d.0+-1d.1+-1e.0-[0000:01]----0c.0+-1f.0+-1f.1\-1f.3
In this output, only the slot and function identifiers are shown to conserve space.
The bus is identified by the two-digit number in the brackets.6The slot numbers
are unique within a bus segment but not unique across bus segments. You willnotice that there is a slot labeled 
1eon buses 2 and 0, for example.
The point of this output is to see where your cards lie in the bus hierarchy.
Quality computer hardware usually comes with a bus diagram pasted to the insidecover of the box. For those times when you don’t have access to such a diagram oraren’t physically present at the hardware 
lspci helps you see for yourself what’s
going on inside the box.9.2 System Performance 465
6. The four-digit number is the PCI domain. Most PCI motherboards have only one domain.
In the example above, bus segment 4 has three slots occupied. In this case, it hap-
pens to be populated by an onboard SCSI controller and two dual-channel networkcards. Here’s the plain 
lspci output:
$ lspci -s 04: Show all the cards on bus segment 04.
04:07.0 SCSI storage controller: Adaptec AIC-7902 U320 (rev 03)
04:07.1 SCSI storage controller: Adaptec AIC-7902 U320 (rev 03)04:09.0 Ethernet controller: Intel Corporation 82546GB Gigabit Ethernet Controller (rev 03)04:09.1 Ethernet controller: Intel Corporation 82546GB Gigabit Ethernet Controller (rev 03)04:0a.0 Ethernet controller: Intel Corporation 82546GB Gigabit Ethernet Controller (rev 03)04:0a.1 Ethernet controller: Intel Corporation 82546GB Gigabit Ethernet Controller (rev 03)
This illustrates another use for this tool: It allows you to see when your cards are
sharing a bus segment with a device that is soldered onto the motherboard. Here,you see that two dual-port Ethernet adapters (devices 9 and 0xa) are sharing bus 4with the onboard SCSI controller. In this case, each card happens to be capable ofoperating at 133MHz, but there’s no way that the BIOS is going to allow such afull bus segment to run that fast. To improve performance here, you would need tofind another slot for at least one of the Ethernet cards.
Because a PCI bus segment runs only as fast as the slowest card on the bus seg-
ment, you could inadvertently slow your onboard SCSI controller just by putting aslow card in one of these slots! Unfortunately, 
lspci does not tell you how fast your
bus segment is running or the capability of your cards. Some intuition, however,along with some trial and error, is enough to uncover issues due to overpopulatedbus segments. 
lspci is extremely useful for this purpose.
Although lspci cannot tell you exactly how fast a parallel PCI device is operat-
ing, it can tell you about PCIe devices. To get this information, you need to use the
-vvoption (doubly verbose). Buried in the copious output, you will find the card’s
capability as well as the current settings. Following is the output from a system withan eight-lane PCI device installed in an eight-lane slot:
$ lspci -vv
...04:00.0 InfiniBand: Mellanox Technologies MT25204 [InfiniHost III Lx HCA] (rev a0)
Subsystem: Mellanox Technologies MT25204 [InfiniHost III Lx HCA]
...
Capabilities: [60] Express Endpoint IRQ 0
Device: Supported: MaxPayload 128 bytes, PhantFunc 0, ExtTag-Device: Latency L0s <64ns, L1 unlimitedDevice: AtnBtn- AtnInd- PwrInd-466 Chapter 9 • Performance T uning
Device: Errors: Correctable+ Non-Fatal+ Fatal+ Unsupported-
Device: RlxdOrd- ExtTag- PhantFunc- AuxPwr- NoSnoop-Device: MaxPayload 128 bytes, MaxReadReq 128 bytesLink: Supported Speed 2.5Gb/s, Width x8, ASPM L0s, Port 8Link: Latency L0s unlimited, L1 unlimitedLink: ASPM Disabled RCB 64 bytes CommClk- ExtSynch-Link: Speed 2.5Gb/s, Width x8
Supported Speed represents the capabilities of the card. The actual speed is the
second highlighted line, which represents the speed of the slot where the cardresides.
9.2.4 Tools for Finding System Performance Issues
You have several tools at your disposal for tracking down processes that are causingyour system to run slow. In an earlier section, I used the 
topcommand with some
detailed examples. In this section, I show you some other tools that can help illus-trate what your system is doing. Most of the features in these tools overlap. Whatmakes one tool more useful than another often depends on the application.
9.2.4.1 Virtual-Memory Status and More with vmstat
I used 
vmstat in an earlier chapter to illustrate the system use of disk cache. This
is a familiar tool that has been around for a long time. One appeal of vmstat is its
simplicity; just type vmstat , and you get one line of dense but useful information.
Type vmstat 1 to get that information printed every second. For example:
$ vmstat 1
procs -----------memory---------- ---swap-- -----io---- --system-- ----cpu----
r  b   swpd   free   buff  cache   si   so    bi    bo   in    cs us sy id wa1  1      0   3940  14740 147440    0    0    51    15  263    91  1  8 90  10  1      0   3764  15236 145516    0    0  2952     0  534   606  3 10  0 870  1      0  12368  16320 132804    0    0  1576     0  565   644  0  9  0 910  1      0   8216  16728 135116    0    0  2768     0  455   430  2  5  0 930  1      0   3984  16732 139260    0    0  4164     0  402   354  1  5  0 940  1      0  14452  16764 128484    0    0  3596  1020  438   397  1  5  0 940  1      0  11236  17796 129696    0    0  2196     0  588   695  2  9  0 890  1      0   8028  18724 130944    0    0  2220     0  556   610  2  7  0 910  1      0   4040  19604 132240    0    0  2180     0  532   601  1 12  0 871  1      0   4140  19744 131624    0    0  2964     0  491   506  1  5  0 940  1      0   4368  20024 131004    0    0  3484   932  481   488  1  6  0 930  1      0   9748  20212 124968    0    0  3840     0  502   535  1  8  0 91
The default output is plain text and fits on an 80-column text display. Aside from
the column headers, that’s as fancy as it gets. The output is dynamic and can change9.2 System Performance 467
with every sample. Each line of output is one sample (in this case, one per second).
The following sections look at some of the details.
Process Information from vmstat
The only process-specific information from the default vmstat output is listed in
the rand bcolumns under the heading procs . The rcolumn lists the number of
processes that are in the runnable state at that time, and the bcolumn lists the num-
ber of processes in an uninterruptible sleep.
You may have seen or used the system loadas a metric of how busy the system is.
The term loadusually is defined as some kind of average of the number of processes
in the run queue (that is, that are runnable) over time. In some applications, loadis a useful metric. In a general-purpose server or a desktop, this value is not alwaysthe most helpful.
Processes that are in an uninterruptible sleep are blocked in a system call (most
likely, a device driver). The most common reason for an uninterruptible sleep is towait for I/O. So a frequent nonzero number in the 
bcolumn is an indicator of a
process that is being blocked by a slow device.
Memory Usage Information from vmstat
Under the heading memory are four columns labeled swpd , free , buff , and cache .
These are the current values of the amount of memory swapped to disk, free, in sys-tem buffers, or in cache, respectively. All values are in kilobytes by default. You canchange this with the 
-Soption (see vmstat(8) for details).
Periodic output from vmstat is a good way to look for trends. When you’re look-
ing for memory leaks, the value of the swpd column will tend to go up as one or
more processes neglect to free memory. The free column is not as helpful as you
may think, because it represents the amount of RAM the kernel can use for alloca-tions without resorting to swapping pages or flushing cache buffers.
The amount of RAM available to applications without swapping is more accu-
rately reflected by the sum of free memory plus the buffers and the cache. This ishow the 
free program represents the same information.
I/O Information from vmstat
I/O is shown under two headings from the default output. The swap heading shows
the rate of memory going to and from the swap device. These are the major page
faults I discussed earlier in the chapter. The sicolumn indicates the rate of reads468 Chapter 9 • Performance T uning
from the swap device (page in), and the socolumn indicates the rate of writes to
the swap device (page out). The values here are expressed in units of KB/sec bydefault. Optimal values for both columns are zero, of course. The upper limit ofthese values is determined by the speed and capability of the swap device.
Next to the 
swap heading, labeled simply io, is the rest of the system I/O. This
includes reads and writes to or from disk other than swap. This includes any I/O,not just I/O caused by the 
read and write system calls, but page faults due to
mappings created by mmap calls as well. The bicolumn contains the input rate, and
the bocolumn contains the output rate. Here again, the values are measured in
KB/sec by default.
Other Information from vmstat
The final two headings in the default output are labeled system and cpu. Under
the system heading, the incolumn indicates the number of interrupts per second.
This value is always in interrupts per second regardless of the reporting intervalspecified on the command line. On an idle system, this value should be very closeto the system clock tick, which requires one interrupt per tick.
The 
cscolumn indicates the number of context switches per second. A context
switch requires a certain amount of overhead, but this overhead has been reducedby techniques such as lazy TLB flushing, which I discussed in Chapter 5. By itself,the context switch rate does not mean much, but with the other data, it can pro-vide some useful insight into the system behavior.
Finally, the 
cpu heading lists the percentage of time the system spent in user
mode (us), kernel mode ( sy), and idle ( id). The last column ( wa) indicates the
amount of time the system spends waiting for I/O. This value is printed in all ver-sions of 
vmstat but is valid only in 2.6-series kernels. In 2.4 and earlier kernels, this
value is always zero; the amount of time spent waiting for I/O is not available andis included as idletime.
This value in the 
wacolumn is a very useful value, because if the kernel is spend-
ing a great deal of time waiting for I/O, processes are contending for I/O. Exactlywhat device they are contending for is another story.
9.2.4.2 Tools from the sysstat Package
The 
syststat package is a vital package that comes with virtually every distribu-
tion. The tools here allow you to monitor system performance interactively and ret-rospectively. The primary tool in this package is 
sar, which is short for system9.2 System Performance 469
activity reporting. The data it provides is similar to the output of vmstat except that
sarhas many more options. The ability to collect system data over time and review
it later is unique to sar.
Introducing sar
sartakes an interval and count argument, like vmstat . The default output, how-
ever, tells you only about CPU utilization. For example:
$ sar 1 4
Linux 2.6.16np (redhat)         06/01/2006
09:41:05 PM       CPU     %user     %nice   %system   %iowait    %steal     %idle
09:41:06 PM       all      6.00      0.00     11.00     83.00      0.00      0.0009:41:07 PM       all      0.99      0.00      8.91     90.10      0.00      0.0009:41:08 PM       all     95.92      0.00      1.02      3.06      0.00      0.0009:41:09 PM       all      5.94      0.00      8.91     85.15      0.00      0.00Average:          all     26.75      0.00      7.50     65.75      0.00      0.00
This command tells sarto report statistics four times, once per second. Each line
shows basic CPU utilization information along with the time of day of each sam-ple. These values reflect an average taken over the interval specified (1 second, inthis case). This is essentially the same output that 
vmstat provides except that there
are additional fields labeled %nice and %steal . The %nice column indicates the
percentage of time spent executing processes with a positive nice value (that is,lower priority). The 
%steal column shows the percentage of time forced to wait by
the hypervisor7on a virtual machine.
Virtual-Memory Information from sar
The -roption provides virtual-memory information like vmstat , only with more
details:
$ sar -r 1 4
Linux 2.6.16np (redhat)         05/31/2006
11:04:54 PM kbmemfree kbmemused  %memused kbbuffers  kbcached kbswpfree kbswpused  %swpused  kbswpcad
11:04:55 PM      5504    250120     97.85     20708    128008    524280         0      0.00         011:04:56 PM      5504    250120     97.85     20708    128008    524280         0      0.00         011:04:57 PM      5504    250120     97.85     20708    128008    524280         0      0.00         011:04:58 PM      5504    250120     97.85     20708    128008    524280         0      0.00         0Average:         5504    250120     97.85     20708    128008    524280         0      0.00         0470 Chapter 9 • Performance T uning
7. A hypervisor is software used to run an OS in a virtual machine. Open source examples include Xen,
QEMU, and Bochs.
You can also see paging and virtual-memory activity by using the -Boption as
follows:
$ sar -B 1 5
Linux 2.6.16np (redhat)         05/31/2006
11:08:53 PM  pgpgin/s pgpgout/s   fault/s  majflt/s
11:08:54 PM      0.00      0.00     53.00      0.0011:08:55 PM      0.00      0.00     31.00      0.0011:08:56 PM      0.00      0.00     11.00      0.0011:08:57 PM      0.00      0.00     11.00      0.0011:08:58 PM      0.00     48.00     11.00      0.00Average:         0.00      9.60     23.40      0.00
Unlike that of vmstat , the output is in units of pages (or faults) per second and
cannot be changed. You can get similar output for each block device in the systemor for specific devices as well.
Process Information from sar
You can see utilization by process, as with the top command, by using the -x
option. This takes a single argument, which can be a single process ID, or you canspecify multiple process IDs with multiple 
-xoptions as follows:
$ jobs -x sar -x 3942 -x 3970 1 1
Linux 2.6.16np (redhat)         06/01/2006
10:04:09 PM       PID  minflt/s  majflt/s     %user   %system   nswap/s   CPU
10:04:10 PM      3942  43231.00      0.00      5.00     48.00      0.00     010:04:10 PM      3970      0.00      0.00     46.00      0.00      0.00     0
Average:          PID  minflt/s  majflt/s     %user   %system   nswap/s
Average:         3942  43231.00      0.00      5.00     48.00      0.00Average:         3970      0.00      0.00     46.00      0.00      0.00
One nice thing about the sar command is that the output units are always
unambiguous. These units should be familiar to you by now. Output consists of
minflt/s (minor faults per second), majflt/s (major faults per second), and
columns for percentage of time spent in user space ( %user ) and kernel space
(%system ). The nswap/s column is valid only in 2.4 and earlier kernels. The CPU
column identifies the CPU with which the runtime is associated. In this example,it’s a uniprocessor machine, so the output is always CPU 
0. Even on an SMP
machine, however, the output lists only one CPU, even if the process migratesbetween CPUs. SMP support is a fairly recent addition to 
sar, and it appears to
need more work.9.2 System Performance 471
The lowercase xoption above shows usage only for the processes listed, whereas
an uppercase Xshows the statistics for onlythe children of the processes listed. If a
process has no children, the output will be all 0. One use for this is when you want
to look at processes as they are being forked, which may occur too fast for you totype. By tracing a common parent process with 
X, you can see the usage of child
processes currently executing, as well as any new processes as they are forked.
sarcomes with the sysstat package,8which includes mpstat and iostat . The
Fedora distribution includes a startup service named sysstat that periodically logs
the system activity into log files stored in a directory named /var/log/sa . These
files are available for postmortem debugging of system activity.
I/O Information from sar
sarcan show detailed information on block devices and network devices. In addi-
tion, you can see the interrupt activity over time. Ideally, there is only device perinterrupt line, although as I discussed earlier, that is not always the case.
For block devices, the output is all or nothing. You cannot choose which block
devices to monitor, but the output is useful if you are trying to debug disk utiliza-tion. Here, I use the 
-poption to show device names instead of major/minor num-
bers. For example:
$ sar -d -p 1 1
Linux 2.6.16np (redhat)         06/01/2006
10:40:12 PM       DEV       tps  rd_sec/s  wr_sec/s  avgrq-sz  avgqu-sz     await     svctm     %util
10:40:13 PM       hda      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.0010:40:13 PM       hdb    457.00      8.00   8248.00     18.07      1.00      2.19      2.18     99.6010:40:13 PM       hdc      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.0010:40:13 PM     nodev   1037.00      8.00   8288.00      8.00      3.54      3.41      0.96     99.6010:40:13 PM     nodev      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
sar allows you to monitor network device usage with the -noption, which
requires a single argument. This argument allows you to report specific types of net-work objects defined by the 
sarprogram. This argument can take one of the fol-
lowing values:
•DEV—Ethernet device statistics
•EDEV —Ethernet device error statistics472 Chapter 9 • Performance T uning
8. http://perso.wanadoo.fr/sebastien.godard
•NFS—NFS client statistics
•NFSD —NFS server statistics
•SOCK —socket statistics
•ALL—all of the above
As with block devices, the statistics are all or nothing with respect to specific
Ethernet ports, so the output can be dense. Here is a sample of Ethernet statisticsfrom my machine:
$ sar -n DEV 1 1
Linux 2.6.16np (redhat)         06/01/2006
10:57:43 PM     IFACE   rxpck/s   txpck/s   rxbyt/s   txbyt/s   rxcmp/s   txcmp/s  rxmcst/s
10:57:44 PM        lo     15.00     15.00  11068.00  11068.00      0.00      0.00      0.0010:57:44 PM      eth0     14.00     15.00   1186.00  18082.00      0.00      0.00      0.00
Interrupt activity can be an important indicator of system activity. sarallows
you to monitor interrupt activity with the -Ioption. For example:
$ sar -I 0 1 1
Linux 2.6.16np (redhat)         06/01/2006
11:00:52 PM      INTR    intr/s
11:00:53 PM         0    250.00Average:            0    250.00
In this case, I chose interrupt 0, which happens to be the timer interrupt. The
output matches the system clock tick, which is 250Hz on this system. You mustspecify a specific interrupt to monitor or 
ALLto show all interrupts. If you need to
know which interrupt belongs to which device, look at the contents of
/proc/interrupts .
Some Concluding Comments on sar
The most common use for saris for looking at performance retroactively. The sys-
tem can create a cron job to run the sysstat service periodically and save the
results to a file. The cron job that comes with the sysstat package is stored in
/etc/cron.d/sysstat and contains commented-out entries that you can use.
The sysstat script uses a couple of helper programs to store the output in files.
The naming convention for these files is safollowed by a two-digit day. To prevent
the files from being unwieldy, process information is not stored in the files.Everything I discussed in this section is available except processes.9.2 System Performance 473
If, for example, you wanted to check on server utilization on the 13th from 3 AM
to 4 AM, you could use the following query:
$ sar -f sa13 -s 03:00:00 -e 04:00:00
Linux 2.6.7 (somemachine)      06/13/06
03:00:00          CPU     %user     %nice   %system   %iowait     %idle
03:10:00          all      2.19      0.34      4.35      0.06     93.0603:20:00          all      2.31      0.21      4.33      0.10     93.0603:30:00          all      1.98      0.00      4.11      0.06     93.8503:40:00          all      2.25      0.20      4.34      0.08     93.1203:50:00          all      5.29      0.01      4.96      0.34     89.4104:00:00          all      2.10      0.00      4.34      0.06     93.49Average:          all      2.69      0.13      4.40      0.12     92.67
By default, sysstat is configured to save statistics at 10-minute intervals. You
could run sarmanually as well with shorter intervals to track system behavior over
a shorter time. As the interval gets shorter, of course, the file gets larger and theoverhead gets bigger. Use your judgment.
9.2.4.3 iostat and mpstat
The 
sysstat package comes with two more useful tools for looking at system per-
formance: iostat and mpstat . The data that these tools provide is also available
from sar, but these tools are for command-line use exclusively. They do not store
or retrieve archived system information, but they may be easier to use than remem-bering a bunch of options.
The output is similar to the 
-doutput of sarexcept that you can target a single
device with the -poption. Typing iostat with no arguments, for example, gives you
system information on I/O devices since the system was booted:
# iostat
Linux 2.6.16np (redhat)         06/03/2006
avg-cpu:  %user   %nice %system %iowait  %steal   %idle
4.56    0.42    1.55    1.65    0.00   91.83
Device:            tps   Blk_read/s   Blk_wrtn/s   Blk_read   Blk_wrtn
hda               0.01         0.04         0.00        201          0hdb               5.66       169.67        39.79     833342     195444hdc               0.00         0.01         0.00         28          0dm-0              9.42       169.51        39.60     832570     194512dm-1              0.03         0.01         0.19         72        928474 Chapter 9 • Performance T uning
As with sar, the units are self explanatory. You can get a periodic update every
second by providing the interval in seconds as the first argument. When periodicupdates are requested, the first sample always contains the cumulative statistics sinceboot time. Subsequent samples reflect the current I/O activity.
The 
mpstat program shows activity for multiple processors in an SMP system.
Because this functionality has been integrated into sar, the mpstat command
doesn’t offer much. The output is virtually identical to the default output from sar.
9.3 Application Performance
You have seen how to monitor the performance of the system as a whole, but whathappens when you want to focus on a single application? Perhaps you have nar-rowed a system issue down to a single unruly process. What then? There are toolsto help, but sometimes, they can be difficult to decipher. It helps to understand thearchitecture of the system and CPU you are working on.
Tools are available on all platforms, but if you are working on an Intel architec-
ture processor, you’re in luck: Several sophisticated tools are available that use thebuilt-in performance monitoring registers to debug performance issues. In this sec-tion, I look in detail at tools that work on all architectures, as well as some that workonly on the Intel architecture.
9.3.1 The First Step with the time Command
When you’re tuning performance, the time command is perhaps the quickest and
easiest way to get some basic answers. I have used this command extensively in ear-lier examples, so I won’t dwell on it here except to say that this command should beyour first step when debugging performance issues.
I’ll begin with a simple example using the 
ddcommand, which reads from a
device and writes to another device. In this case, I’ll read the random numbers from
/dev/urandom and write them to the bit bucket:
$ time dd if=/dev/zero of=/dev/null count=1000
1000+0 records in1000+0 records out
real    0m0.226s
user    0m0.000ssys     0m0.212s9.3 Application Performance 475
You may recall that bash has a built-in time command that provides only tim-
ing information, which is what you see above. GNU provides a stand-alone time
command that implements the POSIX standard features, as well as many additionalones. This command is provided with most Linux distributions. You can specify thestand-alone command by escaping the 
time command with a single backslash, as
follows:
$ \time dd if=/dev/zero of=/dev/null count=1000
1000+0 records in1000+0 records out0.00user 0.22system 0:00.22elapsed 96%CPU (0avgtext+0avgdata 0maxresident)k0inputs+0outputs (0major+161minor)pagefaults 0swaps
This version of the time command provides additional useful information
regarding CPU utilization, page faults, and swapping, as well as the elapsed time.The default output is fairly dense, as you can see. The GNU version, however,allows you to zoom in on the relevant details by using formats. The format string isgiven with the 
foption, and specific formats are documented in the time(1) man
page. To look at minor page faults, for example, you could use %Rformat with the
following command:
$ \time -f "%R" bash -c "read -n 4096 x" < /dev/zero
286$ \time -f "%R" bash -c "read -n 40960 x" < /dev/zero295
Here, you can see that minor faults go up with the number of bytes read by the
Bash read function. There are many additional formats to allow you to focus on
whatever measurement you are interested in.
9.3.2 Understanding Your Processor Architecture with x86info
x86info9summarizes your processor configuration based on the information pro-
vided by the cpuid instruction defined on the Intel architecture. Most distributions
come with this tool, but the Intel architecture is a moving target. If your distribu-tion is older than your processor, it’s likely that the information it provides will beincomplete or just wrong.
9. http://sourceforge.net/projects/x86info476 Chapter 9 • Performance T uning
One problem in particular is the way the Intel architecture describes the cache.
For example:
$ x86info
x86info v1.17.  Dave Jones 2001-2005Feedback to <davej@redhat.com>.
Found 1 CPU
--------------------------------------------------------------------------Found unknown cache descriptors: 64 80 91 102 112 122Family: 15 Model: 1 Stepping: 2 Type: 0 Brand: 8CPU Model: Pentium 4 (Willamette) [D0] Original OEMProcessor name string: Intel(R) Pentium(R) 4 CPU 1.70GHz
Feature flags:
fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflsh
ds acpi mmx fxsr sse sse2 ss ht tmExtended feature flags:
Instruction trace cache:
Size: 12K uOps  8-way associative.
L1 Data cache:
Size: 8KB       Sectored, 4-way associative.line size=64 bytes.
L2 unified cache:
Size: 256KB     Sectored, 8-way associative.line size=64 bytes.
Instruction TLB: 4K, 2MB or 4MB pages, fully associative, 64 entries.Found unknown cache descriptors: 64 80 91 102 112 122Data TLB: 4KB or 4MB pages, fully associative, 64 entries.The physical package supports 1 logical processors
This is my humble 1.7 GHz Pentium 4, which is getting a bit old. Still, x86info
says it doesn’t recognize several of the cache descriptors. For Intel processors, cache
descriptors are described in Intel Application note 485,10which is updated frequently.
In this case, it appears to be a bug in x86info , because these cache descriptors
are reported correctly but the tool still says they’re unknown . Generally speaking, the
specific information provided by x86info usually is correct, especially for mature
processors. Beware if you have a newer processor, and if in doubt, check the appli-cation note to be sure.
Another place to look is 
/proc/cpuinfo , but this information is baked into the
kernel. Chances are that this information is even more out of date. If your processor9.3 Application Performance 477
10. http://developer.intel.com
is newer than your kernel, the information here is likely to be incomplete or
wrong. Furthermore, the information here is much less specific than x86info . In
particular, /proc/cpuinfo does not distinguish among level 1, 2, or 3 caches.
Again, using my humble P4 as an example:
$ cat /proc/cpuinfo
processor       : 0vendor_id       : GenuineIntelcpu family      : 15model           : 1model name      : Intel(R) Pentium(R) 4 CPU 1.70GHzstepping        : 2cpu MHz         : 1700.381cache size      : 256 KBfdiv_bug        : nohlt_bug         : nof00f_bug        : nocoma_bug        : nofpu             : yesfpu_exception   : yescpuid level     : 2wp              : yesflags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmovpat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tmbogomips        : 3406.70
Now that you have this information, it’s time to put it to use. First, you need an
example program to work with: cache-miss.c , shown in Listing 9-3.
LISTING 9-3 cache-miss.c: A Program to Demonstrate Tools That Monitor Cache Hits
and Misses
1 #include <stdio.h>
2 #include <string.h>3 #include <stdlib.h>4 #include <unistd.h>5 #include <malloc.h>6 #include <assert.h>78 int main(int argc, char *argv[])9 {
10     // Default to 64 TLBs (max on my processor).11     int num_pages = 64;1213     // User can specify any number of pages.478 Chapter 9 • Performance T uning
14     if (argc > 1) {
15         num_pages = atoi(argv[1]);16     }1718     const size_t page_size = sysconf(_SC_PAGESIZE);1920     int num_bytes = num_pages * page_size;21     int alloc_bytes = num_bytes;2223     if (alloc_bytes == 0) {24         // allocate one page, just for a baseline.25         // we won't use it though.26         alloc_bytes = page_size;27     }2829     // Allocate memory aligned on a page boundary30     char *buf = memalign(page_size, alloc_bytes);31     assert(buf != NULL);3233     printf("%d pages %d KB\n", num_pages, num_bytes / 1024);3435     // User requested zero pages. We allocated one page, but36     // did not touch it, therefore we caused no additional page faults.37     if (num_pages == 0) {38         exit(0);39     }4041     // Need a place to store the bytes that we will read.42     static volatile char store;4344     /*45      ** We read one byte from the base of each page46      ** until we've done 1,000,000 reads.47      */4849     int i;50     char *c = buf;5152     for (i = 0; i < 1000000; i++) {53         store = *c;54         c += page_size;55         if (c - buf >= num_bytes)56             c = buf;57     }5859     return 0;
60 }9.3 Application Performance 479
Note that this program reads 1 byte from each page allocated until it does 1 mil-
lion reads. It writes the byte to a variable so that the compiler doesn’t discard theread instructions due to optimization. Now I’ll use this example to demonstratesome tools.
9.3.3 Using Valgrind to Examine Instruction Efficiency
Valgrind is available on Intel architectures (IA32 and X86_64) as well as PowerPC32-bit architectures. It is actually a suite of tools for checking memory leaks andmemory corruption. Here, I’ll focus on the tool named 
cachegrind , which reports
the cache efficiency of your code. Compile the example from Listing 9-3:
$ cc -O2 -o cache-miss cache-miss.c
You need to compile with optimization to get the best results. This program takes
a single argument, which is the number of pages to allocate. When it runs, the pro-gram does 1 million reads—one per page, cycling through the pages as necessary.First, run this with an argument of 
0pages, which will give you a baseline against
which you can compare. The command to invoke Valgrind with the cachegrind
tool is
$ valgrind --tool=cachegrind ./cache-miss 0
==18902== Cachegrind, an I1/D1/L2 cache profiler.
==18902== Copyright (C) 2002-2005, and GNU GPL'd, by Nicholas Nethercote et al.==18902== Using LibVEX rev 1471, a library for dynamic binary translation.==18902== Copyright (C) 2004-2005, and GNU GPL'd, by OpenWorks LLP.==18902== Using valgrind-3.1.0, a dynamic binary instrumentation framework.==18902== Copyright (C) 2000-2005, and GNU GPL'd, by Julian Seward et al.==18902== For more details, rerun with: -v==18902==--18902-- warning: Pentium 4 with 12 KB micro-op instruction trace cache--18902--          Simulating a 16 KB I-cache with 32 B lines0 pages 0 KB==18902====18902== I   refs:      137,569
Instruction cache
==18902== I1  misses:      1,216==18902== L2i misses:        694==18902== I1  miss rate:    0.88%==18902== L2i miss rate:    0.50%==18902====18902== D   refs:       62,808  (47,271 rd + 15,537 wr)
Data cache – L1 and L2
==18902== D1  misses:      2,253  ( 1,969 rd +    284 wr)==18902== L2d misses:      1,251  ( 1,041 rd +    210 wr)==18902== D1  miss rate:     3.5% (   4.1%   +    1.8%  )==18902== L2d miss rate:     1.9% (   2.2%   +    1.3%  )480 Chapter 9 • Performance T uning
==18902==
==18902== L2 refs:         3,469  ( 3,185 rd +    284 wr) Data cache – L2 only
==18902== L2 misses:       1,945  ( 1,735 rd +    210 wr)==18902== L2 miss rate:      0.9% (   0.9%   +    1.3%  )
That’s a lot of output, but I’ve highlighted the important parts. It helps to know
what you’re looking for to see through the noise. In this case, the program exitsimmediately, so most of the activity you see is generated by the process of loadingthe application. Given the same input parameters, the output will be the same forevery run (except for the 
pid, of course).
In this case, you can see that there were 62,808 data references. Of these, 47,271
were read requests, and the other 15,537 were writes. The interesting part is on thenext line, which tells you that there were 2,253 cache misses in the L1 data cache(abbreviated as 
D1). A cache miss is a read or write that requested data that wasn’t in
the cache. A cache miss occurs on the first read or write of a cache line. Because thisexample reads only 1 byte from each page, this is the only read it is counting.
When a cache miss occurs, the instruction stalls while the cache line is filled. This
in turn causes the instruction to take more clock cycles and possibly delay otherinstructions from completing. You can see a summary on the lines labeled 
miss
rate , where the misses are reflected as a percentage of the total. Here, you see that
the read miss rate was 3.5 percent.
Now see what happens when you run this while the program actually does some-
thing. Run it with only two pages so that the data access fits inside the L1 cache:
$ valgrind --tool=cachegrind ./cache-miss 2 Allocate two pages.
==18908== Cachegrind, an I1/D1/L2 cache profiler.
...1 pages 4 KB==18908====18908== I   refs:      10,136,737==18908== I1  misses:         1,213==18908== L2i misses:           692==18908== I1  miss rate:       0.01%==18908== L2i miss rate:       0.00%==18908====18908== D   refs:       2,062,373  (1,046,952 rd + 1,015,421 wr)==18908== D1  misses:         2,236  (    1,951 rd +       285 wr)==18908== L2d misses:         1,252  (    1,041 rd +       211 wr)==18908== D1  miss rate:        0.1% (      0.1%   +       0.0%  )==18908== L2d miss rate:        0.0% (      0.0%   +       0.0%  )==18908====18908== L2 refs:            3,449  (    3,164 rd +       285 wr)==18908== L2 misses:          1,944  (    1,733 rd +       211 wr)==18908== L2 miss rate:         0.0% (      0.0%   +       0.0%  )9.3 Application Performance 481
Notice that you see just over 2 million data references. This is the application
reading and writing 1 million times, as planned. Because the data fits in cache, thenumber of L1 data cache misses does not go up. In fact, the number of read missesgoes down—from 1,969 read misses to 1,951. Most likely, this is due to readprefetching going on inside the cache controller, although there is no way to knowfor sure. The number of write misses goes up by only 1—from 284 to 285. Withso many references hitting the cache, the misses are negligible compared with thehits, so the miss rate is effectively zero.
Now I’ll make it interesting. This processor has 256K of cache (64 pages), so allo-
cate the full L2 cache and see what it looks like:
$ valgrind --tool=cachegrind ./cache-miss 64
...64 pages 256 KB==18914====18914== I   refs:      9,152,148==18914== I1  misses:        1,176==18914== L2i misses:          670==18914== I1  miss rate:      0.01%==18914== L2i miss rate:      0.00%==18914====18914== D   refs:      2,062,236  (1,046,866 rd + 1,015,370 wr)==18914== D1  misses:    1,002,231  (1,001,947 rd +       284 wr)==18914== L2d misses:        1,315  (    1,105 rd +       210 wr)==18914== D1  miss rate:      48.5% (     95.7%   +       0.0%  )==18914== L2d miss rate:       0.0% (      0.1%   +       0.0%  )==18914====18914== L2 refs:       1,003,407  (1,003,123 rd +       284 wr)==18914== L2 misses:         1,985  (    1,775 rd +       210 wr)==18914== L2 miss rate:        0.0% (      0.0%   +       0.0%  )
Now virtually all the L1 data cache reads are misses, and the majority of the L1
writes are hits. That’s because the read buffer no longer fits in the L1 data cache,but because you are writing to the same memory location all the time, the writesalways hit the cache. At 256K, however, the data still fits in the L2 data cache, asyou can see in the output that follows the L1 data cache statistics.
The L2 cache on this processor is unified, which means that the same cache is
used for both instructions and data. Here, you see that 1,003,407 references wentto the L2 cache controller, whereas before, there were only 3,449 references. Thisnumber includes both instruction fetches and data reads. The processor does notdistinguish between the two types of references, but it does distinguish between the482 Chapter 9 • Performance T uning
two types of misses. The ones you are interested in are the data cache misses, listed
as L2d misses .
The vast majority of the L2 references were reads, which is as expected. The num-
ber of misses was negligible, which tells you that most of the reads were hits. Thisis reflected in an effective L2 miss rate of 0 percent. Finally, see what this outputlooks like when the data no longer fits in L2:
$ valgrind --tool=cachegrind ./cache-miss 256
...256 pages 1024 KB==18918====18918== I   refs:      9,140,561==18918== I1  misses:        1,176==18918== L2i misses:          670==18918== I1  miss rate:      0.01%==18918== L2i miss rate:      0.00%==18918====18918== D   refs:      2,062,295  (1,046,911 rd + 1,015,384 wr)==18918== D1  misses:    1,002,230  (1,001,946 rd +       284 wr)==18918== L2d misses:    1,001,251  (1,001,041 rd +       210 wr)==18918== D1  miss rate:      48.5% (     95.7%   +       0.0%  )==18918== L2d miss rate:      48.5% (     95.6%   +       0.0%  )==18918====18918== L2 refs:       1,003,406  (1,003,122 rd +       284 wr)==18918== L2 misses:     1,001,921  (1,001,711 rd +       210 wr)==18918== L2 miss rate:        8.9% (      9.8%   +       0.0%  )
This run uses 256 pages, which is four times the size of the L2 cache. You expect
that every read from the block of memory will produce a cache miss. In fact, theoutput shows that the total number of L2 data misses exceeds 1 million. Valgrindexpresses the miss rate as a percentage of total data references, which is listed as 
D refs . This includes reads that were issued to load the code, so the result in this
example comes out to 95.6 percent instead of 100 percent.
The overall L2 data miss rate ( L2d miss rate ) is reported as only 48.5 percent
because it includes both reads and writes. Recall that the program does just as manyreads as writes, except that the writes are all to a single page that certainly neverleaves the cache.
Likewise, the L2 miss rate is somewhat misleading for this example. Here, it
shows 8.9 percent, which is the relationship of L2 misses (both instruction anddata) against all reads and writes (9,140,561). If you look at this number, it lookslike the program is not so bad. This number is misleading because the instruction9.3 Application Performance 483
fetches that occur inside a tight loop are all cache hits, which tend to drive down
the overall average miss rate.
Valgrind has other tools that are worth exploring. It is an excellent tool that is
being improved continually and should be in every developer’s toolbox. Theanswers that Valgrind produces are useful even when your target architecture is notsupported by Valgrind. As long as you can port your code to a supported platformand run it there, you can use the answers from Valgrind to fix your source code.
9.3.4 Introducing ltrace
ltrace traces an application’s use of library calls. Like strace , which I’ve used in
earlier examples, ltrace can show which functions are called, as well as their argu-
ments. The difference is that strace shows you system calls only, whereas ltrace
is able to show you library calls. In C and C++, system calls are made via standard
library wrappers, so ltrace can show you the same information as well.
For performance, it can be useful to see a histogram of calls with the -coption:
$ ltrace -c dd if=/dev/urandom of=/dev/null count=1000
1000+0 records in1000+0 records out% time     seconds  usecs/call     calls      function------ ----------- ----------- --------- --------------------
94.44    2.298177        2298      1000 read
3.35    0.081500          81      1000 write2.04    0.049663          49      1000 memcpy0.04    0.000869         434         2 dcgettext0.03    0.000674          84         8 sigaction0.02    0.000582         582         1 setlocale0.02    0.000450         225         2 fprintf0.02    0.000369          92         4 close0.01    0.000279          93         3 strchr0.01    0.000250          62         4 sigemptyset0.01    0.000201         100         2 open640.00    0.000115          57         2 malloc0.00    0.000098          49         2 free0.00    0.000061          61         1 __strtoull_internal0.00    0.000059          59         1 bindtextdomain0.00    0.000055          55         1 __errno_location0.00    0.000054          54         1 textdomain0.00    0.000051          51         1 getpagesize0.00    0.000050          50         1 __ctype_b_loc0.00    0.000050          50         1 __cxa_atexit
------ ----------- ----------- --------- --------------------100.00    2.433607                  3037 total484 Chapter 9 • Performance T uning
With the -coption, ltrace prints the library calls made by the process, sorted
by the time spent in each call. The example above reads from the urandom device,
which generates random numbers, and writes to the null device, which discards
everything written. Intuitively, you should expect that it takes virtually no time towrite to a null device. Likewise, it takes some amount of time in kernel mode togenerate the random numbers required by the read. Therefore, the reads should takesignificantly longer than the writes. Because virtually all that this process does isread and write, the reads should dominate the runtime. That is, in fact, what theoutput shows, with the read function dominating 94 percent of the runtime.
The catch to using 
ltrace is that the program takes significantly longer to run.
ltrace works much like attaching to a process with a debugger, which accounts for
the extra time it takes. Each function call is like a breakpoint, which causes ltrace
to store timing statistics.
By default, ltrace produces detailed output of all library calls. You can filter 
the list with the -eoption to show only functions of interest. You also can trace the
duration of each individual function call with the -Toption. For example:
$ ltrace -T -e read,write dd if=/dev/urandom of=/dev/null count=1
read(0,"\007\354\037\024b\316\t\255\001\322\222O\b\251\224\335\357w\302\351\207\323\n$\032\342\211\315\2459\006{"..., 512) = 512 < 0.000327 >
write(1,"\007\354\037\024b\316\t\255\001\322\222O\b\251\224\335\357w\302\351\207\323\n$\032\342\211\315\2459\006{"..., 512) = 512 < 0.000111 >
1+0 records in1+0 records out+++ exited (status 0) +++
Here, you can see additional information about each call, but the -Toption adds
the duration of the call (indicated within the angle brackets). Here, too, you can seethat the read takes longer than the write, but only by three times. The precise out-put can vary based on the overhead consumed by the program. Beware of readingtoo much into these numbers.
The limitation of 
ltrace is that it can trace only dynamic library calls—not calls
to functions in statically linked libraries.
9.3.5 Using strace to Monitor Program Performance
strace and ltrace take many of the same options and can be used in much the
same way. The difference is that strace tracks system calls exclusively. Unlike9.3 Application Performance 485
ltrace , however, strace does not require a dynamic library to trace system calls.
Calls are traced whether or not they use wrapper functions, because system calls useinterrupts to transition between user mode and kernel mode. This results in lessoverhead than 
ltrace , even though strace slows your program execution. It’s not
as extreme a penalty as ltrace .
You should not take timing results from strace too seriously, however. An
unfortunate side effect of the way strace monitors program execution is that it
tends to underestimate system time severely. Consider the earlier example in which
ltrace indicated that the read function (which is a system call) took 94 percent
of the overall runtime, and see what strace has to say:
$ strace -c dd if=/dev/urandom of=/dev/null count=1000
1000+0 records in1000+0 records outProcess 22820 detached% time     seconds  usecs/call     calls    errors syscall------ ----------- ----------- --------- --------- ----------------100.00    0.013654          14      1003           read
0.00    0.000000           0      1002           write0.00    0.000000           0        12         6 open0.00    0.000000           0         8           close0.00    0.000000           0         1           execve0.00    0.000000           0         1         1 access0.00    0.000000           0         3           brk0.00    0.000000           0         6           old_mmap0.00    0.000000           0         2           munmap0.00    0.000000           0         1           uname0.00    0.000000           0         2           mprotect0.00    0.000000           0         8           rt_sigaction0.00    0.000000           0         2           mmap20.00    0.000000           0         4           fstat640.00    0.000000           0         1           set_thread_area
------ ----------- ----------- --------- --------- ----------------100.00    0.013654                  2056         7 total
According to strace , the program just got 16 times faster, but don’t believe
everything you read! Notice that although there were 1,002 write calls, they are listedas 0 percent of the time, which doesn’t mean that they took no time to execute—justthat 
strace couldn’t measure them. More important, the read calls, which you
know are slow, consumed only 13 ms. This is not possible if the output from the
time command is to be believed.
This discrepancy is an artifact of the way strace traces work. strace forks and
executes your process on your behalf, so there actually are two processes running. A486 Chapter 9 • Performance T uning
certain amount of system time that would be consumed by the process being traced
is charged to strace . You can see this for yourself by timing strace with the time
command:
$ time strace -c dd if=/dev/urandom of=/dev/null count=1000
...
0.00    0.000000           0         1           set_thread_area
------ ----------- ----------- --------- --------- ----------------100.00    0.013590                  2056         7 total
real    0m0.318s
user    0m0.016ssys     0m0.292s
Despite the timing discrepancies, strace is still useful for profiling because it
can give you a count of the system calls a process makes. The ranking of system callsusually is accurate, as it is in this example, and it can steer you to system calls inyour application that may be causing performance issues.
The ability to attach to running processes is a very useful technique as well. This
comes in handy when you have a process that appears to be hung. You can attachto the process with 
strace -p , for example:
$ strace -ttt -p 23210
Process 23210 attached - interrupt to quit1149372951.205663 write(1, "H", 1)      = 11149372951.206036 select(0, NULL, NULL, NULL, {1, 0}) = 0 (Timeout)1149372952.209712 write(1, "e", 1)      = 11149372952.210045 select(0, NULL, NULL, NULL, {1, 0}) = 0 (Timeout)1149372953.213802 write(1, "l", 1)      = 11149372953.214133 select(0, NULL, NULL, NULL, {1, 0}) = 0 (Timeout)1149372954.217911 write(1, "l", 1)      = 11149372954.218244 select(0, NULL, NULL, NULL, {1, 0}) = 0 (Timeout)1149372955.221994 write(1, "o", 1)      = 1Process 23210 detached
In this contrived example, I created a process that writes Hello World 1 byte at
a time with a sleep in between. It’s silly, but it illustrates what the output looks like.You also can use the 
-coption to get a histogram in this mode.
9.3.6 Traditional Performance Tuning Tools: gcov and gprof
These tools have origins that go back to UNIX and are useful but somewhat hardto use. One problem with these tools is that they require you to instrument yourexecutable, which can be tricky and also degrades your performance. Optimization9.3 Application Performance 487
can make the output challenging to read, as the source code may not accurately
reflect what the CPU is doing. T urning off optimization and turning on debuggingensure that each line of source code has machine instructions associated with it.Running an executable without optimization, however, is unacceptable in manyapplications. Fortunately, GNU has done a great deal to make sure that you canoptimize your code and still produce usable results.
9.3.6.1 Using gprof
gprof is a tool for profiling your executable, which means that it helps you deter-
mine where your program is spending most of its time. The catch is that it measuresonly code that has been instrumented by the compiler. This instrumentation is gen-erated when you compile with the 
-pgflag using gcc. Any code that is not instru-
mented is not measured, so any modules that you do not compile with -pgwill not
be measured. This usually is the case with the libraries that you link with. If yourcode calls a library function that is taking quite a bit of time, the time is charged tothe calling function, leaving it up to you to figure out what is causing your func-tion to consume so much time.
Listing 9-4 is a simple example that illustrates how the profiler can help, as well
as some of its limitations.
LISTING 9-4 profme.c: Simple Profiling Demonstration Program
1 #include <stdio.h>
2 #include <string.h>3 #include <stdlib.h>4 #include <math.h>56 /* Raise x to the power of something. Not very fast. */7 double slow(double x)8 {9     return pow(x, 1.12345);
10 }1112 /* Floating point division - very slow. */13 double slower(double x)14 {15     return 1.0 / x;16 }1718 /* Square root - perhaps the slowest. */19 double slowest(double x)488 Chapter 9 • Performance T uning
20 {
21     return sqrt(x);22 }2324 int main(int argc, char *argv[])25 {26     int i;27     double x;2829     /* Need a large number here to get a good sample. */30     for (i = 0; i < 3000000; i++) {31         x = 100.0;32         x = slow(x);33         x = slower(x);34         x = slowest(x);35     }
36 }
Before you can use the profiler, you must build and link your code with the -pg
option. Other than that, you should use the same flags that you normally compile
with. If you normally compile with optimization, compile with optimization; ifnot, don’t turn on optimization. If you change the flags just for profiling, what youprofile may not represent what you normally run. To build this example, use 
-O2
and -pgas follows:
$ cc -pg -O2 -o profme profme.c -lm
$ ./profme
Each time you run this program, it creates a file named gmon.out in the current
directory. This, along with the instrumented executable, is the input to the gprof
program. The simplest and most useful output from gprof is the flat profile. This
is the default:
$ gprof ./profme
Flat profile:
Each sample counts as 0.01 seconds.
%   cumulative   self              self     total
time   seconds   seconds    calls  ns/call  ns/call  name46.88      0.15     0.15  3000000    50.00    50.00  slowest37.50      0.27     0.12  3000000    40.00    40.00  slower12.50      0.31     0.04                             main
3.12      0.32     0.01  3000000     3.33     3.33  slow
...9.3 Application Performance 489
I truncated the output to highlight the important points. Notice that the slow
function is the fastest function in the group. In fact, main takes almost as long to
run. These are so close that from one run to the next, they will likely exchangeplaces. Based on what you know about the application, the only thing 
main does is
loop and call functions; the time spent in main is pure overhead. The fact that the
slow function is practically tied with main tells you that it probably is not that slow
after all. To get a better picture, you can accumulate multiple gmon.out files and
feed them to gprof as follows:
$ export GMON_OUT_PREFIX=gmon.out Causes GLIBC to create mon.out with the PIDas the suffix.
(This feature is not documented.)
$ ./profme Creates a file named gmon.out.PID .
$ ./profme Repeat three more times.
$ ./profme
$ ./profme
$ gprof ./profme gmon.out.* Generate profile based on four runs.
Flat profile:Each sample counts as 0.01 seconds.
%   cumulative   self              self     total
time   seconds   seconds    calls  ns/call  ns/call  name55.22      0.74     0.74 12000000    61.67    61.67  slowest32.84      1.18     0.44 12000000    36.67    36.67  slower
6.72      1.27     0.09 12000000     7.50     7.50  slow5.22      1.34     0.07                             main
...
Notice that slow now runs a little slower than main in the timing produced with
the additional samples. Also notice that the calls column reflects the total func-
tion calls produced by all the runs of the program. This is about as straightforwardas it gets with 
gprof . More often, the output can be hard to interpret. In this triv-
ial example, I wrapped several standard library functions, which helps profiling butperhaps impedes performance. If I had not done this, however, 
gprof would have
charged all the runtime to main , because that would be the only function it had
instrumented. Standard library calls like powand sqrt are not instrumented and,
therefore, do not show up in the profile.11To get around this, gprof allows you to
produce a line-by-line profile with the -loption. To use this option, however, your490 Chapter 9 • Performance T uning
11. Some distributions provide profiled versions of the libraries.
executable must be compiled with debugging enabled ( -g) for line-number infor-
mation. After recompiling with the debugging flag, the output for the examplelooks like this:
Each sample counts as 0.01 seconds.
%   cumulative   self              self     total
time   seconds   seconds    calls  ns/call  ns/call  name54.55      0.18     0.18                             slowest (profme.c:21 @ 8048546)24.24      0.26     0.08                             slower (profme.c:16 @ 8048535)
6.06      0.28     0.02                             main (profme.c:34 @ 8048592)6.06      0.30     0.02                             slowest (profme.c:22 @ 804855c)3.03      0.31     0.01  3000000     3.33     3.33  slow (profme.c:8 @ 8048504)3.03      0.32     0.01  3000000     3.33     3.33  slowest (profme.c:20 @ 8048538)3.03      0.33     0.01                             main (profme.c:33 @ 8048587)0.00      0.33     0.00  3000000     0.00     0.00  slower (profme.c:14 @ 8048528)
This output can get hard to read when you have a great deal of code. But you can
compile only selected modules with profiling ( -pg) so that only those modules will
be included in the profiling output. As long as you include the -pgoption on the
link line, your executable will produce an appropriate gmon.out file.
gprof can also produce annotated source, similar to gcov , for modules compiled
with debugging. This also can be a useful tool for performance tuning.
9.3.6.2 Using gcov with gprof for profiling
Normally, gcov is for determining code coverage —that is, how much of your code
has executed during a particular run. Code coverage is a good predictor of how wellyour code has been tested, but it also can help in optimization, particularly withunfamiliar code.
Suppose that you have an application that was written by a summer intern, who
has left to go back to school. The application is useful but slow. Throwing compileroptimizations at the code does not improve performance significantly; your internhas stumped the optimizer. In this case, you have to dive into the code and do somehand optimization. Upon doing so, you find that in addition to being inefficient,the source code is incomprehensible. This is where 
gcov can help.
Instead of poring over thousands of lines of code, trying to reverse-engineer a
design that may not exist, it probably makes more sense to target your effort at thelines of code that execute most often. These lines are not necessarily where theapplication is spending most of its time, but they’re a good place to start. Handoptimization should combine both coverage testing and profiling.
Listing 9-5 shows the hypothetical intern’s work.9.3 Application Performance 491
LISTING 9-5 summer-proj.c: Example to Illustrate Profiling and Coverage
1 #include <stdio.h>
2 #include <string.h>3 #include <stdlib.h>4 #include <time.h>5 #include <math.h>67 volatile double x;89 int main(int argc, char *argv[])
10 {11     int i;12     for (i = 0; i < 16000000; i++) {13         x = 1000.0;1415         /* 0 <= r < 16 */16         int r = i & 0xf;1718         if (r <= 8) {19             x = pow(x, 1.234);  /* called 9/16 of the time. */20         }21         else if (r <= 11) {22             x = sqrt(x);        /* called 3/16 of the time. */23         }24         else {25             x = 1.0 / x;        /* called 4/16 of the time. */26         }27     }
28 }
To enable coverage testing, you have to compile with two special flags for this
purpose:
$ cc -g -O2 -ftest-coverage -fprofile-arcs summer-proj.c -o summer-proj -pg -lm
For the purpose of coverage testing, it is preferable to leave optimization off,
because this preserves the line structure of the code more accurately. For your purposes, you don’t care, so you turn on optimization. Compiling with the 
-ftest-coverage flag causes the compiler to produce a file named 
summer-proj.gcno in addition to the executable. The -fprofile-arcs causes
your executable to produce a file named summer-proj.gcda when it runs. Both
files are used as input to the gcov program.12Finally, you kept the -pgoption so492 Chapter 9 • Performance T uning
12. The .gcda file can also be used as feedback to the compiler to optimize based on branch probabilities
with the -fbranch-probabilities option.
that you can use gprof as well. You must run the program at least once before
you can run gcov :
$ ./summer-proj Produces summer-proj.gcda
$ gcov ./summer-proj Produces summer-proj.c.gcov
File 'summer-proj.c'
Lines executed:100.00% of 9summer-proj.c:creating 'summer-proj.c.gcov'
Not surprisingly in this example, you have 100 percent coverage, which means that
all the lines of code that are executable were executed during this run. That’s not whatyou’re interested in, however. You need to look at the counts, which are containedinside 
summer-proj.c.gcov , to see what happened. This is shown in Listing 9-6.
LISTING 9-6 summer-proj.c.gcov: Example of gcov Output
-:    0:Source:summer-proj.c
-:    0:Graph:summer-proj.gcno-:    0:Data:summer-proj.gcda-:    0:Runs:1-:    0:Programs:1-:    1:#include <stdio.h>-:    2:#include <string.h>-:    3:#include <stdlib.h>-:    4:#include <time.h>-:    5:#include <math.h>-:    6:-:    7:volatile double x;-:    8:-:    9:int main(int argc, char *argv[])1:   10:{-:   11:    int i;
16000000:   12:    for (i = 0; i < 16000000; i++) {16000000:   13:        x = 1000.0;
-:   14:-:   15:        /* 0 <= r < 16 */
16000000:   16:        int r = i & 0xf;
-:   17:
16000000:   18:        if (r <= 8) {
9000000:   19:            x = pow(x, 1.234);  /* called 9/16 of the time. */
-:   20:        }
7000000:   21:        else if (r <= 11) {3000000:   22:            x = sqrt(x);        /* called 3/16 of the time. */
-:   23:        }-:   24:        else {
4000000:   25:            x = 1.0 / x;        /* called 4/16 of the time. */
-:   26:        }-:   27:    }
-:   28:}9.3 Application Performance 493
Notice that the counts line up nicely with the predictions in the comments.
Because most code doesn’t come with such helpful comments, tools like gcov are
available. Based on this output, it’s clear that line 19 is called most often. Recall thatlines 19, 22, and 25 are the same as the 
slow , slower , and slowest functions used
in Listing 9-4, but without the wrappers. Based on that output, you know that
sqrt is significantly slower than the other two. But where is this program spending
its time? Here again, gprof comes to the rescue:
$ gprof --no-graph -l summer-proj| head -10
Flat profile:
Each sample counts as 0.01 seconds.
%   cumulative   self              self     total
time   seconds   seconds    calls  Ts/call  Ts/call  name34.00      0.17     0.17                             main (summer-proj.c:22 @ 8048926)20.00      0.27     0.10                             main (summer-proj.c:25 @ 8048879)16.00      0.35     0.08                             main (summer-proj.c:19 @ 80488c6)11.00      0.41     0.06                             main (summer-proj.c:18 @ 80488b3)
6.00      0.43     0.03                             main (summer-proj.c:21 @ 8048870)
...
Note that because this program does not call any instrumented functions, you
have to specify the option nograph to gprof . Otherwise, you will get nothing but
the following complaint:
gprof: gmon.out file is missing call-graph data
In the output, you can see that the lines are ranked by the amount of time spent
on each line. Now it is clear that although the most counts occurred on line 19, theprogram spends most of its time on line 22 (
sqrt ). So in this case, the line counts
from gcov were misleading. In larger, more complicated programs, the output of
gcov tends to be a good indicator of where your code is spending most of its time.
The annotated listings often are easier to interpret than the output of gprof . One
more tool, however, can do both of these jobs, and with less interference: OProfile.
9.3.7 Introducing OProfile
A recent addition to the Linux kernel is a built-in profiler called OProfile. Your ker-nel must be compiled with OProfile support enabled for this to work.
13OProfile is
a very complex tool but has numerous advantages over gcov and gprof .
13. This flag can be found when you configure the kernel. Look under Instrumentation Support ; then select
Profiling Support. When you enable profiling, you will be prompted with an OProfile option.494 Chapter 9 • Performance T uning
Although OProfile is most useful to kernel developers to determine where the
kernel is spending its time, it’s also very useful for programmers, because it can tellyou what’s going on in user space as well. What’s more, you can operate exclusivelyin user space if you want to. You don’t need kernel source or instrumented objectfiles either in the kernel or user space. In other words, you don’t need to rebuildyour code to profile it.
There’s no such thing as a free lunch, of course. Running the profiler
(
oprofiled ) requires root privilege. When it runs, your code will run a little
slower, but the impact is much less than instrumenting the object files with com-piler flags. The following sections show OProfile in action.
OProfile consists of a package of user-space tools, including a daemon as well as
some command-line tools, which I will discuss shortly. These tools, however, workonly with a kernel that has been built with OProfile support enabled.
The command-line tools include
•
opcontrol —starts and stops the oprofiled daemon
•opreport —prints a report based on the most recent oprofiled statistics
•opgprof —produces a gmon.out file that can be used by gprof
•opannotate —produces annotated source from profile data
The opcontrol program is the central tool for managing the oprofiled
daemon that does the work of collecting the statistics.
Controlling the Profiler with opcontrol
opcontrol comes with numerous defaults to make the job of profiling as easy as
possible. There are numerous options as well, which are there for when you are readyfor more advanced work. In addition, 
opcontrol saves the command-line options
from each run, so that these become defaults the next time you run opcontrol .
To start the daemon, you need to call opcontrol with the start option. The
first time you call opcontrol , you must tell it whether you want to profile the ker-
nel or just user space. Because you will be working in user space for these examples,you will use the 
--no-vmlinux option to exclude kernel profiling as follows:
$ sudo opcontrol --start --no-vmlinux
Using default event: GLOBAL_POWER_EVENTS:100000:1:1:1Using 2.6+ OProfile kernel interface.Using log file /var/lib/oprofile/oprofiled.logDaemon started.Profiler running.9.3 Application Performance 495
opcontrol saves your most recent settings in your home directory in a directory
named .oprofile . The next time you start OProfile, you won’t need any options
other than --start , because the tool stores these options in your home directory.
Only root can start and stop the daemon, so this information is stored in root ’s
home directory. Ordinary users can only dump statistics with the --dump option—
but I’m getting ahead of myself.
When the daemon is running, it collects statistics from everything that runs in
the system— everything. This can be overwhelming the first time you look at it. The
output is available to any user, but first, you must tell the oprofiled to dump the
statistics with the --dump option. After you dump the statistics, you can view them
with opreport , opgprof , or opannotate . Any user can dump statistics or call
opreport , not just root . Here’s an example of how this is done:
$ opcontrol --dump
$ opreport | head -15CPU: P4 / Xeon, speed 1700.38 MHz (estimated)Counted GLOBAL_POWER_EVENTS events (time during which processor is not stopped)with a unit mask of 0x01 (mandatory) count 100000GLOBAL_POWER_E...|
samples|      %|
------------------
29141 62.2445 no-vmlinux
Placeholder for kernel events
3380  7.2196 libc-2.3.3.so2641  5.6411 libglib-2.0.so.0.400.82234  4.7718 libgobject-2.0.so.0.400.81824  3.8960 libgdk-x11-2.0.so.0.400.131076  2.2983 libvte.so.4.4.0
911  1.9459 libgtk-x11-2.0.so.0.400.13861  1.8391 libcrypto.so.0.9.7a738  1.5764 libpthread-2.3.3.so659  1.4076 libX11.so.6.2
...
The output appears as a list of object files containing the code that was running
when the profiler took each sample. The files are listed with the most frequentlyencountered file at the top. Because you are not profiling the kernel, any samplesthat occurred in kernel mode show up listed as 
no-vmlinux . This output format is
not the most useful for application programmers, however.
Notice that the output indicates that it counted GLOBAL_POWER_EVENTS . This is
the default on IA32. OProfile uses performance counters that are embedded in the496 Chapter 9 • Performance T uning
CPU, which is how it is able to have such a small impact on the system. Each
processor supports different set of events, but all IA32 processors have a commonset of events that can be trapped. This particular event counts everything that occurswhile the processor is not powered off.
oprofiled records everything from the time you call opcontrol --start . By
calling opcontrol--dump , you produce a snapshot of the statistics, but oprofiled
continues to collect and accumulate data. Subsequent calls to opcontrol --dump
will make the newly accumulated data available for inspection. Only when you call
opcontrol --stop does oprofiled stop collecting data.
Profiling an Application with OProfile
The steps to profiling an application are straightforward but must be followedexactly if you are to get useful results. Because 
oprofiled accumulates data con-
tinuously, it’s important to clear and dump the statistics at the appropriate times.To get accurate results, it’s also important to work on an idle system; otherwise,other running processes may affect your results. In general, the steps are
• Start the daemon/start collecting data
• Reset the statistics (if necessary)• Run the application• Dump the statistics and/or stop the daemon
After you have collected statistics with your application, you can use the report-
ing tools. You can profile an application that has not been compiled with any spe-cial flags. You can compile Listing 9-4 with optimization and no other flags, forexample, and get useful results:
$ cc -o profme -O2 -lm
$ opcontrol --start –no-vmlinux Start collecting statistics.
Using default event: GLOBAL_POWER_EVENTS:100000:1:1:1Using 2.6+ OProfile kernel interface.Using log file /var/lib/oprofile/oprofiled.logDaemon started.Profiler running.$ ./profme
Run our application to be profiled.
$ opcontrol dump Write a snapshot of the statistics so we can run opreport.
$ opreport9.3 Application Performance 497
Counted GLOBAL_POWER_EVENTS events (time during which processor is not stopped)
with a unit mask of 0x01 (mandatory) count 100000GLOBAL_POWER_E...|
samples|      %|
------------------
16477 52.9807 libm-2.3.3.so11597 37.2894 profme
1795  5.7717 no-vmlinux
325  1.0450 libc-2.3.3.so162  0.5209 bash140  0.4502 libglib-2.0.so.0.400.8103  0.3312 oprofiled
The output of opreport shows that the application spends most of its time in
libm , the standard math library. This makes sense, because you already know that
the slowest functions reside there. The difference is that you are running a fullyoptimized executable that you have not instrumented in any way.
That’s not all you can do, however. The OProfile package comes with another
tool called 
opgprof , which creates a gmon.out file that can be used by gprof . It’s
just an extra step, as follows:
$ opgprof profme Create gmon.out from saved statistics.
$ gprof --no-graph profme Resulting gmon.out does not have call graph data.
Flat profile:
Each sample counts as 1 samples.
%   cumulative   self              self     total
time   samples   samples    calls  T1/call  T1/call  name70.37   8161.00  8161.00                             slow22.78  10803.00  2642.00                             slower
5.40  11429.00   626.00                             slowest1.41  11593.00   164.00                             main0.03  11597.00     4.00                             _init
...
There are a few things to notice about this output. One is that there is no tim-
ing. Everything is based on sample counts exclusively. Because these samples (andthe percentage of time) reflect everything that is going on in the system, they maynot accurately reflect how much time your code is taking. If you were to collect thisdata on a busy system with your process running at a low priority, this profile couldbe misleading.
Notice also that there is no call count in the output. You know that the
oprofiled collected 8,161 samples when the CPU was running the slow function,498 Chapter 9 • Performance T uning
but you don’t know how many times the slow function was actually called. That is
a side effect of the fact that OProfile is looking at the entire system, not just oneprocess.
Finally, you had to use the 
--no-graph option again because the gmon.out file
produced by opgprof does not have call graph information. As with a normally
instrumented program, if you want line-by-line profiling, you must compile theexecutable with debugging (
-g). To see this, look again at Listing 9-5:
$ cc -g -O2 -o summer-proj summer-proj.c -lm
$ opcontrol --reset Clear statistics.
Signalling daemon... done$ opcontrol --startProfiler running.$ ./summer-proj
Run our code.
$ opcontrol –-dump T ell daemon to dump stats.
$ opgprof ./summer-proj Create gmon.out for gprof .
$ gprof --no-graph -l ./summer-proj | headFlat profile:
Each sample counts as 1 samples.
%   cumulative   self              self     total
time   samples   samples    calls  T1/call  T1/call  name46.81   3496.00  3496.00                     main (summer-proj.c:19 @ 8 048444)20.10   4997.00  1501.00                     main (summer-proj.c:12 @ 8 0483fb)14.82   6104.00  1107.00                     main (summer-proj.c:19 @ 8 04842c)
8.19   6716.00   612.00                     main (summer-proj.c:22 @ 8 048410)4.69   7066.00   350.00                     main (summer-proj.c:21 @ 8 0483ea)
This produces the familiar line-by-line profile. Again, there is no timing information—
just sample counts. But OProfile does gprof one better by providing annotated
source listings with opannotate . This does not even require the opgprof step:
$ opannotate --source ./summer-proj Output goes to stdout (below).
/*
* Command line: opannotate --source ./summer-proj** Interpretation of command line:* Output annotated source file with samples* Output all files** CPU: P4 / Xeon, speed 1700.38 MHz (estimated)* Counted GLOBAL_POWER_EVENTS events (time during which processor is not
stopped) with a unit mask of 0x01 (mandatory) count 100000
*/
/*9.3 Application Performance 499
* Total samples for file : "/home/john/examples/ch-07/prof/summer-proj.c"
**   7458 99.8527*/
:#include <stdio.h>
:#include <string.h>:#include <stdlib.h>:#include <time.h>:#include <math.h>::volatile double x;::int main(int argc, char *argv[]):{ /* main total:   7458 99.8527 */:    int i;
1554 20.8060 :    for (i = 0; i < 16000000; i++) {
27  0.3615 :        x = 1000.0;
::        /* 0 <= r < 16 */
6  0.0803 :        int r = i & 0xf;
:
115  1.5397 :        if (r <= 8) {
4678 62.6322 :            x = pow(x, 1.234);  /* called 9/16 of the time. */
:        }
350  4.6860 :        else if (r <= 11) {612  8.1939 :            x = sqrt(x);        /* called 3/16 of the time. */
:        }:        else {
116  1.5531 :            x = 1.0 / x;        /* called 4/16 of the time. */
:        }:    }:}
Here, you see the best of gcov and gprof . Each line is annotated with a sample
count—the number of times the oprofiled caught the processor executing this
code. Next to the sample count is the percentage of time spent on that line of code.This percentage is normalized for the file being collected, so the numbers shouldadd up to 100 percent regardless of what else was running at the same time.
Closing Remarks about OProfile
I barely scratched the surface of what OProfile can do. In particular, I didn’t exam-ine any other types of events that OProfile is capable of monitoring. The exactevents are different on every processor, and you can see what your processor is capa-ble of with the following command:
$ opcontrol --list-events500 Chapter 9 • Performance T uning
The output should be lengthy. The default event is good for application profil-
ing, but for system profiling, you will want to explore some of the other eventsavailable to you. You can find more information at the OProfile home page.
14
9.4 Multiprocessor Performance
The term for an operating system that runs on more than one CPU is Symmetric
Multiprocessing (SMP), and it has been around for many years.15When SMP is used,
overall system throughput on a busy server increases because the scheduler can dis-tribute processes across the CPUs. An individual single-threaded application, on theother hand, does not run any faster on a multiprocessor machine because it can runon only one processor at a time. Applications that benefit the most from SMP aremultithreaded applications, which allow the OS to distribute threads across proces-sors to realize higher throughput.
Recently, microprocessor vendors have been offering multicore and multi-
threaded CPUs. This means that many desktop and laptop machines now needSMP operating systems to take advantage of these new processors. Before, onlyservers and high-end workstations used SMP hardware.
9.4.1 Types of SMP Hardware
SMP comes in a few varieties, and new ones are on the way. Each SMP approachhas advantages and drawbacks. Understanding these can help you design code thatwill take the most advantage of multiprocessor hardware.
Before I go into the details of SMP implementations, you need some back-
ground. If you’re already familiar with these terms, you may want to skip the fol-lowing sections.
9.4.1.1 Some Background in CPU Parallelism
To increase the speed of the fastest CPUs, manufacturers have resorted to several
tricks over the years. It helps to understand these tricks so that you can understandwhy SMP and multicore CPUs are important.
RISC versus CISC
It’s hard to imagine, but there once was a time when people didn’t trust compilers.Programmers wrote code in assembly, even if a compiler was available. Assembly9.4 Multiprocessor Performance 501
14. http://oprofile.sourceforge.net
15. Linux 2.0 came out in 1996 and was the first version of Linux that supported SMP . 
language was considered to be the most efficient language, and most assembly pro-
grammers didn’t trust compilers to produce efficient or even correct output.
Most processors of the day were CISC (Complex Instruction Set Computers),
which provided a rich instruction set with many addressing modes to make assem-bly-language programming easier. T wo prominent examples of CISC processors arethe Motorola 68000 and the Intel 8086. The Motorola processor was the processorused in the original Macintosh computer, whereas the 8086 was the processor usedin the IBM PC.
16The 8086 instruction set survives today as a subset in the IA32
architecture.
Studies by IBM, Stanford, and Berkeley demonstrated that they could use the
transistors on a CPU more effectively by creating fewer instructions with limitedaddressing modes and lower instruction latency. This was the motivation for thecreation of the Reduced Instruction Set Computer (RISC) processor. In general,RISC instructions are more compiler friendly than user friendly. Examples of RISCprocessors include the PowerPC, SPARC, and MIPS processors.
Pipelining and Execution Parallelism
One innovation that allowed CISC and RISC processors to run faster was the useof pipelining —the computerized equivalent of the familiar manufacturing assem-
bly line, used by Henry Ford in the early 20th century to speed automobile man-ufacturing. When the assembly of a car was broken into several steps, each workercould specialize in one particular assembly step. Because all workers were special-ists, they could work more efficiently than if they each had to assemble cars fromstart to finish.
Instead of assembling cars, the CPU pipeline executes instructions one piece at a
time. The CPU designer breaks each instruction into discrete steps that can be exe-cuted independently. Each step is accompanied by special-purpose hardware dedi-cated to that step in the pipeline. The specialized hardware is referred to as anexecution unit (sometimes a supersalar unit). What the pipeline is to the assembly
line, the execution unit is to the worker.
The similarity to the assembly line ends there, however. Different instructions
require different stages, and some instructions take longer to execute than others.An instruction pipeline would be something like an assembly line that assembles502 Chapter 9 • Performance T uning
16. Technically, it was the 8088—the 8-bit version of the 8086.
cars, trucks, bicycles, and airplanes on a single line. We call the time it takes for an
instruction to go through the pipeline the instruction latency.
Each stage of a pipeline completes in one CPU clock, so a CPU with a 20-stage
pipeline can require up to 20 clock cycles to complete a single instruction. Thisseems like a lot until you consider that the CPU can work on 20 instructions at thesame time, so the net throughput can be one instruction per clock. This theoretical
maximum throughput is rarely achieved due to pipeline stalls.
A pipeline stall occurs most often as a result of a conditional branch instruction.
By the time the branch instruction and its condition have been decoded, there canbe several instructions behind it in the pipeline. These instructions may be dis-carded as a result of the branch. In this case, the throughput drops because thepipeline must be filled before the CPU starts executing instructions again. Thedepth of the pipeline determines how much of a penalty this will be, as illustratedin Figure 9-3.9.4 Multiprocessor Performance 503
FIGURE 9-3 Simplified Four-Stage Pipeline Stall: When the CPU Knows That xIs False,
All the Other Stages of the Pipeline Must Be Flushed and RefilledFull Pipeline St alled Pipeline
0: if (x) x == false 0: if (x)
1: Empty!
2: Empty!
3: Empty!1: Do this
2: Do that
3: Do other
Else ... Else ...
Pipeline stalls are unavoidable; you can’t write code without ifstatements. Many
CPUs come with features that try to minimize the effects of branching, but theresults will vary based on the application. In general, the pipeline allows the proces-sor to achieve peak instruction throughput in bursts. In real-world applications,that throughput cannot be sustained.
A common feature of RISC processors is that they have low instruction latencies,
requiring short instruction pipelines compared with CISC processors. This meansthat RISC processors should not suffer as badly from a pipeline stall as CISCprocessors do. This is one reason why IA32 processors have faster clocks than com-parable RISC processors. A deeper pipeline requires a faster clock to keep thelatency low in terms of nanoseconds. But sometimes, to make the clocks faster, it’snecessary to make the pipeline deeper. It’s a vicious cycle.
Cache
I discussed cache in detail in Chapter 5, so I won’t dwell on it here. This is animportant feature that enables processors to go faster than the DRAM otherwisewould allow. Cache allows the core to operate at a very high clock frequency thattoday’s DRAM technology is not capable of.
Because cache is so integral to performance, it’s important to understand how the
cache is allocated in a multicore design to predict how well your application willperform. The cache architecture of a multicore design could be the deciding factorfor you when choosing one chip over another.
9.4.1.2 Multiprocessor Motherboard
The oldest SMP implementation places two or more processors on the same moth-
erboard. The processors use a common FSB, RAM, and north bridge (refer toFigure 9-1), which presents many design challenges. For one thing, it tends to limitthe speed of the FSB bus because of the signal-quality issues associated with havingmany devices on a common bus. As a result, the FSB speed of the fastest multi-processor systems typically is slower than that of the fastest single-processor systems.
AMD’s Opteron CPUs have integrated DRAM controllers on the CPU that
allow each CPU on a multiprocessor motherboard to have high-speed access to itsown DRAM. In addition, the Opteron replaces the FSB bus with a Hypertransportbus, which is a packet-based point-to-point interface. This eliminates many of thesignal-quality issues that exist with a traditional FSB and allows AMD to placemany more CPUs on a single motherboard. The drawback to this approach is that504 Chapter 9 • Performance T uning
unlike FSB speed, which varies all over the map, the Hypertransport bus speed stays
constant until the next version of the standard is published. Hypertransport is apublished standard that other chip vendors design to, so changes to the standard areinfrequent. Likewise, the integrated DRAM controller on the Opteron tends to lagthe latest DRAM technology, because adopting a new memory technology requiresa new CPU design.
As you can see, each approach has advantages and drawbacks.
9.4.1.3 Symmetric Multithreading (SMT)
This is the generic Linux term for Intel’s Hyperthreading feature found in some
Pentium and Xeon processors. A processor with Hyperthreading looks like a multi-ple-core CPU to the operating system. These are not true multicore machines,because the SMT CPUs share most of the on-chip resources and, therefore, contendwith one another for them.
Each logical processor has its own register set and instruction pipeline, but that’s
about it. The processors share the on-chip cache, MMU, TLB, and all the execu-tion units. The upshot of this is that two logical cores, for example, cannot processinstructions twice as fast as two single-core processors.
SMT is an opportunity for parallelism unique to CISC processors. Because the
pipeline is deeper on a CISC processor, the effect of a pipeline stall is more drasticthan on a RISC processor, which has a shorter pipeline. Providing multiplepipelines in the form of multiple logical CPUs is one way to keep the executionunits busy in the presence of a pipeline stall.
Hyperthreading (and SMT) got its name because it is most effective at accelerat-
ing multithreaded software. Threads, unlike processes, share memory and page tableentries, which makes them ideal for distributing across logical CPUs. Because thereis only one MMU on an SMT CPU, threads experience more of a performanceboost than processes do.
9.4.1.4 Multicore CPUs
CPU vendors say they have reached the limit of how fast they can make a single CPU
run in terms of clock frequency. The problem is largely with the amount of heat thefast processors produce. When multiple cores are used, the chip can run at a lowerclock frequency but execute more code in the same number of clocks. A dual-coreCPU can run with a slower clock but still claim to be faster than a single-core CPUrunning at a faster clock rate.9.4 Multiprocessor Performance 505
By the time you read this book, multicore CPUs should be available for almost
every architecture. To keep the discussion simple, I will refer exclusively to the Intel-architecture implementations from AMD and Intel. The issues addressed by bothmanufacturers apply to all architectures and should provide a good basis of under-standing.
The first generation of multicore CPUs from Intel and AMD are dual core.
Functionally, a dual-core CPU is equivalent to two single-core CPUs (for example,on a multiprocessor motherboard). Each core has its own registers, cache, instruc-tion pipeline, execution units, MMU, and so on. A dual-core processor should, inprinciple, perform as well as an SMP system with two single-core processors run-ning at the same clock frequency.
Future dual-core and quad-core CPUs will share the on-chip cache at some level,
which has drawbacks and advantages. On one hand, it limits the amount of cachea single CPU can access without contending with another CPU. On the otherhand, sharing the cache cuts down on the number of cycles used to synchronize sep-arate caches. So some applications will benefit from sharing cache across multipleprocessors, and some will suffer. There is no simple answer as to which is the betterapproach.
9.4.2 Programming on an SMP Machine
Most applications never need to know that they are running on a machine with mul-tiple CPUs. Most of the details are dealt with in hardware and the operating system.The operating system is responsible for distributing tasks and balancing the loadacross CPUs. There are some applications, however, in which user space should beaware of the number and types of CPUs to make most efficient use of the hardware.
9.4.2.1 Linux Scheduler and SMP
Linux introduced SMP in version 2.0 of the kernel. The SMP scheduler tries to dis-
tribute tasks and threads efficiently across CPUs, making the most effective use ofthe hardware. The heuristics involved are based on several assumptions, the mostbasic of which is that all CPUs are equal. Indeed, that’s what the symmetric in
Symmetric Multiprocessing means.
This assumption is being challenged by innovations such as SMT and multicore
processors. In advanced multiprocessor architectures, it is often left up to the appli-cation to understand the nature of the hardware and clue in the scheduler.506 Chapter 9 • Performance T uning
The SMP scheduler tends to keep the process on the same CPU, because (thanks
to lazy TLB flushes) there is a good probability that it will be able to reuse TLBs.In fact, with an SMT CPU, this is a waste, because both logical CPUs use the sameMMU and cache. This could mean that a process will be forced to wait for oneCPU because the scheduler thinks it will be more costly to queue the process on theother CPU.
One of the ways that user applications can give the scheduler a clue is via the
affinity mask. The scheduler maintains an affinity mask for each process (and
thread) in the system. It is a bitmap with 1 bit for each CPU in the system. Thedefault affinity mask is all ones, which means that any processor may execute theprocess.
When the scheduler finds a task in the runnable state, it checks to see which CPUs
are available to execute code. Then it compares this against the affinity mask of theprocess to determine which CPU will execute the process. You can restrict a processto executing on one or more processors by setting the affinity mask appropriately.
As the kernel matures, the scheduler catches up to the technology. In Linux 2.6.7,
the scheduler added support for SMT
17in addition to SMP . So presumably, the
scheduler can make smarter choices when it comes to choosing CPUs for execution.
9.4.2.2 Using Affinity to Force a Process to Use a Particular CPU
The schedutils package includes the taskset command, which can be used to
set the affinity mask for a particular process. It can be applied to a running processor a single command. You can use the 
taskset command, for example, to force a
process to run only on the first processor of an SMP system by setting bit 0 of theaffinity mask and no others:
$ taskset 1 ./myprogram Set the affinity mask to 1.
You can use taskset to set or examine the affinity mask on running processes
as well as by using the -poption:
$ taskset -p 1234
pid 1234's current affinity mask: 1
Linux allows any user to inspect the affinity mask of any process, but only root
can change the affinity mask of a process, regardless of who owns it.9.4 Multiprocessor Performance 507
17. Available under Processor Type and Features in the kernel configuation menu.
9.4.2.3 When and Why to Modify Process Affinity
Whenever possible, it is preferable to leave the affinity mask alone and leave sched-
uling up to the Linux scheduler. Hard-coding process affinity into an application islikely to embed many assumptions in your design. Most likely, the code will runfine on whatever target you are testing on but may be suboptimal on a newer or dif-ferent architecture.
Changing the affinity mask is called for in only a few circumstances. One exam-
ple would be if you have a memory-intensive application that you want to keep ona single processor. Although the Linux scheduler tries to keep this on a single CPU,there is no guarantee that it will stay on one CPU. If you have two such processesrunning on a dual-CPU system, it may make sense to lock them down on individ-ual processors.
Another good use of affinity would be when you have dedicated hardware, such
as an embedded computer. In this case, the underlying hardware is well understoodbeforehand, and the code running on the computer is under full control of the sys-tem designers. Using affinity can be an excellent way to make sure that the under-lying hardware is utilized most efficiently.
9.4.2.4 Process Affinity API
Processes and threads can inspect and modify their affinity mask via system calls
defined for this purpose, but they, too, must have root privileges to change the affin-ity mask. A process can inspect the affinity mask of itself or another process by usingthe following GLIBC extensions:
int sched_setaffinity(pid_t  pid,  size_t setsize,  cpu_set_t *cpuset);
int sched_getaffinity(pid_t  pid,  size_t setsize,  cpu_set_t *cpuset);
These functions return 0for success and -1when there is an error. The
cpu_set_t is the bit mask I discussed earlier, and the setsize parameter is the
size of the mask. cpu_set_t is defined to provide a bit mask that allows many
more CPUs than can fit in an unsigned long. As a result, you need special macrosto set and clear the bits in this mask. These are defined as follows:
CPU_ZERO(p)    - Clears the mask pointed to by p.
CPU_SET(n,p)   – Sets the bit for CPU n in mask pointed to by p.CPU_CLR(n,p)   – Clears the bit for CPU n in the mask pointed to by p.CPU_ISSET(n,p) – Returns nonzero when bit n of the mask pointed to by p is set.508 Chapter 9 • Performance T uning
To call one of the setaffinity functions, you must use these macros to initial-
ize a cpu_set_t . The process must have root privileges; otherwise, the function will
return an error. These functions should not be used for threads. GLIBC providesextensions to the POSIX 
pthreads API for this purpose.
9.4.2.5 Thread Affinity API
The GNU Native POSIX Threads Library (NPTL) contains functions to support
thread affinity. With these functions, you can restrict a running thread to one ormore CPUs in the system. This can be helpful for maximizing performance bykeeping threads that use common memory on one CPU, which could help reducecache misses.
The POSIX 
pthreads standard does not currently accommodate affinity, so the
functions in NPTL are extensions, as indicated by the _np(non-POSIX) suffix.
These functions are defined as follows:
int pthread_setaffinity_np(pthread_t tid, size_t setsize, cpu_set_t *cpuset);
int pthread_setaffinity_np(pthread_t tid, size_t setsize, cpu_set_t *cpuset);
The functions above require a running thread, given by tid, to operate cor-
rectly. To affect the currently running thread, the caller can pass the return valueof 
pthread_self as the value for tid. If you want to initialize the affinity before
the thread is launched, you can do this via the thread attributes. Given a properlyinitialized 
pthread_attr_t object, you can set the affinity with the following
functions:
int pthread_attr_setaffinity_np(pthread_attr_t *attr, size_t setsize, cpu_set_t *cpuset);
int pthread_attr_getaffinity_np(pthread_attr_t *attr, size_t setsize, cpu_set_t *cpuset);
Note that these functions do not take a thread ID as an argument. The caller pro-
vides the storage for attr and uses it as an argument to pthread_create . The
thread created will have the affinity mask set to the value set in the attributes.
9.5 Summary
This chapter covered the basics of performance monitoring and tuning. It also cov-ered some basic concepts of multiprocessing (SMP) and things you can do to tuneyour applications to make the most of multiprocessor and multicore machines.9.5 Summary 509
9.5.1 Performance Issues in This Chapter
• Bus contention—I looked at some example architectures based on PCI and
illustrated what to look out for to identify bus contention in your system.
• Interrupts—I examined the interrupt architecture of the typical PC and
explored some of the interrupt-related issues that software encounters.
• Memory, page faults, swapping—I described how these issues affect your
application and how to measure them.
9.5.2 Terms Introduced in This Chapter
• Affinity—a technique used by the operating system to lock a process to a sub-
set of the CPUs in a multiprocessor system
• Multicore—a chip with more than one CPU• Multiprocessor—a computer with more than one CPU• SMP , SMT (Symmetric Multiprocessing, Symmetric Multithreading)—terms
used to describe an operating system that runs on a multiprocessor machine
9.5.3 Tools Used in This Chapter
•gprof , gcov —used to help optimize code at the source level
• OProfile—a powerful system tool (I demonstrated how it can be used to help
optimize applications)
•strace , ltrace —used to monitor code behavior with minimum invasiveness
•time , top, vmstat , iostat , mpstat —used to identify memory issues and
system throughput issues510 Chapter 9 • Performance T uning
9.5.4 Online Resources
• http://sourceforge.net/projects/procps—the home page for the procps
project, which provides many useful tools for tracking process and system
resources
• http://sourceforge.net/projects/strace—the home page for the strace project
9.5.5 References
• Dowd, K., and C. Severance. High Performance Computing, RISC Architectures,
Optimization & Benchmarks. Sebastopol, Calif.: O’Reilly Media, Inc., 1998.9.5 Summary 511
This page intentionally left blank 
10.1 Introduction
This chapter explores some of the most common debugging tools and techniques
for Linux.
Once upon a time, there were no debuggers to speak of, and programmers relied
almost exclusively on printed messages to the terminal. In some embedded envi-ronments today, there is not enough memory or CPU power to do anything else.This technique is always valuable, no matter what the environment is. In this chap-ter, I discuss some techniques to use printed messages effectively.
I also present a detailed look at the features of the 
gdbdebugger by example.
Although a few excellent GUIs are available that enhance gdb’s functionality, the text
interface is still exceptionally powerful and full featured. Unfortunately, the GUIs donot expose all the capabilities of 
gdb. Many programmers never take the time to
learn this interface, which is still available in the GUI versions.
10
513Debugging
In this chapter, I compare several tools available for memory checking and dis-
cuss the features and limitations of each. Finally, I present some unconventionaltechniques that you can use when other debugging techniques fail.
10.2 The Most Basic Debugging Tool: printf
Many programmers fall back on printf debugging out of laziness. When the code
is small, it often is easier to type a couple of printf statements than to bring up a
debugger, set breakpoints, and peek at variables. In an embedded system, it may bedifficult or impossible to run the code under a debugger. In this environment, aprinted message may be your only method of debugging. This is also how kerneldevelopers do most of their debugging, by the way.
10.2.1 Problems with Using printf
The first problem with using printf is the clutter it leads to. If you rely on printf
exclusively, you probably are adding printf statements in several places in the code.
Most programmers are loath to remove a useful message after it has served its pur-pose; there’s always a chance that it will come in handy again. A thoughtful pro-grammer might at least comment out the message so that it doesn’t clutter the screen,but more often than not, these messages scroll by on the screen at an alarming rate.
Cluttering the code with messages (even if they’re commented out) can distract
programmers who may be trying to understand the code and fix bugs. Clutteringthe screen with messages alarms and confuses unsuspecting users and programmersalike. Worse, it can affect performance.
10.2.1.1 Performance Effects of Using printf
Depending on the output device you are using, a 
printf statement can affect your
code’s performance. Pseudoterminals go a long way toward sheltering you from thisimpact. When your code prints to an X terminal, chances are that it’s writing to adeep pipe:
$ time od -v /dev/zero -N200000
Many, many lines later . . .
0606440 000000 000000 000000 000000 000000 000000 000000 000000
0606460 000000 000000 000000 000000 000000 000000 000000 0000000606500
real    0m1.257s
user    0m0.052ssys     0m0.092s514 Chapter 10 • Debugging
This command took 1.257 seconds to execute, which is misleading. Because it
ran on an X terminal, the actual time it took for all the data to scroll by was a bitlonger than the runtime of the program. Because of buffering in the pseudotermi-nal, the text continues to scroll by after the program has terminated. The same com-mand redirected to 
/dev/null runs much faster:
$ time od -v /dev/zero -N200000 > /dev/null
real    0m0.059s
user    0m0.048ssys     0m0.012s
Although this is an extreme example, the point is that text output, in any form,
affects your performance.
10.2.1.2 Synchronization Issues with printf
When I looked at IPC in Chapter 7, I discussed the issue of buffering. When the
program printout is directed at the screen, you take it for granted that the textappears when the code is run. This generally is true for any short message that endswith a newline, but this behavior usually changes when the output is redirected toanother device. Consider this simple example:
for (i = 0; i < 3; i++) {
printf("Hello World\n");sleep(1);
}
When you run this command, you see HelloWorld printed once per second for
3 seconds. Now pretend that this is a program you are debugging, and you want tosave the output to a file while you are watching it. The tool for this is the 
teecom-
mand, which allows you to do exactly that. But there’s a surprise in store:
$ ./hello | tee hello.out
You should see nothing for 3 seconds, followed by three lines of Hello World .
If you were expecting your messages to show up when the printf statement was
executed, you would be wrong. The problem is that the C standard library usesstreams built on top of plain file descriptors. These streams are used for standardI/O and any file that uses a 
FILE* pointer. The C library maintains a buffer for each
stream and uses different buffering strategies based on whether or not the output isa terminal. When the output is a terminal, file streams use what is called line buffer-
ing.This causes the stream to defer writing characters to the device until a newline10.2 The Most Basic Debugging Tool: printf 515
is encountered. Then all the characters up to and including the newline are printed.
Because most messages you print to the screen have a newline, this fools you intothinking that your 
printf statements will be synchronous all the time. When the
output is not a terminal, characters are not sent to the device until the buffer is fullor the program explicitly flushes the buffer with 
fflush .
10.2.1.3 Buffering and C File Streams
It is always more efficient to send characters to a device in blocks of characters
rather than one character at a time. The user-space buffers allow the driver to sendcharacters to the output device in blocks. Because terminal devices are often char-acter devices, they do not have buffering the way a block device (such as a disk)does. The user-space buffer is a workaround for this.
Buffering doesn’t always work well with terminals, because humans expect to see
their output immediately when they press a key, rather than after they type somenumber of bytes. To accommodate this, the C library allows the buffering strategyto be tuned to the type of device attached to the file stream. C file streams allowthree basic strategies that can be selected with the 
setvbuf function, which is
defined as follows:
int setvbuf(FILE *stream, char *buf, int mode, size_t size);
The stream argument tells the function which stream you want to modify—for
example, stdout . The bufand size arguments allow you to provide your own
buffer for streaming purposes. There are some applications for which this is desir-able, but it’s easiest to allow the C library to allocate the buffer for you. To use thedefaults, you can pass 
NULL for the bufand 0for the size . The interesting argu-
ment, however, is the mode argument, which takes one of three values:
•_IONBF —unbuffered. No buffering is done on the stream. Characters are
written one at a time.
•_IOLBF —line buffered. Characters are buffered up to the first newline
character.
•_IOFBF —fully buffered. Characters are buffered until the buffer is full.
The default behavior of stdout is to use line buffering ( _IOLBF ) when it is
attached to a terminal. When you redirect stdout to a file or a pipe (as I did above),516 Chapter 10 • Debugging
the behavior changes to fully buffered ( _IOFBF ). Most of the time, this is what you
want. The penalties you pay for writing to the terminal are diminished by the useof buffering.
If you really want to force your printouts to be synchronous, you have a couple
of choices. One option is to force buffering back to line-buffering mode with
setvbuf as follows:
setvbuf(stdout,NULL,_IOLBF,0);
You also could turn off buffering altogether with _IONBF , but either way, you
force your code to run slower than it would with buffering. This is true even if youredirect to 
/dev/null . It’s not due to a bug in /dev/null ; the fact is that turn-
ing off buffering causes your code to make more system calls—specifically, write
calls. So intuitively, fully buffered streams are faster than line-buffered streamsbecause they require fewer system calls, regardless of the device. Likewise, a line-buffered stream is faster than an unbuffered stream because it also requires fewersystem calls.
A second option to force synchronous output is to flush the buffer manually
using the 
fflush command. The advantage of this is that you can target specific
printouts for flushing and take advantage of buffering at other times. You may wantto print a message when a counter hits a certain value and see it instantly, for exam-ple. A single 
fflush call in this case is a good solution. The disadvantage of fflush
is that it adds more lines of code and clutter.
10.2.1.4 Buffering and File Systems
In addition to the user-space buffers that the C library provides, the file system
maintains buffers in the kernel. Figure 10-1 shows a diagram of where the buffersreside and the library calls that move the data among them. Note that flushing theuser-space buffer with 
fflush does not flush the file-system buffer. The data is not
written to disk until the system determines that it needs to write it or the applica-tion calls 
fsync .
When the program you are using to view the file runs on the same system where
the file is written, this is not a big issue. The file-system cache is visible to everyprocess on the system, so every process in the system sees the data just as though ithad been written to disk. It just may not have been written to the physical media yet.10.2 The Most Basic Debugging Tool: printf 517
File-system buffering can be a problem when you are looking at a file from a dif-
ferent computer. Perhaps the media resides on a remote NFS server. In this case,what you see is only as up to date as the most recent write to the media. Data sit-ting in the file-system cache on another client is not visible to you. This sort ofproblem is an issue only in very specific circumstances, but when it happens, youcan force updates to the file system with the 
fsync and fdatasync functions,
which are defined as follows:
int fsync(int fd);
int fdatasync(int fd);
Both functions force the user data to be written from the file-system cache to the
device. Both block the caller until the device driver indicates that the data has been518 Chapter 10 • Debugging
FIGURE 10-1 I/O Buffering OverviewProcess A
fwrite
fflushUser Sp ace
BufferUser Sp ace
Buffer
Kernel Sp ace
DiskFilesystem BufferProcess B
write
readfread
fsyncApplic ation Data Applic ation Data
written to the device.1The difference is that fdatasync writes the only user data
to the device, whereas fsync updates the file-system metadata as well.
Note that these functions take a file descriptor, not a file stream, as an argument,
so these functions do not replace the fflush or setvbuf calls for file streams but
are required in addition to them. You can get the file descriptor for any C file streamwith the 
fileno function. One pattern to follow is as follows:
printf("Hello World\n");
fflush(stdout); Flush the file stream buffer (in user space); must be done first.
fsync(fileno(stdout)); Flush the file-system buffer (in kernel space).
It is interesting to note that fread does not behave as you might expect based on
Figure 10-1. GLIBC does not use the user-space buffer as a cache in the traditionalsense; it is used only to coalesce reads and writes so that the underlying system callsuse larger blocks. Although it is possible to get stale data from the buffer, you canprevent this by calling 
fseek before calling fread , which will cause fread to
refresh the data in the buffer.
10.2.2 Using printf Effectively
Based on the previous discussion, perhaps one of the most important things toknow about using 
printf is knowing when not to use it. If you can’t avoid it, you
can at least turn it off when you don’t need it.
10.2.2.1 Preprocessor Help
The C preprocessor can be very helpful for formatting and controlling debug mes-
sages. One useful pattern is to wrap calls to printf in a macro, which can be used
to cut down on clutter and also allows you remove the messages easily when they’renot needed. For example:
#ifdef DEBUG
#define DEBUGMSG(...) printf(__VA_ARGS__) /* Uses a C99 / GNU extension */#else#define DEBUGMSG /* nop */#endif10.2 The Most Basic Debugging Tool: printf 519
1. Devices such as hard drives have internal cache as well, which may not be flushed when this function
returns. There is no standard way to control the drive’s internal cache.
Using variable argument lists in macros began as a GNU extension but was
adopted by the C99 standard as well. If you are using a non-GNU compiler or onethat does not support C99 extensions, there is a more clumsy alternative syntax thatgives you the effect of variable argument lists:
#define DEBUGMSG(msg) printf msg /* Caller passes in the parentheses. */
/* Only works like this ... */
DEBUGMSG(("Hello World %d\n",123)); /* becomes printf("Hello World %d\n",123);*/
Note the double parentheses enclosing the arguments. The inner parentheses
become part of the macro argument, which is why they are left off in the definition.This is a bit awkward, a bit ugly, and hard to explain, but it is a useful substitute forvariable argument lists in macros.
Wrapping 
printf calls in a macro also allows you to hide much ugliness that can
otherwise be useful. You can include a filename and line number in each message asfollows:
#define DEBUGMSG(fmt,...) printf("%s %d " fmt, __FILE__,__LINE__, ## __VA_ARGS__)
The double pound sign, used in this context, is another GNU-only extension.
Normally, this is used for string concatenation inside a macro, but in this context,GNU assigns a different meaning. When used with 
__VA_ARGS__ it strips the trail-
ing comma for you when you use the macro with a format string that takes no argu-ments. That is, if 
__VA_ARGS __ is empty, gccwill remove the trailing comma for
you. Without this extension, you would be required to provide at least one argu-ment even when the format strings take no arguments.
Note that the format string 
"%s %d" is concatenated with the fmtargument by
the C compiler—not the C preprocessor. Concatenating fixed strings like this hasbeen a feature of C since the original ANSI (1989) standard.
Quick and Dirty Preprocessor Tricks
Here are some of my favorite printf tricks using the C preprocessor.520 Chapter 10 • Debugging
Printing Variables to the Screen with Minimal Typing
#define PHEX(x)printf("%#10x %s\n", x, #x)
...PHEX(foo);PHEX(bar);PHEX(averylongname);
The trick here is using the #character to wrap a macro argument in quotes. The
preprocessor expands arguments that are preceded with a #by enclosing this in
quotes. This can save some typing. The following line
PHEX(averylongname);
expands to
printf("%#10x %s\n", averylongname, "averylongname");
The other trick is to put the values in a fixed-width field on the left. I find that
the output is much easier to read, because the variable names can vary all over themap but the values always line up. For example:
0x1 foo
0x2 bar
0xdeadbeef averylongname
I used hexadecimal in this example, but you can use this trick for whatever for-
mat you like. The magic is on the right side of the format string.
Inline Synchronization
Based on the earlier discussion of synchronization, it’s possible to wrap this uglinessin a macro to cut down on clutter. One solution could look like this:
#define DEBUGMSG(...) \
do { \printf(__VA_ARGS__);\fflush(stdout);\} while(0)
You can stuff anything you need to in this block of code. If this block of code
grows over time, it may start to affect the size of your code. It may be time to cre-ate a function instead.10.2 The Most Basic Debugging Tool: printf 521
Using do / while(0) in macros
This pattern is used extensively in the Linux kernel source. When a macro contains
more than one statement, it is dangerous to define it without enclosing braces.Suppose that you want to print a message before exiting. The following simple macrois syntactically correct but defective:
#define EXITMSG(msg) printf(msg); exit(EXIT_FAILURE)
This will compile fine, but a typical use case is likely to behave incorrectly.
Consider this example:
if ( x != 0 ) EXITMSG("x is not zero\n");
The resulting code after preprocessing is equivalent to this:
if ( x != 0 ) printf("x is not zero\n");
exit(EXIT_FAILURE);
When used this way, the exit call is unconditional. The code will always exit at
this point, which clearly is not what was intended. An intuitive fix might look like thefollowing definition:
#define EXITMSG(msg) { printf(msg); exit(EXIT_FAILURE); }
The addition of the braces allows the macro to behave as a single statement. This
works in most cases, but it still has problems in some contexts. For example:
if ( x != 0 )
EXITMSG("x is not zero\n");
else
printf("no problem\n");
This does not compile. After expansion, it looks like this:
if (x != 0)
{ printf("x is not zero\n"); exit(1); };
else /* Syntax error here! */
printf("no problem\n");
The semicolon following the closing brace terminates the ifstatement. The else
clause looks like a stand-alone statement to the compiler, which of course is incorrect.
The fix for this is to place the bracketed code inside a do/while statement. The
behavior of the do/while block is to execute the code inside the block at least once.522 Chapter 10 • Debugging
The condition (following the while keyword) is evaluated after the block is executed.
So when you use a condition of 0(false), the block is executed once and only once.
The redefined macro looks like this.
#define EXITMSG(msg) \
do { printf(msg); exit(EXIT_FAILURE); } while(0)
The code inside the block is perfectly encapsulated so that it can be used in any
context. Because the condition is a literal, the optimizer is able to omit any looping
code that it otherwise would have generated.
10.2.2.2 Using a Wrapper Function
There are drawbacks to using the preprocessor exclusively to control printed out-
put. In the most recent example, I wrapped a block of code inside a macro. This isa primitive form of function inlining. The drawback of this is that macros don’thave a function signature, so there is no argument checking in the macro. Any syn-tax errors that occur while calling one of these macros can produce misleading errormessages.
The alternative is to wrap this code in a function. This may be unattractive on
some architectures because of the increase in overhead due to function calls. Thisdrawback is minimized thanks to the 
inline keyword supported in C++, C99, and
gccin general. An inline function is a function that isn’t actually called. Instead,
each time the compiler encounters a call to an inline function, it places the com-piled instructions at that spot in the code. In effect, the function produces the samecode that the macro produces, except that now there are function signature forenhanced error checking and warnings. Inlining saves the overhead of a functioncall but increases the code size because the compiled instructions appear many timesin the object file instead of only once. For this reason, inlining is treated as a com-piler hint (not a directive). The optimizer is free to use its own heuristics to decidewhen inlining is beneficial.
2
The second hurdle to overcome using a wrapper function is variable argument
lists. The printf function uses a variable argument list to support formatted strings10.2 The Most Basic Debugging Tool: printf 523
2.gccdoes not use inlining unless compiled using -O3or explicitly directed via the -finline-functions
option.
that can take any number of arguments. The API for using variable arguments is
defined in stdarg.h . A simple wrapper can look like this:
inline int myprintf( const char *fmt, ... )
{
int n;va_list ap;
va_list holds the information needed for the API.
va_start(ap, fmt); Indicate where the variable arguments start (i.e.. after fmt).
n = vprintf(fmt,ap); vprintf takes a format string and a va_list .
va_end(ap); Must call this before exiting the function.
return n;
}
This function adds no value but illustrates how to use variable arguments to cre-
ate your own printf wrapper. The key is that the underlying functions you are
calling must take a va_list as an argument. The C standard library contains
equivalents for virtually all the printf -like functions. You can recognize these by
the fact that they all begin with v: vprintf , vsprintf , vfprintf , and so on.
Creating a function wrapper for printf has the drawback of disabling type
checking for the variable arguments. You don’t have this problem with a macro,because in all the examples, the format string is a string literal and is never storedin a variable. So the expanded macro contains a call to 
printf with the format in
a string literal and all the arguments. As you shall see, GNU provides an extensionto get around this drawback.
printf is the exception when it comes to checking variable arguments.
Normally, the compiler cannot make any assumptions about the type or number ofarguments in a variable argument list. In a 
printf call, these can be inferred from
the format string. The gcccompiler has the ability to parse literal format strings
and check them against the variable arguments for correctness. vprintf and other
stdarg -friendly functions cannot check format strings because they take only a
va_list to hold the arguments to the format string. What’s more, in myprintf ,
you pass a string variable to vprintf instead of a string literal, so the compiler is
unable to parse the format string.
GNU allows you to apply printf format checking to any function by using the
__attribute__ directive. This syntax is the same used in stdio.h for printf
and friends. For myprintf , you could add the attribute to the function prototype
as follows:
inline int myprintf( const char *fmt, ... )
__attribute__ ((format (printf, 1, 2)));524 Chapter 10 • Debugging
The __attribute__ directive is a general-purpose directive. This instance
specifically tells the compiler that the function follows the printf formatting rules
and that the format string is in argument 1, with the first format argument startingat argument 2. Note that the 
__attribute__ directive is available only on GNU
compilers and is not standard; therefore, it is not portable to other compilers.
10.2.2.3 Don’t Ignore the printf Format Warnings
The addition of format warnings to printf is useful, but unfortunately, the major-
ity of these warnings are portability issues and not necessarily bugs. One of the mostcommon warnings you will see involves mismatched integer types, which look likethe following:
warning: int format, long int arg (arg 2)
warning: long unsigned int format, int arg (arg 2)
The C standard is vague on how big an intor a long should be, so these mes-
sages are cautious. In fact, with gccvirtually all 32-bit architectures use the same
size for long and int. Because printf uses variable arguments, the compiler pro-
motes all small integral values to intand all floating-point values to double . So
although printf has format support for these other types, there is no danger of see-
ing the wrong value in the output due to the argument size.
There are warnings you should pay attention to, however; they could be embed-
ded in the noise from these earlier warnings. There are data-type mismatches thatwill garble your output or possibly crash your program.
64-Bit Types
The off_t type, for example, is used in the POSIX APIs such as lseek and mmap .
It can be either a 32-bit size or a 64-bit size, depending on compiler flags. Bydefault, you get a 32-bit type, but GNU allows you to replace 
off_t with a 64-bit
type by compiling with the flag
-D_FILE_OFFSET_BITS=64
When a 64-bit type appears as an argument to an integer or long integer format,
the output will not be correct. Worse, because the size is different, subsequent argu-ments will also be incorrect or could even cause your application to crash. Thewarning in this case differs in different versions of 
gcc:
gcc 3.x: warning: int format, different type arg
gcc 4.x: warning: format '%x' expects type 'unsigned int', but argument 2 has type 'off_t'10.2 The Most Basic Debugging Tool: printf 525
This warning is relevant whether you are compiling on 64-bit or 32-bit architec-
tures. The type long long has been used by gccand other compilers to represent
64-bit types on 32-bit architectures. The format for a long long type is ll, for
example:
printf("%lld\n", x);
The situation is a bit messy. On x86_64 architectures, gccrecognizes both long
and long long as 64-bit types. And wouldn’t you know, printf requires the for-
mat to match the type, even though the types are the same size. On IA32, for exam-ple, 
gccimplements C99’s int64_t type as a longlong , but on x86_64, gccuses
type long . So to build on IA32 without warnings, a printf format requires
"%lld" , whereas the same code compiled on x86_64 requires "%ld" .
Floating-Point Types
Mixing floating-point and integer types is just as dangerous as mixing 64-bit and32-bit types. That’s because the double type is 64 bits. Interestingly though, mixingfloat with double arguments is not a problem, because C promotes a float to a dou-ble when it appears in a variable argument list. For the same reason, using a 
float
argument to an integer format can be catastrophic, because the float is promoted
to double , and integer arguments are all 32-bits. Again the warning is slightly dif-
ferent in different versions of gcc:
gcc 3.x: warning: int format, double arg (arg 2)
gcc 4.x: warning: format '%d' expects type 'int', but argument 2 has type 'double'
Note that in both cases, the actual argument was float , but the compiler warn-
ing takes into account the fact that the compiler promotes it to double .
String Types
String types are perhaps the most likely sources of error, because they take pointersas arguments, and pointers can go wrong in so many ways. Again, the warningvaries from one release to the next of 
gcc:
gcc 3.x: warning: format argument is not a pointer (arg 2)
gcc 3.x: warning: char format, different type arg (arg 2)gcc 4.x: warning: format '%s' expects type 'char *', but argument 2 has type 'int'gcc 4.x: warning: format '%s' expects type 'char *', but argument 2 has type 'int *'
The code responsible for these warnings is likely to produce crashes at runtime.
If you see one of these warnings, you should inspect the code and fix it. If the codeis correct as is, use an explicit type cast to remove the warning.526 Chapter 10 • Debugging
10.2.2.4 Tips for Creating Good Debug Messages
A formatting convention can give you indication at a glance of what is going on
without requiring you to read every line of printout.
Use an Easily Recognizable Format
Adults read by recognizing the shapes of words, not by sounding out every letter ina word. A consistent format will take advantage of this fact by giving “good” mes-sages one shape and “bad” messages another. Consider this rubbish, which couldhave come from your favorite program:
debug>   The  LAST  time I  cHecked,   the DVD drive is  within   DEfined
tolerances.
Relax,   THE hard   drive  is  runniNg.
-- The   USB   port   is  not  In   flames.It's  a good thing   that  the pcI  bus is  running.
I'm optimistic   because the  usb  port is better.Don't paNic, but   the DVd  Drive is going tO  explode.
The   last  time   I  checKed,  THE   heap  is   OptImal.
debug> The  moUse IS  not  in   flames.
*** EverythinG's  fine, I checked anD the memory is   running   at  full   speed.
The same text, using a uniform format, easily highlights the errors, no matter
how silly:
info - The last time I checked, the DVD drive is within defined tolerances.
info - Relax, the hard drive is running.info - The USB port is not in flames.info - It's a good thing that the PCI bus is running.info - I'm optimistic because the USB port is better.**** ERROR - Don't panic, but the DVD drive is going to explode.info - The last time I checked, the heap is optimal.info - The mouse is not in flames.info - Everything's fine, I checked and the memory is running at full speed.
Creating your own printf wrapper is ideal for enforcing predictable formats in
your output. It can’t help you separate the relevant from the irrelevant information,though.
One Line Per Message
Messages can get cryptic when you try to squeeze them on a single line, but thereare advantages to doing so. If your program dumps a great deal of printout to a file,for example, you can use a simple 
grep command to look for particular messages.10.2 The Most Basic Debugging Tool: printf 527
If the information you need is on more than one line, you may have to resort to
manual inspection.
Keep Chattiness to a Minimum
It’s fun to keep things lighthearted, but when chat starts to clutter the screen orworse, slow the application, it’s time to cut back.
10.2.3 Some Final Words on printf Debugging
Using printf is not without side effects, as I have demonstrated. One side effect
that I have not mentioned is inadvertent synchronization. This is more often aproblem in multithreaded code but can be an issue in single-threaded code as well.Perhaps you have encountered a bug that went away when 
printf was turned on.
This can happen when a strategically placed printf hides a race condition in a
multithreaded application. In single-threaded applications, a printf can cause the
compiler to store a float in memory that otherwise was in a register. Because float-ing-point registers on IA32 have higher precision than an IEEE float, the additionof a 
printf may change your numerical results, as the example in Listing 10-1
illustrates.
LISTING 10-1 side-effects.c: A Demonstration of printf Side Effects on IA32
1 #include <stdio.h>
2 #include <string.h>3 #include <stdlib.h>4 #include <math.h>56 int main(int argc, char *argv[])7 {8     // Make argument a volatile variable. This prevents9     // the optimizer from taking any shortcuts.
10     volatile double arg = 7.0;1112     // Square root of a prime is 'irrational';13     // i.e. digits go on forever. On IA32, x14     // will be in an 88-bit floating point register.15     double x = sqrt(arg);1617 #ifndef NOPRINT18     // If we print it, x must be stored on the stack.19     // double has only 64-bits so we lose precision20     printf("x    = % 0.20f\n", x);21 #endif22528 Chapter 10 • Debugging
23     // By calling printf, we changed the value of x,
24     // which will show up as a non-zero diff.25     volatile double diff = sqrt(arg) - x;26     printf("diff = % 0.20f\n", diff);2728     if (diff == 0.0) {29         printf("Zero diff!\n");30     }31     else {32         printf("Nonzero diff!!!\n");33     }
34 }
I compiled this program with gcc 4.0.1, both with and without the macro
NOPRINT defined as follows:
$ cc -o print   -O2           -lm side-effects.c$ cc -o noprint -O2 -DNOPRINT -lm side-effects.c
$ ./noprint
diff =  0.00000000000000000000Zero diff!
$ ./print
x    =  2.64575131106459071617diff = -0.00000000000000012577Nonzero diff!!!
As the annotation in the code shows, simply calling printf changes the behav-
ior of the program enough to change the results. It forces the compiler to store thevariable 
xin memory, which has less precision than the internal floating-point reg-
isters on the IA32. As a result, the value stored in xno longer is the same as what
was stored in the register.3
10.3 Getting Comfortable with the GNU Debugger: gdb
gdbis a text-based debugger with intuitive commands. Most Linux programmers
have had at least some experience with it. Every command can be abbreviated—some with only one letter. More often, you need to type the first few letters of thecommand. 
gdbneeds enough letters to identify the command uniquely, so when10.3 Getting Comfortable with the GNU Debugger: gdb 529
3. Assembly output can vary from one version of gccto the next. Your results may vary. You can avoid this
sort of error by using -ffloat-store option to gcc, which forces the compiler not to use floating-
point registers for storage.
you don’t type enough letters, gdbwill give you a helpful hint. Type shat the gdb
prompt, for example, and you will see the following message:
(gdb) sh
Ambiguous command "sh": sharedlibrary, shell, show.
After typing a few letters, you can press Tab, which will complete the command
based on the letters you have already typed, if possible. If no completion is offered,you can press 
Tabagain to see a list of matching commands. For example:
(gdb) b<Tab><Tab>
backtrace  break      bt(gdb) b
I typed b, and gdboffered backtrace , break , and btas possible completions.4
I return to the gdbprompt with the letters I just typed. I still need to type enough
letters for gdbto identify the command unambiguously.
10.3.1 Running Your Code with gdb
When you launch gdb, you can specify your program on the command line or just
start gdband load your program. Either pattern works:
$ gdb ./hello ... or
$ gdb
(gdb) file ./helloReading symbols from /home/john/hello...done.
Both commands read your file and its symbols into memory. The latter form also
allows you to switch programs without exiting gdb.
At the gdbprompt, you can jump into your code with the runcommand, if you
like. This is useful if you already know that it’s going to crash. When it crashes, youcan get a stack backtrace to see where the code died. More often, you want to stepthrough the code or set a breakpoint.
Here are the basic commands you need to know for starting your code in 
gdb:
•set args —a special case of the setcommand used to pass command-line
arguments. gdbstores the arguments you specify for use with subsequent calls530 Chapter 10 • Debugging
4. Note that bby itself is the abbreviation for break and that btis the abbreviation for backtrace .
to the runcommand, although you can specify new arguments with each call
to run. To clear the arguments after they have been set, however, you must use
set args .
•run—starts the program from the beginning. The program executes with the
arguments specified here or whatever was set by the most recent call to set
args . Execution continues until the program exits, aborts, or hits a breakpoint.
•start —the same as runexcept that the program stops as though a break-
point were set in main . This allows you to single-step from the beginning of
the program.
•step —executes a single line of code. If the line contains a function that is com-
piled with debugging, gdbwill step into that function. You must start your pro-
gram with the runor start command before you can use this command.
•next —the same as step except that gdbwill not step into functions, regard-
less of whether they are compiled with debugging.
•kill —terminates your program. This is not equivalent to the kill system
call,which sends a signal to the running program. For that, see the signal
command.
10.3.2 Stopping and Restarting Execution
You can stop a running program at any time by pressing Ctrl+C . This tells gdbto
stop the currently running program but does not send the program a SIGINT , so
you can continue program execution from where it left off. Some applications actu-ally lend themselves to this kind of debugging without breakpoints, but most don’t.For those that don’t, explicit breakpoint commands are available from the 
gdb
prompt.
Here are the basic commands:
•break,tbreak —set a breakpoint ( break ) or temporary breakpoint ( tbreak ).
With no arguments, the commands set a breakpoint at the next instruction tobe executed; more often, you specify a function or line number at which tostop. These commands also can be followed by a logical expression to create a10.3 Getting Comfortable with the GNU Debugger: gdb 531
conditional breakpoint (see the section “Using Conditional Breakpoints” later in
this chapter).
•watch —similar to a breakpoint but uses hardware registers (if available) to
monitor a particular location for a change. This can allow your program toexecute much more quickly than a conventional breakpoint (see the section“Using Watchpoints” later in this chapter).
•
continue —resumes program execution from where it stopped. The
continue command allows an optional numeric argument ( N) that tells gdb
to ignore the following N-1breakpoints. In other words, “Continue until the
process hits the breakpoint for the Nth time.”
•signal —sends a signal to your program and continues. The command takes
a single argument, which can be the signal number or the signal name. Youcan see a list of signal names by typing 
info signal .
•info breakpoints —lists the currently active breakpoints and watches.
•delete —deletes all breakpoints. To delete a single breakpoint, use info
breakpoints to get the breakpoint number, and pass that number to the
delete command.
10.3.2.1 Breakpoint Syntax
The breakpoint command is abbreviated as bbecause it is used so frequently. The
argument to the breakpoint command is always an instruction address, which can
be provided in one of the following ways:
(gdb) b Break at the next instruction in the current stack frame.
(gdb) b foo Break at function foo.
(gdb) b foobar.c:foo Break at function foodefined in foobar.c .
(gdb) b 10 Break at line 10 in the current module.
(gdb) b foobar.c:10 Break at line 10 in foobar.c .
(gdb) b *0xdeadbeef Break at address 0xdeadbeef .
Breakpoints can be set almost anywhere in memory. The underlying code does
not need to be compiled with debug , although it helps. Listing 10-2 creates a buggy
program that overruns the heap upon request.532 Chapter 10 • Debugging
LISTING 10-2 nasty.c: A Nasty Buggy Program
1 #include <stdio.h>
2 #include <string.h>3 #include <stdlib.h>45 void *nasty(char *buf, int setlen)6 {7     // We don't check setlen! Naughty!8     return memset(buf, 'a', setlen);9 }
1011 // Use a fixed buffer length...12 const int buflen = 16;1314 int main(int argc, char *argv[])15 {16     char *buf = malloc(buflen);1718     // Default to same length as buffer19     int len = buflen;20     if (argc > 1)21         // Allow command line arguments to override buffer length.22         len = atoi(argv[1]);2324     // If len > buflen, then this corrupts the heap.25     nasty(buf, len);2627     // Some versions of glibc detect errors here, but not always.28     free(buf);2930     // Get here and everything should be okay.31     printf("buflen=%d len=%d okay\n", buflen, len);32     return 0;
33 }
You set  a breakpoint on memset , which is the offending function, even though
memset is part of the standard library, which is not compiled with debug . A debug
session would look like this:
$ gdb ./nastyGNU gdb Red Hat Linux (6.1post-1.20040607.43.0.1rh)Copyright 2004 Free Software Foundation, Inc.GDB is free software, covered by the GNU General Public License, and you arewelcome to change it and/or distribute copies of it under certain conditions.Type "show copying" to see the conditions.10.3 Getting Comfortable with the GNU Debugger: gdb 533
There is absolutely no warranty for GDB.  Type "show warranty" for details.
This GDB was configured as "i386-redhat-linux-gnu"...Using host libthread_dblibrary "/lib/tls/libthread_db.so.1".
(gdb) b memset
Function "memset" not defined.Make breakpoint pending on future shared library load? (y or [n]) y
Breakpoint 1 (memset) pending.
(gdb) run 100Starting program: /home/john/examples/ch-10/debug/nasty 100Reading symbols from shared object read from target memory...done.Loaded system supplied DSO at 0xffffe000Breakpoint 2 at 0xb7e4e050Pending breakpoint "memset" resolved
Breakpoint 2, 0xb7e4e050 in memset () from /lib/tls/libc.so.6
(gdb) bt#0  0xb7e4e050 in memset () from /lib/tls/libc.so.6#1  0x0804844e in nasty (buf=0x804a008 "", setlen=100) at nasty.c:8#2  0x080484b5 in main (argc=2, argv=0xbf836564) at nasty.c:25
This session illustrates several concepts. First, when you load your program, the
shared libraries are not yet loaded. gdbdoes not recognize memset because it is part
of the standard C library, which is implemented as a shared library and has not beenloaded yet. 
gdbprompts you with the option of setting a pending breakpoint, which
means that it will look for this symbol as each shared library is loaded. When gdb
encounters the pending breakpoint in the shared library, it will set the breakpointyou requested. If 
gdbnever sees the symbol from the pending breakpoint (say, you
misspelled memset ), you will hear nothing more from gdbabout it.
You start the program by using the runcommand with an argument of 100,
which will cause memset to overrun the buffer. The program stops on memset as
expected, but you can’t see anything useful in this stack frame because memset is
not compiled with debug . You can use the btcommand (the abbreviation for
backtrace ) to see the call stack. This shows you the arguments passed to the
nasty function, which called memset . This also shows the offending length that
was passed to nasty .
10.3.2.2 Using Conditional Breakpoints
You also could debug the program in Listing 10-2 by using a conditional breakpoint.
You can stop on the nasty function whenever setlen is greater than buflen , for534 Chapter 10 • Debugging
example. The syntax for this is to include an ifstatement after the breakpoint, as
follows:
(gdb) b nasty if setlen > buflen
Breakpoint 1 at 0x804843e: file nasty.c, line 8.(gdb) run 100Starting program: /home/john/examples/ch-10/debug/nasty 100Reading symbols from shared object read from target memory...done.Loaded system supplied DSO at 0xffffe000
Breakpoint 1, nasty (buf=0x804a008 "", setlen=100) at nasty.c:8
8           return memset(buf, 'a', setlen);
You can include any address or condition in the conditional breakpoint. The only
restriction is that any variables used must be in the same scope as the address of thebreakpoint. The following conditional breakpoint does not work:
(gdb) b nasty if len > buflen
No symbol "len" in current context.
In this case, lenis a local variable inside main and is not in scope when the
nasty function is called, so gdbdoes not allow it. You can specify the scope explic-
itly by using C++-style scoping operations. The same conditional breakpoint can beset as follows:
(gdb) b nasty if main::len > buflen
Breakpoint 1 at 0x804845e: file nasty.c, line 8.
Notice that the code does not need to be written in C++ for you to use this syntax.
10.3.2.3 Setting Breakpoints with C++ Code
C++ programs can be challenging to debug. With namespaces, overloading, and
templates, it can be hard to narrow down symbols for breakpoints. Fortunately, gdb
provides some helpful shortcuts to make debugging easier.
T ry debugging the program in Listing 10-3. This is particularly difficult due to
the long function names, which are very similar. On top of that, the program placesthem all in a namespace and overloads one of them to maximize the amount of typ-ing required. 
gdballows tab completion of all commands and symbols, which is
very helpful for cutting down the amount of typing you need to do and eliminat-ing opportunities for typos. Unfortunately, because all the functions are in a name-space, you must know the namespace before you can use tab completion. Justtyping annoy<Tab> will not work.10.3 Getting Comfortable with the GNU Debugger: gdb 535
LISTING 10-3 cppsym.c: Only a C++ Programmer Could Love This
1 // Three inconveniently named functions
2 // wrapped inside a namespace, just to make them more annoying.3 // And for good measure, we overload one of the functions.45 namespace inconvenient {6     void *annoyingFunctionName1(void *ptr) {7         return ptr;8     };9     void *annoyingFunctionName2(void *ptr) {
10         return ptr;11     };12     void *annoyingFunctionName3(void *ptr) {13         return ptr;14     };15     void *annoyingFunctionName3(int x) {16         return (void *) x;17     };18 };1920 // Too bad the 'using' statement is not an option in gdb...21 using namespace inconvenient;2223 int main(int argc, char *argv[])24 {25     annoyingFunctionName1(0);26     annoyingFunctionName2(0);27     annoyingFunctionName3(0);28     annoyingFunctionName3((int) 0);
29 }
Because this module is so small, it’s easy to see that these functions are in a name-
space. In real-world examples, this is normally not the case. For those times, the
info command is very helpful. For example:
(gdb) info function  annoy Look for any function with the word “annoy” in it.
All functions matching regular expression "annoy":
File cppsym.cpp:
void *inconvenient::annoyingFunctionName1(void*);void *inconvenient::annoyingFunctionName2(void*);void *inconvenient::annoyingFunctionName3(int);void *inconvenient::annoyingFunctionName3(void*);536 Chapter 10 • Debugging
This shows the namespace as well as all the matching function names. Now that
you know the namespace, you can set a breakpoint using tab completion, buthere’s one more trick to know: Tab completion works for the namespace(
inconvenient ), but stops there, because gdb’s tab completion does not include
the colons that are part of the namespace. To work around this, you need to beginthe function name with a single quote and then use tab completion, as follows:
(gdb) b 'inc<Tab><Tab>
inconvenientinconvenient::annoyingFunctionName1(void*)inconvenient::annoyingFunctionName2(void*)inconvenient::annoyingFunctionName3(int)inconvenient::annoyingFunctionName3(void*)
The Tabgets you as far as the first colon. Pressing Tabagain shows you a list of
possible matches. To get any further with tab completion, you must type the twocolons by hand and then use tab completion to continue. For example:
(gdb) b 'incon<tab> becomes b ‘inconvenient
(gdb) b 'inconvenient::<Tab> becomes b ‘inconvenient::annoyingFunctionName
(gdb) b 'inconvenient::annoyingFunctionName3<Tab><Tab>
inconvenient::annoyingFunctionName3(int)inconvenient::annoyingFunctionName3(void*)(gdb) b 'inconvenient::annoyingFunctionName3(
Finally, when you picked the function you want, you must close the quotes and
press Enter . The complete command would look like this:
(gdb) b 'inconvenient::annoyingFunctionName3(void*)'
Breakpoint 2 at 0x804836f: file cppsym.cpp, line 13.
Tab completion sure beats all that typing.
10.3.2.4 Using Watchpoints
Many processors come with special purpose registers to assist in breakpoint debug-
ging. gdbmakes registers available to you via the watchpoint command. A watch-
point allows you to stop the program whenever a specific memory location is read
or written. Contrast this with a breakpoint, which takes an instruction address asan argument and stops when the code at that location is executed. Watchpoints areespecially useful when you’re looking for memory corruption by defective code.
gdb also implements watchpoints for architectures that don’t have supporting
hardware. In this case, gdbwill single-step your executable and monitor the memory10.3 Getting Comfortable with the GNU Debugger: gdb 537
with each step. This causes your code to run orders of magnitude slower than
normal.
To set a watchpoint to stop the program any time it changes the value of a vari-
able named foo, simply use the following command:
(gdb) watch foo Watch the value of foofor changes.
Beware: Watchpoints trigger only when the value in memory changes. If the ini-
tial value of foois 123and the code writes 123, for example, this watchpoint will
not trigger. Notice that the watch command automatically takes the address of foo
to be used as the watchpoint. If foohappens to be a pointer to a location that you
want to monitor, you would need to use the following syntax:
(gdb) watch *foo Watch the location pointed to by foofor changes.
If you forget the asterisk, you will end up monitoring the value of the pointer!
These watchpoints stop whenever the variable in the expression is modified, nomatter where the program is. That means that you could wind up breaking in amodule that was compiled without debugging. In this case, you can go up the stackand (ideally) find a frame that has useful debugging information.
Watchpoints can be combined with logical conditions to create conditional
watchpoints. You can stop any time 
foois written with the value 123, as follows:
(gdb) watch foo if foo == 123
This syntax is identical to the conditional breakpoint syntax I discussed earlier in
the chapter. The condition does not have to contain the value being watched. Youcould just as easily use an expression like this:
(gdb) watch foo if someflag == true
Any logical statement works, provided that all the scoping requirements are met
when the watchpoint is hit. Because watchpoints can trigger anywhere in your code,
gdbmakes no assumptions about the scope of the variables in the condition state-
ment and does not check their scope when you set the watchpoint. If a variable inthe conditional expression is not in scope when the watchpoint is hit, the watch-point simply does not trigger.538 Chapter 10 • Debugging
A Detailed Example Using Watchpoints
A more detailed example should illustrate the usefulness of watchpoints. Listing 10-4
contains a defective program that overruns a heap buffer, but only sometimes. Thisis the sort of bug that can be very hard to catch, even with a debugger.
I created a function called 
ovrrun that is a thin wrapper around memcpy .
Because there is no bounds checking inside this function, there is opportunity tooverrun the target buffer. I added a 
memcpy to slow things and simulate a process-
ing-intensive program. The target buffer is allocated from the heap using buflen
as a size. I artificially created a 1-in-800,000 chance of overrunning the target bufferby 1 byte. This sort of overrun often has no side effects due to the padding that the
malloc function typically performs. You can detect the overrun after the fact by
using strlen , but normally, that would be too late. If this were a very large over-
run, it could cause the program to crash.
Without watches, your first inclination may be to set a conditional breakpoint.
You can stop whenever the ovrrun function is called with a msglen greater than
buflen . The syntax for this would be
(gdb) b ovrrun if msglen > buflen
This works as expected, but it is extremely slow. The reason is that every call to
msglen causes the program to stop and transfer control to gdb. gdbexamines the
value of msglen and compares it with buflen each time, deciding whether to con-
tinue or stop the program. Because this program calls ovrrun 800,000 times, the
overhead of this conditional breakpoint affects performance dramatically. On my1.7 GHz P4, the 
overrun program takes only about 700 ms to execute when run-
ning under gdbwith no breakpoints. With the conditional breakpoint set, the pro-
gram takes more than 2 minutes and 17 seconds.
The same thing done with a watchpoint does not affect the code at all; the code
runs as fast as it does under gdbwith no watchpoints. The watchpoint is set as
follows:
(gdb) watch buf[buflen]
Here, you are looking for a write to the byte at location buf+buflen , which
would indicate an overrun. The reason why this is so fast is that the trigger is con-trolled in the processor hardware, and the processor does not generate a trigger until10.3 Getting Comfortable with the GNU Debugger: gdb 539
the write takes place. So instead of stopping the program 800,000 times, gdbstops
the program only once.
Watchpoints come in three flavors:
•watch —breaks when the location is written by the program and the value
changes
•rwatch —breaks when the location is read by the program
•awatch —breaks when the location is read or written by the program
gdbmanages watchpoints just like breakpoints. Watchpoints are listed with the
info watchpoints command, which is a synonym for info breakpoints . Just
like breakpoints, watchpoints can be removed with the delete command.
LISTING 10-4 overrun.c: Defective Code Example to Illustrate Watchpoints
1 #include <stdio.h>
2 #include <string.h>3 #include <stdlib.h>4 #include <time.h>56 // Source text for copying7 const char text[] = "0123456789abcdef";89 // This function will overrun if you tell it to.
10 void ovrrun(char *buf, const char *msg, int msglen)11 {12     // Pointless memcpy - just to slow us down and illustrate13     // the usefulness of watchpoints14     char dummy[4096];15     memset(dummy, msglen, sizeof(dummy));1617     // Here's the culprit...18     memcpy(buf, msg, msglen);19 }2021 // Carefullly chosen malloc size.22 // malloc a small buffer so that space comes from the heap (not mmap).23 // malloc will also pad the buffer, which means that a one-byte overrun24 // should not cause the program to crash.25 const int buflen = 13;2627 int main(int argc, char *argv[])28 {29     char *buf = malloc(buflen);30     int i;540 Chapter 10 • Debugging
31
32     // Seed the random number generator so that each run is different.33     srand(time(NULL));3435     // Loop count - a nice high number.36     int n = 800000;3738     // We want the chance of overrun to be 1 in N just to make this hard39     // to catch.40     int thresh = RAND_MAX / n;4142     for (i = 0; i < n; i++) {43         // Overrun if the random number is less than the threshold44         int len = (rand() < thresh) ? buflen + 1 : buflen;45         ovrrun(buf, text, len);46     }4748     // Overrun is easy to detect but hard to catch.49     int overran = (strlen(buf) > buflen);50     if (overran)51         printf("OVERRUN!\n");52     else53         printf("No overrun\n");5455     free(buf);56     return overran;
57 }
10.3.3 Inspecting and Manipulating Data
gdbhas very powerful features for inspecting data using only a few commands with
rich syntax. Before you explore them, I’ll go over the basic commands involved:
•print —provides a unique, rich formatting syntax that lets you display all
types of data, such as including strings and arrays. The objects printed can beobjects in memory or any valid C or C++ expression.
•
x—short for examine and similar to the print command except that xworks
with memory addresses and raw data, whereas print can handle abstract
expressions. Both commands accept modifiers, discussed in the next section.
•printf —just like the C function of the same name. It follows identical rules
for formatting. Don’t forget to include a newline in your format string unlessyou really don’t want one.
•
whatis —tells you everything gdbknows about the type of a given symbol.10.3 Getting Comfortable with the GNU Debugger: gdb 541
•backtrace —shows the call stack of the current program, including local vari-
ables, if desired.
•up, down —changes the stack frame so you can examine local variables in dif-
ferent parts of the call stack.
•frame —an alternative to the upand down commands that allows you to spec-
ify exactly which frame to go to. Frames are specified using the numbers listedin the 
backtrace command.
•info locals —a subcommand of the info command that shows all the local
variables in the current stack frame.
10.3.3.1 print Expression Syntax
Printing a single variable or dumping memory is done with the print and xcom-
mands. (print is abbreviated as p.) The print command can take almost any valid
C or C++ expression as an argument,5whereas the xcommand takes an address as
an argument and displays the memory at that address. When you use a variable asan argument to the 
xcommand, it is treated as an address even if the variable is not
a pointer. For example:
(gdb) whatis foo
type = long long int(gdb) p foo$2 = 4096
The value of foois 0x1000 .
(gdb) x foo0x1000: Cannot access memory at address 0x1000
foois treated as an address!
(gdb) x &foo0xbf9af240:     0x00001000
Memory is dumped as 32-bit (default).
In this case, the variable foois a 64-bit integer that contains the value 4096 . The
print command works as expected, but when you pass footo the xcommand, it
fails, because the value of xin this case is an invalid address. When you use the
address of fooas the argument, you get a dump of the memory in hexadecimal
using the default word size.
Defaults are made to be changed, however, and gdbmakes it easy to change the
default behavior of these commands. Both print and xallow you to provide
modifiers to change the output behavior. xallows you to specify a count as well. For542 Chapter 10 • Debugging
5. gdb also understands expressions in languages other than C/C++. See info gdb languages support
for details.
both commands, gdbrequires that you separate the modifiers from the command
with a forward slash. For example,
(gdb) p/x foo Print foousing hexadecimal.
$2 = 0x1000
(gdb) x/d &foo Dump memory at location &foo in decimal.
0x22eec4:       4096
The complete list of modifiers is shown in Table 10-1.
TABLE 10-1 Output Modifiers for the print and x Commands
Modifier Format print x
x Hexadecimal Yes Yes
d Signed decimal Yes Yes
u Unsigned decimal Yes Yes
o Octal Yes Yes
t Binary Yes Yes
a Address Prints hexadecimal and
shows its relationship tonearby symbols.
c Character D umps memory in pairs—
an ASCII character witha decimal byte.
f Floating point Display memory in
floating point, using thecurrent word size. Use 
gfor IEEE double and 
wfor IEEE float on 
32-bit machines.
i Instructions No Disassembly memory at
the given location.
s No Display memory as an
ASCII string. Outputstops at the first 
NUL
character.Null-terminatedASCII stringDisplay memory asdouble.Least significantbyte.Prints hexadecimaland shows itsrelationship tonearby symbols.10.3 Getting Comfortable with the GNU Debugger: gdb 543
In addition, xallows you to specify the word size used when dumping memory
as well as the number of words to dump. The count is specified immediately afterthe slash. For example:
(gdb) x/8bx &foo Dump 8 bytes in hexadecimal at address &foo .
0x22eec4:       0x00    0x10    0x00    0x00    0xd8    0xef    0x22    0x00
Because the xcommand dumps memory, it uses a fixed word size to display the
data. This word size can be specified with one of the suffixes listed in Table 10-2.Here, I used 
xwith the word size specified with the bflag. print , on the other
hand, knows the size of the data from the type of the variable.
gdbremembers your modifiers for the next time you use the command, so you
need only specify the modifiers once. If that’s what you want to use for the remain-der of the session, you do not need to specify any modifiers again. The modifiersfollowing the count may occur in any order, so 
8bxis the same as 8xb.
10.3.3.2 Print Examples
Using Table 10-1 and Table 10-2, I’ll show some quick examples. I mentioned ear-
lier in the chapter that print can take any valid C syntax as an argument. gdbalso
can call functions, which means that you can do some interesting things from the
gdbcommand line:
(gdb) p getpid() Print the process ID of the current process.
$1 = 12903
(gdb) p kill(getpid(),0) T est to see if the process exists.
$2 = 0(gdb) p kill(getpid(),9)
Kill the process via the C API. ( gdbwill not be happy.)
Program terminated with signal SIGKILL, Killed.The program no longer exists.The program being debugged stopped while in a function called from GDB.When the function (kill) is done executing, GDB will silentlystop (instead of continuing to evaluate the expression containingthe function call).
print uses the type of the variable it is printing to format the output, whereas x
dumps memory using an explicit word size as specified by the format. You can
demonstrate this with the following C variables:
double dblarr[] = {1,2,3,4};
float  fltarr[] = {1,2,3,4};int    intarr[] = {1,2,3,4};544 Chapter 10 • Debugging
Now see the difference between xand print in gdb:
(gdb) p intarr
$5 = {10, 20, 30, 40} Output is formatted as an array of ints.
(gdb) x/4wx intarr Output is in 32-bit hex (as requested).
0x8049610 <intarr>:     0x0000000a      0x00000014      0x0000001e0x00000028
(gdb) x/2gx intarr
Output is in 64-bit hex (as requested).
0x8049610 <intarr>:     0x000000140000000a      0x000000280000001e
Using xwith floating-point numbers can get weird if you are not careful. An
IEEE float is 4 bytes, for example, but if you inadvertently use an 8-byte word size
(g) with a float , you get gibberish. An IEEE double is 8 bytes, so the same for-
mat looks fine with the array of doubles:
(gdb) p fltarr phas no problem with floats.
$7 = {10, 20, 30, 40}
(gdb) x/4wf fltarr Word size whappens to be the same as sizeof(float) .
0x8049600 <fltarr>:     10      20      30      40(gdb) x/2gf fltarr
Word size gis too big for floats.
0x8049600 <fltarr>:     134217760.5625  34359746808(gdb) x/4gf dblarr
Word size gis just right for doubles.
0x80495e0 <dblarr>:     10      20
0x80495f0 <dblarr+16>:  30      40
In these examples, I specified the format explicitly, which is a good idea when you
can remember to do it. The problem is when you forget to specify the format and10.3 Getting Comfortable with the GNU Debugger: gdb 545
TABLE 10-2 Word Sizes Used with the x Command
Suffix Word Size
b Byte (8 bits)
h Half word (2 bytes)
w Word (4 bytes)
g Giant (8 bytes)
can’t understand the results. In that case, you should check and recheck to make
sure that you are using the correct format before jumping to any conclusions.
These are just a few of the many variations you can apply when printing data.
print allows even more flexibility with variables because it allows you to use C syn-
tax. With arrays, for example, you can use C syntax to print individual values, oryou can print out multiple elements by using the ampersand suffix:
(gdb) p *intarr Just like C, array can be used like a pointer.
$4 = 10
(gdb) p intarr[1] Use C subscript notation to look at the second element in the array.
$5 = 20(gdb) p intarr[1]@2
Use a combination of subscripts and @ to look at two elements starting at element 1.
$6 = {20, 30}
There are some subtle differences to be aware of when you print strings, how-
ever. Some formats recognize ASCII NULs, and some ignore them. Consider these
declarations:
const char ccarr[] = "This is NUL terminated.\0Oops! you shouldn't see this.";
const char *ccptr = ccarr;
The ccarr is an array, with a NULcharacter in the middle of some ASCII text.
ccptr is a pointer that points to the same memory. Notice that the print com-
mand distinguishes between the two variables based on their types, whereas the x
command, with an explicit /smodifier, treats both types the same:
(gdb) p ccarr Array type does not recognize ASCII NUL.
$1 = "This is NUL terminated.\000Oops! you shouldn't see this."
(gdb) p ccptr Pointer to char recognizes NUL.
$2 = 0x8048440 "This is NUL terminated."(gdb) x/s ccarr
/sexplicitly tells xto print a null-terminated string.
0x8048440 <ccarr>:       "This is NUL terminated."
With print , you can coerce the types using regular C syntax to force the output
to look the way you want. For example:
(gdb) p (char*) ccarr
$3 = 0x403040 "This is NUL terminated."
Finally, you may never need it, but you can disassemble machine code anywhere
in memory by using the iformat with the xcommand:546 Chapter 10 • Debugging
(gdb) x/10i main
0x401050 <main>:        push   %ebp0x401051 <main+1>:      mov    %esp,%ebp0x401053 <main+3>:      sub    $0x28,%esp0x401056 <main+6>:      and    $0xfffffff0,%esp0x401059 <main+9>:      mov    $0x0,%eax0x40105e <main+14>:     add    $0xf,%eax0x401061 <main+17>:     add    $0xf,%eax0x401064 <main+20>:     shr    $0x4,%eax0x401067 <main+23>:     shl    $0x4,%eax0x40106a <main+26>:     mov    %eax,0xffffffe4(%ebp)
This could come in handy if you are trying to look for buffer overflow attacks.
10.3.3.3 Calling Functions from gdb
gdballows you to call any function that is visible in your program. The function
executes in the context of your running process and consumes stack and otherresources from the process being debugged. Although this is cool, it can have unin-tended side effects if not used carefully.
A function call can be included as an argument to almost every command. I used
this earlier in the chapter to illustrate use of the 
print command, where I called
the kill function as an argument to print . If you simply want to call a function
and nothing else, use the call command:
(gdb) call getpid()
$1 = 27274
The value $1is a temporary value that is allocated by gdbto hold the return
value of the function. This memory resides in gdb’s space (not the running pro-
gram). gdballocates these variables automatically for you whenever it needs to store
a return value. You can use these values as arguments to functions. You can pass theprevious result of 
getpid to the kill command as follows:
(gdb) call kill($1,0)
$2 = 0
If you want to modify values in the running program’s space, you can use the
set command. set takes many different arguments, but like most gdb com-
mands, it accepts almost any valid C expression as an argument. Due to gdb’s free
syntax, you can set a variable using any command that allows C expressions as anargument, not just the 
set command. It’s easy to remember to use set with
assignment expressions.10.3 Getting Comfortable with the GNU Debugger: gdb 547
10.3.3.4 Some Notes about the C++ and Templates
C++ templates pose a unique debugging challenge. Templates allow a programmer
to define code in a generic fashion such that the compiler can generate source codefrom a more abstract specification. Consider the following trivial example, whichswaps two values:
template <class Typ>
void swapvals( Typ &a, Typ &b){
Typ tmp = a;a = b;b = tmp;
}
The token Tmpis a placeholder for a type name. Defining this template in your
source will not generate any code until you use it. When you use it, you must spec-ify a type that will take the place of 
Typ. This is called instantiation. To create a
function to swap two doubles, you would call this function as follows:
swapvals<double>(a,b);
This causes the compiler to create a swapvals function that works exclusively
with double s. If you need to swap two variables of type int, you can use
swapvals<int> , which causes the compiler to generate a completely different func-
tion with a unique function signature. Because the template defines a whole familyof functions, setting a breakpoint on a function defined by a template requires somefinesse. Start by looking for the function with 
gdb’s info functions command:
(gdb) info func swapvals
All functions matching regular expression "swapvals":
File templ.cpp:
void void swapvals<Foo>(Foo&, Foo&); gdb6.3 prints ‘void’ twice, for some reason.
void void swapvals<double>(double&, double&);void void swapvals<int>(int&, int&);
.
Notice that there is a unique function for each type. There is no command that
will apply a breakpoint on all functions generated by this template. You can set abreakpoint on only one of these functions at a time. To set a breakpoint on the 
int
version, you can start with an open quote and use the tab expansion:548 Chapter 10 • Debugging
(gdb) b 'void swap <Tab>
(gdb) b 'void swapvals< <Tab>
(gdb) b 'void swapvals<int>(int&, int&)'Breakpoint 3 at 0x8048434: file templ.cpp, line 8.
The closest you can come to setting a breakpoint on all functions that match a
template is the rbreak command. This sets a breakpoint on all functions that
match a given regular expression. For example:
(gdb) rbreak swapvals
Breakpoint 2 at 0x8048456: file templ.cpp, line 8.void void swapvals<Foo>(Foo&, Foo&);Breakpoint 3 at 0x8048400: file templ.cpp, line 8.void void swapvals<double>(double&, double&);Breakpoint 4 at 0x8048434: file templ.cpp, line 8.void void swapvals<int>(int&, int&);
If your template has a short name that matches many other functions, you may
end up setting unintended breakpoints. Use this carefully.
10.3.3.5 Some Notes about the C++ Standard Template Library
Although C++ includes the ANSI C standard library, C++ adds its own standard
library implemented almost exclusively with templates. Technically, the Standard
T emplate Library (STL) is now the C++ standard library, although many program-
mers still refer to it as STL.
One of the features in the standard template library is the container. A container
is a template that implements dynamic storage. Containers can save a great deal ofcoding by implementing common storage algorithms such as lists, queues, andmaps. Debugging code that uses containers can be a challenge, however.
The problem is that C++ containers go to great lengths to hide the underlying
implementation from the user. Data inside a container is accessible only via methodcalls. You can use 
gdbto call these methods just like functions. This way, you can
inspect containers at run time.
Look at one of the simplest C++ containers: the vector . The vector is designed
to behave like a regular C array except that the storage is dynamic. Compare the twoby using a type 
int:
int myarray[3]; C array of three ints
std::vector<int> myvector(3); C++ vector of three ints10.3 Getting Comfortable with the GNU Debugger: gdb 549
When you debug code that has these two declarations, you can find out what you
are looking at by using the whatis command:
(gdb) whatis myarray
type = int [3] Size is fixed.
(gdb) whatis myvectortype = std::vector<int,std::allocator<int> >
Size is dynamic—not reported.
All C++ containers have a size method that tells you how many elements are
stored in the container. You can call this method from gdb just like any other
function:
(gdb) p myvector.size()
$1 = 3
There is a catch, however: C++ templates do not generate code unless the code is
used (instantiated). So unless your code actually uses the size method above, it will
not be instantiated, and there will be no size method in the executable to call. In
this case, you might see a message like the following:
(gdb) p myvector.size()
Cannot evaluate function -- may be inlined
To make matters more confusing, the method could get instantiated indirectly by
using other methods. So you might be able to use this technique to debug in oneprogram but not another. To make your life easier, you can consciously add super-fluous method calls to your code for the purpose of debugging. Also note that instan-tiation must be done for each unique type, so instantiating 
vector<int>::size
does not instantiate vector<float>::size .
Note that gdbis able to treat containers of std::vector as regular arrays, so
most syntax that works with arrays also works with vectors. Most other containersrequire help from method calls and iterators. Recall that an iterator is the C++
equivalent of a pointer used to access data in containers. Iterators use the same syn-tax as pointers, but they are not pointers. Luckily, 
gdbunderstands iterators, so you
can print data using equivalent syntax. Consider the following code fragment:
std::list<int> mylist;
...std::list<int>::iterator x = mylist.begin();550 Chapter 10 • Debugging
When xis instantiated inside the code, you can print use the same syntax with
this iterator as you would with a pointer inside gdb:
(gdb) p *x
$1 = (int &) @0x804c1f8: 100
That’s about as much as you can do with an iterator inside gdb. You cannot use
pointer math or walk through a sequence container by using the ++operator in gdb.
If you need to see data inside C++ containers (other than a vector) from gdb, you
will need to add some code to facilitate this.
10.3.3.6 The display Command
display prints the specified expression each time the program stops. You can dis-
play as many expressions as you want and format each expression using the samesyntax as the 
xcommand.
Compile and run the program in Listing 10-5 with gdb. This program also
demonstrates using gdbwith containers.
LISTING 10-5 permute.cpp: A C++ Program to Demonstrate gdb’s display Command
1 #include <stdio.h>
2 #include <string>3 #include <algorithm>45 int main(int argc, char *argv[])6 {7     int i = 0;8     std::string token = "ABCD";9
10     // Simple loop goes through every permutation of a string11     // using std::next_permutation.12     do {13         i++;14     } while (std::next_permutation(token.begin(), token.end()));1516     printf("%d permutations\n", i);17     return 0;
18 }10.3 Getting Comfortable with the GNU Debugger: gdb 551
token is a four-character string that this program permutes using the C++ algo-
rithm std::next_permutation . The laws of combinatorics states that there are
four factorial6(4!) permutations of this string. You set a breakpoint on line 13 so
you can watch what it does with the display command:
Breakpoint 2, main (argc=1, argv=0xbffce1a4) at permute.cpp:13
13                      i++;(gdb) display/xw token.c_str()
Display four chars as a longword.
1: x/xw token.c_str ()  0x804a014:      0x44434241(gdb) display/s token.c_str()
Display same thing as ASCII.
2: x/s token.c_str ()  0x804a014:        "ABCD"(gdb) contContinuing.
Breakpoint 2, main (argc=1, argv=0xbffce1a4) at permute.cpp:13
13                      i++;2: x/s token.c_str ()  0x804a014:        "ABDC"1: x/xw token.c_str ()  0x804a014:      0x43444241(gdb)
No need to retype “cont.”
Continuing.
Breakpoint 2, main (argc=1, argv=0xbffce1a4) at permute.cpp:13
13                      i++;2: x/s token.c_str ()  0x804a014:        "ACBD"1: x/xw token.c_str ()  0x804a014:      0x44424341
etc...
Breakpoint 2, main (argc=1, argv=0xbffce1a4) at permute.cpp:1313                      i++;2: x/s token.c_str ()  0x804a014:        "DCBA"
Final permutation.
1: x/xw token.c_str ()  0x804a014:      0x41424344(gdb)Continuing.24 permutations
4 factorial = 24
You created two displays, which gdbnumbers 1and 2for reference. Each time
the program stops, both expressions are evaluated. In this case, gdb calls the
c_str method of std::string once for each display. You can stop these displays
at any time. To stop displaying the ASCII string in this example, you could type
undisplay 2 to remove display 2.552 Chapter 10 • Debugging
6. For the math impaired:  N! = N * (N-1) * (N-2) ... * 2, so 4! = 4 * 3 * 2 = 24.
10.3.4 Attaching to a Running Process with gdb
From the command line, you can attach to a running process with gdbusing the
following syntax:
$ gdb programname pid
The pid is the process ID of the process you want to attach to, and the
programname is the filename of the executable. These must match for the debug-
ging session to be relevant. If you try to work with an executable that has beenrecompiled since the process was launched, there is no guarantee that the results youget will make any sense.
If you want to debug a different program, you don’t need to quit 
gdbor termi-
nate the currently running process. You can stop debugging and let the process con-tinue running by using the 
detach command. This terminates the debugging
session without terminating your process. When gdbis detached, you can change
the program and debug a different process with the attach command. If necessary,
you can use the file command to set the executable to match the new process.
You could use these commands as an alternative to starting gdbthe way you did
earlier in the chapter, as follows:
(gdb)file programname
(gdb)attach pid
Note that if the process is compiled with debug , it may be possible to skip the
file command and let gdbfigure out where the executable is.
10.3.5 Debugging Core Files
When a process dumps a core file, it leaves behind the state of its virtual memory.
Core files are always the result of a signal. The most common core-file-generatingsignals encountered while debugging include
•
SIGSEGV —segmentation violation. This signal is generated when a process
attempts to read from or write to an invalid memory address; it also can occurwhen attempting to write to a read-only page or read from a page with no readpermission.
•
SIGFPE —floating-point error. Oddly enough, this signal usually is not gener-
ated by floating-point functions. Instead, you will see SIGFPE on the x86
architecture when a process attempts an integer divide by zero.10.3 Getting Comfortable with the GNU Debugger: gdb 553
•SIGABRT —abort; used by the abort and assert functions.
•SIGILL —illegal instruction. This signal is most likely to occur in hand-coded
assembly routines that attempt to use privileged instructions from user mode.
•SIGBUS —bus error, but not in the hardware sense. This signal can be the result
of a page fault that could not complete, such as running out of swap space.
The core file includes information about which signal caused the core file. As
long as you have a copy of the same executable that generated the core file (as well
as all the same shared objects), you can debug the program using the core file. Todebug, use the following command line:
$ gdb exec-filename core-filename
T raditionally, core files are named simply core , although many distributions
setup the kernel to dump core files with the process ID as part of the name—forexample, 
core.pid .7When you bring up a file for postmortem debugging in this
way, one of the first things gdbtells you is what caused the core dump. If it can,
gdbwill place you at the line of source where the signal occurred. For example:
$ gdb seldomcrash core.27078
...Reading symbols from shared object read from target memory...done.Loaded system supplied DSO at 0xffffe000Core was generated by './seldomcrash'.Program terminated with signal 8, Arithmetic exception.
...
#0  0x0804836c in main (argc=1, argv=0xbfc9acd4) at seldomcrash.c:1010              return someint / 0;
Oops, divided by 0!
(gdb)
As you might expect, you can bring up programs for postmortem debugging from
within gdbby using the file command followed by the core-file command.
When a program severely corrupts the stack, the core file that is generated often
is of limited use with gdb, because gdbneeds the stack to navigate the local vari-
ables. Even when the stack is corrupt, however, global and static variables will stillbe useful.554 Chapter 10 • Debugging
7. This feature is disabled by the command sysctl -w kernel.core_uses_pid = 0 .
Knowing that your program corrupted the stack is useful information, but unfor-
tunately, gdbdoesn’t come right out and tell you this. The only clue you get to this
event is when the functions listed in the backtrace command make no sense for
the program you are running, or gdbmight just tell you no stack .
The most common cause of stack corruption is a buffer overflow of a local vari-
able. You might declare an array on the stack and use memset or memcpy to ini-
tialize the array to too large a size. In general, any time you take a pointer to a localvariable and give it to another function, you create the risk of an overrun. Thisopportunity for buffer overflow is possible with 
scanf , read , and many other
functions.
The program in Listing 10-6 is an implementation of the factorial function I
described earlier in the chapter. This is commonly used in statistical applicationsand in computer-science classes as an example of recursive programming.
8
LISTING 10-6 factorial.c: An Example of Stack Corruption
1 #include <stdio.h>
2 #include <string.h>3 #include <stdlib.h>45 // factorial function. Traditional programming example6 // of a recursive function.7 int factorial(int x)8 {9     int overflow;
10     static int depth = 0;1112     if (x <= 1)13         return 1;1415     if (++depth > 6) {16         // Overflow n bytes. Will cause a SIGSEGV, but we don't know where.17         int n = 0x100;18         memset((char *) &overflow, 0xa5, sizeof(overflow) + n);19     }2021     return x * factorial(x - 1);22 }2310.3 Getting Comfortable with the GNU Debugger: gdb 555
8. Infinite recursion is a common programming bug that leads to a SIGSEGV but not necessarily to stack
corruption.continues
24 int main(int argc, char *argv[])
25 {26     int n = 3;27     printf("Command line argument > 7 will cause stack corruption\n");28     if (argc > 1) {29         n = atoi(argv[1]);30     }31     printf("%d! == %d\n", n, factorial(n));32     return 0;
33 }
In this contrived example, you deliberately trash the stack after the function has
called itself seven times. To do this, you write 256 bytes to the address of an inte-ger local variable. Here it is in action:
$ ./factorial 7
Command line argument > 7 will cause stack corruption7! == 5040$ ./factorial 8Command line argument > 7 will cause stack corruptionSegmentation fault (core dumped)
Told you so!
$ gdb ./factorial coreGNU gdb Red Hat Linux (6.3.0.0-1.21rh)...Core was generated by `./factorial 8'.Program terminated with signal 11, Segmentation fault....#0  0xa5a5a5a5 in ?? ()
This is not looking good.
(gdb) bt Let’s look at the backtrace.
#0  0xa5a5a5a5 in ?? ()#1  0xa5a5a5a5 in ?? ()#2  0xa5a5a5a5 in ?? ()#3  0xa5a5a5a5 in ?? ()...
OK, that wasn’t helpful.
#21 0xa5a5a5a5 in ?? ()#22 0xa5a5a5a5 in ?? ()---Type <return> to continue, or q <return> to quit---qQuit(gdb)p factorial::depth
Static variables are still OK.
$1 = 7
After the program dumps core, you use gdbto attempt postmortem debugging
and find that the stack is unusable. Your clue is that the backtrace command ( bt)
shows nothing useful. You can still use this core file, however. You only need stackframes to look at local variables. All static and global variables are still visible. To lookat the static variable 
depth defined inside the function factorial , use C++ syntax:
factorial::depth . Examining this value tells you that you called factorial
seven times recursively. In this case, that’s almost as good as a stack trace.556 Chapter 10 • Debugging
Unfortunately, most programs aren’t this easy to debug. If you are debugging a
program that is exhibiting stack corruption, you can consider adding global or staticvariables for the purpose of postmortem debugging. I explore this idea further inthe section “Creating Your Own Black Box” later in this chapter.
10.3.6 Debugging Multithreaded Programs with gdb
Debugging programs with multiple threads can be a difficult task. gdbis thread
aware and provides several features to make this task easier.
All multithreaded programs start out single threaded, so debugging is straight-
forward until the threads are created. The default behavior of gdb in a multi-
threaded program probably is what you would expect. When you’re at thecommand prompt, all threads are stopped. When you single-step through the code,
gdb attempts to stay in the current thread. If a breakpoint occurs in a different
thread, however, the command prompt will switch to that thread’s stack frame.
The info threads command shows you the currently running threads. This is
illustrated below with the code omitted:
(gdb) info threads
4 Thread -1225585744 (LWP 6703)  0x0804854c in the_thread (ptr=0x2)
at thread-demo.c:17
3 Thread -1217193040 (LWP 6702)  0x0804854c in the_thread (ptr=0x1)
at thread-demo.c:17
2 Thread -1208800336 (LWP 6701)  0x0804854c in the_thread (ptr=0x0)
at thread-demo.c:17
* 1 Thread -1208797504 (LWP 6698)  main (argc=1, argv=0xbf940e84)
at thread-demo.c:45
The first column is a gdb-defined thread identifier. These are simple incre-
menting values that gdbuses to make switching between threads easier for the user.
The number increases with each new thread, which can be helpful for identifyingwhich thread is which. The large negative number is the 
pthread_t used by the
pthreads API.
At the command prompt, gdb uses the stack frame of the current thread for
local variables. The current thread is wherever gdb happened to stop. To switch
from one thread to another, use the thread command followed by the gdbiden-
tifier of that thread.
Any breakpoint you set without a qualifier applies to all threads. The first thread
that hits the breakpoint will stop the program, and gdbwill make that thread the
current thread. To make a breakpoint apply to only one thread, use the thread10.3 Getting Comfortable with the GNU Debugger: gdb 557
qualifier to the break command. If you want to stop at function fooin thread 3,
use the following command:
(gdb) break foo thread 3
Whenever gdbhits a breakpoint, the thread that caused the breakpoint becomes
the current thread. Any single-stepping from that point occurs in the current thread.
Using gdb’s scheduler-lock Feature with Threads
When you’re single-stepping in a multithreaded program using the step or next
commands, it’s important to know that other threads will execute while the current
thread single-steps. Usually, this is what you want, because it provides some degree ofsimultaneity among threads while you single-step through the application.
There are occasions when this might be undesirable, however. While you are step-
ping through the code with 
gdbin one thread, the other threads can execute and hit
breakpoints. If this happens gdbwill switch threads automatically. To prevent this,
you can modify the scheduler behavior while stepping through the code with the
set scheduler-lock command. This takes one of three settings:
set scheduler-lock off
set scheduler-lock onset scheduler-lock step
The default setting is off, which I just described. The onsetting causes gdbto dis-
able all other threads while you single-step through code. The other threads do notrun until you resume execution with the 
continue command.
The step setting causes the threads to stop during a step but not during a next
command—that is, other threads do not run during single stepping unless you step
over a function in the current thread with the next function.
10.3.7 Debugging Optimized Code
gccallows you to compile code with optimization and debugging, although these
options seem mutually exclusive. There are times when the performance penalty fornot using optimization is unacceptable. At the same time, the behavior of optimizedcode running under the debugger can be downright bizarre. Sometimes, you can558 Chapter 10 • Debugging
compromise by compiling only selected modules without optimization, but that’s
not always possible.
Optimization is architecture specific, so there are no hard and fast guidelines that
I can give you. There are some typical things to look out for:
• Unused variables—The optimizer is free to remove unused variables from the
code, which can be confusing when running under debug . Sometimes pro-
grammers set aside variables to store data that you look at only while debug-ging. Such a write-only variable is a prime candidate for removal by theoptimizer. That makes debugging harder, of course. To protect such variablesfrom the optimizer, declare them 
volatile .
• Inline functions—This is a typical optimization that compilers make to save
the overhead of a function call. When a function is inlined, it will not appearon the call stack. To disable function inlining, compile your code using the
gccflag -fno-inline .
• Out-of-order steps—Single-stepping through an optimized function can chal-
lenge your sensibilities. Where unoptimized code proceeds nicely from top tobottom, an optimized function often jumps all over the place with no rhymeor reason because the optimizer rearranges the code to execute in a more effi-cient manner.
Sometimes, looking at the assembly language can help. 
gdbdoes not provide list-
ings of source and assembly together, but you can see what the compiler is telling
gdbwith the objdump command. Consider this screwed-up function:
1 void vscale2(double *vec, int len, double arg)
2 {3     int i;45     // Temporaries to confuse the reader...6     double a, b, c;7     for (i = 0; i < len; i++) {89         // Don't ask me what this does
10         a = arg * arg;11         b = arg * a / 2;12         c = a * b * arg;13         vec[i] *= c;14     }15 }10.3 Getting Comfortable with the GNU Debugger: gdb 559
The temporary variables defined on line 6 serve only to confuse the reader. By
reusing the temporary variables on multiple lines, you give the optimizeropportunities to combine these lines into a single operation. Compiling this func-tion with optimization and running it through 
objdump shows that all these refer-
ences are collapsed into one line of code from the debugger’s point of view—that is,the optimizer is able to realize these are temporary variables and exploit this by com-bining all this code.
The 
objdump command can be helpful here. It can produce a dump of the
machine code, disassembly, and debug information annotated with the original Csource. When we run this on our object file compiled with optimization and debug,we can see things more clearly. For example:
$ objdump -S -l vscale2.o
vscale2.o:     file format elf32-i386Disassembly of section .text:00000000 <vscale2>:
vscale2():/home/john/examples/ch-10/debug/vscale2.c:2
Line numbers show where gdbcan set breakpoints.
void vscale2(double *vec, int len, double arg){
0: 55                   push   %ebp1: 89 e5                mov    %esp,%ebp3: 8b 4d 0c             mov    0xc(%ebp),%ecx6: dd 45 10             fldl   0x10(%ebp)
/home/john/examples/ch-10/debug/vscale2.c:7
The following code is attributed to line 7.
int i;
// Temporaries to confuse the reader...
double a, b, c;for (i = 0; i < len; i++) {
9: 85 c9                test   %ecx,%ecxb: 7e 25                jle    32 <vscale2+0x32>d: d9 c0                fld    %st(0)f: d8 c9                fmul   %st(1),%st
11: d9 c1                fld    %st(1)13: d8 c9                fmul   %st(1),%st15: d8 0d 00 00 00 00    fmuls  0x01b: de c9                fmulp  %st,%st(1)1d: de c9                fmulp  %st,%st(1)1f: 31 d2                xor    %edx,%edx21: 8b 45 08             mov    0x8(%ebp),%eax
/home/john/examples/ch-10/debug/vscale2.c:13
Notice that there is no code attributed to lines 8–12!560 Chapter 10 • Debugging
// Don't ask me what this does This gibberish has been optimized away.
a = arg * arg;
b = arg * a / 2;c = a * b * arg;vec[i] *= c;
24: d9 c0                fld    %st(0)26: dc 08                fmull  (%eax)28: dd 18                fstpl  (%eax)
/home/john/examples/ch-10/debug/vscale2.c:7
2a: 42                   inc    %edx2b: 83 c0 08             add    $0x8,%eax2e: 39 d1                cmp    %edx,%ecx30: 75 f2                jne    24 <vscale2+0x24>32: dd d8                fstp   %st(0)
/home/john/examples/ch-10/debug/vscale2.c:15
}
}
34: c9                   leave35: c3                   ret36: 89 f6                mov    %esi,%esi
What this shows is that when you run this code, you cannot set breakpoints on
lines 10, 11, and 12 because the temporaries have been coalesced by the optimizer.If you try to set a breakpoint on line 10, for example, the debugger will appear tocomply, but the code will actually stop on line 15. As you might expect, there is nostorage allocated for 
a, b, or c, so you can’t try to print these variables at runtime.
If you try, gdbwill respond with a message like this:
No symbol "a" in current context.
There is no trace of these variables in the debug output (try info locals from
inside vscale2 as well). Again, if you feel you need to keep these temporary values
for debugging, declare them volatile . But use the volatile keyword sparingly,
because it degrades your performance.
10.4 Debugging Shared Objects
Shared objects have many names on different platforms, but the concept is thesame. Microsoft Windows calls these dynamic linked libraries, whereas many UNIX
tools refer to them as dynamic libraries. The generic term is shared objects because
these need not be libraries, although that is how they are most commonly used.Debugging code in a shared object is not much different from debugging a normalprogram. Sometimes, however, getting the correct binary into memory can be achallenge.10.4 Debugging Shared Objects 561
10.4.1 When and Why to Use Shared Objects
By default, all Linux applications use shared objects for the standard libraries,
because this is the most efficient way to use memory. You can see this with the ldd
command.9T ry this on any Hello World application:
$ ldd hello
linux-gate.so.1 =>  (0xffffe000)libc.so.6 => /lib/libc.so.6 (0xb7e32000)/lib/ld-linux.so.2 (0xb7f69000)
Here, you can see that the file was linked against the standard library libc.so ,
and the runtime version can be found at /lib/libc.so.6 . The program also must
be linked with the dynamic linker itself, which is /lib/ld-linux.so.2 in this
executable. linux-gate.so.1 is a pseudo shared object that appears on Intel
architectures. It allows shared libraries to use the faster sysenter and sysexit
opcodes if the processor supports it. This is faster than the normal mechanism formaking system calls, which uses a software interrupt.
Normally, shared objects are used to share a library of routines among many
processes. This saves physical memory, because the read-only segments of the sharedlibrary can occupy the same physical memory across processes.
Although most programmers do not write code that will be shared by many
processes, there are other reasons to use shared objects. One use for shared objectsis to provide extensions to scripting languages. Perl and Python, for example, allowprogrammers to create shared libraries that can be called from scripts. This gives youthe flexibility of a script while keeping the efficiency of C for processor-intensiveparts. The shared object is pulled in as a module, and the functions within are vis-ible to the script interpreter.
10
Another, less common application is to use shared objects to implement overlays.
Overlays once were a common technique used to save memory on 16-bit platformswithout virtual memory. Today, such techniques are hardly necessary, but just incase, POSIX has an API for you. Interested readers should look at the 
dlopen(3)
manpage.562 Chapter 10 • Debugging
9.LDD stands for list dynamic dependencies.
10. Sound interesting? Check out www.swig.org.
10.4.2 Creating Shared Objects
Conceptually, the only difference between a shared object and a program is that the
shared object typically does not have a main function. This is not a requirement,
however. You can create shared objects that can be called just like an executablewhile retaining the ability to be linked dynamically into a larger program. Thedynamic linker itself is just such a shared object; it is used by the 
lddcommand I
introduced earlier in the chapter.
Creating a simple shared object is easy enough; just build it as though it were a
program, but use the -shared and -fpic flags. For example:
$ cc -shared -fpic -o libmylib.so mylib1.c mylib2.c
The -shared flag is for the linker, which tells it to produce a shared object
instead of an executable. The -fpic flag informs the compiler to generate position-
independent code. This is important because unlike those of a conventional exe-
cutable, the shared object’s virtual addresses are not known until runtime.
Linking a program with a shared object is deceptively simple:
$ cc -o myprog myprog.o -L . -lmylib
Here, I informed the linker that my shared library is located in the current direc-
tory with the -Loption. The problem is that the runtime linker ld-linux.so
needs to know where to find this shared object as well. This is a problem, as youcan see when you try to run this program:
$ ./myprog
./myprog: error while loading shared libraries: libmylib.so: cannot open sharedobject file: No such file or directory
The problem is that the system has no clue where the shared object is located.
Programs linked with shared objects do not contain any information about whereto find the shared objects. This is deliberate, because shared objects are located inspecific locations on every system. If this application were to run on a different sys-tem, it should make no assumptions about where to find a shared object. Instead,each shared object provides what is called a soname, which is the name that the
dynamic linker uses to identify the object. The library you created does not have ansoname, because you did not specify one. This is optional, because the dynamiclinker will fall back to using the filename if it does not recognize the soname.10.4 Debugging Shared Objects 563
10.4.3 Locating Shared Objects
Locating shared objects is the job of the dynamic linker ldlinux.so , located in
/lib . The dynamic linker will always search in the standard paths /lib and
/usr/lib . If you want to store shared objects in different places, you can use the
environment variable LD_LIBRARY_PATH . More often, systems have shared libraries
in several places. To prevent the dynamic linker from having to search throughmany paths, the system keeps a cache of sonames and the shared object locations in
/etc/ld.so.cache . This cache is created and updated by the /sbin/ldconfig
program, which searches the directories listed in /etc/ld.so.conf .
Whenever you install new libraries, it is necessary to run the ldconfig program
to update the cache. In addition, ldconfig creates symbolic links so that the file-
names of the shared object files can be uniquely different from the sonames. Usingthe 
hello program on my Fedora Core 3 machine, libc.so.6 points to a file
named libc-2.3.6.so :
$ ls -l /lib/libc.so.6
lrwxrwxrwx  1 root root 13 Jul  2 16:03 /lib/libc.so.6 -> libc-2.3.6.so
libc.so.6 is a generic soname encountered by the compiler, whereas 
libc-2.3.6.so is the filename used by the GNU C library package ( glibc ). In
principle, you don’t need to use glibc to provide libc.so.6 . You could substi-
tute your own library. As long as it uses the correct soname and is found in thepath, the dynamic linker will use it. In reality, replacing 
glibc probably would
break all the GNU tools that require glibc extensions.
10.4.4 Overriding the Default Shared Object Locations
Unprivileged users can use the environment variable LD_LIBRARY_PATH to tell the
dynamic linker where to look. You can finally get the myprog example to run as
follows:
$ LD_LIBRARY_PATH=./ ./myprog
This tells the dynamic linker to look in the current directory for shared objects,
which is where libmylib.so is located. For objects that have a soname, you can
save a little typing by using the LD_PRELOAD environment variable as follows:
$ LD_PRELOAD=libc.so.6 ./hello-world libc.so.6 is the soname of glibc on my system.564 Chapter 10 • Debugging
This tells the dynamic linker specifically to link against libc before linking the
rest of hello-world . This particular technique can be useful if your program links
with a library that re-implements a vital function from libc . LD_PRELOAD works
only with libraries that have sonames listed in /etc/ld.so.cache .
Both these techniques are very useful for debugging shared objects, because they
allow you to create a shared object in a private directory. There, you can link withthe shared object without interfering with other processes that may be using aninstalled version of the same shared object. You can debug a new version of theobject without fear of crashing other processes.
10.4.5 Security Issues with Shared Objects
I have discussed the benefits of shared objects, but shared objects also pose a seri-ous security risk if used improperly. Some shared objects are shared by many pro-grams, such as 
libc , which is used by virtually every system command. These
objects are used by many programs, including many that run with root privilege. Ifa malicious programmer can compromise a commonly used shared object, he cancompromise your whole system.
Suppose that you create a program with 
setuid root privileges to allow ordi-
nary users on your system to do some routine maintenance task. Whenever an ordi-nary user runs this program, the process will execute with 
root ’s privileges. Perhaps
this program uses a shared object that is located in an insecure location. A maliciousprogrammer theoretically could replace that shared object with malware. Your orig-inal program remains untouched but unknowingly compromises the system by call-ing one of the functions in this hijacked shared object.
For this reason, the dynamic linker goes to great lengths to make sure that shared
objects pulled in by such programs are secure. Only objects in the standard path areallowed, for example (
LD_LIBRARY_PATH is ignored), and all shared objects must
have root ownership and read-only permissions.11
10.4.6 Tools for Working with Shared Objects
The Linux dynamic linker is itself a command-line tool. For historical reasons, the
manpage is listed under ld.so(8) , although the actual program name used in current10.4 Debugging Shared Objects 565
11. For more details, see ld.so(8) .
distributions is ld-linux.so.2 . When invoked from the command line, the linker
takes several options and understands many environment variables, which you canuse to understand your program better. These options are described in the 
ld.so(8)
manpage, but in most cases the preferred tool is ldd, which is actually a wrapper
script that calls ld-linux.so.2 and has some more user-friendly options.
10.4.6.1 List Shared Objects Required by an Executable
With no options, lddwill show you all the shared objects required by an executable:
$ ldd hello
linux-gate.so.1 =>  (0xffffe000)libm.so.6 => /lib/libm.so.6 (0xb7f1b000)libpthread.so.0 => /lib/libpthread.so.0 (0xb7f09000)libc.so.6 => /lib/libc.so.6 (0xb7de0000)/lib/ld-linux.so.2 (0xb7f4e000)
Strictly speaking, this is a list of objects that the file was linked with—not nec-
essarily the objects that the executable requires. In this case, I deliberately linked a
Hello World program with the math library and the pthread library, neither of
which is required:
$ gcc -o hello hello.c -lm -lpthread
Unlike static libraries, the linker does not remove shared object code from the
executable. Recall that a static library is just an archive. The linker uses the archiveto pull in object files that it needs and only the object files that it needs. In this way,the static linker is able to eliminate unneeded object files from the executable.When you specify a shared object on the command line, the linker includes it in theexecutable whether it’s necessary or not.
You can see this with the 
lddcommand you used earlier in the chapter. In this
case, you happen to know that libpthread.so and libm.so are not required, but
what if you didn’t know that? The -uoption will show you unused dependencies,
as follows:12
$ ldd -u ./hello
Unused direct dependencies:
/lib/libm.so.6
/lib/libpthread.so.0566 Chapter 10 • Debugging
12. Curiously, the -uoption is missing from the ldd man page but shows up with --help .
It’s up to you to do the mental gymnastics to figure out howthose shared objects
found their way into your executable. Knowing the naming convention for librariesis one way to work backward into the command-line options that got you here.Many open source projects use the 
pkg-config tool to create the command-line
options that are used to link a project. In some cases, these rules can pull in extrashared objects.
10.4.6.2 Why Worry about Unused Shared Objects?
For each shared object a program links with, the dynamic linker must search the
object for unresolved references and call initialization routines. This increases theamount of time it takes for your program to start. On a fast desktop machine,unused shared objects probably don’t amount to much extra time, but if there aremany of them, it could add up to a significant delay.
Another issue with unused shared objects is the resources they consume.
Whether it is used or not, a shared object may allocate and initialize a large amountof physical memory. If initialized and left unused, this memory eventually will findits way to the swap partition. Objects that don’t consume physical memory mayconsume virtual memory. This is memory that is allocated but uninitialized. Ifnever used, it will never be swapped and will never consume physical RAM, but itlimits the number of available virtual addresses that can be used by a program.Usually, this is a problem only in applications on 32-bit architectures that requirevery large datasets—on the order of gigabytes of RAM.
As a rule, it’s always a good idea to avoid unnecessary shared objects. In particu-
lar, if you have a system with a slow CPU or limited RAM, you should be extra care-ful not to link with shared libraries you do not need. On a modern server or desktopsystem, none of these issues is a serious problem by itself. Nevertheless, when manyapplications use many shared objects that they don’t need, the system as a whole canstart to feel the effects.
10.4.6.3 Looking for Symbols in Shared Objects
Occasionally, you might download some source code that compiles but does not
link because it is missing a symbol. The 
nmand objdump commands are the tools
of choice for looking at program symbol tables. In addition, there is the readelf
command. All these tools do basically the same thing, but you may find that basedon what you need to know, only one of these tools can help.10.4 Debugging Shared Objects 567
Suppose that you have a shared object file and want to know its soname before
you install it. Recall that the ldconfig command does the job of reading sonames
and putting them in the cache. Before you install this library, you may want toknow whether it conflicts with any existing sonames. You can look at the cache atany time with 
ldconfig p . To find the soname of a single (uninstalled) shared
object, you need to look at the so-called DYNAMIC section. The nmtool is not suited
for this, but objdump and readelf are:
$ objdump -x some-obj-1.0.so | grep SONAME
SONAME      libmylib.so
$ readelf  -a libmylib.so |grep SONAME
0x0000000e (SONAME)                     Library soname: [libmylib.so]
Another problem that arises is when the linker complains about unresolved sym-
bols. This occurs perhaps most often due to a missing library or attempting to linkwith the wrong version of a library. There are many ways this can happen. In C++,the problem can be caused by a function signature that has changed or perhaps justa typo.
All three tools can print out symbol tables, but 
nmmay be the easiest to use. To
look through the object code to find references to a particular symbol, use the fol-lowing command:
$ nm -uA *.o | grep foo
The -uoption restricts the output to unresolved symbols in each object file. The
-Aoption displays the filename information with each symbol, so that when you
pipe the output to the grep command, you can see which object file contains that
symbol. For C++ code, there is also the -Coption, which demangles the symbols
for you as well. This can help in debugging libraries that may have unwisely chosenfunction signatures, such as the following:
int foo(char p);
int foo(unsigned char p);
C++ allows both functions to have unique signatures but silently typecasts the
input parameters to use one if the other is missing.13To look for libraries that have568 Chapter 10 • Debugging
13. By the way, if you change the input argument types to const references, the input types are strictly
enforced.
these functions, just drop the -uoption. Adding the -Coption never hurts unless
you really want to see mangled function names:
$ nm -gCA lib*.a | grep foo
libFoolib.a:somefile.o:00000000 T foo(char)libFoolib.a:somefile.o:00000016 T foo(unsigned char)
As you might guess, the objdump and readelf commands can do the same
thing as well. The equivalent of the nmcommand using objdump is
$ objdump -t
objdump also has a -Coption to demangle symbol names. The equivalent
readelf command is
$ readelf -s
Unlike nmand objdump , readelf has no options to demangle symbol names as
of version 2.15.94. All three utilities are available as part of the binutils package.
10.5 Looking for Memory Issues
Problems with memory can take many forms, from buffer overflows to memoryleaks. Many tools try to help, but there are limits to what you can do. Nevertheless,some tools are easy enough to use that they’re worth a try. Sometimes when one tooldoesn’t work, another will. Even 
glibc has features to help you debug dynamic
memory issues.
10.5.1 Double Free
Freeing a pointer twice is an easy-enough mistake to make, but the consequencescan be dire. The problem is that until recently, 
glibc would not check your point-
ers for you and would blindly accept any pointer you give it. Freeing a pointer toan invalid virtual address will cause a 
SIGSEGV at the point where it occurred. That’s
easy to find. Freeing a pointer that points to a valid virtual address can be muchmore difficult to find.
Most often, the invalid pointer being freed is one that was initialized by a 
malloc
call but already freed with a free call. It is possible that freeing the pointer twice
will corrupt the free list that glibc uses to track dynamic memory allocations.
When this happens, you will get SIGSEGV , but it might not occur until the nextfree
or malloc call! That’s more difficult to find.10.5 Looking for Memory Issues 569
Some idiosyncrasies in glibc make finding such errors more difficult. Blocks
above a certain size are allocated using mmap calls instead of a conventional heap, for
example. T raditionally, the heap is a large pool of memory that grows and shrinksas the process requires. 
glibc uses anonymous mmap s to allocate large blocks and a
traditional heap for small blocks. This creates different failure modes for differentblock sizes that may be difficult to interpret.
Recent versions of 
glibc include checking for invalid free pointers that cause a
program to terminate with a core dump no matter what the circumstances. I lookat this topic in detail later in the chapter.
10.5.2 Memory Leaks
A memory leak occurs when a process allocates a block of memory, discards it, andthen neglects to free it. Often, small leaks are harmless, and the program continuesto run with no ill effects. Given time, though, even a small leak can grow to becomea problem. A simple utility that executes for a short time can tolerate small leaksbecause it discards its heap after it exits. A daemon process that may run for monthscannot tolerate any leaks, however, because they accumulate over time.
The effect of a memory leak is that your process’s memory footprint continues to
grow. When allocated memory has not been touched, the leaked memory may con-sume only virtual addresses. As long as your program doesn’t run out of user-spacevirtual addresses (typically, 3GB on a 32-bit machine), you will never see any illeffects. In most cases, the leaked memory has been modified by the program, sothese pages must consume physical storage (either RAM or swap). As the unusedmemory pages age and the demand for system memory increases, these pages getpaged out to disk.
The swapping is perhaps the most insidious side effect of memory leaks, because
a single leaky program can slow the entire system. Fortunately, memory leaks arenot very hard to find. Following are some tools to help.
10.5.3 Buffer Overflows
A buffer overflow occurs when an application writes beyond the end of a block ofmemory, overwriting memory that may be in use for other purposes. An overflowmay result in writing to unmapped or read-only memory, which will result in a
SIGSEGV . Overflows are a common type of error and can occur in any type of570 Chapter 10 • Debugging
memory: stack, dynamic, or static. There are several tools for detecting overflows in
dynamic memory, but detecting overflows in static memory and local variables isharder.
The best advice for dealing with overflows is to avoid them. Certain standard
library functions present ample opportunities for overflows to occur and should beavoided. In most instances, a safer alternative is available. A good tool that canuncover vulnerabilities is 
flawfinder .14This is a Python script that parses your
source code for dangerous functions and reports them to you.
10.5.3.1 Stack Buffer Overflows
A stack buffer overflow represents a security risk. Several attackers have used over-
flow vulnerabilities in commercial software to implant malware on otherwise-securesystems.
A typical vulnerability involves a text input field defined as a local variable using
a function that has no overflow checking. When the input consists of plain text orgarbage, the program will simply crash. This is bad enough, but a clever attackercan input binary machine code into a text field to overflow the input buffer. Withsome trial and error, a clever attacker can figure out just the right bytes to write tocoax the program to run his code and take over the process.
The precise details of how this occurs are beyond the scope of this book, but it is
important to know that stack buffer overflows are a security risk. Under normal cir-cumstances, a stack buffer overflow is difficult to find when it occurs. The typicalsignature of a stack buffer overflow is a 
SIGSEGV followed by a core dump that
includes no useful backtrace. When this occurs, it may be quicker to do a codereview looking for known problem functions than to try debugging it.
10.5.3.2 Heap Buffer Overflows
When a program overflows a heap buffer, the consequences are not always imme-
diate. When 
malloc uses an mmap call to allocate a block of memory (as it does for
large blocks), it pads the requested block size so that it is a multiple of the page size.Therefore, if the requested block size is not an integral number of pages, theamount of space allocated for the block includes extra bytes. Your code can overrun10.5 Looking for Memory Issues 571
14. www.dwheeler.com/flawfinder
the end of the block, and you may never know. Only when your code overruns into
the address beyond the end of the last page will it terminate with a SIGSEGV . The
good news is that it terminates immediately, and there is no opportunity for suchan overrun to corrupt the heap. With smaller blocks, that do not use 
mmap , the
problem can be more difficult.
With small blocks, a small overflow can go undetected as well. Most heap imple-
mentations will pad the block size so that it falls on an efficient boundary in mem-ory. This allows you to overrun a few bytes occasionally with no ill effects. Such anerror may cause a crash only sometimes. The details depend on the implementationof the standard library, the size of the block, and the size of the overflow.
When the code overflows a small block beyond end of the padding, it corrupts
the internal lists that 
malloc and free use to maintain the heap. Typically, such an
overflow isn’t detected until the next malloc or free call. To make matters more
confusing, the free call that fails need not be freeing the block that has overflowed.
If the overflow is large enough, it may extend into invalid virtual addresses, in whichcase you will get a 
SIGSEGV .
The problems of dynamic memory overflows are essentially the same for C++. At
the core of the default operators for newand delete is a conventional heap that
may even use the C library versions of malloc and free . The GNU implementa-
tion of newand delete appears to be intolerant of even a single-byte overflow,
although like C, you don’t find out about it until the delete operation. C++ allows
you to overload these operators, however. When you do this, you create new failuremodes for overflows. This is one reason why the decision to overload 
operatornew
and delete should not be taken lightly.
Several tools are available to check for heap overflows; I look at them later in this
chapter.
10.5.4 glibc Tools
The GNU standard library ( glibc ) has had built-in debugging features for
dynamic memory for a long time. Until recently, these features were turned off bydefault and enabled only by the environment variable 
MALLOC_CHECK_ . The ration-
ale for not checking the heap with each allocation and free was that it decreased effi-ciency. Some checks are inexpensive enough that recent versions of 
glibc have
some of these basic checks turned on by default.572 Chapter 10 • Debugging
10.5.4.1 Using MALLOC_CHECK_
glibc inspects the environment variable MALLOC_CHECK_ and alters its behavior as
follows:
•MALLOC_CHECK_=0 —disables all checking
•MALLOC_CHECK_=1 —prints a message to stderr when an error is detected
•MALLOC_CHECK_=2 —aborts when an error is detected; no message is printed
When MALLOC_CHECK_ is not set, older versions of glibc behave as though
MALLOC_CHECK_ were set to 0. Newer versions behave as though MALLOC_CHECK_
were set to 2. It will dump core as soon as it detects an inconsistency and print a
lengthy traceback as well. The default output is more verbose than the output youget when you set 
MALLOC_CHECK_ to 2. Here is the output from a trivial program
(not shown) I created that does a double-free and links with glibc version 2.3.6:
$ ./double-free MALLOC_CHECK_ not set
*** glibc detected *** ./double-free: double free or corruption (top): 0x0804a008 ***
======= Backtrace: =========/lib/libc.so.6[0xb7e8f1e0]/lib/libc.so.6(__libc_free+0x77)[0xb7e8f72b]./double-free[0x80483f8]/lib/libc.so.6(__libc_start_main+0xdf)[0xb7e40d7f]./double-free[0x804832d]======= Memory map: ========08048000-08049000 r-xp 00000000 fd:00 576680     /home/john/examples/ch-10/memory/double-free08049000-0804a000 rw-p 00000000 fd:00 576680     /home/john/examples/ch-10/memory/double-free
[ rest of memory map omitted ]
The memory map may seem like overkill, but recall that for large block sizes,
glibc will use mmap instead of a conventional heap. The stack trace is not as user
friendly as a gdbstack trace,15but because this is a SIGABRT , there should be a core
file to go with it. Then you can use gdbin postmortem mode to view the backtrace
as well as any variable values.
10.5.4.2 Looking for Memory Leaks with mtrace
mtrace is a tool provided with glibc that Fedora packages with the glibc-utils
package. This may not be installed by default on your system. On other distributions,10.5 Looking for Memory Issues 573
15. See also addr2line(1) from binutils .
it may be packaged similarly. The main purpose of mtrace is to look for leaks.
There are better tools for this purpose, but because this one comes as part of glibc
it’s worth mentioning.
To use the mtrace utility, you must instrument your code with the mtrace and
muntrace functions provided by glibc . In addition, you must set the environment
variable MALLOC_TRACE to the name of a file where glibc will store data for the
mtrace utility. After you run your code, data is stored in the file you specify. This
data is overwritten with each run. I’ll skip the listing here just to show you how
mtrace runs:
$ MALLOC_TRACE=foo.dat ./ex-mtrace mtrace data will be stored in foo.dat .
leaking 0x603 bytes
leaking 0x6e2 bytesleaking 0x1d8 bytesleaking 0xd9f bytesleaking 0xc3 bytesleaking 0x22f bytes$ mtrace ./ex-mtrace foo.dat
mtrace needs the name of the executable and the data.
Memory not freed:-----------------
Address     Size     Caller
0x0804a378    0x603  at /home/john/examples/ch-10/memory/ex-mtrace.c:230x0804a980    0x6e2  at /home/john/examples/ch-10/memory/ex-mtrace.c:230x0804b068    0x1d8  at /home/john/examples/ch-10/memory/ex-mtrace.c:230x0804b248    0xd9f  at /home/john/examples/ch-10/memory/ex-mtrace.c:230x0804bff0     0xc3  at /home/john/examples/ch-10/memory/ex-mtrace.c:230x0804c0b8    0x22f  at /home/john/examples/ch-10/memory/ex-mtrace.c:23
While mtrace works with C++ code, it’s not very useful. It correctly reports the
number and size of memory leaks in C++ code, but it fails to identify the line num-ber of the leak. Perhaps this is because 
mtrace follows the malloc call but not the
newcall. Because malloc is called by the C++ standard library, the return pointer
does not point to a module with debugging symbols.
10.5.4.3 Gathering Memory Statistics with memusage
To use the memusage utility, you do not need to instrument your code at all. This
utility also comes with the glibc-utils package in Fedora. It tells you how much
memory your program is using in the form of a histogram. The default output goesto the standard output and uses ASCII text to show a graphical histogram. Here isan example:574 Chapter 10 • Debugging
$ memusage awk 'BEGIN{print "Hello World"}' Show the memory usage of awk.
Hello World
Memory usage summary: heap total: 3564, heap peak: 3548, stack peak: 8604
total calls   total memory   failed calls
malloc|         28           3564              0
realloc|          0              0              0   (in place: 0, dec: 0)
calloc|          0              0              0
free|         10             48
Histogram for block sizes:
0-15             21  75% ==================================================
16-31              3  10% =======32-47              1   3% ==48-63              1   3% ==
112-127             1   3% ==
3200-3215            1   3% ==
Like all the functions in glibc-utils , memusage has no manpage and no info
page. The --help option indicates some useful features, such as the ability to trace
mmap and munmap calls in addition to malloc and free :
$ memusage --helpUsage: memusage [OPTION]... PROGRAM [PROGRAMOPTION]...Profile memory usage of PROGRAM.
-n,--progname=NAME     Name of the program file to profile
-p,--png=FILE          Generate PNG graphic and store it in FILE-d,--data=FILE         Generate binary data file and store it in FILE-u,--unbuffered        Don't buffer output-b,--buffer=SIZE       Collect SIZE entries before writing them out
--no-timer          Don't collect additional information though timer
-m,--mmap              Also trace mmap & friends
-?,--help              Print this help and exit
--usage             Give a short usage message
-V,--version           Print version information and exit
The following options only apply when generating graphical output:
-t,--time-based        Make graph linear in time-T,--total             Also draw graph of total memory use
--title=STRING      Use STRING as title of the graph
-x,--x-size=SIZE       Make graphic SIZE pixels wide-y,--y-size=SIZE       Make graphic SIZE pixels high
Mandatory arguments to long options are also mandatory for any corresponding
short options.
For bug reporting instructions, please see:
<http://www.gnu.org/software/libc/bugs.html>.10.5 Looking for Memory Issues 575
One interesting feature is the ability to graph the output into a PNG file.16An
example of this is shown in Figure 10-2.
Finally, the memusagestat utility produces a PNG file from the data produced
with the doption of memusage . These tools appear to be works in progress.
10.5.5 Using Valgrind to Debug Memory Issues
In Chapter 9 I used Valgrind17to demonstrate its ability to debug cache issues, but
it’s more commonly used for debugging memory issues. This is in fact the default576 Chapter 10 • Debugging
16.PNG stands for Portable Network Graphics, an open format for sharing images (www.libpng.org).
17. www.valgrind.orgFIGURE 10-2 Graphical Output from memusage

option for the valgrind command if you do not specify the --tool option.
Calling valgrind with no arguments is equivalent to the following command:
$ valgrind --tool=memcheck ./myprog
The advantage of Valgrind is that you do not need to instrument your code to
debug your application. The price you pay for this is performance. Valgrind causesyour code’s performance to drop dramatically. The other disadvantage of using atool like Valgrind is that some results do not show up until the program exits.Specifically, leaks by nature cannot always be detected until the program exits.
10.5.5.1 Using Valgrind to Detect Leaks
With no arguments, the 
valgrind command will print a summary of what it
believes are leaks when the program exits. To get more details, use the 
--leakcheck=full option. Listing 10-7 contains examples of two types of mem-
ory leaks that Valgrind looks for.
LISTING 10-7 leaky.c: Leak Example
1 #include <stdio.h>
2 #include <string.h>3 #include <stdlib.h>45 char *possible_leak(int x)6 {7     // Static pointer - valgrind doesn't know if this is needed.8     static char *lp;9     char *p = (char *) malloc(x);
10     lp = p + x / 2;11     return p;12 }1314 int main(int argc, char *argv[])15 {16     // Definitely lost.17     char *p1 = malloc(0x1000);1819     // Possibly lost20     char *p2 = possible_leak(0x1000);21     return 0;
22 }10.5 Looking for Memory Issues 577
The first type of leak that Valgrind reports is a simple leak. In Listing 10-7,
pointer p1is allocated and discarded without being freed. Because p1goes out of
scope, the block is definitely leaked. The second allocation occurs inside the func-
tion possible_leak . This time, I included a static pointer that points to some-
where in the middle of the most recently allocated block. This could be a value thatwill be used on the next call, or it could just be an oversight in programming.Valgrind can’t tell the difference, so it reports it as possibly leaked:
$ valgrind --quiet --leak-check=full ./leaky
==22309====22309== 4,096 bytes in 1 blocks are possibly lost in loss record 1 of 2==22309==    at 0x40044C9: malloc (vg_replace_malloc.c:149)==22309==    by 0x804838D: possible_leak (leaky.c:9)==22309==    by 0x80483E8: main (leaky.c:20)==22309====22309====22309== 4,096 bytes in 1 blocks are definitely lost in loss record 2 of 2==22309==    at 0x40044C9: malloc (vg_replace_malloc.c:149)==22309==    by 0x80483D5: main (leaky.c:17)
Notice that the possible leak points the finger at the possible_leak function as
the source of the leak. When you encounter this in your code, now you’ll knowwhat to look for.
10.5.5.2 Looking for Memory Corruption with Valgrind
Valgrind is capable of looking for heap memory corruption as well as memory leaks.
Specifically, Valgrind can detect single-byte overruns that might go unnoticed by
glibc . Listing 10-8 shows a program with a single-byte overflow.
LISTING 10-8 new-corrupt.cpp: Example of Heap Corruption in a C++ Program
1 #include <string.h>
23 int main(int argc, char *argv[])4 {5     int *ptr = new int;6     memset(ptr, 0, sizeof(int) + 1); // One byte overflow7     delete ptr;
8 }
Running this program with the valgrind command uncovers this flaw:
$ valgrind --quiet ./new-corrupt==14780== Invalid write of size 1578 Chapter 10 • Debugging
==14780==    at 0x80484B9: main (new-corrupt.cpp:6)
==14780==  Address 0x402E02C is 0 bytes after a block of size 4 alloc'd==14780==    at 0x4004888: operator new(unsigned) (vg_replace_malloc.c:163)==14780==    by 0x80484A9: main (new-corrupt.cpp:5)
It’s important to point out that the overflows that Valgrind detects are in heap
only. There is no way for Valgrind to detect overflows of stack or static memory.
10.5.5.3 Heap Analysis with Massif
The massif tool that comes with Valgrind is useful for showing a summary of
heap usage by function. To illustrate this, I need an example. The program inListing 10-9, called 
funalloc, contains two functions that simply leak memory.
LISTING 10-9 funalloc.c: Memory Allocation Functions
1 #include <stdio.h>
2 #include <string.h>3 #include <stdlib.h>4 #include <unistd.h>56 void func1(void)7 {8     malloc(1024);               // Deliberate leak9 }
1011 void func2(void)12 {13     malloc(1024);               // Deliberate leak14 }1516 int main(int argc, char *argv[])17 {18     srand(0);19     int i;20     for (i = 0; i < 256; i++) {21         int r = rand() % 100;22         if (r > 75) {23             func2();            // Called about 25% of the time24         }25         else {26             func1();            // Called about 75% of the time27         }28         // Space out the samples on the graph.29         usleep(1);30     }
31 }10.5 Looking for Memory Issues 579
The funalloc program uses a pseudorandom sequence to make it unpre-
dictable, but statistically it will call func1 about 75 percent of the time. Because
both functions allocate the same amount of memory, you should see that func1
accounts for about 75 percent of the heap in use.
To run the example, use the following command line:
$ valgrind --tool=massif ./funalloc
This creates two files: massif.PID.ps and massif.PID.txt . The text file
shows what you already knew:
Command: ./funalloc
== 0 ===========================
Heap allocation functions accounted for 97.9% of measured spacetime
Called from:
73.5% : 0x8048432: func1 (funalloc.c:8)
24.4% : 0x804844A: func2 (funalloc.c:13)
(additional data deleted)
The graph is also interesting to look at and is shown in Figure 10-3. Each color
represents one of the functions in the program. The height of the graph representsthe total heap in use. Dwarfed by the heap allocations is the stack usage, which canbe especially important in multithreaded applications.
10.5.5.4 Valgrind Final Thoughts
With all the Valgrind tools, line number information is available only for modules
compiled with debugging. Without debugging, you still get a summary of errors. Inmany cases, it’s preferable to redirect the output to a log file for viewing in an edi-tor. The option for this is 
--log-file .
Valgrind is a big gun. I only scratched the surface of what Valgrind is capable of.
In some cases, the performance penalties of Valgrind may make you want to leaveit on the shelf. Other tools are faster and provide rough answers, but for fine-grained details, few tools compare with Valgrind.
In addition to 
memcheck , massif , and cachegrind , you can use callgrind
and helgrind . callgrind is a function profiler similar to gprof , although it com-
bines some of the features of cachegrind as well. helgrind claims to look for data
race conditions in multithreaded programs. These tools are too complex to cover inthis book, but you can explore them on your own system.580 Chapter 10 • Debugging
10.5.6 Looking for Overflows with Electric Fence
Electric Fence uses some clever techniques to find overflows in heap memory when
they happen, unlike glibc , which can detect overflows only after the fact.
Although Valgrind does this as well, Electric Fence uses the Memory ManagementUnit (MMU) to trap the offending code. Because the MMU does the real work, theperformance penalty of using Electric Fence is minimal.
You do not need to instrument your code to use Electric Fence. Instead, it pro-
vides a dynamic library that implements alternative versions of the dynamic alloca-tion functions. A wrapper script called 
efis provided to take care of the necessary
LD_PRELOAD environment variable setting. As with Valgrind, all you do is invoke
your process with the efcommand. You can run the new-corrupt program in
Listing 10-8 with Electric Fence as follows:
$ ef ./new-corrupt
Electric Fence 2.2.0 Copyright (C) 1987-1999 Bruce Perens <bruce@perens.com>
/usr/bin/ef: line 20: 23227 Segmentation fault      (core dumped) ( exportLD_PRELOAD=libefence.so.0.0; exec $* )10.5 Looking for Memory Issues 581
FIGURE 10-3 Heap Usage by Function Created by Massif./funalloc 739,284,724 bytes x ms
ms 0.0 500.0 1000.0 1500.0 2000.0 2500.0 3000.0 3500.0 4000.0 4500.0 5000.0 5500.0 bytes
0k20k40k60k80k100k120k140k160k180k200k220k
stack(s)x804844B:func2x8048433:func1
Then you can bring up gdbin postmortem mode to get an accurate backtrace of
where the overflow occurred. If you don’t want to use a core file, you can run yourprogram in 
gdbwith Electric Fence in either of two ways. One way is to link your
application with the static library that comes with Electric Fence, as follows:
$ g++ -g -o new-corrupt new-corrupt.cpp -lefence libefence.a is linked in.
Otherwise, you can use dynamic libraries from within gdb by setting the
LD_PRELOAD environment variable inside the gdbcommand shell, as follows:
$ gdb ./new-corrupt
...(gdb) set environment LD_PRELOAD libefence.so(gdb) runStarting program: /home/john/examples/ch-10/memory/new-corrupt
Electric Fence 2.2.0 Copyright (C) 1987-1999 Bruce Perens <bruce@perens.com>
Reading symbols from shared object read from target memory...done.Loaded system supplied DSO at 0xffffe000
Electric Fence 2.2.0 Copyright (C) 1987-1999 Bruce Perens <bruce@perens.com>
Program received signal SIGSEGV, Segmentation fault.
0x080484b9 in main (argc=1, argv=0xbfb8a404) at new-corrupt.cpp:77               memset(ptr,0,sizeof(int)+1);(gdb)
This prevents the core file that you otherwise would get when you run this pro-
gram. Intuitively, you might be inclined to run gdbunder Electric Fence with the
efcommand. Actually, this does not work in version 2.2.2 of Electric Fence
because of the way the efscript is written and because gdb6.3 contains mallocs
of 0 bytes:
$ ef gdb ./new-corrupt
Electric Fence 2.2.0 Copyright (C) 1987-1999 Bruce Perens <bruce@perens.com>
ElectricFence Aborting: Allocating 0 bytes, probably a bug.
/usr/bin/ef: line 20: 23307 Illegal instruction     (core dumped) ( exportLD_PRELOAD=libefence.so.0.0; exec $* )
This can be prevented by setting the environment variable EF_ALLOW_MALLOC_0 ,
which tells libefence to relax about 0-byte allocations. You can set several other
environment variables to alter the behavior of Electric Fence, as described in the man
page efence(3) .582 Chapter 10 • Debugging
Another feature of Electric Fence is the ability to detect underruns as well as
overruns. An underrun occurs when a process writes to an address preceding ablock of memory. This type of error can happen with pointer arithmetic, such asthe following:
char *buf = malloc(1024);
...char *ptr = buf + 10;
It’s poor style, but you can use negative indexes with ptr.
...*(ptr - 11) = '\0';
You asked for it: an underrun!
To detect this underrun with Electric Fence, you must set the environment vari-
able EF_PROTECT_BELOW :
$ EF_PROTECT_BELOW=1 ef ./underrun
Electric Fence 2.2.0 Copyright (C) 1987-1999 Bruce Perens <bruce@perens.com>
/usr/bin/ef: line 20:  4644 Segmentation fault      (core dumped) ( exportLD_PRELOAD=libefence.so.0.0; exec $* )
The complete example is not shown, but the debugging mechanism is exactly the
same as before. The error causes a SIGSEGV , which leads to the line of code that
generated the error.
Electric Fence works by allocating an extra read-only page after each allocated
block for the purpose of causing a SIGSEGV when the code overruns. Because it allo-
cates an extra page for every block, regardless of its size, the library can make yourcode use much more memory than normal. Applications that allocate many smallblocks will see their heap usage increase dramatically.
Another problem occurs because of block alignment in the allocation libraries. It
is very difficult to detect a single-byte overrun under some circumstances, such asexample in Listing 10-4. Electric Fence fails to detect this overrun as well.
Finally, the only mechanism Electric Fence uses to inform you of errors is a
SIGSEGV , which is not very informative. So although you may find a problem by
using Electric Fence, you may need to use another tool to understand the problem.Nevertheless, Electric Fence provides a quick-and-dirty way to check your code forserious errors.
10.6 Unconventional Techniques
With all the tools available for debugging, there are still occasions when unconven-tional techniques are called for. For some applications, running under a debugger istoo difficult, or using replacement libraries causes problems.10.6 Unconventional Techniques 583
10.6.1 Creating Your Own Black Box
You probably are familiar with the so-called black box in commercial airliners. This
is the device that crash investigators search for to determine the cause of an acci-dent. It contains a history of measurements from some time in the past up to thepoint of the crash. By examining the history leading up to the crash, it may be pos-sible to determine the cause.
You can create the software equivalent of a black box for use with your applica-
tions. There are advantages to using this technique instead of a debugger:
• It gives you fine-grained control over what gets logged and what doesn’t. In
this way, you can preserve performance while keeping some debugging infor-mation available.
• You can use this technique with optimized executables; you don’t necessarily
need to compile with debugging.
• This technique can be especially effective when you are trying to debug a stack
overflow. Recall that a stack overflow typically causes your traceback informa-tion to be invalid, making debugging almost useless.
Listing 10-10 contains a complete example of a program that creates a black box
in the form of a trace buffer (the more common term for this technique when used
in software).
LISTING 10-10 trace-buffer.c: A Complete Example Using a Software Black Box
1 #include <stdio.h>
2 #include <string.h>3 #include <stdlib.h>4 #include <stdarg.h>56 // Global message buffer. Give it a name that's easy to remember.7 // Pick a size that works for you.8 char tracebuf[4096] = "";9 char *mstart = tracebuf;
1011 // Prototype for our printf-like function. We use the GNU __attribute__12 // directive to include format checking for free.13 int dbgprintf(const char *fmt, ...)14     __attribute__ ((__format__(__printf__, 1, 2)));1516 // Printf-like function sends data to the trace buffer.584 Chapter 10 • Debugging
17 int dbgprintf(const char *fmt, ...)
18 {19     int n = 0;2021     // ref. stdarg(3)22     va_list ap;23     va_start(ap, fmt);2425     // Number of chars available for snprintf26     int nchars = sizeof(tracebuf) - (mstart - tracebuf);2728     if (nchars <= 2) {29         // Circular buffer.30         mstart = tracebuf;31         nchars = sizeof(tracebuf);32     }3334     // Write the message to the buffer35     n = vsnprintf(mstart, nchars, fmt, ap);36     mstart += n + 1;3738     va_end(ap);39     return n;40 }4142 int defective(int x)43 {44     int y = 1;45     dbgprintf("defective(%u)", x);46     if (x == 10) {47         dbgprintf("time to corrupt the stack!");4849         // Overflow the stack by 128 bytes (that should be enough).50         memset(&y, 0xa5, sizeof(y) + 128);5152         // Most likely won't die until we try to return.53         dbgprintf("I'm still here; returning now.");54         return 0;55     }5657     return defective(x + 1);58 }5960 int main(int argc, char *argv[])61 {62     defective(1);63     dbgprintf("exiting...");64     return 0;
65 }10.6 Unconventional Techniques 585
The function dbgprintf is a printf -like function that writes to a block of
global memory instead of writing to the standard output. The memory is used as acircular buffer so that when the buffer is full, the oldest messages are overwritten bythe newest messages. The amount of history you get is determined by the size of thememory you allocate.
The rest of the program contains a single function that will overflow the stack
under the right conditions. It’s a recursive function that increments its input param-eter and calls itself with the new value. When this value reaches 
10, the program
will overflow the stack. The result, as you might expect, is a core file. As you alsomight expect, the core file does not contain a usable backtrace. For example:
$ cc -g -o trace-buffer trace-buffer.c
./trace-bufferSegmentation fault (core dumped)$ gdb ./trace-buffer core.25347...#0  0xa5a5a5a5 in ?? ()
Senseless address is the first sign of trouble!
(gdb) bt Try a backtrace and see what you get...
#0  0xa5a5a5a5 in ?? ()#1  0xa5a5a5a5 in ?? ()...
More of the same
#22 0xa5a5a5a5 in ?? ()---Type <return> to continue, or q <return> to quit---qQuit(gdb) x/15s &tracebuf
Look at the first 15 messages in tracebuf .
0x8049720 <tracebuf>:    "defective(1)"0x804972d <tracebuf+13>:         "defective(2)"0x804973a <tracebuf+26>:         "defective(3)"0x8049747 <tracebuf+39>:         "defective(4)"0x8049754 <tracebuf+52>:         "defective(5)"0x8049761 <tracebuf+65>:         "defective(6)"0x804976e <tracebuf+78>:         "defective(7)"0x804977b <tracebuf+91>:         "defective(8)"0x8049788 <tracebuf+104>:        "defective(9)"0x8049795 <tracebuf+117>:        "defective(10)"0x80497a3 <tracebuf+131>:        "time to corrupt the stack!"0x80497be <tracebuf+158>:        "I'm still here; returning now."0x80497dd <tracebuf+189>:        ""0x80497de <tracebuf+190>:        ""0x80497df <tracebuf+191>:        ""(gdb)
In this example, you can see that the stack is useless thanks to the defective
function, but the global data is intact. You take advantage of this by looking at the
trace buffer with gdb. The messages give you a history of what took place before the586 Chapter 10 • Debugging
crash. There’s no guarantee that this is going to tell you what you need to know, but
you can always modify the content to help. This also works in code compiled foroptimization. You don’t necessarily need the debugger, either; you can use the
strings command to dump all the ASCII strings from the core file and see these
messages in the order in which they were written.
The implementation in Listing 10-10 is not perfect, but it is simple. When the
buffer is full, for example, the last message will be truncated. If you never fill thebuffer, this is not a problem. If you do fill the buffer, you probably will truncate onemessage. Making it more robust than this is left as an exercise.
10.6.2 Getting Backtraces at Runtime
The simplest way to get a backtrace of your program is to use gdb. The drawback
here is that attaching to a running process with the debugger requires it to stop theprocess, type some commands, and restart the process. This stop time can be quiteinvasive if you need to do it often. 
gdbcomes with an undocumented script called
gstack , which does this for you and cuts down the time required to get a trace-
back. To get the traceback for a running process, the syntax is
$ gstack pid All you supply is the pid.
The advantage of gstack is that it requires much less time to execute than it
would take you to type interactively in gdb. gstack prints only the function names
in the stack trace, so your executable need not be compiled with debug to get use-
ful output.
There are times when you might want to include backtrace information in your
program output, such as in an error handler. In this case, you do not want to relyon the programmer to enter any commands; you want to produce the backtraceautomatically. 
glibc provides some unique functions for this purpose. The
backtrace18function gives you an array of pointers that are the program counters
pulled off the stack. The prototype for backtrace is as follows:
int backtrace (void **BUFFER, int SIZE)10.6 Unconventional Techniques 587
18. Reference: info libc backtrace .
By themselves, the pointers don’t help much. You probably want to see symbols.
glibc provides a function called backtrace_symbols that ostensibly translates
the pointers from backtrace into symbols. Unfortunately, backtrace_symbols
appears only to lookup symbols in libc —that is, it does not appear to be intended
for general usage. No problem, though; you can use the addr2line utility to trans-
late the output into line numbers, provided that your executable is compiled withdebug. Even without 
debug , you can still see function names. An example of a
traceback function that calls addr2line could look like the following:
void print_trace(void)
{
int i;const int NTRACE = 32;      // Maximum call stack depth.void *traceback[NTRACE];    // Will hold instruction pointers
// Use a shell command to lookup addresses.
// Ref. addr2line(1), which is part of binutils.char cmd[128] = "addr2line -f -e ";
// Use the symlink /proc/self/exe to get the name of the executable.
// This prevents us from having to keep a copy of argv[0] somewhere.char *prog = cmd + strlen(cmd);int r = readlink("/proc/self/exe",prog,sizeof(cmd)-(prog-cmd)-1);
// ... error checking omitted for brevity// Run the shell command - ref. popen(3)
FILE *fp = popen(cmd, "w");
// ... error checking omitted for brevityint depth = backtrace(traceback, NTRACE);for (i = 0; i < depth; i++) {
// Send the addresses to addr2line.// addr2line prints to stdout.fprintf(fp, "%p\n", traceback[i]);
// ... error checking omitted for brevity
}fclose(fp);
}
addr2line accepts pointers from the standard input, which is what you take
advantage of here. You must specify the executable with the -eoption. In this case,
I took the path of least resistance by reading the symlink in /proc/self/exe to588 Chapter 10 • Debugging
keep the listing short. With no options, addr2line attempts to produce line-
number information for each pointer, which is available only when the executable iscompiled with 
debug . Without debugging information, the information you get is
useless. That is why I include the -foption in the command, which prints the clos-
est function to each pointer. This should be valid even with debugging turned on.19
10.6.3 Forcing Core Dumps
It seems paradoxical that you should want your code to generate a core file, but forhard-to-catch bugs, this can be very useful. One such situation might be a regres-sion test that runs overnight. In this case, you may not be able to predict what willfail, but if something does fail, you want as much information as possible. The corefile will provide you as much information as you can get.
Recall that most new distributions disable core files by default. To enable core-
file generation in 
bash , use the following command:
$ ulimit -c unlimited
The system limit can most likely be found in /etc/rc.local , but as I discussed
in Chapter 6, there are good reasons to disable core files by default.
10.6.3.1 abort and assert
These two functions will terminate your program and generate a core file.
Specifically, they both work by generating a SIGABRT signal. Calling abort is equiv-
alent to calling
raise(SIGABRT);
You should use this when you believe that the program state is unknown (per-
haps because of some memory overflows) and it is unwise to continue. You couldjust as well call 
exit , but that wouldn’t leave a core file for you to examine. The
typical pattern for using abort looks like this:
if ( ! program is sane ) abort();
With that in mind, ANSI C defines the assert macro, which wraps this up into
a single line. assert takes a single argument, which is evaluated as a Boolean10.6 Unconventional Techniques 589
19. Beware: Optimization can alter your call stack in unexpected ways.
expression. When the expression is false, it calls abort . The output includes the line
number and expression that failed:
assert(!insane);
When insane is true, the output looks like this:
$./foo
assert: foo.c:7: main: Assertion '!insane' failed.Aborted (core dumped)
You can disable assert by compiling your code with the preprocessor macro
NDEBUG defined. When assert is disabled in this way, the expression inside the
parentheses is discarded before the compiler sees it. This could lead to side effectscaused by turning off 
assert . Do yourself a favor: Keep your assertions simple.
10.6.3.2 Using gcore
Normally, a core dump is produced by the kernel as the result of an abnormal ter-
mination. gdballows you to dump a core file without terminating the process. The
command for this in gdbis gcore . When you execute the gcore command from
gdb, it writes a core file to a file named core.pid or any name you choose. gdb
comes with a shell script named gcore , which attaches to a process with gdband
dumps the core using the gcore command; then it disconnects and allows the
process to continue.
gcore takes one or more process IDs as arguments and allows you to change the
core filename with the -ooption.
10.6.4 Using Signals
A common use for signals is to dump some debugging information when a partic-ular signal is received. You can, for example, force a program to dump core by send-ing it a core-generating signal such as 
SIGBART .20This way, you can exert control
from the shell without necessarily instrumenting your code. More proactively, youcan create a signal handler that responds to one of the user-defined signals590 Chapter 10 • Debugging
20. You actually can send any core -generating signal, such as SIGSEGV or SIGBUS , and the process will ter-
minate with a core dump.
(SIGUSR1 , SIGUSR2 ), prints some debugging information at the time of the signal,
and allows the program to continue to run.
Another idea is to facilitate just-in-time debugging by using SIGSTOP and
SIGCONT . In some examples in this book, I used SIGSTOP to pause the process
and allow you to examine the process state before it terminated. This is as simpleas adding the following line to your code:
raise(SIGSTOP);
This is equivalent to pressing Ctrl+Z for a foreground process in the shell. You
can stop a background process by sending it SIGSTOP from the shell as follows:
$ kill -STOP pid
After you stop a process, you must send it a SIGCONT to allow it to continue run-
ning. From the shell, this is just as simple:
$ kill -CONT pid
While the process is stopped, you can examine the state of the process without
any inherent race conditions—that is, you can look at things without them chang-ing from underneath you. In particular, you probably will be interested in the
/proc file system.
10.6.5 Using procfs for Debugging
Recall that procfs refers to the file-system driver that is used to report process
information and is mounted on /proc . This file system contains a great deal of sys-
tem information, but it also contains one directory for each running process. Eachsubdirectory is named for the process ID and contains the process-specific infor-mation. One exception is 
/proc/self , which is a symbolic link to the process
directory of the currently running process. More precisely, when a process opensfiles in 
/proc/self , it actually is opening files in its own process-specific directory.
Most of the information about process state is available from the pscommand
and from commands in the procps package. You are strongly encouraged to use
these commands whenever possible, but a few things that are found uniquely in the
/proc file system, and they can be invaluable for debugging.10.6 Unconventional Techniques 591
A running process is a moving target, of course, and when using values from
procfs , you must keep this in mind. It is always preferable to make sure that the
process is in a nonrunning state when you are examining values. The process doesnot need to be stopped; it can just as easily be sleeping or blocking in a system call.As long as it is not executing when you look at the data, you can trust the data thatyou see.
10.6.5.1 Memory Maps
Earlier in the book, I introduced the useful 
pmap command for looking at a
process’s memory map. This can be helpful in tracking down memory leaks, whichusually lead to fragmentation. In particular, when 
malloc uses mmap calls to allo-
cate blocks, these blocks can be visible in the memory map. The blocks may map
directly to allocated blocks in your program.
The raw unfiltered data that pmap gets comes from /proc/PID/maps . In most
cases, the pmap command is preferable. For details on the procfs maps format,
consult proc(5) .
10.6.5.2 Process Environment
Inside each directory is useful information about the process environment, includ-
ing the environment variables as well as the current working directory and file-name of the executable. Environment variables are stored in 
/proc/PID/environ
as a flat array of characters with NULcharacters separating each environment vari-
able. The easiest way to look at this from the shell is with the strings command,
as follows:
$ strings -n1 /proc/ PID/environ
MANPATH=:/home/john/usr/share/manHOSTNAME=redhatTERM=xtermSHELL=/bin/bashHISTSIZE=1000 ...
Output continues.
This file is read-only, which means that you cannot modify the environment of
a running process from the shell. I use the -n1option to the strings command
because by default, strings will filter out strings fewer than four characters in
length. Here, I tell it to show strings as short as one character—that is, all strings.592 Chapter 10 • Debugging
Another useful file is /proc/cmdline , which stores the process’s command line
in the same format. This allows you to see exactly what the process’s argv vector
looked like when it was executed. For example:
$ strings -n1 /proc/self/cmdline | cat -n
1  strings Contents of argv[0]
2  -n1 Contents of argv[1]
3  /proc/self/cmdline Contents of argv[2]
A little mental arithmetic is required, because line 1 contains argv[0] , but you
get the idea.
10.6.5.3 Open Files
The /proc directory of the process also tells you about open files and directories.
/proc/PID/exe contains a symbolic link to the executable file that was used to
launch the process. For example:
$ ls -l /proc/self/exe
lrwxrwxrwx  1 john john 0 Jul 30 11:53 /proc/self/exe -> /bin/ls
Similarly, /proc/PID/cwd is a symbolic link to the current working directory of
the process. This is useful when connecting to a server process that is writing datato its current working directory. It may not tell you where that is; you can use
procfs to find out for yourself.
Finally, you can see every file a process has open by looking at its file descriptors
in /proc/PID/fd . This is a subdirectory that contains a symbolic link for each open
file descriptor. The name of the link is the pathname of the open file if the file resideson a file system. If the file is a socket or pipe, a 
readlink of the file typically con-
tains some textual indication of the nature of the file descriptor. For example:
$ ls -l /proc/self/fd | tee /dev/null
total 4lrwx------  1 john john 64 Jul 30 12:00 0 -> /dev/pts/3l-wx------  1 john john 64 Jul 30 12:00 1 -> pipe:[33209]lrwx------  1 john john 64 Jul 30 12:00 2 -> /dev/pts/3lr-x------  1 john john 64 Jul 30 12:00 3 -> /proc/5487/fd
Note that file descriptors 0and 2(standard input and standard error, respec-
tively) point to the pseudoterminal device. The standard output is piped to the tee
command, so file descriptor 1shows up as pipe:[33209 ]. This is not a file you can10.6 Unconventional Techniques 593
open, but it does tell you that this is a pipe and that 33209 happens to be the inode
number of this pipe. File descriptor 3is an open directory. This is actually
/proc/self/fd , which in this case points to /proc/5487/fd .
In Chapter 6, I introduced the tools lsof and fuser , both of which use this
information extensively. Which tool you prefer depends on the circumstances, butsometimes, a simple 
lscommand in /proc tells you everything you need to know.
10.7 Summary
This chapter covered some of the tools and issues involved with debugging usercode. I covered several techniques for using 
printf to debug your code, and I also
looked at some of the undesirable side effects of this debugging technique.
Also in this chapter, I took a detailed look at the GNU debugger ( gdb) and cov-
ered some basic and not-so-basic usage. I demonstrated how to make most efficientuse of 
gdbin circumstances where otherwise, it may slow your code drastically. I
looked at some issues unique to debugging C++ code with gdb.
This chapter also looked at some of the unique problems associated with shared
objects, which can be challenging to debug. I looked at how to run a process tem-porarily with a specific shared object that may be under development without hav-ing to replace the systemwide copy of that object. I also demonstrated how to dothis using 
gdb.
Finally, I demonstrated several tools for resolving memory issues in your code,
such as memory overflows and corrupted stacks. I looked at some popular tools aswell as a few unconventional techniques.
10.7.1 Tools Used in This Chapter
• Electric Fence—a tool to look for memory overruns and underruns that uses
the MMU
•gdb—the GNU debugger; used for interactive debugging of a process
•mtrace —part of glibc that allows you to look for memory leaks after your
code has run
• Valgrind—a powerful tool that can look for memory leaks, memory corrup-
tion, and many other things594 Chapter 10 • Debugging
10.7.2 Online Resources
• http://duma.sourceforge.net—a fork of Electric Fence
• http://perens.com/FreeSoftware/ElectricFence—the original home of Electric
Fence
• www.valgrind.org—the home page of the Valgrind project
10.7.3 References
• Robbins, A. GDB Pocket Reference. Sebastopol, Calif.: O’Reilly Media, Inc.,
2005.10.7 Summary 595
This page intentionally left blank 
A
a command, vi, 151
.a extension, 73a modifier, print and x, gdb, 543ABI (Application Binary Interface)
incompatible, 97–98for system calls, 225
abort function, 589–590accept function, 388, 391Acrobat Reader, 131Add/remove feature in revision control, 191addr2line utility, 588–589Adopted processes, 243Advanced Programmable Interrupt Controller
(APIC), 462–463
Advisory locks, 362Affinity masks, 507–509aic79xx module, 137–138alarm function, 237all target, 77Ampersands (&) in Ex mode, 159Analyzing code
cflow for, 207–210compiler for, 214–216ctags for, 210
Anonymous mapping, 73, 302, 308Anticipatory I/O scheduler, 284–285API (Application Programming Interface), 225APIC (Advanced Programmable Interrupt
Controller), 462–463
Apostrophes (‘)
Ex mode, 159vi, 149–150Application Binary Interface (ABI)
incompatible, 97–98for system calls, 225
Application performance, 475
gcov, 491–494gprof, 488–491instruction efficiency, 480–484ltrace, 484–485OProfile, 494–501processor architecture, 476–480strace, 485–487time, 475–476
Application Programming Interface (API), 225Application timing, 255–256apropos command, 118, 122apt-cache command, 35apt-get command, 35apt-key command, 35apt-setup command, 35Apt tool, 34–35ar files, 5, 12Archive files, 4–5
extracting, 11identifying, 6–7querying, 7–11
as scheduler, 285ascii man page, 113Assembly language
inline, 292in linker process, 68system calls in, 224–225
assert function, 589–590astyle artistic style, 206–207attach command, gdb, 553Index
597
598 Index
Attaching running processes, 553
__attribute__ directive, 524–525Authentication, 19–21
digital signatures for, 21–25problems in, 25–26
Autocompletion feature, 145
Kate, 183vi and Vim, 147, 169
autoconf script, 82Autoindentation
Emacs, 171Vim, 167
automake tool, 77Automatic indenting, 145Automatic storage, 338Automatic variables, 57–58auxv files, 344awatch watchpoints, 540awk script, 322
B
b command, vi, 150b suffix, x, gdb, 545Background processes, 244Backticks (`), 49, 60–61backtrace command, gdb, 542, 555backtrace_symbols function, 588Backtraces, 587–589basename function, make, 60basic.S listing, 224bcopy function, 112Beautifiers and browsers, source code,
203–204
astyle, 206–207cflow, 207–210compiler, 214–216cscope, 211ctags, 210Doxygen, 212–213indent code, 204–206
Big Endian byte order, 424, 426Binaries, miscellaneous, 324–326Binary packages, 14–16Binary semaphores, 403—bindir option, 75binutils package, 569Black box for debugging, 584–587Block devices, 257–258, 274–276Blocked signals, 373Blocking
in process scheduler, 228–229timing with, 248–249
Bookmarks, 175boot man page, 113bootparam man page, 113Braces ({})
matching, 144, 182missing, 93–94variables, 52
Branching in revision control, 192break command, gdb, 531, 558breakpoint command, gdb, 532Breakpoints
conditional, 534–535gdb, 531–532setting, 535–537syntax, 532–534and templates, 548–549
brk system call, 310Browsing and building code
Emacs, 175–176Vim, 164–166
Brute-force man page matching, 110bsd autoindentation style, 171BSD syntax, 435Buffers
Emacs, 173file streams, 516–517file systems, 517–519I/O devices, 276–280overflows, 555–556, 571–572stack, 338text editor, 145trace, 584–587
Index 599
Builds, 41–42
alternative approaches, 43–44background, 42documentation, 135–136errors and warnings, 78
build stage, 83–86compiler errors, 86–88compiler warnings, 88–98configure stage, 82–83linker errors, 98–100Makefile mistakes, 79–82
GNU tools, 43, 74–77imake, 42–43libraries for, 69–73linking, 67–69make program. Seemake program and
Makefiles
process, 74
Bus contention, 459–461Bus error (SIGBUS) signal, 554bvi editor, 180, 421–422bzip2 tool, 4
C
C command
Emacs, 173vi, 153
C language
compiler warnings, 93–94file streams, 516–517preprocessor, 81
C++ language
breakpoints, 535–537compiler warnings, 95–98STL, 549–551and templates, 548–549
c modifier, print and x, gdb, 543cache-miss.c listing, 478–480Cache misses, 458, 481, 483cachegrind tool, 480–481Caches
CPU, 291–295I/O devices, 276–280memory, 295–296multiprocessor performance, 504
call command, gdb, 547call function, make, 63callgrind tool, 580cb filter, 204cfile command, Vim, 166CFLAGS, 66cflow command, 207–210cfq scheduler, 285–286Change commands, vi, 153Character devices, 257–258, 273–274charsets man page, 113Check in in revision control, 191Check out in revision control, 191–192chrt command, 236–237CISC (Complex Instruction Set Computer)
processors, 501–502
Clean blocks, 280clean target, 77Client/server example
local sockets, 387–392network sockets, 392–393
client_un.c listing, 390–392clock function, 249–251clock_getres function, 253clock_gettime function, 251–252CLOCK_ IDs, 252–253clock_t type, 249–251Clock ticks, 249, 253–255CLOCKS_PER_SEC macro, 249clone system call, 320Closing
device drivers, 257POSIX message queues, 398
cmdline file, in /proc, 344, 593cnext command, Vim, 166Code. SeeSource code
Coherency, memory, 295–296Coloring, stack, 339
600 Index
Command mode, vi, 147–148
Communication, interprocess. SeeInterprocess
communication (IPC)
Compatibility
ABI, 97–98text editor, 146
Compiler
code analysis, 214–216errors, 86–88warnings, 88–89
C, 93–94C++, 95–98format, 91–93, 525–526implicit declarations, 89–91
Complete Fair Queuing I/O scheduler (CFQ),
285
Complex Instruction Set Computer (CISC)
processors, 501–502
Concurrent Version System (CVS), 190–193,
201
Conditional breakpoints, 532, 534–535Conditional constructs, 64–66configure.in file, 77configure script
build process, 74–77errors in, 82–83
Conflicts in revision control, 201CONFORMING TO man section, 112Cons project, 44Containers, 549Contention, bus, 459–461continue command, gdb, 532, 558control.tar.gz file, 32Cooperative multitasking, 227Copy on write technique, 319–320Core dumps, 589–591core-file command, gdb, 554Core files, debugging, 553–557coreutils info page, 119Corruption
memory, 578–579stack, 555–556
Counters, semaphore, 402–403cpio files, 5, 8, 11cpp info page, 119CPPFLAGS variable, 82–83cppsym.c listing, 536cprev command, Vim, 166CPU
cache, 291–295multicore, 505–506multiprocessing, 456–458parallelism, 501–504utilization
affinity masks for, 507–509vs. efficiency, 458–459vmstat for, 469
CPU_ macros, 508cpu_idle function, 458cpu_set_t type, 508–509crazy-malloc.c listing, 303–304crazy-malloc2.c listing, 308–310crazy-malloc3.c listing, 312–315Cross-references
help, 121–122man pages, 111
cscope tool, 211ctags command, 164–166, 210Ctrl key
vi, 151, 154Vim, 161, 165
Cursor movement commands
Emacs, 173vi, 149–151
Cut commands
Emacs, 174vi, 155–158
CVS (Concurrent Version System), 190–193,
201
cwd file, in /proc, 344, 593
D
d command
Ex mode, 159, 161vi, 155
d modifier, print and x, gdb, 543
Index 601
-d option, GCC, 214–215
data.tar.gz file, 32—datadir option, 75dbgprintf function, 586dd command, 246–247, 475–476deadline scheduler, 284–285Debian distribution
contents, 28–33package format for, 14updating, 34–35
Debugging, 513–514
backtraces for, 587–589black box for, 584–587core dumps, 589–591GNU. Seegdb debugger
IPC. SeeInterprocess communication (IPC)
memory issues. SeeMemory
preprocessor macros, 214–216printf for. Seeprintf function
procfs for, 416, 591–594shared objects. SeeShared objects
shell commands, 85–86signals for, 590–591
Declaration warnings, 89–91Decrypted passwords in memory, 343Default editors, 143–144Default shared object locations, 564–565#define directive, 520–521delete command
Ex mode, 160gdb, 532
Delete commands
Emacs, 174vi, 155–158
delete operator, C++, 572Dependencies
compiler, 214make program, 45–48, 79package, 13, 27–28in updates, 38
Deprecated features, 96–97Desktop help tools, 120Destructors, 95detach command, gdb, 553/dev directory, 9, 241, 260–263devfs, 266Device Independent (DVI) system, 124Devices and device drivers, 257, 272–273
block, 257–258, 274–276buffer cache, 276–280bus contention, 459–461character, 257–258, 273–274file-system, 259file-system cache, 276–281interrupts, 461–462kernel modules, 259–260lspci for, 464–467network, 259nodes, 257, 260–262
major numbers, 264–265minor numbers, 263–264sources, 266–271
PIC vs. APIC, 462–463slots, 463–467
diff command, 193–197diff3 command, 198–201diffstat command, 123Digital signatures
authentication with, 21–25ELF files, 324renaming functions based on, 99
-dir option, 197Direct Memory Access (DMA), 295Dirty blocks, 280Dirty caches, 296display command, gdb, 551–552Displaying
expressions, 551–552variable values, 542–547
distclean target, 77Distribution formats, 3DMA (Direct Memory Access), 295do/while loops, 522–523doc-linux package, 121DocBook format, 126–127, 138docbook-utils package, 126
602 Index
Document Type Definition (DTD), 126
Documentation. See also Help
builds, 135–136code, 212–213cross referencing and indexing, 121–122Emacs commands, 172formats, 124–131
Dollar signs ($), Ex mode, 159Domains, socket, 383down command, gdb, 542Doxyfile file, 212–213Doxygen tool, 212–213dpkg command, 32Drivers. SeeDevices and device drivers
DTD (Document Type Definition), 126Dual-core CPUs, 505–506Dumping
core files, 553–557file data, 420–426
DVI (Device Independent) format, 124dvipdf tool, 125Dynamic libraries, 69–73, 561Dynamic linker, 69, 300, 562–565Dynamic priority, 230–231
E
e command, Ex mode, 161-E option, GCC, 214–215echo command, 50ECOFF format, 324edit command, Ex mode, 160Editing source files. SeeText editors
EF_ALLOW_MALLOC_0 variable, 582ef, Electric Fence tool, 581EF_PROTECT_BELOW variable, 583Effective scheduler priority, 231Electric Fence tool, 581–583Elevator algorithms, 282–284ELF format, 324ellemtel autoindentation style, 171else clause, 65.emacs file, 177–178Emacs text editor, 143, 146, 170
browsing and building code, 175–176commands and shortcuts, 172–173cursor movement, 173deleting, cutting, and pasting, 174features, 170GUI mode, 179modes, 170–172search and replace, 174–175settings, 177–178text mode menus, 176–177for vi users, 178–179
environ file, in /proc, 344, 592ENVIRONMENT man section, 111–112error function, 63errorformat setting, 168Errors and warnings, build process, 78
build stage, 83–86compiler errors, 86–88compiler warnings, 88–98configure stage, 82–83linker, 98–100Makefile, 79–82
etags utility, 175etime format, 349evince tool, 124, 130Ex mode, 158–163Exact matching for man pages, 110Exception conditions, 370exe file, in /proc, 345, 593exec function
child processes, 318, 320, 330, 332, 334chrt, 238control transfer, 320–321
Executables
object files, 324profiling
gcov for, 491–494gprof for, 488–491OProfile for, 494–501
scripts, 321–323
Execution parallelism, 502
Index 603
execve function, 320–326
expandtabs setting, 167.exrc file, 166–167Extracting archive files, 11
F
f command, vi, 156f modifier, print and x, gdb, 543factorial.c listing, 555–556FAQs, 133fcntl function, 335FD_CLOEXEC flag, 335fd directory, 336
description, 345for open files, 594for pipes, 437
fdatasync function, 281, 518–519Fedora distribution
GUI-enabled vim package for, 169man pages in, 107package format for, 14
fflush function, 517, 519fg command, 244Fibonacci function, 205Field groups with top, 450FIFO (first in, first out)
pipes, 437–438scheduling, 236–237
file command, 6–7, 418–419, 554File descriptors, 331–336
leaked, 415limits, 337–338lsof command, 336–337
file-ipc-better.c listing, 360–362file-ipc-naive.c listing, 358–360File streams, 516–517File systems
buffering, 517–519caches, 276–280drivers, 259I/O, 274–276filedat.c listing, 420–422Files
dumping, 420–426locking, 362permissions, 8–10
Filesystem Hierarchy Standard, 121
for man pages, 105overview, 113
filter function, 62filter-out function, 62Filters, Synaptic, 37Finding
info pages, 118man pages
with apropos, 107–110with whatis, 110–111
open source tools, 2–3processes, 352–353
findstring function, 62firefox package, 23First in, first out (FIFO)
pipes, 437–438scheduling, 236–237
Fixed scheduler priority, 230flawfinder tool, 571Floating-point error (SIGFPE) signal, 553Floating-point types with printf, 526flock function, 413Flushing caches, 296Folding in Kate text editor, 183Footprint, process, 329–331fopen function, 362Forcing core dumps, 589–591Foreground processes, 244fork-file.c listing, 332–336fork function, 238, 242, 318, 320, 332, 334Forks, timing in, 255Formats
help documentation, 124–131man pages, 105package, 14printf, 91–92, 525–526process information, 349–351
604 Index
fprintf function, 91
Fragmented memory, 305–306frame command, gdb, 542free command, 449, 452–453, 572Frequency, tick, 253–255Frontside bus (FSB), 456–458fsync function, 281, 517–519ftok function, 413ftpd function, 320ftruncate function, 366–367full device, 262funalloc.c listing, 579–580Functions
calling from gdb, 547declarations, 90hook, 178renaming, 99signal safe, 370user-defined, 63–64variable manipulation, 60wrapper, 523–525
fuser command, 417–418
G
G command, vi, 149g suffix, x, gdb, 545GCC. SeeGNU Compiler Collection (GCC)
gcc info page, 119gcc project, 76gcore function, 590gcov tool, 491–494gdb debugger, 529–530
attaching to running process with, 553for core files, 553–557with Electric Fence, 582inspecting and manipulating data with gdb,
541–542
C++ STL, 549–551display command, 551–552function calling, 547print, 542–547templates, 548–549
for multithreaded programs, 557–558for optimized code, 558–561running code with, 530–531stopping and restarting execution with,
531–532
breakpoints, 532–537watchpoints, 537–541
Gedit text editor, 184–185GENERATE_ tags, Doxygen, 213Gentoo distribution, 14getprotoent function, 384getrlimit function, 314–315, 340–342getrusage function, 250–251, 255, 328, 342Ghostscript tools, 129–130glibc, 569–570
MALLOC_CHECK_ for, 573memusage, 574–576mtrace, 573–574
glibc-utils package, 574–576gmon.out file, 498–499gnome-help tool, 107, 117, 120gnome-terminal, 176gnometerminal window, 171GNU arch tool, 190gnu autoindentation style, 171GNU Compiler Collection (GCC)
build tools, 43, 74–77C compiler warnings, 93–94C++ compiler warnings, 95–98code analysis, 214debugger. Seegdb debugger
Ghostscript tools, 129–130heap, 305–307info pages, 115–119
GNU Privacy Guard (GPG), 22–25gprof tool, 488–491, 498–499Graphviz tool, 212groff language tool, 131gs command, 130gstack script, 587
Index 605
GUI text editors, 143, 182–183
Emacs, 179Gedit, 184–185Kate and Kwrite, 183–184NEdit, 184–186SciTE, 186–187vi, 147Vim, 147, 168–169
gvim program, 168gvimdiff tool, 198–199gzip tool, 4, 7
H
h command, vi, 149h suffix, x, gdb, 545Hard limits, 340Hard real-time application, 236Hashes, 19–20HAVE_DOT tag, Doxygen, 213Heap memory
analysis, 579–581libraries for, 305–307overflows, 571–572, 581–583and virtual memory, 308–310
helgrind function, 580hello-sync.c listing, 405hello-unsync.c listing, 403–404Help, 103
cross referencing and indexing, 121–122documentation formats, 124–131Internet information sources, 131–134kernel information, 134–138miscellaneous documentation, 138online tools, 103–104
desktop, 120info pages, 115–119man pages. See man pages
package queries, 122–123/usr/share/doc directory, 121
help command, Ex mode, 161hexdump command, 424–425hier man page, 113Hints, cache, 294–295hog.c listing, 448Hook functions, 178Hotplug feature, 266–268HOWTO documents, 133HTML for documentation, 127–129htons macro, 392Hyperthreading feature, 339, 505Hypertransport connections, 460HZ macro, 253
I
i command, vi, 151–152i modifier, print and x, gdb, 543IA32 architecture, 92ifdef conditional, 64, 66ifeq conditional, 64–66ifndef conditional, 64ifneq conditional, 64Illegal instruction (SIGILL) signal, 435, 554imake program, 42Implicit declarations, 89–91Implicit Makefile rules, 52–57in_addr structure, 393INADDR_ANY macro, 392Include files, 87Incompatible ABI, 97–98Incremental search mode, 175indent command, 204–206Indentation
code beautifiers, 204–206Emacs, 171
Indexes
Emacs, 175help, 121–122Vim, 164–166
info command, gdb, 532, 536, 540, 542, 548,
557
606 Index
info pages, 104, 115
recommended, 119searching, 118viewing, 115–118
init function, 242, 317–318, 320Initializers, reordered, C++, 95–96Inline assembly, 292Inline functions, 523, 559Inline synchronization, 521Inodes, 440–442insecure.c listing, 342–343Insert commands, vi, 151–152Insert mode
vi, 147Vim, 161–162
Inspecting and manipulating data, gdb
debugger, 541–542
C++ standard template library, 549–551display command, 551–552function calling, 547print, 542–547templates, 548–549
install target, 77–78Installation scripts, 27–28Instantiation, 548Instructions
efficiency, 480–484latency, 503–504
Interactive processes, 230Internationalization, text editor, 188–189Internet information sources, 131–134Interprocess communication (IPC), 357
debugging, 415
dumping file data, 420–426inodes, 440–442open files, 415–420pipes, 437–438POSIX, 431–434signals, 434–437sockets, 438–440System V, 426–431message queues, 393
POSIX, 397–402System V, 393–397
pipes, 381–382plain files, 358–363semaphores, 402–407
POSIX, 407–410System V, 410–412
shared memory, 363
POSIX, 364–367System V, 367–370
signals, 370–371
handling, 371–373masks for, 373–376real-time, 376–378sending, 371sigqueue and sigaction, 378–381
sockets, 382
client/server, 387–393creating, 383–385debugging, 438–440example, 385–387
Interruptible process state, 239Interrupts
device, 461–462sar for, 473
Interrupts per second information, 469Invalidating cache entries, 295I/O
buffering, 517–519devices, 272–273
block devices and file systems, 274–276buffer cache and file-system cache, 276–280character, 273–274
information
sar for, 472–473vmstat for, 468–469
scheduler, 282–286
I/O APIC, 462iostat tool, 473–475IPC. SeeInterprocess communication (IPC)
Index 607
IPC_PRIVATE macro, 393
ipcrm command, 426, 430ipcs command, 370, 426, 428–430IPPROTO_ flags for socket protocols, 385Iterators, C++, 550
J
j command, vi, 149jadetex package, 126java autoindentation style, 171JBellz Trojan, 17–18Jed text editor, 181–182Jiffies, 253Jobs, process, 346Joe text editor, 181–182
K
k command, vi, 149Kate text editor, 183–184Kconfig files, 136kdvi tool, 124Kernel, 221–222
clock ticks, 253–255devices and device drivers. SeeDevices and
device drivers
information about, 134–138I/O scheduler, 282–286memory management. SeeMemory; Virtual
memory
process scheduler. SeeProcess scheduler
user mode vs. kernel mode, 222–226
Kernel modules, 259–260Kernel space, 222–223, 226Kernel threads, 281Key mapping, vi, 148khelpcenter program, 107, 117–118, 120kill function, 354, 371, 378–380, 531, 591kmem device, 262kmsg device, 262Knoppix distribution
GUI-enabled vim package for, 169man pages in, 107package format for, 14
.ko extension, 260konqueror tool, 130K&R autoindentation style, 171kswapd thread, 449kvim program, 168–169Kwrite text editor, 183
L
l command, vi, 149L1 cache, 291, 481–482L2 cache, 291–292, 482–483L3 cache, 292Labels in revision control, 192LAPIC (Local APIC), 462Last in, first out (LIFO) buffers, 338Latency
instruction, 503–504scheduling, 235
LaTeX system, 124Lazy TLB flushing, 290LBA (logical block addressing), 258ld command, 67LD_LIBRARY_PATH variable, 564ld info page, 119LD_PRELOAD variable, 564–565, 581–582ldconfig command, 564, 568ldd command, 68–69, 562–563, 566LDFLAGS variable, 82–83ldlinux.so linker, 564Leaks
file descriptors, 415memory, 570
mtrace for, 573–574valgrind for, 577–578
Leaky.c listing, 577–578
608 Index
Leaning toothpick syndrome, 163
Least recently used (LRU) pages, 448less program, 116libc info page, 119—libdir option, 75Libraries
for builds, 69–73for heap, 305–307naming convention, 73STL, 549–551
librt library, 433Lifetime, variable, 52LIFO (last in, first out) buffers, 338Line buffering, 515–516#line directives, 215Linker errors, 98–100Linking builds, 67–69lint program, 88–89Linus Elevator algorithm, 282–284linux autoindentation style, 171Linux Documentation Project, 133listen function, 388, 391Listen sockets, 388Listing shared objects, 566–567Little Endian byte order, 424, 426Local APIC (LAPIC), 462Local sockets
client/server example, 387–392debugging, 439
Locating shared objects, 564–565Locked memory, 310–315, 339–340lockf function, 360, 362Logical block addressing (LBA), 258LRU (least recently used) pages, 448ls command, 418lsof command, 336–337
in debugging, 416–417inodes, 441pipes, 437semaphores, 434shared memory, 428sockets, 438–439
lspci tool, 464–467ltrace command, 434, 484–485M
m command, vi, 154Macros
debugging, 214–216do/while in, 522–523POSIX scheduling policy, 238printf, 519–521
madvise function, 295–296magic field, 325Magic numbers, 324–326mailing-list archive (MARC), 134Mailing lists, 134Major device numbers, 264–265Major page faults, 289, 447make program and Makefiles, 42, 44–45. 
See also Builds
conditional constructs, 64–66dependencies, 214error detection by, 84–86install stage, 77–78minimal, 52–57mistakes, 79–82, 84rules and dependencies, 45–48sources, 66–67targets, 77–78user-defined functions, 63–64variables
automatic, 57–58defining, 48–49in implicit rules, 53–55manipulating, 59–63referencing and modifying, 52white space and newlines in, 50–51
Vim, 166
make target, 77makeinfo program, 125–126makeprg setting, 168makewhatis tool, 122MALLOC_CHECK_ variable, 573malloc function, 303–304
overcommitment by, 309–310and overflow, 572page table mappings with, 446–447with pointers, 569
Index 609
MALLOC_TRACE variable, 574
mallopt function, 307Malware, 17–18man pages, 103–105
contents, 111–112organization, 105–107recommended, 112–114searching
with apropos, 107–110with whatis, 110–111
Mandatory locks, 362mandb version, 122—mandir option, 76Mandriva distribution, 14Mangling, name, 99MAP_ANONYMOUS flag, 308map command, Ex mode, 160Maps, memory, 592
anonymous, 73, 302, 308virtual memory, 329–330
maps file, in /proc, 345MARC (mailing-list archive), 134Masks
affinity, 507–509magic numbers, 325signals, 435
massif tool, 579–581md5sum command, 20–21, 33md5sums file, 33mebibytes, 114mem driver, 258, 261–262, 264mem file, in /proc, 345memcheck function, 577memcpy function, 112Memory
debugging, 569–570
buffer overflows, 571–572corruption, 578–579heap analysis, 579–581heap overflow, 581–583leaks, 570, 573–574, 577–578MALLOC_CHECK_, 573mtrace, 573–574pointers, 569–570statistics, 574–576Valgrind tools, 576–581
decrypted passwords in, 343kernel modes, 222for libraries, 71locked, 310–315, 339–340mapping, 73, 302, 308, 592for processes, 303–310
resident and locked, 339–340usage, 353–354
system, 310in system performance, 446
page faults, 446–448swapping, 448–456
text editors, 187–188virtual. SeeVirtual memory
vmstat information, 468
Memory coherency, 295–296Memory leaks, 570
mtrace for, 573–574valgrind for, 577–578
Memory Management Unit (MMU), 288–289,
581
Memory-related ps formats, 350memset function, 533memusage function, 574–576memusagestat function, 576mepis distribution, 14Merging revision changes, 192–197Message queues, IPC, 393
POSIX, 397–402, 432–433System V, 393–397, 429–430
Meta keys, 171Metadata, 5Minibuffer mode, 173Minimal Makefiles, 52–57Minor device numbers, 263–264Minor page faults, 289, 447Minus signs (-), vi, 149Missing braces, 93–94Missing include files, 87Missing parentheses, 93
610 Index
Missing public keys, 24–25
Missing tabs, 79–80mkfifo function, 382mkfs command, 274mknod command, 260–261, 382mkswap command, 298mlock function, 310–311mlockall function, 311mmap function
devices, 274–275memory coherency, 295–296memory fragmentation, 306–308pointers, 570shared memory, 364
MMU (Memory Management Unit), 288–289,
581
Modeless text editors, 170Modes
Emacs, 170–172vi, 147–148
Modifying
process affinity, 508variables, 52
modinfo command, 137–138Modules
documentation, 137–138kernel, 259–260
monotone tool, 190mostlyclean target, 77Motherboards, multiprocessor, 504–505mount command, 276mounts file, in /proc, 345mpg123 program, 17–18mpstat tool, 473–475mq_close function, 398mq_getattr function, 398mq_notify function, 433mq_open function, 397–398mq_receive function, 399, 402mq_send function, 399mq_setattr function, 398–399mq_unlink function, 398–399mqueue file system, 433mqueuepseudo file system, 432MS_INVALIDATE flag, 296MS_SYNC flag, 296msg_receive function, 402msgctl function, 394msgget function, 393–394msgrcv function, 396–397msgsnd function, 394msync function, 295–296, 364Multicore CPUs, 505–506Multiprocessor performance, 501
Frontside Bus, 456–458SMP hardware, 501
CPU parallelism, 501–504motherboards, 504–505multicore CPUs, 505–506SMT, 505
SMP programming, 506
affinity masks, 507–509scheduler, 506–507
Multitasking
cooperative, 227preemptive, 228process. SeeProcess scheduler
Multithreaded programs, 557–558munlock function, 310munmap function, 364muntrace function, 574
N
n command, vi, 150-N flag, diff, 195Names
finding processes by, 352–353in libraries, 73in man pages, 106mangling, 99package, 28sending process signals by, 354–355
Nano text editor, 181–182
Index 611
nasty.c listing, 533
Native POSIX Threads Library (NPTL), 509ncurses library, 450NEdit text editor, 184–186netstat command, 438–440Network devices, 259Network sockets
client/server example, 392–393debugging, 441
Network Time Protocol (NTP) package, 127new-corrupt.cpp listing, 578–579, 581new operator, 572Newlines
printf, 515–516in variables, 50–51
Newsgroups, 134next command, gdb, 531, 558nice command, 234–235, 238nm command
for linking, 72for symbols, 99, 567–568
Nodes, device, 257, 260–262
major numbers, 264–265minor numbers, 263–264sources, 266–271
Nonvirtual destructors, 95noop scheduler, 282–284NPTL (Native POSIX Threads Library), 509NTP (Network Time Protocol) package, 127null device, 262Null pointer arguments, 93Number of buffers per session feature, 147NVRAM driver, 264nvram module, 267–268
O
o command, vi, 151o modifier, print and x, gdb, 543objdump command, 559–560, 567–568Object files, executable, 324Octal dump command, 425–426od command, 425–426off_t type, 525Official package names, 28offset field for magic numbers, 325Offsets, memory pages, 288Online help tools, 103–104
desktop, 120info pages, 115–119man pages. Seeman pages
OOM (out-of-memory) killer, 297, 309oom_adj file, in /proc, 345oom_score file, in /proc, 345opannotate tool, 496, 499opcontrol tool, 495–497Open files
information for, 593IPC debugging, 415–420
open function
device drivers, 257permissions for, 332
Open source tools, 1
archive files, 4–5
extracting, 11identifying, 6–7querying, 7–11
defined, 2distribution formats, 3finding, 2–3packages. SeePackages
significance, 2–3
Opening POSIX message queues, 397–398OpenSSH source code, 18operator man page, 113opgprof tool, 496, 498–499opreport tool, 496, 498.oprofile file, 496OProfile profiler, 494–495
for applications, 497–500opcontrol tool, 495–497remarks, 500–501
oprofiled tool, 496–498Opteron CPUs, 504–505
612 Index
Optimized code, debugging, 558–561
Out-of-memory (OOM) killer, 297, 309Out-of-order steps in optimized code, 559Output modifiers, print and x, gdb, 543–544Overcommit, memory, 309–310Overflow
buffer, 555–556, 571–572heap, 581–583stack, 338, 555–556
Overlays, shared objects for, 562overrun.c listing, 540–541
P
p command, vi, 155-p option, patch, 197Packages
contents, 27–33package managers, 12–14queries, 29–30, 122–123security for, 17–26source and binary, 14–16updating, 33–39working with, 16–17
PAE (Physical Address Extension), 302Page faults, 289, 446–448Page tables, 289Pages in virtual memory. SeeVirtual memory
Parallel bus, 463, 465Parallelism, CPU, 501–504Parentheses ()
macros, 520missing, 93
Partitions, swap, 297–298Passwords in memory, 343Pasting
Emacs, 174vi, 155–158
patch command, 193–197Patching, libraries for, 70patsubst function, 60, 62Pattern rules, 55pause function, 242pax utility, 5PCI bus, 459, 463–465PCI Express (PCIe) connections, 460–461,
463
PCI-X bus, 463PDF (Portable Document Format), 130–131PDF_HYPERLINKS tag, Doxygen, 213pdflatex tool, 125pdflush daemon, 281Percent signs (%)
Ex mode, 159vi, 149
Performance, 445
application. SeeApplication performance
multiprocessor. SeeMultiprocessor perform-
ance
printf, 514–515system. SeeSystem performance
Periods (.)
Ex mode, 159vi, 154
Permissions, 8–10, 332permute.cpp listing, 551–552PF_ flags for socket domains, 383pgrep command, 352–353Physical Address Extension (PAE), 302PIC (Programmable Interrupt Controller),
462–463
Pico text editor, 181–182PID directory, 344–345PIDs
with gdb, 553System V memory, 428System V message queues, 430
pinfo command, 117pipe function, 381–382Pipelines, 502–505Pipes, IPC, 381–382, 437–438pkg-config tool, 567pkgtool package manager, 13–14pkill command, 354–355Plain files for IPC, 358–363Plus signs (+), vi, 149pmap function, 299–301, 305, 353–354, 592
Index 613
Pointers
freeing, 569–570null arguments, 93
Polling in IPC, 376pop command, vi, 165Popping, stack, 338port device, 262Portable Document Format (PDF), 130–131portage package manager, 13–14Position-independent code, 563POSIX API
IPC debugging, 431–434library, 69message queues, 397–402, 432–433semaphores, 407–410, 433–434shared memory, 364–367, 431–432
posix-msgq-ex.c listing, 399–402posix_sem.c listing, 407–410, 433–434posix-shm.c listing, 365–367Postmortem debugging, 554PostScript language, 129–130Pound signs (#)
Ex mode, 163macros, 520–521
Preemptable kernels, 229Preemption in process scheduler, 228–229Preemptive multitasking, 228Prefetching, cache, 294—prefix option, 75Preprocessor macros
debugging, 214–216printf, 519–521
print command, gdb, 541–547print_trace function, 588–589printf function, 514
buffering
and file streams, 516–517and file systems, 517–519
format warnings, 91–92, 525–526gdb, 541performance effects, 514–515preprocessor help, 519–521remarks, 528–529synchronization issues, 515–516tips, 527–528wrapper functions, 523–525
Priorities
process scheduler
and fairness, 229–233and nice, 234–235real-time, 235–237
queued messages, 397
/proc directory, 268–269, 344–346/proc/cpuinfo file, 477–478/proc/pid/fd directory, 437Process scheduler, 226
blocking, preemption, and yielding in,
228–229
overview, 226–227priorities
and fairness, 229–233and nice, 234–235real-time, 235–237
process states, 239–245real-time processes, 238–239timing, 246–249
applications, 255–256kernel clock tick, 253–255system time units, 249–253
Processes, 317–318
clone, 320communication between. SeeInterprocess
communication (IPC)
copy on write, 319–320environment, 592–593exec, 320–321executable object files, 324executable scripts, 321–323file descriptors, 331–338finding, 352–353footprints, 329–331fork and vfork, 318information for
displaying, 346–348formats for, 349–351sar for, 471–472vmstat for, 468
limits, 340–343
614 Index
Processes, continued
memory for, 303–310
resident and locked, 339–340usage, 353–354virtual, 298–302
miscellaneous binaries, 324–326and procfs, 343–346real-time, 238–239scheduling. SeeProcess scheduler
signals for, 354–355stack for, 338–339states, 239
sleeping vs. running, 240–241stopped, 243–245zombies and Wait, 241–243
synchronizing, 327–329
Processor architecture
instruction efficiency, 480–484x86info for, 476–480
procfs file system
for debugging, 416, 442, 591–594process information provided by, 343–346vs. sysfs, 268–269
procps package, 346, 349–351Profiling executables
gcov for, 491–494gprof for, 488–491OProfile for, 494–501
profme.c listing, 488–491—program-prefix option, 76—program-suffix option, 76Programmable Interrupt Controller (PIC),
462–463
Projects in revision control, 191Protected output, 403Protocols, socket, 384–385ps function, 235, 243
for finding processes, 352–353formats, 349–351options, 233overview, 346–348for semaphores, 431for states, 239–240Pseudotargets, 47–48pthread_create function, 320, 509pthread_self function, 509pthread_setaffinity_np function, 509pthread_setschedparam function, 238Public keys, missing, 24–25Pushing, stack, 338Put commands, vi, 155–158python autoindentation style, 171
Q
q command, Ex mode, 161Quad-core CPUs, 506Quantum, 228, 237Queries
archive file, 7–11package, 29–30, 122–123
queryformat option, 30Question marks (?)
Ex mode, 159vi, 150
Queues, message, 393
POSIX, 397–402, 432–433System V, 393–397, 429–430
quit command, vi, 160
R
-R flag, patch, 195Race conditions, 334, 357raise function, 380, 591ramdisk device
memory for, 310vs. tmpfs, 280
random device, 262rbreak command, gdb, 549RCS (Revision Control System), 190–191Read operations
blocking, 248–249character devices, 273POSIX message queues, 399
Index 615
Readability
automatic variables for, 57–58indentation, 204–206
readelf command, 567–569README files, 121, 123, 135Real-time applications, 235–236Real-time priorities, 235–236
FIFO scheduling, 236–237round-robin scheduling, 237
Real-time processes, 238–239Real-time signals, 376–378Reaping processes, 327Red Hat distribution
package format, 14updating, 37–39
Reduced Instruction Set Computer (RISC)
processors, 501–502
Referencing variables, 52regex man page, 114register pseudofile, 325Registers
SMT, 505vi, 157
Regular expressions
man pages, 110text editors, 145
Removing POSIX message queues, 398–399Renaming functions, 99renice command, 234Reordered initializers, 95–96Repeated vi commands, 148Resident memory, 339–340Restarting execution with gdb, 531–532
breakpoints, 532–537watchpoints, 537–541
Reviewing revision changes, 193–197Revision control, 189
basics, 189–190diff and patch, 193–197RCS, 190–191reviewing and merging changes, 193–197terms, 191–193
Revision Control System (RCS), 190–191RISC (Reduced Instruction Set Computer)
processors, 501–502
RLIMIT_ flags, 341–342rlimit structure, 314, 340–341rmmod command, 260, 267root file, in /proc, 345Round-robin scheduling, 237RPM (RPM Package Manager) package, 13–14
contents, 28–31updating, 35–36
rpm utility, 23rpm2cpio command, 31rt-sig.c listing, 377–378Rules, Makefiles, 45–48, 52–57run command, gdb, 531runex script, 232Running out of process memory, 303–310Running processes, 239
attaching to, 553vs. sleeping, 240–241
rwatch command, gdb, 540
S
S command, vi, 153s modifier, print and x, gdb, 543SA_SIGINFO flag, 379sar tool, 469–470
interrupt activity, 473I/O information, 472–473process information, 471–472remarks, 473–474virtual memory information, 470–471
—sbindir, configure, 75scanf function, 92SCHED_FIFO macro, 238–239sched_getaffinity function, 508SCHED_OTHER macro, 238SCHED_RR macro, 238sched_setaffinity function, 508sched_setscheduler function, 238–239sched_yield function, 228
616 Index
Scheduler
I/O, 282–286process. SeeProcess scheduler
SMP programming, 506–507
Scheduler-lock feature, 558Scheduler-related process formats, 351schedutils package, 507SciTE text editor, 186–187Scons project, 44Scope, make variable, 52Scripts
executable, 321–323installation, 27–28typos in, 323
SCSI devices, 263, 269–271Search and replace feature
Emacs, 174–175Ex mode, 163
Searching
info pages, 118man pages
with apropos, 107–110with whatis, 110–111
Security
kernel modes for, 222packages, 17–26shared objects, 565
SEE ALSO man section, 111Segmentation violation signal. SeeSIGSEGV
(segmentation violation) signal
select function, 388self directory, in /proc, 336sem_destroy function, 410sem_init function, 410sem_open function, 408–410sem_post function, 407sem_t type, 433sem_unlink function, 409sem_wait function, 406Semaphores, IPC, 402–407
POSIX, 407–410, 433–434System V, 410–412, 430–431semop function, 412semtimedop function, 431Sending signals to processes, 371server_un.c listing, 388–390set command
Ex mode, 160gdb, 547
set args command, gdb, 530–531set scheduler-lock command, gdb, 558setaffinity functions, 509setgid bit, 8–10setrlimit function, 312, 314–315, 339–342Settings
Emacs, 177–178vi, 166–168
setuid bit, 8–10setvbuf function, 516–517, 519SGML Document Type Definition, 126Shared memory, IPC, 363
POSIX, 364–367, 431–432System V, 367–370, 426–429
Shared objects, 561
creating, 563listing, 566–567locating, 564–565security with, 565symbols in, 567–569unused, 567using, 562
Shell commands
debugging. SeeDebugging
Makefiles, 79–80syntax errors, 85–86
shell function, make, 60shiftwidth setting, 167shm_open function, 364, 366–367, 369shm_unlink function, 364shmat function, 368shmctl function, 368shmdt function, 368shmget function, 368–369Shortcuts, Emacs, 172–173
Index 617
SI_ flags used with struct siginfo, 381
Side effects, printf, 528–529side-effects.c listing, 528–529SIGABRT signal, 554, 589sigaction function, 372–373, 378–381sigaddset function, 373SIGBUS signal, 554SIGCHLD signal, 243, 327, 375–376, 378SIGCONT signal, 244–245, 591sigdelset function, 373SIGFPE signal, 553sighander_t type, 372SIGILL signal, 435, 554siginfo structure, 380SIGINT signal, 436sigismember function, 373SIGKILL signal, 244, 372signal system call, 379, 532Signal safe functions, 370Signals
for debugging information, 590–591IPC, 370–371
debugging, 434–437handling, 371–373masks for, 373–376real-time, 376–378sending, 371sigqueue and sigaction, 378–381
for processes, 354–355
Signatures
authentication with, 21–25ELF files, 324renaming functions based on, 99
sigpending function, 376sigprocmask.c listing, 374–376sigprocmask function, 373sigqueue function, 378–381SIGQUIT signal, 240SIGRTMAX macro, 376SIGRTMIN macro, 376, 378SIGSEGV (segmentation violation) signal
core files, 553with Electric Fence, 583heap overflow, 572invalid addresses, 289pointers, 569stack overflow, 338, 342, 571
sigset_t type, 373SIGSTOP signal, 372, 591sigsuspend function, 376SIGTERM signal, 240, 244SIGTSTP signal, 244SIGTTIN signal, 244–245SIGTTOU signal, 245Single-stepping, 55864-bit types with printf, 525–526Size
container, 550executables, 72memory pages, 288
size method, 550skill command, 354–355Slackware distribution, 14Slashes (/)
Ex mode, 159vi, 150
sleep command, 246Sleeping process state, 240–241Slots, device, 463–467smaps file, in /proc, 345SMP . SeeSymmetric Multiprocessing System
(SMP) computers
SMT (Symmetric Multithreading), 505snprintf function, 91.so extension, 73SOCK_ socket types, 384sockaddr structure, 393sockaddr_un structure, 391socket system call, 383, 387socketpair.c listing, 385–387socketpair system call, 385–387
618 Index
Sockets
domains, 383IPC, 382
client/server, 387–393creating, 383–385debugging, 438–440example, 385–387
protocols, 384–385types, 383
Soft limits, 340Soft real-time applications, 236son-of-hog.c listing, 451–456sort function, make, 62Source code, 141
analysis
cflow, 207–210compiler for, 214–216ctags, 210
beautifiers and browsers, 203–204
astyle, 206–207cflow, 207–210compiler, 214–216cscope, 211ctags, 210Doxygen, 212–213indent code, 204–206
code coverage, 491–494editing. SeeText editors
packages, 14–16revision control, 189
basics, 189–190diff and patch, 193–197RCS, 190–191reviewing and merging changes, 193–197terms, 191–193
VPATH for, 66–67
SourceForge.net site, 132–133Spaces in scripts, 323Split windows in SciTE, 187sprintf function, 91Square brackets ([]), vi, 149ssh-keygen tool, 115sshd daemon, 320, 331–332Stack coloring, 339Stacks
corruption, 555–556overflow, 338, 342, 571for processes, 338–339tag, 164–166
Stale caches, 295Stalls, pipeline, 503–504Standard Template Library (STL), 549–551start command, gdb, 531Starved requests, 284stat command, 419–420, 442stat file, in /proc, 345States, process, 239
sleeping vs. running, 240–241stopped, 243–245zombies and Wait, 241–243
Static libraries, 69Static linking, 71–73Static priority, 231Static storage, 338statm file, in /proc, 345status file, in /proc, 345stdarg.h file, 524stdout stream, 516step command, gdb, 531, 558sticky bit, 8, 11STL (Standard Template Library), 549–551Stopped process state, 239, 243–245Stopping execution with gdb, 531–532
breakpoints, 532–537watchpoints, 537–541
strace command
for program performance, 485–487purpose, 225for signals, 436–437timing with, 246–248, 360
strace package, 132strftime function, 92String types
for file data dumps, 422–423with printf, 526
Index 619
strings command, 422–423
strip function, make, 62Stroustrup autoindentation style, 171strptime function, 92stty command, 245subst function, make, 62substitute command, vi, 163Substitution references, 59Subversion tool, 190, 201Suffix rules for object code, 54–55suffixes man page, 114summer-proj.c listing, 492summer-proj.c.gcov listing, 493–494Superscalar units, 502swapoff command, 297, 452swapon command, 297, 452Swapped memory. SeeVirtual memory
swapvals function, C++ template example, 548Switched fabric architectures, 460Symbols in shared objects, 567–569Symmetric Multiprocessing System (SMP)
computers, 295, 457
hardware, 501
CPU parallelism, 501–504motherboards, 504–505multicore CPUs, 505–506
programming, 506
affinity masks, 507–509scheduler, 506–507
Symmetric Multithreading (SMT), 505Synaptic tool, 36–37sync system call, 106, 281Synchronization
caches, 296inline, 521printf issues, 515–516processes, 327–329
Syntax highlighting, 144, 147syntax setting, 168/sys directory, in /proc, 268–269syscall function, 224–225sysconf function, 249, 254, 337sysfs feature, 268–272sysstat package
iostat and mpstat, 474–475sar, 469–474
sysstat service, 472–474System calls, 223–226System memory, 310System performance, 445
CPU utilization, 456–459devices and interrupts, 459–467memory issues, 446
page faults, 446–448swapping, 448–456
sysstat package
iostat and mpstat, 474–475sar, 469–474
virtual-memory status, 467–469
System software installation, 18System time units, 249–253System V API
IPC debugging, 426–431message queues, 393–397, 402, 429–430semaphores, 410–412, 430–431shared memory, 367–370, 426–429
sysv-msgq-example.c listing, 394–396sysv_sem.c listing, 410–412sysv-shm.c listing, 368–370
T
t command, vi, 156t modifier, print and x, gdb, 543Tab completion, gdb, 537Tab expansion, vi, 147Tabs, Makefiles, 79–80tabstop setting, 167tag command, vi, 165Tags, Vim, 164–166tags command, vi, 165Tape archive (tar) files, 4–6, 8, 12Targets, make, 77taskset command, 507tbreak command, gdb, 531
620 Index
Templates, C++, 548–549
Temporary variables, 560Terminal-based text editors, 143tetex-bin package, 125tex command, 126TeX system, 124texi2dvi tool, 126Texinfo language, 115, 125–126Text editors, 141
categories, 142–143clones, 179–182default, 143–144Emacs. SeeEmacs text editor
features, 144–146GUI, 182–187memory usage, 187–188summary, 188–189vi. Seevi text editor; Vim text editor
Text functions, 62–63Text mode menus, 176–177Text substitution patterns, 5932-bit types with printf, 525–526Thrashing, 297, 310–311, 455Thread Affinity API, 509Threads
affinity masks, 509gdb for, 557–558hyperthreading technology, 339kernel, 281timing, 255
3G/1G kernel split, 222, 329Throughput, pipeline, 503Ticks, 249, 253–255time command, 246, 256, 447, 475–476Time formats, 349–350times function, 250–251timespec structure, 226, 251–252timeval structure, 250–251Timing in process scheduler, 246–249
applications, 255–256kernel clock tick, 253–255system time units, 249–253
TLB (Translation Lookaside Buffer), 289–291TLDP FAQs, 133tmpfs file system
mounting, 367vs. ramdisk, 280semaphores, 433shared objects, 431
tms structure, 250tnext command, vi, 165–166top command, 450–456tostop setting, 245tprevious command, vi, 165trace-buffer.c listing, 584–587Translation Lookaside Buffer (TLB), 289–291troff language, 104, 124, 131Trojans, 17–18tselect command, vi, 165Typos in scripts, 323
U
u command, vi, 152, 154u modifier, print and x, gdb, 543Ubuntu distribution
GUI-enabled vim package for, 169man pages in, 107package format for, 14
udev package, 128, 266–268ulimit function, 315, 339, 589UML_LOOK tag, 213Underruns, 583Undo feature, 147undocumented man page, 123Unified L2 cache, 482Uninitialized variables, 94Uninstall features, 13uninstall target, 77–78Uninterruptible process state, 239–241units man page, 114Unnamed semaphores, 409–410Unused shared objects, 567Unused variables, 559up command, gdb, 542up2date tool, 34, 37–39
Index 621
Updating packages, 33–39
Upgrading packages, 13urandom device, 262uri man page, 114url man page, 114urn man page, 114USE_PDFLATEX tag, Doxygen, 213Usenet, 134User-defined functions, 63–64USER_HZ macro, 254User mode vs. kernel mode, 222–226User space
kernel modes, 222–223, 226memory management. SeeMemory; Virtual
memory
V
Valgrind tools, 576–577
heap analysis, 579–581instruction efficiency, 480–484memory corruption, 578–579memory leaks, 577–578remarks, 580
Variables
displaying values of, 542–547make program
automatic, 57–58defining, 48–49in implicit rules, 53–55manipulating, 59–63referencing and modifying, 52white space and newlines in, 50–51
in optimized code, 559–560uninitialized, 94
Vectors, 549vfork system call, 318, 320vfprintf function, 524vi text editor, 143, 146. See also Vim text editor
change commands, 153command mode, 147–148cursor movement commands, 149–151cut, paste, and delete commands, 155–158Ex mode, 158–161features, 147for info pages, 116–117insert commands, vi, 151–152miscellaneous commands, vi, 154modes, vi, 147–148settings, 166–168
vi users, Emacs for, 178–179vim-gnome package, 168–169vim-gtk package, 169Vim text editor, 143, 146, 181–182
browsing and building code, 164–166features, 147GUI mode, 168–169insert mode, 161–162
vim-X11 package, 168–169.vimrc file, 166–167Viper mode, Emacs, 178Virtual memory, 286–287
caches, 291–296coherency, 295–296and heap, 308–310map, 329–330MMU for, 288–289for processes, 298–302sar for, 470–471status, 467–469swap space, 296–298swapping, 448–456TLB, 289–291
Viruses, 17–18vmstat command, 278–279, 467–469volatile keyword, 561VPATH variable, 66–67, 80–82vprintf function, 524vsprintf function, 524
W
w command
Ex mode, 161vi, 150
622 Index
w suffix, x, gdb, 545
Wait for zero operation, 412wait system call, 241–243, 327–329Wait states, socket, 438wait3 system call, 327–328wait4 system call, 327–328waitpid system call, 327–328, 334-Wall option, 89–92Warnings, compiler, 88–89
C, 93–94C++, 95–98format, 91–93, 525–526implicit declarations, 89–91
watch command, gdb, 532, 538watch watchpoints, gdb, 540watchpoint command, gdb, 537Watchpoints, gdb, 537–541wchan file, in /proc, 345-Wformat-nonliteral option, gcc, 92-Wformat-y2k option, gcc, 92whatis command, 110–111, 541, 550White space, 50–51whitesmith autoindentation style, 171Windows text editor compatibility, 146WNOHANG flag, 328–329word function, make, 63wordlist function, make, 63words function, make, 63WordStar text editor, 180Wrapper functions, 523–525wrapscan setting, 167Write back, cache, 293–294Write combining, cache, 294write command, vi, 160write function, 366Write operations
blocking, 248–249character devices, 273CPU, 293–294Ex, 160POSIX message queues, 399shared memory, 366Write through, cache, 294WUNTRACED flag, 328www.gnu.org site, 131–132
X
x command
Ex mode, 161gdb, 541–547
x modifier, print and x, gdb, 543x86info tool, 476–480Xandros distribution, 14xdvi tool, 124xinetd daemon, 320, 441xit command, Ex mode, 160xman tool, 107xpdf tool, 130xxd command, 423–424xxdiff tool, 198, 201–202
Y
y command, vi, 155Yank commands, 155–156Yellowdog Updater Modified (yum), 34–36yelp tool, 120Yielding in process scheduler, 228–229
Z
zb command, vi, 154zero device, 262Zile text editor, 181–182.zip files, 6Zombie process state, 239, 241–243zt command, vi, 154zz command, vi, 154
