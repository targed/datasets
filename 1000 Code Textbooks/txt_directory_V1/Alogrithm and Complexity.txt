Algorithms and Complexity
Herbert S. Wilf
University of Pennsylvania
Philadelphia, PA 19104-6395
Copyright Notice
Copyright 1994 by Herbert S. Wilf. This material may be reproduced for any educational purpose, multiple
copies may be made for classes, etc. Charges, if any, for reproduced copies must be just enough to recover
reasonable costs of reproduction. Reproduction for commercial purposes is prohibited. This cover page must
be included in all distributed copies.
Internet Edition, Summer, 1994
This edition of Algorithms and Complexity is available at the web site <http://www/cis.upenn.edu/ wilf> .
It may be taken at no charge by all interested persons. Comments and corrections are welcome, and should
be sent to wilf@math.upenn.edu
CONTENTS
Chapter 0: What This Book Is About
0 . 1 B a c k g r o u n d ......................................1
0 . 2 H a r d v s . e a s y p r o b l e m s .................................2
0 . 3 A p r e v i e w .......................................4
Chapter 1: Mathematical Preliminaries
1 . 1 O r d e r s o f m a g n i t u d e ..................................5
1 . 2 P o s i t i o n a l n u m b e r s y s t e m s............................... 1 1
1.3 Manipulations with series ............................... 1 4
1 . 4 R e c u r r e n c e r e l a t i o n s .................................. 1 6
1 . 5 C o u n t i n g ...................................... 2 1
1 . 6 G r a p h s ....................................... 2 4
Chapter 2: Recursive Algorithms
2 . 1 I n t r o d u c t i o n..................................... 3 0
2 . 2 Q u i c k s o r t ...................................... 3 1
2.3 Recursive graph algorithms ............................... 3 8
2.4 Fast matrix multiplication ............................... 4 7
2 . 5 T h e d i s c r e t e F o u r i e r t r a n s f o r m ............................. 5 0
2.6 Applications of the FFT ................................ 5 6
2 . 7 A r e v i e w ....................................... 6 0
Chapter 3: The Network Flow Problem
3 . 1 I n t r o d u c t i o n..................................... 6 3
3.2 Algorithms for the network ï¬‚ow problem ......................... 6 4
3.3 The algorithm of Ford and Fulkerson .......................... 6 5
3 . 4 T h e m a x - ï¬‚ o w m i n - c u t t h e o r e m............................. 6 9
3.5 The complexity of the Ford-Fulkerson algorithm ..................... 7 0
3 . 6 L a y e r e d n e t w o r k s................................... 7 23.7 The MPM Algorithm ................................. 7 6
3.8 Applications of network ï¬‚ow .............................. 7 7
Chapter 4: Algorithms in the Theory of Numbers
4 . 1 P r e l i m i n a r i e s..................................... 8 1
4 . 2 T h e g r e a t e s t c o m m o n d i v i s o r.............................. 8 24.3 The extended Euclidean algorithm ........................... 8 5
4 . 4 P r i m a l i t y t e s t i n g ................................... 8 7
4.5 Interlude: the ring of integers modulo n......................... 8 9
4 . 6 P s e u d o p r i m a l i t y t e s t s................................. 9 2
4 . 7 P r o o f o f g o o d n e s s o f t h e s t r o n g p s e u d o p r i m a l i t y t e s t .................... 9 44.8 Factoring and cryptography .............................. 9 7
4.9 Factoring large integers ................................ 9 9
4 . 1 0 P r o v i n g p r i m a l i t y...................................1 0 0
iii
Chapter 5: NP-completeness
5 . 1 I n t r o d u c t i o n.....................................1 0 4
5.2 Turing machines ...................................1 0 9
5 . 3 C o o k â€™ s t h e o r e m ....................................1 1 25 . 4 S o m e o t h e r N P - c o m p l e t e p r o b l e m s ...........................1 1 6
5 . 5 H a l f a l o a f . . ......................................1 1 9
5.6 Backtracking (I): independent sets ...........................1 2 2
5.7 Backtracking (II): graph coloring ............................1 2 4
5.8 Approximate algorithms for hard problems . . . .....................1 2 8
iv
Preface
For the past several years mathematics majors in the computing track at the University of Pennsylvania
have taken a course in continuous algorithms (numerical analysis) in the junior year, and in discrete algo-
rithms in the senior year. This book has grown out of the senior course as I have been teaching it recently.
It has also been tried out on a large class of computer science and mathematics majors, including seniors
and graduate students, with good results.
Selection by the instructor of topics of interest will be very important, because normally Iâ€™ve found
that I canâ€™t cover anywhere near all of this material in a semester. A reasonable choice for a rst try might
be to begin with Chapter 2 (recursive algorithms) which contains lots of motivation. Then, as new ideas
are needed in Chapter 2, one might delve into the appropriate sections of Chapter 1 to get the conceptsand techniques well in hand. After Chapter 2, Chapter 4, on number theory, discusses material that is
extremely attractive, and surprisingly pure and applicable at the same time. Chapter 5 would be next, since
the foundations would then all be in place. Finally, material from Chapter 3, which is rather independent
of the rest of the book, but is strongly connected to combinatorial algorithms in general, might be studied
as time permits.
Throughout the book there are opportunities to ask students to write programs and get them running.
These are not mentioned explicitly, with a few exceptions, but will be obvious when encountered. Students
should all have the experience of writing, debugging, and using a program that is nontrivially recursive,for example. The concept of recursion is subtle and powerful, and is helped a lot by hands-on practice.
Any of the algorithms of Chapter 2 would be suitable for this purpose. The recursive graph algorithms are
particularly recommended since they are usually quite foreign to studentsâ€™ previous experience and therefore
have great learning value.
In addition to the exercises that appear in this book, then, student assignments might consist of writing
occasional programs, as well as delivering reports in class on assigned readings. The latter might be found
among the references cited in the bibliographies in each chapter.
I am indebted rst of all to the students on whom I worked out these ideas, and second to a num-
ber of colleagues for their helpful advice and friendly criticism. Among the latter I will mention Richard
Brualdi, Daniel Kleitman, Albert Nijenhuis, Robert Tarjan and Alan Tucker. For the no-doubt-numerous
shortcomings that remain, I accept full responsibility.
This book was typeset in T EX. To the extent that itâ€™s a delight to look at, thank T EX. For the deciencies
in its appearance, thank my limitations as a typesetter. It was, however, a pleasure for me to have had thechance to typeset my own book. My thanks to the Computer Science department of the University of
Pennsylvania, and particularly to Aravind Joshi, for generously allowing me the use of T EX facilities.
Herbert S. Wilf
v
Chapter 0: What This Book Is About
0.1 Background
An algorithm is a method for solving a class of problems on a computer. The complexity of an algorithm
is the cost, measured in running time, or storage, or whatever units are relevant, of using the algorithm to
solve one of those problems.
This book is about algorithms and complexity, and so it is about methods for solving problems on
computers and the costs (usually the running time) of using those methods.
Computing takes time. Some problems take a very long time, others can be done quickly. Some problems
seemto take a long time, and then someone discovers a faster way to do them (a â€˜faster algorithmâ€™). The
study of the amount of computational eort that is needed in order to perform certain kinds of computationsis the study of computational complexity .
Naturally, we would expect that a computing problem for which millions of bits of input data are
required would probably take longer than another problem that needs only a few items of input. So the time
complexity of a calculation is measured by expressing the running time of the calculation as a function of
some measure of the amount of data that is needed to describe the problem to the computer.
For instance, think about this statement: â€˜I just bought a matrix inversion program, and it can invert
annnmatrix in just 1 :2n
3minutes.â€™ We see here a typical description of the complexity of a certain
algorithm. The running time of the program is being given as a function of the size of the input matrix.
A faster program for the same job might run in 0 :8n3minutes for an nnmatrix. If someone were
to make a really important discovery (see section 2.4), then maybe we could actually lower the exponent,
instead of merely shaving the multiplicative constant. Thus, a program that would invert an nnmatrix
in only 7n2:8minutes would represent a striking improvement of the state of the art.
For the purposes of this book, a computation that is guaranteed to take at most cn3time for input of
sizenwill be thought of as an â€˜easyâ€™ computation. One that needs at most n10time is also easy. If a certain
calculation on an nnmatrix were to require 2nminutes, then that would be a â€˜hardâ€™ problem. Naturally
some of the computations that we are calling â€˜easyâ€™ may take a very long time to run, but still, from ourpresent point of view the important distinction to maintain will be the polynomial time guarantee or lack of
it.
The general rule is that if the running time is at most a polynomial function of the amount of input
data, then the calculation is an easy one, otherwise itâ€™s hard.
Many problems in computer science are known to be easy. To convince someone that a problem is easy,
it is enough to describe a fast method for solving that problem. To convince someone that a problem is
hard is hard, because you will have to prove to them that it is impossible to nd a fast way of doing the
calculation. It will notbe enough to point to a particular algorithm and to lament its slowness. After all,
thatalgorithm may be slow, but maybe thereâ€™s a faster way.
Matrix inversion is easy. The familiar Gaussian elimination method can invert an nnmatrix in time
at mostcn
3.
To give an example of a hard computational problem we have to go far aeld. One interesting one is
called the â€˜tiling problem.â€™ Suppose* we are given innitely many identical ï¬‚oor tiles, each shaped like aregular hexagon. Then we can tile the whole plane with them, i.e., we can cover the plane with no empty
spaces left over. This can also be done if the tiles are identical rectangles, but not if they are regular
pentagons.
In Fig. 0.1 we show a tiling of the plane by identical rectangles, and in Fig. 0.2 is a tiling by regular
hexagons.
That raises a number of theoretical and computational questions. One computational question is this.
Suppose we are given a certain polygon, not necessarily regular and not necessarily convex, and suppose we
have innitely many identical tiles in that shape. Can we or can we not succeed in tiling the whole plane?
That elegant question has been proved * to be computationally unsolvable. In other words, not only do
we not know of any fast way to solve that problem on a computer, it has been proved that there isnâ€™t any
* See, for instance, Martin Gardnerâ€™s article in Scientic American , January 1977, pp. 110-121.
* R. Berger, The undecidability of the domino problem, Memoirs Amer. Math. Soc. 66(1966), Amer.
Chapter 0: What This Book Is About
Fig. 0.1: Tiling with rectangles
Fig. 0.2: Tiling with hexagons
way to do it, so even looking for an algorithm would be fruitless. That doesnâ€™t mean that the question is
hard for every polygon. Hard problems can have easy instances. What has been proved is that no single
method exists that can guarantee that it will decide this question for every polygon.
The fact that a computational problem is hard doesnâ€™t mean that every instance of it has to be hard. The
problem is hard because we cannot devise an algorithm for which we can give a guarantee of fast performance
forallinstances.
Notice that the amount of input data to the computer in this example is quite small. All we need to
input is the shape of the basic polygon. Yet not only is it impossible to devise a fast algorithm for this
problem, it has been proved impossible to devise any algorithm at all that is guaranteed to terminate witha Yes/No answer after nitely many steps. Thatâ€™s really hard!
0.2 Hard vs. easy problems
Letâ€™s take a moment more to say in another way exactly what we mean by an â€˜easyâ€™ computation vs. a
â€˜hardâ€™ one.
Think of an algorithm as being a little box that can solve a certain class of computational problems.
Into the box goes a description of a particular problem in that class, and then, after a certain amount oftime, or of computational eort, the answer appears.
A â€˜fastâ€™ algorithm is one that carries a guarantee of fast performance. Here are some examples.
Example 1. It is guaranteed that if the input problem is described with Bbits of data, then an answer
will be output after at most 6B
3minutes.
Example 2. It is guaranteed that every problem that can be input with Bbits of data will be solved in at
most0:7B15seconds.
A performance guarantee, like the two above, is sometimes called a â€˜worst-case complexity estimate,â€™
and itâ€™s easy to see why. If we have an algorithm that will, for example, sort any given sequence of numbersinto ascending order of size (see section 2.2) it may nd that some sequences are easier to sort than others.
For instance, the sequence 1, 2, 7, 11, 10, 15, 20 is nearly in order already, so our algorithm might, if
it takes advantage of the near-order, sort it very rapidly. Other sequences might be a lot harder for it tohandle, and might therefore take more time.
Math. Soc., Providence, RI
2
0.2 Hard vs. easy problems
So in some problems whose input bit string has Bbits the algorithm might operate in time 6 B,a n do n
others it might need, say, 10 BlogBtime units, and for still other problem instances of length Bbits the
algorithm might need 5 B2time units to get the job done.
Well then, what would the warranty card say? It would have to pick out the worst possibility, otherwise
the guarantee wouldnâ€™t be valid. It would assure a user that if the input problem instance can be describedbyBbits, then an answer will appear after at most 5 B
2time units. Hence a performance guarantee is
equivalent to an estimation of the worst possible scenario: the longest possible calculation that might ensue
ifBbits are input to the program.
Worst-case bounds are the most common kind, but there are other kinds of bounds for running time.
We might give an average case bound instead (see section 5.7). That wouldnâ€™t guarantee performance no
worse than so-and-so; it would state that if the performance is averaged over all possible input bit strings of
Bbits, then the average amount of computing time will be so-and-so (as a function of B).
Now letâ€™s talk about the dierence between easy and hard computational problems and between fast
and slow algorithms.
A warranty that would notguarantee â€˜fastâ€™ performance would contain some function of Bthat grows
faster than anypolynomial. Like eB, for instance, or like 2p
B,e t c . It is the polynomial time vs. not
necessarily polynomial time guarantee that makes the dierence between the easy and the hard classes ofproblems, or between the fast and the slow algorithms.
It is highly desirable to work with algorithms such that we can give a performance guarantee for their
running time that is at most a polynomial function of the number of bits of input.
An algorithm is slowif, whatever polynomial Pwe think of, there exist arbitrarily large values of B,
and input data strings of Bbits, that cause the algorithm to do more than P(B) units of work.
A computational problem is tractable if there is a fast algorithm that will do all instances of it.
A computational problem is intractable if it can be proved that there is no fast algorithm for it.
Example 3. Here is a familiar computational problem and a method, or algorithm, for solving it. Letâ€™s see
if the method has a polynomial time guarantee or not.
The problem is this. Let nbe a given integer. We want to nd out if nisprime . The method that we
choose is the following. For each integer m=2;3;:::;bp
ncwe ask ifmdivides (evenly into) n.I fa l lo ft h e
answers are â€˜No,â€™ then we declare nto be a prime number, else it is composite.
We will now look at the computational complexity of this algorithm. That means that we are going to
nd out how much work is involved in doing the test. For a given integer nthe work that we have to do can
be measured in units of divisions of a whole number by another whole number. In those units, we obviouslywill do aboutp
nunits of work.
It seems as though this is a tractable problem, because, after all,pnis of polynomial growth in n.F o r
instance, we do less than nunits of work, and thatâ€™s certainly a polynomial in n, isnâ€™t it? So, according to
our denition of fast and slow algorithms, the distinction was made on the basis of polynomial vs. faster-than-polynomial growth of the work done with the problem size, and therefore this problem must be easy.
Right? Well no, not really.
Reference to the distinction between fast and slow methods will show that we have to measure the
amount of work done as a function of the number of bits of input to the problem . In this example, nis not
the number of bits of input. For instance, if n= 59, we donâ€™t need 59 bits to describe n, but only 6. In
general, the number of binary digits in the bit string of an integer nis close to log
2n.
So in the problem of this example, testing the primality of a given integer n, the length of the input bit
stringBis about log2n. Seen in this light, the calculation suddenly seems very long. A string consisting of
am e r el o g2n0â€™s and 1â€™s has caused our mighty computer to do aboutpnunits of work.
If we express the amount of work done as a function of B;we nd that the complexity of this calculation
is approximately 2B=2, and that grows much faster than any polynomial function of B.
Therefore, the method that we have just discussed for testing the primality of a given integer is slow.
See chapter 4 for further discussion of this problem. At the present time no one has found a fast wayto test for primality, nor has anyone proved that there isnâ€™t a fast way. Primality testing belongs to the
(well-populated) class of seemingly, but not provably, intractable problems.
In this book we will deal with some easy problems and some seemingly hard ones. Itâ€™s the â€˜seeminglyâ€™
that makes things very interesting. These are problems for which no one has found a fast computer algorithm,
3
Chapter 0: What This Book Is About
but also, no one has proved the impossibility of doing so. It should be added that the entire area is vigorously
being researched because of the attractiveness and the importance of the many unanswered questions that
remain.
Thus, even though we just donâ€™t know many things that weâ€™d like to know in this eld , it isnâ€™t for lack
of trying!
0.3 A preview
Chapter 1 contains some of the mathematical background that will be needed for our study of algorithms.
It is not intended that reading this book or using it as a text in a course must necessarily begin with Chapter
1. Itâ€™s probably a better idea to plunge into Chapter 2 directly, and then when particular skills or concepts
are needed, to read the relevant portions of Chapter 1. Otherwise the denitions and ideas that are in thatchapter may seem to be unmotivated, when in fact motivation in great quantity resides in the later chapters
of the book.
Chapter 2 deals with recursive algorithms and the analyses of their complexities.
Chapter 3 is about a problem that seems as though it might be hard, but turns out to be easy, namely the
network ï¬‚ow problem. Thanks to quite recent research, there are fast algorithms for network ï¬‚ow problems,and they have many important applications.
In Chapter 4 we study algorithms in one of the oldest branches of mathematics, the theory of num-
bers. Remarkably, the connections between this ancient subject and the most modern research in computermethods are very strong.
In Chapter 5 we will see that there is a large family of problems, including a number of very important
computational questions, that are bound together by a good deal of structural unity. We donâ€™t know if
theyâ€™re hard or easy. We do know that we havenâ€™t found a fast way to do them yet, and most people suspect
that theyâ€™re hard. We also know that if any one of these problems is hard, then they all are, and if any oneof them is easy, then they all are.
We hope that, having found out something about what people know and what people donâ€™t know, the
reader will have enjoyed the trip through this subject and may be interested in helping to nd out a littlemore.
4
1 . 1O r d e r so fm a g n i t u d e
Chapter 1: Mathematical Preliminaries
1.1 Orders of magnitude
In this section weâ€™re going to discuss the rates of growth of dierent functions and to introduce the ve
symbols of asymptotics that are used to describe those rates of growth. In the context of algorithms, the
reason for this discussion is that we need a good language for the purpose of comparing the speeds with
which dierent algorithms do the same job, or the amounts of memory that they use, or whatever other
measure of the complexity of the algorithm we happen to be using.
Suppose we have a method of inverting square nonsingular matrices. How might we measure its speed?
Most commonly we would say something like â€˜if the matrix is nnthen the method will run in time 16 :8n3.â€™
Then we would know that if a 100 100 matrix can be inverted, with this method, in 1 minute of computer
time, then a 200 200 matrix would require 23= 8 times as long, or about 8 minutes. The constant â€˜16.8â€™
wasnâ€™t used at all in this example; only the fact that the labor grows as the third power of the matrix size
was relevant.
Hence we need a language that will allow us to say that the computing time, as a function of n, grows
â€˜on the order of n3,â€™ or â€˜at most as fast as n3,â€™ or â€˜at least as fast as n5logn,â€™ etc.
The new symbols that are used in the language of comparing the rates of growth of functions are the
following ve: â€˜ oâ€™ (read â€˜is little oh ofâ€™), â€˜ Oâ€™ (read â€˜is big oh ofâ€™), â€˜â€™ (read â€˜is theta ofâ€™), â€˜ â€™ (read â€˜is
asymptotically equal toâ€™ or, irreverently, as â€˜twiddlesâ€™), and â€˜â„¦â€™ (read â€˜is omega ofâ€™).
Now letâ€™s explain what each of them means.Letf(x)a n dg(x) be two functions of x. Each of the ve symbols above is intended to compare the
rapidity of growth of fandg.I f w e s a y t h a t f(x)=o(g(x)), then informally we are saying that fgrows
more slowly than gdoes whenxis very large. Formally, we state the
Denition. We say that f(x)=o(g(x)) (x!1)iflim
x!1f(x)=g(x)exists and is equal to 0.
Here are some examples:
(a)x2=o(x5)
(b) sinx=o(x)
(c) 14:709px=o(x=2+7c o sx)
(d) 1=x=o(1) (?)
(e) 23 logx=o(x:02)
We can see already from these few examples that sometimes it might be easy to prove that a â€˜ oâ€™
relationship is true and sometimes it might be rather dicult. Example (e), for instance, requires the use of
Lâ€™Hospitalâ€™s rule.
If we have two computer programs, and if one of them inverts nnmatrices in time 635 n3and if the
other one does so in time o(n2:8) then we know that for all suciently large values of nthe performance
guarantee of the second program will be superior to that of the rst program. Of course, the rst program
might run faster on small matrices, say up to size 10 ;00010;000. If a certain program runs in time
n2:03and if someone were to produce another program for the same problem that runs in o(n2logn)t i m e ,
then that second program would be an improvement, at least in the theoretical sense. The reason for the
â€˜theoreticalâ€™ qualication, once more, is that the second program would be known to be superior only if n
were suciently large.
The second symbol of the asymptotics vocabulary is the â€˜ O.â€™ When we say that f(x)=O(g(x)) we
mean, informally, that fcertainly doesnâ€™t grow at a faster rate than g. It might grow at the same rate or it
might grow more slowly; both are possibilities that the â€˜ Oâ€™ permits. Formally, we have the next
Denition. We say that f(x)=O(g(x)) (x!1)if9C;x 0such that jf(x)j<Cg(x)(8x>x 0):
The qualier â€˜ x!1â€™ will usually be omitted, since it will be understood that we will most often be
interested in large values of the variables that are involved.
For example, it is certainly true that sin x=O(x), but even more can be said, namely that sin x=O(1).
Alsox3+5x2+7 7c o sx=O(x5)a n d1=(1 +x2)=O(1). Now we can see how the â€˜ oâ€™ gives more precise
information than the â€˜ O,â€™ for we can sharpen the last example by saying that 1 =(1 +x2)=o(1). This is
5
Chapter 1: Mathematical Preliminaries
sharper because not only does it tell us that the function is bounded when xis large, we learn that the
function actually approaches 0 as x!1.
This is typical of the relationship between Oando. It often happens that a â€˜ Oâ€™ result is sucient for
an application. However, that may not be the case, and we may need the more precise â€˜ oâ€™e s t i m a t e .
The third symbol of the language of asymptotics is the â€˜.â€™
Denition. We say that f(x)= (g(x))if there are constants c1>0;c2>0;x0such that for all x>x 0
it is true that c1g(x)<f(x)<c 2g(x).
We might then say that fandgare of the same rate of growth, only the multiplicative constants are
uncertain. Some examples of the â€˜â€™ at work are
(x+1 )2= ( 3x2)
(x2+5x+7 )=(5x3+7x+2 )= ( 1 =x)
q
3+p
2x= (x1
4)
(1 + 3=x)x= ( 1 ):
The â€˜â€™ is much more precise than either the â€˜ Oâ€™o rt h eâ€˜o.â€™ If we know that f(x)= (x2), then we know
thatf(x)=x2stays between two nonzero constants for all suciently large values of x. The rate of growth
offis established: it grows quadratically with x.
The most precise of the symbols of asymptotics is the â€˜ .â€™ It tells us that not only do fandggrow at
the same rate, but that in fact f=gapproaches 1 as x!1.
Denition. We say that f(x)g(x)iflimx!1f(x)=g(x)=1.
Here are some examples.
x2+xx2
(3x+1 )481x4
sin1=x1=x
(2x3+5x+7 )=(x2+4 )2x
2x+7l o gx+c o sx2x
Observe the importance of getting the multiplicative constants exactly right when the â€˜ â€™s y m b o li su s e d .
While it is true that 2 x2= (x2), it is not true that 2 x2x2. It is, by the way, also true that 2 x2= ( 1 7x2),
but to make such an assertion is to use bad style since no more information is conveyed with the â€˜17â€™ than
without it.
The last symbol in the asymptotic set that we will need is the â€˜â„¦.â€™ In a nutshell, â€˜â„¦â€™ is the negation of
â€˜o.â€™ That is to say, f(x)=â„¦ (g(x)) means that it is nottrue thatf(x)=o(g(x)). In the study of algorithms
for computers, the â€˜â„¦â€™ is used when we want to express the thought that a certain calculation takes at least
so-and-so long to do. For instance, we can multiply together two nnmatrices in time O(n3). Later on
in this book we will see how to multiply two matrices even faster, in time O(n2:81). People know of even
faster ways to do that job, but one thing that we can be sure of is this: nobody will ever be able to write
a matrix multiplication program that will multiply pairs nnmatrices with fewer than n2computational
steps, because whatever program we write will have to look at the input data, and there are 2 n2entries in
the input matrices.
Thus, a computing time of cn2is certainly a lower bound on the speed of any possible general matrix
multiplication program. We might say, therefore, that the problem of multiplying two nnmatrices requires
â„¦(n2)t i m e .
The exact denition of the â€˜â„¦â€™ that was given above is actually rather delicate. We stated it as the
negation of something. Can we rephrase it as a positive assertion? Yes, with a bit of work (see exercises 6
and 7 below). Since â€˜ f=o(g)â€™ means that f=g!0, the symbol f=â„¦ (g)m e a n st h a t f=gdoes not approach
zero. If we assume that gtakes positive values only, which is usually the case in practice, then to say that
f=gdoes not approach 0 is to say that 9>0 and an innite sequence of values of x, tending to 1, along
which jfj=g> . So we donâ€™t have to show that jfj=g> for all largex, but only for innitely many large
x.
6
1 . 1O r d e r so fm a g n i t u d e
Denition. We say that f(x)=â„¦ (g(x))if there is an >0and a sequence x1;x2;x3;:::!1 such that
8j:jf(xj)j>g(xj).
Now letâ€™s introduce a hierarchy of functions according to their rates of growth when xis large. Among
commonly occurring functions of xthat grow without bound as x!1, perhaps the slowest growing ones are
functions like log log xor maybe (log log x)1:03or things of that sort. It is certainly true that log log x!1
asx!1, but it takes its time about it. When x=1;000;000, for example, log log xhas the value 2.6.
Just a bit faster growing than the â€˜snailsâ€™ above is log xitself. After all, log (1 ;000;000) = 13:8. So if
we had a computer algorithm that could do nthings in time log nand someone found another method that
could do the same job in time O(log logn), then the second method, other things being equal, would indeed
be an improvement, but nmight have to be extremely large before you would notice the improvement.
Next on the scale of rapidity of growth we might mention the powers of x. For instance, think about
x:01. It grows faster than log x, although you wouldnâ€™t believe it if you tried to substitute a few values of x
and to compare the answers (see exercise 1 at the end of this section).
How would we prove thatx:01grows faster than log x? By using Lâ€™Hospitalâ€™s rule.
Example. Consider the limit of x:01=logxforx!1.A sx!1 the ratio assumes the indeterminate form
1=1, and it is therefore a candidate for Lâ€™Hospitalâ€™s rule, which tells us that if we want to nd the limit
then we can dierentiate the numerator, dierentiate the denominator, and try again to let x!1.I f w e
do this, then instead of the original ratio, we nd the ratio
:01xâˆ’:99=(1=x)=:01x:01
which obviously grows without bound as x!1. Therefore the original ratio x:01=logxalso grows without
bound. What we have proved, precisely, is that log x=o(x:01), and therefore in that sense we can say that
x:01grows faster than logx.
To continue up the scale of rates of growth, we meet x:2,x,x15,x15log2x,e t c . ,a n dt h e nw ee n c o u n t e r
functions that grow faster than every xed power of x,j u s ta sl o g xgrows slower than every xed power of
x.
Considerelog2x. Since this is the same as xlogxit will obviously grow faster than x1000, in fact it will
be larger than x1000as soon as log x>1000,i.e.,a ss o o na s x>e1000(donâ€™t hold your breath!).
Henceelog2xis an example of a function that grows faster than every xed power of x.A n o t h e r s u c h
example is epx(why?).
Denition. A function that grows faster than xa, for every constant a, but grows slower than cxfor
every constant c>1is said to be of moderately exponential growth . More precisely, f(x)is of moderately
exponential growth if for every a>0we havef(x)=â„¦ (xa)and for every >0we havef(x)=o((1 +)x).
Beyond the range of moderately exponential growth are the functions that grow exponentially fast.
Typical of such functions are (1 :03)x,2x,x97x, and so forth. Formally, we have the
Denition. A function fis of exponential growth if there exists c>1such thatf(x)=â„¦ (cx)and there
existsdsuch thatf(x)=O(dx).
If we clutter up a function of exponential growth with smaller functions then we will not change the
fact that it is of exponential growth. Thus epx+2x=(x49+ 37) remains of exponential growth, because e2xis,
all by itself, and it resists the eorts of the smaller functions to change its mind.
Beyond the exponentially growing functions there are functions that grow as fast as you might please.
Liken!, for instance, which grows faster than cnfor every xed constant c, and like 2n2, which grows much
faster than n!. The growth ranges that are of the most concern to computer scientists are â€˜betweenâ€™ the very
slowly, logarithmically growing functions and the functions that are of exponential growth. The reason is
simple: if a computer algorithm requires more than an exponential amount of time to do its job, then it will
probably not be used, or at any rate it will be used only in highly unusual circumstances. In this book, the
algorithms that we will deal with all fall in this range.
Now we have discussed the various symbols of asymptotics that are used to compare the rates of growth
of pairs of functions, and we have discussed the pecking order of rapidity of growth, so that we have a small
catalogue of functions that grow slowly, medium-fast, fast, and super-fast. Next letâ€™s look at the growth of
sums that involve elementary functions, with a view toward discovering the rates at which the sums grow.
7
Chapter 1: Mathematical Preliminaries
Think about this one:
f(n)=nX
j=0j2
=12+22+32++n2:(1:1:1)
Thus,f(n) is the sum of the squares of the rst npositive integers. How fast does f(n) grow when nis
large?
Notice at once that among the nterms in the sum that denes f(n), the biggest one is the last one,
namelyn2. Since there are nterms in the sum and the biggest one is only n2, it is certainly true that
f(n)=O(n3) ,a n de v e nm o r e ,t h a t f(n)n3for alln1.
Suppose we wanted more precise information about the growth of f(n), such as a statement like f(n)?.
How might we make such a better estimate?
The best way to begin is to visualize the sum in (1.1.1) as shown in Fig. 1.1.1.
Fig. 1.1.1: How to overestimate a sum
In that gure we see the graph of the curve y=x2,i nt h ex-yplane. Further, there is a rectangle drawn
over every interval of unit length in the range from x=1t ox=n. The rectangles all lie under the curve.
Consequently, the total area of all of the rectangles is smaller than the area under the curve, which is to say
that
nâˆ’1X
j=1j2Zn
1x2dx
=(n3âˆ’1)=3:(1:1:2)
If we compare (1.1.2) and (1.1.1) we notice that we have proved that f(n)((n+1 )3âˆ’1)=3.
Now weâ€™re going to get a lower bound onf(n) in the same way. This time we use the setup in Fig.
1.1.2, where we again show the curve y=x2, but this time we have drawn the rectangles so they lie above
the curve.
From the picture we see immediately that
12+22++n2Zn
0x2dx
=n3=3:(1:1:3)
Now our function f(n) has been bounded on both sides, rather tightly. What we know about it is that
8n1:n3=3f(n)((n+1 )3âˆ’1)=3:
From this we have immediately that f(n)n3=3, which gives us quite a good idea of the rate of growth of
f(n)w h e nnis large. The reader will also have noticed that the â€˜ â€™ gives a much more satisfying estimate
of growth than the â€˜ Oâ€™d o e s .
8
1 . 1O r d e r so fm a g n i t u d e
Fig. 1.1.2: How to underestimate a sum
Letâ€™s formulate a general principle, for estimating the size of a sum, that will make estimates like the
above for us without requiring us each time to visualize pictures like Figs. 1.1.1 and 1.1.2. The general idea
is that when one is faced with estimating the rates of growth of sums, then one should try to compare thesums with integrals because theyâ€™re usually easier to deal with.
Let a function g(n) be dened for nonnegative integer values of n, and suppose that g(n) is nondecreasing.
We want to estimate the growth of the sum
G(n)= nX
j=1g(j)(n=1;2;:::): (1:1:4)
Consider a diagram that looks exactly like Fig. 1.1.1 except that the curve that is shown there is now the
curvey=g(x). The sum of the areas of the rectangles is exactly G(nâˆ’1), while the area under the curve
between 1 and nisRn
1g(t)dt. Since the rectangles lie wholly under the curve, their combined areas cannot
exceed the area under the curve, and we have the inequality
G(nâˆ’1)Zn
1g(t)dt (n1): (1:1:5)
On the other hand, if we consider Fig. 1.1.2, where the graph is once more the graph of y=g(x),
the fact that the combined areas of the rectangles is now not less than the area under the curve yields the
inequality
G(n)Zn
0g(t)dt (n1): (1:1:6)
If we combine (1.1.5) and (1.1.6) we nd that we have completed the proof of
Theorem 1.1.1. Letg(x)be nondecreasing for nonnegative x.T h e n
Zn
0g(t)dtnX
j=1g(j)Zn+1
1g(t)dt: (1:1:7)
The above theorem is capable of producing quite satisfactory estimates with rather little labor, as the
following example shows.
Letg(n)=l o gnand substitute in (1.1.7). After doing the integrals, we obtain
nlognâˆ’nnX
j=1logj(n+1 )l o g(n+1 )âˆ’n: (1:1:8)
9
Chapter 1: Mathematical Preliminaries
We recognize the middle member above as log n!, and therefore by exponentiation of (1.1.8) we have
(n
e)nn!(n+1 )n+1
en: (1:1:9)
This is rather a good estimate of the growth of n!, since the right member is only about netimes as large as
the left member (why?), when nis large.
By the use of slightly more precise machinery one can prove a better estimate of the size of n!t h a ti s
called Stirlingâ€™s formula , which is the statement that
x!(x
e)xp
2x: (1:1:10)
Exercises for section 1.1
1. Calculate the values of x:01and of logxforx= 10, 1000, 1,000,000. Find a single value of x>10 for
whichx:01>logx, and prove that your answer is correct.
2. Some of the following statements are true and some are false. Which are which?
(a) (x2+3x+1 )3x6
(b) (px+1 )3=(x2+1 )=o(1)
(c)e1=x= ( 1 )
(d) 1=x0
(e)x3(log logx)2=o(x3logx)
(f)plogx+1=â„¦ ( l o gl o g x)
(g) sinx=â„¦ ( 1 )
(h) cosx=x=O(1)
(i)Rx
4dt=tlogx
(j)Rx
0eâˆ’t2dt=O(1)
(k)P
jx1=j2=o(1)
(l)P
jx1x
3. Each of the three sums below denes a function of x. Beneath each sum there appears a list of ve
assertions about the rate of growth, as x!1, of the function that the sum denes. In each case state
which of the ve choices, if any, are true (note: more than one choice may be true).
h1(x)=X
jxf1=j+3=j2+4=j3g
(i)logx(ii) =O(x) (iii) 2l o gx(iv) = (log x)( v )=â„¦ ( 1 )
h2(x)=X
jpxflogj+jg
(i)x=2 (ii) =O(px) (iii) = (pxlogx)( i v )=â„¦ (px)( v )=o(px)
h3(x)=X
jpx1=p
j
(i) =O(px) (ii) = â„¦(x1=4) (iii) =o(x1=4)( i v ) 2x1=4(v) = (x1=4)
4. Of the ve symbols of asymptotics O;o;;;â„¦, which ones are transitive (e.g.,i ff=O(g)a n dg=O(h),
isf=O(h)?)?
5. The point of this exercise is that if fgrows more slowly than g, then we can always nd a third function
hwhose rate of growth is between that of fand ofg. Precisely, prove the following: if f=o(g) then there
10
1.2 Positional number systems
is a function hsuch thatf=o(h)a n dh=o(g). Give an explicit construction for the function hin terms
offandg.
6.fThis exercise is a warmup for exercise 7. gBelow there appear several mathematical propositions. In
each case, write a proposition that is the negation of the given one. Furthermore, in the negation, do not use
the word â€˜notâ€™ or any negation symbols. In each case the question is, â€˜If this isnâ€™ttrue, then what istrue?â€™
(a)9x>03f(x)6=0
(b)8x>0;f(x)>0
(c)8x>0;9>03f(x)<
(d)9x6=038y<0;f(y)<f(x)
(e)8x9y38z:g(x)<f(y)f(z)
(f)8>09x38y>x :f(y)<
Can you formulate a general method for negating such propositions? Given a proposition that contains â€˜ 8,â€™
â€˜9,â€™ â€˜3,â€™ what rule would you apply in order to negate the proposition and leave the result in positive form
(containing no negation symbols or â€˜notâ€™s).
7. In this exercise we will work out the denition of the â€˜â„¦.â€™
(a) Write out the precise denition of the statement â€˜lim x!1h(x)=0 â€™( u s eâ€˜ â€™s).
(b) Write out the negation of your answer to part (a), as a positive assertion.
(c) Use your answer to part (b) to give a positive denition of the assertion â€˜ f(x)6=o(g(x)),â€™ and
thereby justify the denition of the â€˜â„¦â€™ symbol that was given in the text.
8. Arrange the following functions in increasing order of their rates of growth, for large n. That is, list them
so that each one is â€˜little ohâ€™ of its successor:
2pn;elogn3;n3:01;2n2;
n1:6;logn3+1;p
n!;n3l o gn;
n3logn;(log logn)3;n:52n;(n+4 )12
9. Find a function f(x) such that f(x)=O(x1+) is true for every >0, but for which it is not true that
f(x)=O(x).
10. Prove that the statement â€˜ f(n)=O((2 +)n) for every >0â€™ is equivalent to the statement â€˜ f(n)=
o((2 +)n) for every >0.â€™
1.2 Positional number systems
This section will provide a brief review of the representation of numbers in dierent bases. The usual
decimal system represents numbers by using the digits 0 ;1;:::;9. For the purpose of representing whole
numbers we can imagine that the powers of 10 are displayed before us like this:
:::;100000;10000;1000;100;10;1:
Then, to represent an integer we can specify how many copies of each power of 10 we would like to have. If
we write 237, for example, then that means that we want 2 100â€™s and 3 10â€™s and 7 1â€™s.
In general, if we write out the string of digits that represents a number in the decimal system, as
dmdmâˆ’1d1d0, then the number that is being represented by that string of digits is
n=mX
i=0di10i:
Now letâ€™s try the binary system . Instead of using 10â€™s weâ€™re going to use 2â€™s. So we imagine that the
powers of 2 are displayed before us, as
:::;512;256;128;64;32;16;8;4;2;1:
11
Chapter 1: Mathematical Preliminaries
To represent a number we will now specify how many copies of each power of 2 we would like to have. For
instance, if we write 1101, then we want an 8, a 4 and a 1, so this must be the decimal number 13. We will
write
(13) 10= (1101) 2
to mean that the number 13, in the base 10, is the same as the number 1101, in the base 2.
In the binary system (base 2) the only digits we will ever need are 0 and 1. What that means is that if
we use only 0â€™s and 1â€™s then we can represent every number nin exactly one way. The unique representation
of every number, is, after all, what we must expect and demand of any proposed system.
Letâ€™s elaborate on this last point. If we were allowed to use more digits than just 0â€™s and 1â€™s then we
would be able to represent the number (13) 10as a binary number in a whole lot of ways. For instance, we
might make the mistake of allowing digits 0, 1, 2, 3. Then 13 would be representable by 3 22+120or by
222+221+120etc.
So if we were to allow too many dierent digits, then numbers would be representable in more than one
way by a string of digits.
If we were to allow too few dierent digits then we would nd that some numbers have no representation
at all. For instance, if we were to use the decimal system with only the digits 0 ;1;:::;8, then innitely many
numbers would not be able to be represented, so we had better keep the 9â€™s.
The general proposition is this.
Theorem 1.2.1. Letb>1be a positive integer (the â€˜baseâ€™). Then every positive integer ncan be written
in one and only one way in the form
n=d0+d1b+d2b2+d3b3+
if thedigitsd0;d1;:::lie in the range 0dibâˆ’1, for alli.
Remark: The theorem says, for instance, that in the base 10 we need the digits 0 ;1;2;:::; 9, in the base
2 we need only 0 and 1, in the base 16 we need sixteen digits, etc.
Proof of the theorem: Ifbis xed, the proof is by induction on n, the number being represented. Clearly
the number 1 can be represented in one and only one way with the available digits (why?). Suppose,inductively, that every integer 1 ;2;:::;n âˆ’1 is uniquely representable. Now consider the integer n. Dene
d=nmodb.T h e ndis one of the bpermissible digits. By induction, the number n
0=(nâˆ’d)=bis uniquely
representable, say
nâˆ’d
b=d0+d1b+d2b2+:::
Then clearly,
n=d+nâˆ’d
bb
=d+d0b+d1b2+d2b3+:::
is a representation of nthat uses only the allowed digits.
Finally, suppose that nhas some other representation in this form also. Then we would have
n=a0+a1b+a2b2+:::
=c0+c1b+c2b2+:::
Sincea0andc0are both equal to nmodb, they are equal to each other. Hence the number n0=(nâˆ’a0)=b
has two dierent representations, which contradicts the inductive assumption, since we have assumed the
truth of the result for all n0<n.
The basesbthat are the most widely used are, aside from 10, 2 (â€˜binary systemâ€™), 8 (â€˜octal systemâ€™)
and 16 (â€˜hexadecimal systemâ€™).
The binary system is extremely simple because it uses only two digits. This is very convenient if youâ€™re
a computer or a computer designer, because the digits can be determined by some component being either
â€˜onâ€™ (digit 1) or â€˜oâ€™ (digit 0). The binary digits of a number are called its bitsor itsbit string .
12
1.2 Positional number systems
The octal system is popular because it provides a good way to remember and deal with the long bit
strings that the binary system creates. According to the theorem, in the octal system the digits that we
need are 0;1;:::;7. For instance,
(735) 8= (477) 10:
The captivating feature of the octal system is the ease with which we can convert between octal and binary.
If we are given the bit string of an integer n, then to convert it to octal, all we have to do is to group the
bits together in groups of three, starting with the least signicant bit, then convert each group of three bits,independently of the others, into a single octal digit. Conversely, if the octal form of nis given, then the
binary form is obtainable by converting each octal digit independently into the three bits that represent it
in the binary system.
For example, given (1101100101)
2. To convert this binary number to octal, we group the bits in threes,
(1)(101)(100)(101)
starting from the right, and then we convert each triple into a single octal digit, thereby getting
(1101100101) 2= (1545) 8:
If youâ€™re a working programmer itâ€™s very handy to use the shorter octal strings to remember, or to write
down, the longer binary strings, because of the space saving, coupled with the ease of conversion back andforth.
The hexadecimal system (base 16) is like octal, only more so. The conversion back and forth to binary
now uses groups of fourbits, rather than three. In hexadecimal we will need, according to the theorem
above, 16 digits. We have handy names for the rst 10 of these, but what shall we call the â€˜digits 10 through
15â€™? The names that are conventionally used for them are â€˜A,â€™ â€˜B,â€™...,â€˜F.â€™ We have, for example,
(A52C)
16= 10(4096) + 5(256) + 2(16) + 12
= (42284) 10
= (1010) 2(0101) 2(0010) 2(1100) 2
= (1010010100101100) 2
= (1)(010)(010)(100)(101)(100)= (122454)
8:
Exercises for section 1.2
1. Prove that conversion from octal to binary is correctly done by converting each octal digit to a binary
triple and concatenating the resulting triples. Generalize this theorem to other pairs of bases.
2. Carry out the conversions indicated below.
(a) (737) 10=( ? ) 3
(b) (101100) 2=( ? ) 16
(c) (3377) 8=( ? ) 16
(d) (ABCD )16=( ? ) 10
(e) (BEEF )16=( ? ) 8
3. Write a procedure convert (n;b:integer,digitstr :string), that will nd the string of digits that represents
nin the base b.
13
Chapter 1: Mathematical Preliminaries
1.3 Manipulations with series
In this section we will look at operations with power series, including multiplying them and nding their
sums in simple form. We begin with a little catalogue of some power series that are good to know. First we
have the nite geometric series
(1âˆ’xn)=(1âˆ’x)=1+x+x2++xnâˆ’1: (1:3:1)
This equation is valid certainly for all x6= 1, and it remains true when x= 1 also if we take the limit
indicated on the left side.
Why is (1.3.1) true? Just multiply both sides by 1 âˆ’xto clear of fractions. The result is
1âˆ’xn=( 1+x+x2+x3++xnâˆ’1)(1âˆ’x)
=( 1+x+x2++xnâˆ’1)âˆ’(x+x2+x3++xn)
=1âˆ’xn
and the proof is nished.
Now try this one. What is the value of the sum
9X
j=03j?
Observe that we are looking at the right side of (1.3.1) with x= 3. Therefore the answer is (310âˆ’1)=2. Try
to get used to the idea that a series in powers of xbecomes a number if xis replaced by a number , and if
we know a formula for the sum of the series then we know the number that it becomes.
Here are some more series to keep in your zoo. A parenthetical remark like â€˜( jxj<1)â€™ shows the set of
values ofxfor which the series converges.
1X
k=0xk=1=(1âˆ’x)( jxj<1) (1 :3:2)
ex=1X
m=0xm=m!( 1 :3:3)
sinx=1X
r=0(âˆ’1)rx2r+1=(2r+ 1)! (1 :3:4)
cosx=1X
s=0(âˆ’1)sx2s=(2s)! (1 :3:5)
log (1=(1âˆ’x)) =1X
j=1xj=j (jxj<1) (1 :3:6)
Can you nd a simple form for the sum (the logarithms are â€˜naturalâ€™)
1 + log 2 + (log 2)2=2! + (log 2)3=3! +?
Hint: Look at (1.3.3), and replace xby log 2.
Aside from merely substituting values of xinto known series, there are many other ways of using known
series to express sums in simple form. Letâ€™s think about the sum
1+22+34+48+516 ++N2Nâˆ’1: (1:3:7)
14
1.3 Manipulations with series
We are reminded of the nite geometric series (1.3.1), but (1.3.7) is a little dierent because of the multipliers
1;2;3;4;:::;N .
The trick is this. When confronted with a series that is similar to, but not identical with, a known
series, write down the known series as an equation, with the series on one side and its sum on the other.
Even though the unknown series involves a particular value of x, in this case x= 2, keep the known series
with its variable unrestricted. Then reach for an appropriate tool that will be applied to both sides of thatequation, and whose result will be that the known series will have been changed into the one whose sum we
needed.
In this case, since (1.3.7) reminds us of (1.3.1), weâ€™ll begin by writing down (1.3.1) again,
(1âˆ’x
n)=(1âˆ’x)=1+x+x2++xnâˆ’1(1:3:8)
Donâ€™t replace xby 2 yet, just walk up to the equation (1.3.8) carrying your tool kit and ask what kind
of surgery you could do to both sides of (1.3.8) that would be helpful in evaluating the unknown (1.3.7).
We are going to reach into our tool kit and pull out â€˜d
dx.â€™ In other words, we are going to dierentiate
(1.3.8). The reason for choosing dierentiation is that it will put the missing multipliers 1 ;2;3;:::;N into
(1.3.8). After dierentiation, (1.3.8) becomes
1+2x+3x2+4x3++(nâˆ’1)xnâˆ’2=1âˆ’nxnâˆ’1+(nâˆ’1)xn
(1âˆ’x)2: (1:3:9)
Now itâ€™s easy. To evaluate the sum (1.3.7), all we have to do is to substitute x=2;n=N+ 1 in (1.3.9), to
obtain, after simplifying the right-hand side,
1+22+34+48++N2Nâˆ’1=1+(Nâˆ’1)2N: (1:3:10)
Next try this one:
1
232+1
333+ (1:3:11)
If we rewrite the series using summation signs, it becomes
1X
j=21
j3j:
Comparison with the series zoo shows great resemblance to the species (1.3.6). In fact, if we put x=1=3i n
(1.3.6) it tells us that
1X
j=11
j3j=l o g( 3=2): (1:3:12)
The desired sum (1.3.11) is the result of dropping the term with j= 1 from (1.3.12), which shows that the
sum in (1.3.11) is equal to log (3 =2)âˆ’1=3.
In general, suppose that f(x)=Panxnis some series that we know. ThenPnanxnâˆ’1=f0(x)a n dPnanxn=xf0(x). In other words, if the nthcoecient is multiplied by n, then the function changes from
fto (xd
dx)f. If we apply the rule again, we nd that multiplying the nthcoecient of a power series by n2
changes the sum from fto (xd
dx)2f.T h a ti s ,
1X
j=0j2xj=j!=(xd
dx)(xd
dx)ex
=(xd
dx)(xex)
=(x2+x)ex:
15
Chapter 1: Mathematical Preliminaries
Similarly, multiplying the nthcoecient of a power series by npwill change the sum from f(x)t o
(xd
dx)pf(x), but thatâ€™s not all. What happens if we multiply the coecient of xnby, say, 3n2+2n+5 ? I f
the sum previously was f(x), then it will be changed to f3(xd
dx)2+2 (xd
dx)+5gf(x). The sum
1X
j=0(2j2+5 )xj
is therefore equal to f2(xd
dx)2+5gf1=(1âˆ’x)g, and after doing the dierentiations we nd the answer in the
form (7x2âˆ’8x+5 )=(1âˆ’x)3.
Here is the general rule: if P(x) is any polynomial then
X
jP(j)ajxj=P(xd
dx)fX
jajxjg: (1:3:13)
Exercises for section 1.3
1. Find simple, explicit formulas for the sums of each of the following series.
(a)P
j3log 6j=j!
(b)P
m> 1(2m+7 )=5m
(c)P19
j=0(j=2j)
(d) 1 âˆ’x=2! +x2=4!âˆ’x3=6! +
(e) 1âˆ’1=32+1=34âˆ’1=36+
(f)P1
m=2(m2+3m+2 )=m!
2. Explain whyP
r0(âˆ’1)r2r+1=(2r+1 ) !=0 .
3. Find the coecient of tnin the series expansion of each of the following functions about t=0 .
(a) (1 +t+t2)et
(b) (3tâˆ’t2)sint
(c) (t+1 )2=(tâˆ’1)2
1.4 Recurrence relations
A recurrence relation is a formula that permits us to compute the members of a sequence one after
another, starting with one or more given values.
Here is a small example. Suppose we are to nd an innite sequence of numbers x0;x1;:::by means of
xn+1=cxn (n0;x0=1 ): (1:4:1)
This relation tells us that x1=cx0,a n dx2=cx1, etc., and furthermore that x0= 1. It is then clear that
x1=c; x 2=c2;:::;x n=cn;:::
We say that the solution of the recurrence relation (= â€˜dierence equationâ€™) (1.4.1) is given by xn=cn
for alln0. Equation (1.4.1) is a rst-order recurrence relation because a new value of the sequence is
computed from just one preceding value ( i.e.,xn+1is obtained solely from xn, and does not involve xnâˆ’1or
any earlier values).
Observe the format of the equation (1.4.1). The parenthetical remarks are essential. The rst one
â€˜n0â€™ tells us for what values of nthe recurrence formula is valid, and the second one â€˜ x0=1 â€™g i v e st h e
starting value. If one of these is missing, the solution may not be uniquely determined. The recurrence
relation
xn+1=xn+xnâˆ’1 (1:4:2)
needs two starting values in order to â€˜get going,â€™ but it is missing both of those starting values and the range
ofn. Consequently (1.4.2) (which is a second-order recurrence) does not uniquely determine the sequence.
16
1.4 Recurrence relations
The situation is rather similar to what happens in the theory of ordinary dierential equations. There,
if we omit initial or boundary values, then the solutions are determined only up to arbitrary constants.
Beyond the simple (1.4.1), the next level of diculty occurs when we consider a rst-order recurrence
relation with a variable multiplier, such as
xn+1=bn+1xn (n0;x0given): (1:4:3)
Nowfb1;b2;:::gis a given sequence, and we are being asked to nd the unknown sequence fx1;x2;:::g.
In an easy case like this we can write out the rst few xâ€™s and then guess the answer. We nd, successively,
thatx1=b1x0,t h e nx2=b2x1=b2b1x0andx3=b3x2=b3b2b1x0etc. At this point we can guess that the
solution is
xn=fnY
i=1bigx0(n=0;1;2;:::): (1:4:4)
Since that wasnâ€™t hard enough, weâ€™ll raise the ante a step further. Suppose we want to solve the
rst-order inhomogeneous (becausexn=0f o ra l lnis not a solution) recurrence relation
xn+1=bn+1xn+cn+1 (n0;x0given): (1:4:5)
Now we are being given two sequences b1;b2;:::andc1;c2;:::, and we want to nd the xâ€™s. Suppose we
follow the strategy that has so far won the game, that is, writing down the rst few xâ€™s and trying to guess
the pattern. Then we would nd that x1=b1x0+c1;x 2=b2b1x0+b2c1+c2, and we would probably tire
rapidly.
Here is a somewhat more orderly approach to (1.4.5). Though no approach will avoid the unpleasant
form of the general answer, the one that we are about to describe at least gives a method that is muchsimpler than the guessing strategy, for many examples that arise in practice. In this book we are going to
run into several equations of the type of (1.4.5), so a unied method will be a denite asset.
The rst step is to dene a new unknown function as follows. Let
x
n=b1b2bnyn(n1;x0=y0)( 1 :4:6)
dene a new unknown sequence y1;y2;:::Now substitute for xnin (1.4.5), getting
b1b2bn+1yn+1=bn+1b1b2bnyn+cn+1:
We notice that the coecients of yn+1and ofynare the same, and so we divide both sides by that coecient.
The result is the equation
yn+1=yn+dn+1 (n0;y0given) (1 :4:7)
w h e r ew eh a v ew r i t t e n dn+1=cn+1=(b1bn+1). Notice that the dâ€™s areknown .
We havenâ€™t yet solved the recurrence relation. We have only changed to a new unknown function that
satises a simpler recurrence (1.4.7). Now the solution of (1.4.7) is quite simple, because it says that each y
is obtained from its predecessor by adding the next one of the dâ€™s. It follows that
yn=y0+nX
j=1dj(n0):
We can now use (1.4.6) to reverse the change of variables to get back to the original unknowns x0;x1;:::,
and nd that
xn=(b1b2bn)fx0+nX
j=1djg(n1): (1:4:8)
It is not recommended that the reader memorize the solution that we have just obtained. It isrecom-
mended that the method by which the solution was found be mastered. It involves
(a) make a change of variables that leads to a new recurrence of the form (1.4.6), then
17
Chapter 1: Mathematical Preliminaries
(b) solve that one by summation and
(c) go back to the original unknowns.
As an example, consider the rst-order equation
xn+1=3xn+n (n0;x0=0 ): (1:4:9)
The winning change of variable, from (1.4.6), is to let xn=3nyn. After substituting in (1.4.9) and simplifying,
we nd
yn+1=yn+n=3n+1(n0;y0=0 ):
Now by summation,
yn=nâˆ’1X
j=1j=3j+1(n0):
Finally, since xn=3nynwe obtain the solution of (1.4.9) in the form
xn=3nnâˆ’1X
j=1j=3j+1(n0): (1:4:10)
This is quite an explicit answer, but the summation can, in fact, be completely removed by the same method
that you used to solve exercise 1(c) of section 1.3 (try it!).
That pretty well takes care of rst-order recurrence relations of the form xn+1=bn+1xn+cn+1,a n d
itâ€™s time to move on to linear second order (homogeneous) recurrence relations with constant coecients.These are of the form
x
n+1=axn+bxnâˆ’1(n1;x0and x 1given): (1:4:11)
If we think back to dierential equations of second-order with constant coecients, we recall that there
are always solutions of the form y(t)=etwhereis constant. Hence the road to the solution of such a
dierential equation begins by trying a solution of that form and seeing what the constant or constants 
turn out to be.
Analogously, equation (1.4.11) calls for a trial solution of the form xn=n. If we substitute xn=n
in (1.4.11) and cancel a common factor of nâˆ’1we obtain a quadratic equation for ,n a m e l y
2=a+b: (1:4:12)
â€˜Usuallyâ€™ this quadratic equation will have two distinct roots, say +andâˆ’, and then the general solution
of (1.4.11) will look like
xn=c1n
++c2n
âˆ’ (n=0;1;2;:::): (1:4:13)
The constants c1andc2will be determined so that x0;x 1have their assigned values.
Example. The recurrence for the Fibonacci numbers is
Fn+1=Fn+Fnâˆ’1(n1;F0=F1=1 ): (1:4:14)
Following the recipe that was described above, we look for a solution in the form Fn=n. After substituting
in (1.4.14) and cancelling common factors we nd that the quadratic equation for is, in this case, 2=+1.
If we denote the two roots by +=( 1+p
5)=2a n dâˆ’=( 1âˆ’p
5)=2, then the general solution to the
Fibonacci recurrence has been obtained, and it has the form (1.4.13). It remains to determine the constants
c1;c 2from the initial conditions F0=F1=1 .
From the form of the general solution we have F0=1=c1+c2andF1=1=c1++c2âˆ’.I fw es o l v e
these two equations in the two unknowns c1;c 2we nd that c1=+=p
5a n dc2=âˆ’âˆ’=p
5. Finally, we
substitute these values of the constants into the form of the general solution, and obtain an explicit formula
for thenthFibonacci number,
Fn=1p
51+p
5
2n+1
âˆ’1âˆ’p
5
2n+1
(n=0;1;:::): (1:4:15)
18
1.4 Recurrence relations
The Fibonacci numbers are in fact 1 ;1;2;3;5;8;13;21;34;:::It isnâ€™t even obvious that the formula
(1.4.15) gives integer values for the Fnâ€™s. The reader should check that the formula indeed gives the rst few
Fnâ€™s correctly.
Just to exercise our newly acquired skills in asymptotics, letâ€™s observe that since (1 +p
5)=2>1a n d
j(1âˆ’p
5)=2j<1, it follows that when nis large we have
Fn((1 +p
5)=2)n+1=p
5:
The process of looking for a solution in a certain form, namely in the form n, is subject to the same
kind of special treatment, in the case of repeated roots, that we nd in dierential equations. Correspondingto adouble rootof the associated quadratic equation 
2=a+bwe would nd two independent solutions
nandnn, so the general solution would be in the form n(c1+c2n).
Example. Consider the recurrence
xn+1=2xnâˆ’xnâˆ’1(n1;x0=1 ;x1=5 ): (1:4:16)
If we try a solution of the type xn=n, then we nd that satises the quadratic equation 2=2âˆ’1.
Hence the â€˜twoâ€™ roots are 1 and 1. The general solution is xn=1n(c1+nc2)=c1+c2n. After inserting the
given initial conditions, we nd that
x0=1=c1;x1=5=c1+c2
If we solve for c1andc2we obtainc1=1;c 2= 4, and therefore the complete solution of the recurrence
(1.4.16) is given by xn=4n+1 .
Now letâ€™s look at recurrent inequalities , like this one:
xn+1xn+xnâˆ’1+n2(n1;x0=0 ;x1=0 ): (1:4:17)
The question is, what restriction is placed on the growth of the sequence fxngby (1.4.17)?
By analogy with the case of dierence equations with constant coecients, the thing to try here is
xnKn. So suppose it is true that xnKnfor alln=0;1;2;:::;N . Then from (1.4.17) with n=N
we nd
xN+1KN+KNâˆ’1+N2:
Letcbe the positive real root of the equation c2=c+ 1, and suppose that >c.T h e n2>+1a n ds o
2âˆ’âˆ’1=t,s a y ,w h e r e t>0. Hence
xN+1KNâˆ’1(1 +)+N2
=KNâˆ’1(2âˆ’t)+N2
=KN+1âˆ’(tKNâˆ’1âˆ’N2):(1:4:18)
In order to insure that xN+1<KN+1what we need is for tKNâˆ’1>N2. Hence as long as we choose
K>max
N2
N2=tNâˆ’1/bracerightbig
; (1:4:19)
in which the right member is clearly nite, the inductive step will go through.
The conclusion is that (1.4.17) implies that for every xed >0,xn=O((c+)n), wherec=( 1 +p
5)=2.
The same argument applies to the general situation that is expressed in
19
Chapter 1: Mathematical Preliminaries
Theorem 1.4.1. Let a sequence fxngsatisfy a recurrent inequality of the form
xn+1b0xn+b1xnâˆ’1++bpxnâˆ’p+G(n)(np)
wherebi0(8i),Pbi>1. Further, let cbe the positive real root of * the equa tion cp+1=b0cp++bp.
Finally, suppose G(n)=o(cn). Then for every xed >0we havexn=O((c+)n).
Proof: Fix>0, and let=c+,w h e r ecis the root of the equation shown in the statement of the
theorem. Since >c,i fw el e t
t=p+1âˆ’b0pâˆ’âˆ’bp
thent>0. Finally, dene
K=m a x
jx0j;jx1j
;:::;jxpj
p;max
npG(n)
tnâˆ’p/bracerightbig
:
ThenKis nite, and clearly jxjjKjforjp. We claim that jxnjKnfor alln, which will complete
the proof.
Indeed, if the claim is true for 0 ;1;2;:::;n ,t h e n
jxn+1jb0jx0j++bpjxnâˆ’pj+G(n)
b0Kn++bpKnâˆ’p+G(n)
=Knâˆ’pfb0p++bpg+G(n)
=Knâˆ’pfp+1âˆ’tg+G(n)
=Kn+1âˆ’ftKnâˆ’pâˆ’G(n)g
Kn+1:
Exercises for section 1.4
1. Solve the following recurrence relations
(i)xn+1=xn+3 (n0;x0=1 )
(ii)xn+1=xn=3+2 (n0;x0=0 )
(iii)xn+1=2nxn+1 (n0;x0=0 )
(iv)xn+1=( (n+1 )=n)xn+n+1 (n1;x1=5 )
(v)xn+1=xn+xnâˆ’1(n1;x0=0 ;x1=3 )
(vi)xn+1=3xnâˆ’2xnâˆ’1(n1;x0=1 ;x1=3 )
(vii)xn+1=4xnâˆ’4xnâˆ’1(n1;x0=1 ;x1=)
2. Findx1if the sequence xsatises the Fibonacci recurrence relation and if furthermore x0=1a n d
xn=o(1) (n!1).
3. Letxnbe the average number of trailing 0â€™s in the binary expansions of all integers 0 ;1;2;:::;2nâˆ’1.
Find a recurrence relation satised by the sequence fxng, solve it, and evaluate lim n!1xn.
4. For what values of aandbis it true that no matter what the initial values x0,x1are, the solution of the
recurrence relation xn+1=axn+bxnâˆ’1(n1) is guaranteed to be o(1) (n!1)?
5. Suppose x0=1 ,x1= 1, and for all n2i ti st r u et h a t xn+1xn+xnâˆ’1.I si tt r u et h a t 8n:xnFn?
Prove your answer.6. Generalize the result of exercise 5, as follows. Suppose x
0=y0andx1=y1,w h e r eyn+1=ayn+
bynâˆ’1(8n1). If furthermore, xn+1axn+bxnâˆ’1(8n1), can we conclude that 8n:xnyn?I f
not, describe conditions on aandbunder which that conclusion would follow.
7. Find the asymptotic behavior in the form xn?(n!1) of the right side of (1.4.10).
*See exercise 10, below.
20
1.5 Counting
8. Write out a complete proof of theorem 1.4.1.
9. Show by an example that the conclusion of theorem 1.4.1 may be false if the phrase â€˜for every xed
>0:::â€™ were replaced by â€˜for every xed 0:::.â€™
10. In theorem 1.4.1 we nd the phrase â€˜... the positive real root of ...â€™ Prove that this phrase is justied, in
that the equation shown always has exactly one positive real root. Exactly what special properties of that
equation did you use in your proof?
1.5 Counting
For a given positive integer n, consider the set f1;2;:::ng. We will denote this set by the symbol [ n],
and we want to discuss the number of subsets of various kinds that it has. Here is a list of all of the subsetsof [2]: ;,f1g,f2g,f1;2g. There are 4 of them.
We claim that the set [ n] has exactly 2
nsubsets.
To see why, notice that we can construct the subsets of [ n] in the following way. Either choose, or donâ€™t
choose, the element â€˜1,â€™ then either choose, or donâ€™t choose, the element â€˜2,â€™ etc., nally choosing, or not
choosing, the element â€˜ n.â€™ Each of the nchoices that you encountered could have been made in either of 2
ways. The totality of nchoices, therefore, might have been made in 2ndierent ways, so that is the number
of subsets that a set of nobjects has.
N e x t ,s u p p o s ew eh a v e ndistinct objects, and we want to arrange them in a sequence. In how many
ways can we do that? For the rst object in our sequence we may choose any one of the nobjects. The
second element of the sequence can be any of the remaining nâˆ’1 objects, so there are n(nâˆ’1) possible
ways to make the rst two decisions. Then there are nâˆ’2 choices for the third element, and so we have
n(nâˆ’1)(nâˆ’2) ways to arrange the rst three elements of the sequence. It is no doubt clear now that there
are exactly n(nâˆ’1)(nâˆ’2)321=n! ways to form the whole sequence.
Of the 2nsubsets of [ n], how many have exactly kobjects in them? The number of elements in a
set is called its cardinality . The cardinality of a set Sis denoted by jSj, so, for example, j[6]j=6 . As e t
whose cardinality is kis called a â€˜k-set,â€™ and a subset of cardinality kis, naturally enough, a â€˜ k-subset.â€™ The
question is, for how many subsets Sof [n]i si tt r u et h a t jSj=k?
We can construct k-subsetsSof [n] (written â€˜ S[n]â€™) as follows. Choose an element a1(npossible
choices). Of the remaining nâˆ’1 elements, choose one ( nâˆ’1 possible choices), etc., until a sequence of k
dierent elements have been chosen. Obviously there were n(nâˆ’1)(nâˆ’2)(nâˆ’k+ 1) ways in which we
might have chosen that sequence, so the number of ways to choose an (ordered) sequence of kelements from
[n]i s
n(nâˆ’1)(nâˆ’2)(nâˆ’k+1 )=n!=(nâˆ’k)!:
But there are more sequences of kelements than there are k-subsets, because any particular k-subsetS
will correspond to k! dierent ordered sequences, namely all possible rearrangements of the elements of the
subset. Hence the number of k-subsets of [ n] is equal to the number of k-sequences divided by k!. In other
words, there are exactly n!=k!(nâˆ’k)!k-subsets of a set of nobjects.
The quantities n!=k!(nâˆ’k)! are the famous binomial coecients , and they are denoted by
n
k
=n!
k!(nâˆ’k)!(n0;0kn): (1:5:1)
Some of their special values are
n
0
=1 ( 8n0);n
1
=n(8n0);
n
2
=n(nâˆ’1)=2(8n0);n
n
=1 ( 8n0):
It is convenient to dene/parenleftbign
k
to be 0 ifk<0o ri fk>n.
We can summarize the developments so far with
21
Chapter 1: Mathematical Preliminaries
Theorem 1.5.1. For eachn0,as e to fnobjects has exactly 2nsubsets, and of these, exactly/parenleftbign
k
have
cardinality k(8k=0;1;:::;n ). There are exactly n!dierent sequences that can be formed from a set of n
distinct objects.
Since every subset of [ n]h a ssome cardinality, it follows that
nX
k=0n
k
=2n(n=0;1;2;:::): (1:5:2)
In view of the convention that we adopted, we might have written (1.5.2) asP
k/parenleftbign
k
=2n,w i t hn or e s t r i c t i o n
on the range of the summation index k. It would then have been understood that the range of kis from
âˆ’1to1, and that the binomial coecient/parenleftbign
k
vanishes unless 0 kn.
In Table 1.5.1 we show the values of some of the binomial coecients/parenleftbign
k
. The rows of the table
are thought of as labelled â€˜ n=0 , â€™â€˜n= 1,â€™ etc., and the entries within each row refer, successively, to
k=0;1;:::;n . The table is called â€˜Pascalâ€™s triangle.â€™
1
11
121
1331
14641
151 0 1 0 51
1 6 15 20 15 6 1
1 7 21 35 35 21 7 1
1 8 28 56 70 56 28 8 1
...................................................
..
Table 1.5.1: Pascalâ€™s triangle
Here are some facts about the binomial coecients:
(a) Each row of Pascalâ€™s triangle is symmetric about the middle. That is,
n
k
=n
nâˆ’k
(0kn;n0):
(b) The sum of the entries in the nthrow of Pascalâ€™s triangle is 2n.
(c) Each entry is equal to the sum of the two entries that are immediately above it in the triangle.
The proof of (c) above can be interesting. What it says about the binomial coecients is that
n
k
=nâˆ’1
kâˆ’1
+nâˆ’1
k
((n;k)6=( 0;0)): (1:5:3)
There are (at least) two ways to prove (1.5.3). The hammer-and-tongs approach would consist of expanding
each of the three binomial coecients that appears in (1.5.3), using the denition (1.5.1) in terms of factorials,
and then cancelling common factors to complete the proof.
That would work (try it), but hereâ€™s another way. Contemplate (this proof is by contemplation) the
totality ofk-subsets of [ n]. The number of them is on the left side of (1.5.3). Sort them out into two piles:
thosek-subsets that contain â€˜1â€™ and those that donâ€™t. If a k-subset of [ n] contains â€˜1,â€™ then its remaining
kâˆ’1 elements can be chosen in/parenleftbignâˆ’1
kâˆ’1
ways, and that accounts for the rst term on the right of (1.5.3). If a
k-subset does not contain â€˜1,â€™ then its kelements are all chosen from [ nâˆ’1], and that completes the proof
of (1.5.3).
22
1.5 Counting
Thebinomial theorem is the statement that 8n0w eh a v e
(1 +x)n=nX
k=0n
k
xk: (1:5:4)
Proof: By induction on n. Eq. (1.5.4) is clearly true when n= 0, and if it is true for some nthen multiply
both sides of (1.5.4) by (1 + x)t oo b t a i n
(1 +x)n+1=X
kn
k
xk+X
kn
k
xk+1
=X
kn
k
xk+X
kn
kâˆ’1
xk
=X
kn
k
+n
kâˆ’1/bracerightbig
xk
=X
kn+1
k
xk
which completes the proof.
Now letâ€™s ask how big the binomial coecients are, as an exercise in asymptotics. We will refer to the
coecients in row nof Pascalâ€™s triangle, that is, to
n
0
;n
1
; :::;n
n
as the coecients of ordern. Then, by (1.5.2) (or by (1.5.4) with x= 1), the sum of all of the coecients
of ordernis 2n. It is also fairly apparent, from an inspection of Table 1.5.1, that the largest one(s) of the
coecients of order nis (are) the one(s) in the middle.
More precisely, if nis odd, then the largest coecients of order nare/parenleftbign
(nâˆ’1)=2
and/parenleftbign
(n+1)=2
, whereas
ifnis even, the largest one is uniquely/parenleftbign
n=2
.
It will be important, in some of the applications to algorithms later on in this book, for us to be able
to pick out the largest term in a sequence of this kind, so letâ€™s see how we could prove that the biggest
coecients are the ones cited above.
Fornxed, we will compute the ratio of the ( k+1 )stcoecient of order nto thekth. We will see then
that the ratio is larger than 1 if k<(nâˆ’1)=2 and is<1i fk>(nâˆ’1)=2. That, of course, will imply that
the (k+1 )stcoecient is bigger than the kth,f o rs u c hk, and therefore that the biggest one(s) must be in
the middle.
The ratio is /parenleftbign
k+1
/parenleftbign
k=n!=f(k+1 ) ! (nâˆ’kâˆ’1)!g
n!=fk!(nâˆ’k)!g
=k!(nâˆ’k)!
(k+1 ) ! (nâˆ’kâˆ’1)!
=(nâˆ’k)=(k+1 )
and is>1i k<(nâˆ’1)=2, as claimed.
OK, the biggest coecients are in the middle, but how big are they? Letâ€™s suppose that nis even, just
to keep things simple. Then the biggest binomial coecient of order nis
n
n=2
=n!
(n=2)!2
(n
e)np
2n
f(n
2e)n
2png2
=r
2
n2n(1:5:5)
23
Chapter 1: Mathematical Preliminaries
where we have used Stirlingâ€™s formula (1.1.10).
Equation (1.5.5) shows that the single biggest binomial coecient accounts for a very healthy fraction
of the sum of allof the coecients of order n. Indeed, the sum of all of them is 2n, and the biggest one is
p
2=n2n.W h e nnis large, therefore, the largest coecient contributes a fraction p
2=nof the total.
If we think in terms of the subsets that these coecients count, what we will see is that a large fraction
of all of the subsets of an n-set have cardinality n=2, in fact ( nâˆ’:5) of them do. This kind of probabilistic
thinking can be very useful in the design and analysis of algorithms. If we are designing an algorithm thatdeals with subsets of [ n], for instance, we should recognize that a large percentage of the customers for that
algorithm will have cardinalities near n=2, and make every eort to see that the algorithm is fast for such
subsets, even at the expense of possibly slowing it down on subsets whose cardinalities are very small or verylarge.
Exercises for section 1.5
1. How many subsets of even cardinality does [ n]h a v e ?
2. By observing that (1 + x)
a(1 +x)b=( 1 +x)a+b, prove that the sum of the squares of all binomial
coecients of order nis/parenleftbig2n
n
.
3. Evaluate the following sums in simple form.
(i)Pn
j=0j/parenleftbign
j
(ii)Pn
j=3/parenleftbign
j
5j
(iii)Pn
j=0(j+1 ) 3j+1
4. Find, by direct application of Taylorâ€™s theorem, the power series expansion of f(x)=1=(1âˆ’x)m+1about
the origin. Express the coecients as certain binomial coecients.
5. Complete the following twiddles.
(i)/parenleftbig2n
n
?
(ii)/parenleftbign
blog2nc
?
(iii)/parenleftbign
bnc
?
(iv)/parenleftbign2
n
?
6. How many ordered pairs of unequal elements of [ n] are there?
7. Which one of the numbers f2j/parenleftbign
j
gn
j=0is the biggest?
1.6 Graphs
A graph is a collection of vertices , certain unordered pairs of which are called its edges. To describe a
particular graph we rst say what its vertices are, and then we say which pairs of vertices are its edges. The
set of vertices of a graph Gis denoted by V(G), and its set of edges is E(G).
Ifvandware vertices of a graph G, and if (v;w)i sa ne d g eo f G, then we say that vertices v,ware
adjacent inG.
Consider the graph Gwhose vertex set is f1;2;3;4;5gand whose edges are the set of pairs (1,2), (2,3),
(3,4), (4,5), (1,5). This is a graph of 5 vertices and 5 edges. A nice way to present a graph to an audience
is to draw a picture of it, instead of just listing the pairs of vertices that are its edges. To draw a picture of
a graph we would rst make a point for each vertex, and then we would draw an arc between two vertices v
andwif and only if ( v;w) is an edge of the graph that we are talking about. The graph Gof 5 vertices and
5 edges that we listed above can be drawn as shown in Fig. 1.6.1(a). It could also be drawn as shown inFig. 1.6.1(b). Theyâ€™re both the same graph. Only the pictures are dierent, but the pictures arenâ€™t â€˜reallyâ€™
the graph; the graph is the vertex list and the edge list. The pictures are helpful to us in visualizing and
remembering the graph, but thatâ€™s all.
The number of edges that contain (â€˜are incident withâ€™) a particular vertex vof a graphGis called the
degree of that vertex, and is usually denoted by (v). If we add up the degrees of every vertex vofGwe will
have counted exactly two contributions from each edge of G, one at each of its endpoints. Hence, for every
24
1.6 Graphs
Fig. 1.6.1(a) Fig. 1.6.1(b)
graphGwe haveX
v2V(G)(v)=2jE(G)j: (1:6:1)
Since the right-hand side is an even number, there must be an even number of odd numbers on the left side
of (1.6.1). We have therefore proved that every graph has an even number of vertices whose degrees are odd .*
In Fig. 1.6.1 the degrees of the vertices are f2;2;2;2;2gand the sum of the degrees is 10 = 2 jE(G)j.
Next weâ€™re going to dene a number of concepts of graph theory that will be needed in later chapters.
A fairly large number of terms will now be dened, in rather a brief space. Donâ€™t try to absorb them allnow, but read through them and look them over again when the concepts are actually used, in the sequel.
ApathPin a graphGis a walk from one vertex of Gto another, where at each step the walk uses an
edge of the graph. More formally, it is a sequence fv
1;v2;:::;v kgof vertices of Gsuch that 8i=1;kâˆ’1:
(vi;vi+1)2E(G).
A graph is connected if there is a path between every pair of its vertices.
Ap a t h Pissimple if its vertices are all distinct, Hamiltonian if it is simple and visits every vertex of G
exactly once, Eulerian if it uses every edgeofGexactly once.
Asubgraph of a graph Gis a subset Sof its vertices to gether with a subset of just those edges of G
both of whose endpoints lie in S.A ninduced subgraph ofGis a subset Sof the vertices of Gtogether with
alledges ofGboth of whose endpoints lie in S. We would then speak of â€˜the subgraph induced by S.â€™
In a graph Gwe can dene an equivalence relation on the vertices as follows. Say that vandware
equivalent if there is a path of Gthat joins them. Let Sbe one of the equivalence classes of vertices of G
under this relation. The subgraph of GthatSinduces is called a connected component of the graph G.A
graph is connected if and only if it has exactly one connected component.
Acycleis a closed path, i.e., one in which vk=v1. A cycle is a circuit ifv1is the only repeated vertex
in it. We may say that a circuit is a simple cycle. We speak of Hamiltonian and Eulerian circuits of Gas
circuits ofGthat visit, respectively, every vertex, or every edge, of a graph G.
Not every graph has a Hamiltonian path. The graph in Fig. 1.6.2(a) has one and the graph in Fig.
1.6.2(b) doesnâ€™t.
Fig. 1.6.2(a) Fig. 1.6.2(b)
* Did you realize that the number of people who shook hands an odd number of times yesterday is an
even number of people?
25
Chapter 1: Mathematical Preliminaries
Fig. 1.6.3(a) Fig. 1.6.3(b)
Likewise, not every graph has an Eulerian path. The graph in Fig. 1.6.3(a) has one and the graph in
Fig. 1.6.3(b) doesnâ€™t.
There is a world of dierence between Eulerian and Hamiltonian paths, however. If a graph Gis given,
then thanks to the following elegant theorem of Euler, it is quite easy to decide whether or not Ghas an
Eulerian path. In fact, the theorem applies also to multigraphs , which are graphs except that they are allowed
to have several dierent edges joining the same pair of vertices.
Theorem 1.6.1. A (multi-)graph has an Eulerian circuit (resp. path) if and only if it is connected and has
no (resp. has exactly two) vertices of odd degree.
Proof: LetGbe a connected multigraph in which every vertex has even degree. We will nd an Eulerian
circuit inG. The proof for Eulerian paths will be similar, and is omitted.
The proof is by induction on the number of edges of G, and the result is clearly true if Ghas just one
edge.
Hence suppose the theorem is true for all such multigraphs of fewer than medges, and let Ghavem
edges. We will construct an Eulerian circuit of G.
Begin at some vertex vand walk along some edge to a vertex w. Generically, having arrived at a vertex
u, depart from ualong an edge that hasnâ€™t been used yet, arriving at a new vertex, etc. The process halts
when we arrive for the rst time at a vertex v0such that all edges incident with v0have previously been
walked on, so there is no exit.
We claim that v0=v,i.e., weâ€™re back where we started. Indeed, if not, then we arrived at v0one more
time than we departed from it, each time using a new edge, and nding no edges remaining at the end. Thus
there were an odd number of edges of Gincident with v0, a contradiction.
Hence we are indeed back at our starting point when the walk terminates. Let Wdenote the sequence
of edges along which we have so far walked. If Wincludes all edges of Gthen we have found an Euler tour
and we are nished.
Else there are edges of Gthat are not in W. Erase all edges of WfromG, thereby obtaining a (possibly
disconnected multi-) graph G0. LetC1;:::;C kdenote the connected components of G0. Each of them has
only vertices of even degree because that was true of Gand of the walk Wthat we subtracted from G.
Since each of the Cihas fewer edges than Ghad, there is, by induction, an Eulerian circuit in each of the
connected components of G0.
We will thread them all together to make such a circuit for Gitself.
Begin at the same vand walk along 0 or more edges of Wuntil you arrive for the rst time at a vertex
qof component C1. This will certainly happen because Gis connected. Then follow the Euler tour of the
edges ofC1, which will return you to vertex q. Then continue your momentarily interrupted walk Wuntil
you reach for the rst time a vertex of C2, which will surely happen because Gis connected, etc., and the
proof is complete.
It is extremely dicult computationally to decide if a given graph has a Hamilton path or circuit. We
will see in Chapter 5 that this question is typical of a breed of problems that are the main subject of that
chapter, and are perhaps the most (in-)famous unsolved problems in theoretical computer science. Thanksto Eulerâ€™s theorem (theorem 1.6.1) it is easyto decide if a graph has an Eulerian path or circuit.
Next weâ€™d like to discuss graph coloring , surely one of the prettier parts of graph theory. Suppose that
there areKcolors available to us, and that we are presented with a graph G.Aproper coloring of the
vertices of Gis an assignment of a color to each vertex of Gin such a way that 8e2E(G) the colors of
26
1.6 Graphs
the two endpoints of eare dierent. Fig. 1.6.4(a) shows a graph Gand an attempt to color its vertices
properly in 3 colors (â€˜R,â€™ â€˜Yâ€™ and â€˜Bâ€™). The attempt failed because one of the edges of Ghas had the same
color assigned to both of its endpoints. In Fig. 1.6.4(b) we show the same graph with a successful proper
coloring of its vertices in 4 colors.
Fig. 1.6.4(a) Fig. 1.6.4(b)
Thechromatic number (G) of a graph Gis the minimum number of colors that can be used in a proper
coloring of the vertices of G.Abipartite graph is a graph whose chromatic number is 2,i.e., it is a graph
that can be 2-colored. That means that the vertices of a bipartite graph can be divided into two classes â€˜Râ€™
and â€˜Yâ€™ such that no edge of the graph runs between two â€˜Râ€™ vertices or between two â€˜Yâ€™ vertices. Bipartite
graphs are most often drawn, as in Fig. 1.6.5, in two layers, with all edges running between layers.
Fig. 1.6.5: A bipartite graph
Thecomplement Gof a graphGis the graph that has the same vertex set that Ghas and has an edge
exactly where Gdoes not have its edges. Formally,
E(G)=f(v;w)jv;w2V(G);v6=w;(v;w)=2E(G)g:
Here are some special families of graphs that occur so often that they rate special names. The complete
graphKnis the graph of nvertices in which every possible one of the/parenleftbign
2
edges is actually present. Thus
K2is a single edge, K3looks like a triangle, etc. The empty graph Knconsists of nisolated vertices, i.e.,
has no edges at all.
Thecomplete bipartite graph Km;nhas a setSofmvertices and a set Tofnvertices. Its edge set
isE(Km;n)=ST.I t h a s jE(Km;n)j=mnedges. The n-cycle,Cn, is a graph of nvertices that are
connected to form a single cycle. A treeis a graph that (a) is connected and (b ) has no cycles. A tree is
shown in Fig. 1.6.6.
Fig. 1.6.6: A tree
27
Chapter 1: Mathematical Preliminaries
It is not hard to prove that the following are equivalent descriptions of a tree.
(a) A tree is a graph that is connected and has no cycles.
(b) A tree is a graph Gthat is connected and for which jE(G)j=jV(G)jâˆ’1.
(c) A tree is a graph Gwith the property that between every pair of distinct vertices there is a unique
path.
IfGis a graph and SV(G), thenSis anindependent set of vertices of Gif no two of the vertices in
Sare adjacent in G. An independent set Sismaximal if it is not a proper subset of another independent set
of vertices of G. Dually, if a vertex subset Sinduces a complete graph, then we speak of a complete subgraph
ofG. A maximal complete subgraph of Gis called a clique.
A graph might be labeled orunlabeled . The vertices of a labeled graph are numbered 1 ;2;:::;n .O n e
dierence that this makes is that there are a lot more labeled graphs than there are unlabeled graphs. There
are, for example, 3 labeled graphs that have 3 vertices and 1 edge. They are shown in Fig. 1.6.7.
Fig. 1.6.7: Three labeled graphs...
There is, however, only 1 unlabeled graph that has 3 vertices and 1 edge, as shown in Fig. 1.6.8.
Fig. 1.6.8: ... but only one unlabeled graph
Most counting problems on graphs are much easier for labeled than for unlabeled graphs. Consider the
following question: how many graphs are there that have exactly nvertices? Suppose rst that we mean
labeled graphs. A graph of nvertices has a maximum of/parenleftbign
2
edges. To construct a graph we would decide
which of these possible edges would be used. We can make each of these/parenleftbign
2
decisions independently, and
for every way of deciding where to put the edges we would get a dierent graph. Therefore the number of
labeled graphs of nvertices is 2(n
2)=2n(nâˆ’1)=2.
If we were to ask the corresponding question for unlabeled graphs we would nd it to be very hard.
The answer is known, but the derivation involves Burnsideâ€™s lemma about the action of a group on a set,
and some fairly delicate counting arguments. We will state the approximate answer to this question, which
is easy to write out, rather than the exact answer, which is not. If gnis the number of unlabeled graphs of
nvertices then
gn2(n
2)=n!:
Exercises for section 1.6
1. Show that a tree is a bipartite graph.
2. Find the chromatic number of the n-cycle.
3. Describe how you would nd out, on a computer, if a given graph Gis bipartite.
4. Given a positive integer K. Find two dierent graphs each of whose chromatic numbers is K.
5. Exactly how many labeled graphs of nvertices and Eedges are there?
6. In how many labeled graphs of nvertices do vertices f1;2;3gform an independent set?
7. How many cliques does an n-cycle have?
8. True or false: a Hamilton circuit is an induced cycle in a graph.9. Which graph of nvertices has the largest number of independent sets? How many does it have?
10. Draw all of the connected, unlabeled graphs of 4 vertices.
11. LetGbe a bipartite graph that has qconnected components. Show that there are exactly 2
qways to
properly color the vertices of Gin 2 colors.
12. Find a graph Gofnvertices, other than the complete graph, whose chromatic number is equal to 1 plus
the maximum degree of any vertex of G.
28
1.6 Graphs
13. Letnbe a multiple of 3. Consider a labeled graph Gthat consists of n=3 connected components, each
of them aK3. How many maximal independent sets does Ghave?
14. Describe the complement of the graph Gin exercise 13 above. How many cliques does it have?
15. In how many labeled graphs of nvertices is the subgraph that is induced by vertices f1;2;3ga triangle?
16. LetHbe a labeled graph of Lvertices. In how many labeled graphs of nvertices is the subgraph that
is induced by vertices f1;2;:::;L gequal toH?
17. Devise an algorithm that will decide if a given graph, of nvertices and medges, does or does not contain
a triangle, in time O(max(n2;mn)).
18. Prove that the number of labeled graphs of nvertices all of whose vertices have evendegree is equal to
the number of all labeled graphs of nâˆ’1 vertices.
29
Chapter 2: Recursive Algorithms
Chapter 2: Recursive Algorithms
2.1 Introduction
Here are two dierent ways to dene n!, ifnis a positive integer. The rst denition is nonrecursive,
the second is recursive.
(1) â€˜n! is the product of all of the whole numbers from 1 to n, inclusive.â€™
(2) â€˜Ifn=1t h e nn!=1 ,e l s en!=n(nâˆ’1)!.â€™
Letâ€™s concentrate on the second denition. At a glance, it seems illegal, because weâ€™re dening something,
and in the denition the same â€˜somethingâ€™ appears. Another glance, however, reveals that the value of n!i s
dened in terms of the value of the same function at a smaller value of its argument, viz.nâˆ’1. So weâ€™re
really only using mathematical induction in order to validate the assertion that a function has indeed beendened for all positive integers n.
What is the practical import of the above? Itâ€™s monumental. Many modern high-level computer
languages can handle recursive constructs directly, and when this is so, the programmerâ€™s job may be
considerably simplied. Among recursive languages are Pascal, PL/C, Lisp, APL, C, and many others.
Programmers who use these languages should be aware of the power and versatility of recursive methods(conversely, people who like recursive methods should learn one of those languages!).
A formal â€˜functionâ€™ module that would calculate n!nonrecursively might look like this.
functionfact(n);
fcomputesn! for givenn>0g
fact:= 1;
fori:= 1tondofact:=ifact;
end.
On the other hand a recursiven! module is as follows.
functionfact(n);
ifn=1thenfact:= 1
elsefact:=nfact(nâˆ’1);
end.
The hallmark of a recursive procedure is that it calls itself , with arguments that are in some sense
smaller than before. Notice that there are no visible loops in the recursive routine. Of course there will
be loops in the compiled machine-language program, so in eect the programmer is shifting many of the
bookkeeping problems to the compiler (but itdoesnâ€™t mind!).
Another advantage of recursiveness is that the thought processes are helpful. Mathematicians have
known for years that induction is a marvellous method for proving theorems, making constructions, etc.Now computer scientists and programmers can protably think recursively too, because recursive compilers
allow them to express such thoughts in a natural way, and as a result many methods of great power are being
formulated recursively, methods which, in many cases, might not have been developed if recursion were notreadily available as a practical programming tool.
Observe next that the â€˜trivial case,â€™ where n= 1, is handled separately, in the recursive form of the n!
program above. This trivial case is in fact essential, because itâ€™s the only thing that stops the execution of
the program. In eect, the computer will be caught in a loop, reducing nby 1, until it reaches 1, then it will
actually know the value of the function fact, and after that it will be able to climb back up to the original
input value of n.
The overall structure of a recursive routine will always be something like this:
30
2.2 Quicksort
procedurecalculate (list of variables);
ifftrivialcase gthen doftrivialthing g
elsedo
fcallcalculate (smaller values of the variables) g;
fmaybe do a few more things g
end.
In this chapter weâ€™re going to work out a number of examples of recursive algorithms, of varying
sophistication. We will see how the recursive structure helps us to analyze the running time, or complexity,
of the algorithms. We will also nd that there is a bit of art involved in choosing the list of variables that
a recursive procedure operates on. Sometimes the rst list we think of doesnâ€™t work because the recursivecall seems to need more detailed information than we have provided for it. So we try a larger list, and then
perhaps it works, or maybe we need a still larger list ..., but more of this later.
Exercises for section 2.1
1. Write a recursive routine that will nd the digits of a given integer nin the base b. There should be no
visible loops in your program.
2.2 Quicksort
Suppose that we are given an array x[1];:::;x [n]o fnnumbers. We would like to rearrange these
numbers as necessary so that they end up in nondecreasing order of size. This operation is called sorting the
numbers.
For instance, if we are given f9;4;7;2;1g, then we want our program to output the sorted array
f1;2;4;7;9g.
There are many methods of sorting, but we are going to concentrate on methods that rely on only
two kinds of basic operations, called comparisons andinterchanges . This means that our sorting routine is
allowed to
(a) pick up two numbers (â€˜keysâ€™) from the array, compare them, and decide which is larger.
(b) interchange the positions of two selected keys.
Here is an example of a rather primitive sorting algorithm:
(i) nd, by successive comparisons, the smallest key
(ii) interchange it with the rst key
(iii) nd the second smallest key
(iv) interchange it with the second key, etc. etc.
Here is a more formal algorithm that does the job above.
procedureslowsort (X: array[1::n]);
fsorts a given array into nondecreasing order g
forr:= 1tonâˆ’1d o
forj:=r+1tondo
ifx[j]<x[r]thenswap(x[j];x[r])
end.fslowsort g
If you are wondering why we called this method â€˜primitive,â€™ â€˜slowsort,â€™ and other pejorative names, the
reason will be clearer after we look at its complexity.
What is the cost of sorting nnumbers by this method? We will look at two ways to measure that cost.
First letâ€™s choose our unit of cost to be one comparison of two numbers, and then we will choose a dierent
unit of cost, namely one interchange of position (â€˜swapâ€™) of two numbers.
31
Chapter 2: Recursive Algorithms
How many paired comparisons does the algorithm make? Reference to procedure slowsort shows that it
makes one comparison for each value of j=r+1;:::;n in the inner loop. This means that the total number
of comparisons is
f1(n)=nâˆ’1X
r=1nX
j=r+11
=nâˆ’1X
r=1(nâˆ’r)
=(nâˆ’1)n=2:
The number of comparisons is ( n2), which is quite a lot of comparisons for a sorting method to do. Not
only that, but the method does that many comparisons regardless of the input array, i.e.its best case and
its worst case are equally bad.
The Quicksort* method, which is the main object of study in this section, does a maximum ofcn2
comparisons, but on the average it does far fewer, a mere O(nlogn) comparisons. This economy is much
appreciated by those who sort, because sorting applications can be immense and time consuming. One
popular sorting application is in alphabetizing lists of names. It is easy to imagine that some of those lists
are very long, and that the replacement of ( n2) by an average of O(nlogn) comparisons is very welcome.
An insurance company that wants to alphabetize its list of 5,000,000 policyholders will gratefully notice the
dierence between n2=2 5;000;000;000;000 comparisons and nlogn=7 7;124;740 comparisons.
If we choose as our unit of complexity the number of swaps of position, then the running time may
depend strongly on the input array. In the â€˜slowsortâ€™ method described above, some arrays will need no
swaps at all while others might require the maximum number of ( nâˆ’1)n=2 (which arrays need that many
swaps?). If we average over all n! possible arrangements of the input data, assuming that the keys are
distinct, then it is not hard to see that the average number of swaps that slowsort needs is ( n2).
Now letâ€™s discuss Quicksort. In contrast to the sorting method above, the basic idea of Quicksort is
sophisticated and powerful. Suppose we want to sort the following list:
26;18;4;9;37;119;220;47;74 (2 :2:1)
The number 37 in the above list is in a very intriguing position. Every number that precedes it is smaller
than it is and every number that follows it is larger than it is. What that means is that after sorting the list,
the 37 will be in the same place it now occupies, the numbers to its left will have been sorted but will still beon its left, and the numbers on its right will have been sorted but will still be on its right .
If we are fortunate enough to be given an array that has a â€˜splitter,â€™ like 37, then we can
(a) sort the numbers to the left of the splitter, and then
(b) sort the numbers to the right of the splitter.
Obviously we have here the germ of a recursive sorting routine.
The ï¬‚y in the ointment is that most arrays donâ€™t have splitters, so we wonâ€™t often be lucky enough to
nd the state of aairs that exists in (2.2.1). However, we can make our own splitters, with some extra work,
and that is the idea of the Quicksort algorithm. Letâ€™s state a preliminary version of the recursive procedureas follows (look carefully for how the procedure handles the trivial case where n=1).
procedurequicksortprelim (x:an array of nnumbers);
fsorts the array xinto nondecreasing order g
ifn2then
permute the array elements so as to create a splitter;
letx[i] be the splitter that was just created;
quicksortprelim (the subarray x
1;:::;x iâˆ’1) in place;
quicksortprelim (the subarray xi+1;:::;x n) in place;
end.fquicksortprelim g
* C. A. R. Hoare, Comp. J. ,5(1962), 10-15.
32
2.2 Quicksort
This preliminary version wonâ€™t run, though. It looks like a recursive routine. It seems to call itself twice
in order to get its job done. But it doesnâ€™t. It calls something thatâ€™s just slightly dierent from itself in
order to get its job done, and that wonâ€™t work.
Observe the exact purpose of Quicksort, as described above. We are given an array of length n,a n d
we want to sort it, all of it . Now look at the two â€˜recursive calls,â€™ which really arenâ€™t quite. The rst one
of them sorts the array to the left of xi. That is indeed a recursive call, because we can just change the â€˜ nâ€™
to â€˜iâˆ’1â€™ and call Quicksort. The second recursive call is the problem. It wants to sort a portion of the
array that doesnâ€™t begin at the beginning of the array. The routine Quicksort as written so far doesnâ€™t have
enough ï¬‚exibility to do that. So we will have to give it some more parameters.
Instead of trying to sort allof the given array, we will write a routine that sorts only the portion of the
given array xthat extends from x[left]t ox[right], inclusive, where leftandright are input parameters.
This leads us to the second version of the routine:
procedureqksort (x:array;left;right :integer);
fsorts the subarray x[left];:::;x [right]g
ifright âˆ’left1then
create a splitter for the subarray in the itharray position;
qksort (x;left;i âˆ’1);
qksort (x;i+1;right )
end.fqksort g
Once we have qksort, of course, Quicksort is no problem: we call qksort with left:= 1 andright :=n.
The next item on the agenda is the little question of how to create a splitter in an array. Suppose we
are working with a subarray
x[left];x[left+1 ];:::;x [right]:
The rst step is to choose one of the subarray elements (the element itself, not the position of the element)
to be the splitter, and the second step is to make it happen. The choice of the splitter element in the
Quicksort algorithm is done very simply: at random . We just choose, using our favorite random number
generator, one of the entries of the given subarray, letâ€™s call it T, and declare it to be the splitter. To repeat
the parenthetical comment above, Tis the value of the array entry that was chosen, not its position in the
array. Once the value is selected, the position will be what it has to be, namely to the right of all smaller
entries, and to the left of all larger entries.
The reason for making the random choice will become clearer after the smoke of the complexity discussion
has cleared, but brieï¬‚y itâ€™s this: the analysis of the average case complexity is realtively easy if we use the
random choice, so thatâ€™s a plus, and there are no minuses.
Second, we have now chosen Tto be the value around which the subarray will be split. The entries of
the subarray must be moved so as to make Tthe splitter. To do this, consider the following algorithm.*
* Attributed to Nico Lomuto by Jon Bentley, CACM 27 (April 1984).
33
Chapter 2: Recursive Algorithms
proceduresplit(x;left;right;i )
fchooses at random an entry Tof the subarray
[xleft;xright], and splits the subarray around Tg
fthe output integer iis the position of Tin the
output array: x[i]=Tg;
1L:= a random integer in [ left;right ];
2swap(x[left];x[L]);
3fnow the splitter is rst in the subarray g
4T:=x[left];
5i:=left;
6forj:=left+1toright do
begin
7 ifx[j]<Tthen
begin
8 i:=i+1
swap(x[i];x[j])
end;
end
9swap(x[left];x[i])
10 end. fsplitg
We will now prove the correctness of split.
Theorem 2.2.1. Proceduresplitcorrectly splits the array xaround the chosen value T.
Proof: We claim that as the loop in lines 7 and 8 is repeatedly executed for j:=left+1to right ,t h e
following three assertions will always be true just aftereach execution of lines 7, 8:
(a)x[left]=Tand
(b)x[r]<Tfor allleft<r iand
(c)x[r]Tfor alli<rj
Fig. 2.2.1 illustrates the claim.
Fig. 2.2.1: Conditions (a), (b), (c)
To see this, observe rst that (a), (b), (c) are surely true at the beginning, when j=left+1 . N e x t ,i f
for somejthey are true, then the execution of lines 7, 8 guarantee that they will be true for the next value
ofj.
Now look at (a), (b), (c) when j=right. It tells us that just prior to the execution of line 9 the
condition of the array will be
(a)x[left]=Tand
(b)x[r]<Tfor allleft<r iand
(c)x[r]Tfor alli<rright.
When line 9 executes, the array will be in the correctly split condition.
Now we can state a â€˜nalâ€™ version of qksort (and therefore of Quicksort too).
34
2.2 Quicksort
procedureqksort (x:array;left; right :integer);
fsorts the subarray x[left];:::;x [right]g;
ifright âˆ’left1then
split(x;left;right;i );
qksort (x;left;i âˆ’1);
qksort (x;i+1;right )
end.fqksort g
procedureQuicksort (x:array;n:integer)
fsorts an array of length ng;
qksort (x;1;n)
end.fQuicksort g
Now letâ€™s consider the complexity of Quicksort. How long does it take to sort an array? Well, the
amount of time will depend on exactly which array we happen to be sorting, and furthermore it will depend
on how lucky we are with our random choices of splitting elements.
If we want to see Quicksort at its worst, suppose we have a really unlucky day, and that the random
choice of the splitter element happens to be the smallest element in the array. Not only that, but suppose
this kind of unlucky choice is repeated on each and every recursive call.
If the splitter element is the smallest array entry, then it wonâ€™t do a whole lot of splitting. In fact, if
the original array had nentries, then one of the two recursive calls will be to an array with no entries at all,
and the other recursive call will be to an array of nâˆ’1e n t r i e s .I f L(n) is the number of paired comparisons
that are required in this extreme scenario, then, since the number of comparisons that are needed to carry
out the call to splitan array of length nisnâˆ’1, it follows that
L(n)=L(nâˆ’1) +nâˆ’1(n1;L(0) = 0):
Hence,
L(n)=( 1+2+ +(nâˆ’1)) = (n2):
The worst-case behavior of Quicksort is therefore quadratic in n. In its worst moods, therefore, it is as bad
as â€˜slowsort â€™a b o v e .
Whereas the performance of slowsort is pretty much always quadratic, no matter what the input is,
Quicksort is usually a lot faster than its worst case discussed above.
We want to show that on the average the running time of Quicksort is O(nlogn).
The rst step is to get quite clear about what the word â€˜averageâ€™ refers to. We suppose that the entries
of the input array xare all distinct. Then the performance of Quicksort can depend only on the sequence of
size relationships in the input array and the choices of the random splitting elements.
The actual numerical values that appear in the input array are not in themselves important, except that,
to simplify the discussion we will assume that they are all dierent. The only thing that will matter, then,
will be the set of outcomes of all of the paired comparisons of two elements that are done by the algorithm.
Therefore, we will assume, for the purposes of analysis, that the entries of the input array are exactly the
set of numbers 1 ;2;:::;n in some order.
There aren! possible orders in which these elements might appear, so we are considering the action of
Quicksort on just these n! inputs.
Second, for each particular one of these inputs, the choices of the splitting elements will be made by
choosing, at random, one of the entries of the array at each step of the recursion. We will also average over
all such random choices of the splitting elements.
Therefore, when we speak of the function F(n), theaverage complexity of Quicksort, we are speaking of
the average number of pairwise comparisons of array entries that are made by Quicksort, where the averaging
35
Chapter 2: Recursive Algorithms
is done rst of all over all n! of the possible input orderings of the array elements, and second, for each such
input ordering, we average also over all sequences of choices of the splitting elements.
Now letâ€™s consider the behavior of the function F(n). What we are going to show is that F(n)=
O(nlogn).
The labor that F(n) estimates has two components. First there are the pairwise comparisons involved
in choosing a splitting element and rearranging the array about the chosen splitting value. Second there are
the comparisons that are done in the two recursive calls that follow the creation of a splitter.
As we have seen, the number of comparisons involved in splitting the array is nâˆ’1. Hence it remains
to estimate the number of comparisons in the recursive calls.
For this purpose, suppose we have rearranged the array about the splitting element, and that it has
turned out that the splitting entry now occupies the ithposition in the array.
Our next remark is that each value of i=1;2;:::;n is equally likely to occur. The reason for this is that
we chose the splitter originally by choosing a random array entry. Since all orderings of the array entries are
equally likely, the one that we happened to have chosen was just as likely to have been the largest entry as
to have been the smallest, or the 17th-from-largest, or whatever.
Since each value of iis equally likely, each ihas probability 1 =nof being chosen as the residence of the
splitter.
If the splitting element lives in the itharray position, the two recursive calls to Quicksort will be on
two subarrays, one of which has length iâˆ’1 and the other of which has length nâˆ’i. The average numbers
of pairwise comparisons that are involved in such recursive calls are F(iâˆ’1) andF(nâˆ’i), respectively. It
follows that our average complexity function Fsatises the relation
F(n)=nâˆ’1+1
nnX
i=1fF(iâˆ’1) +F(nâˆ’i)g (n1): (2:2:2)
together with the initial value F(0) = 0.
How can we nd the solution of the recurrence relation (2.2.2)? First letâ€™s simplify it a little by noticing
thatnX
i=1fF(nâˆ’i)g=F(nâˆ’1) +F(nâˆ’2) ++F(0)
=nX
i=1fF(iâˆ’1)g(2:2:3)
and so (2.2.2) can be written as
F(n)=nâˆ’1+2
nnX
i=1F(iâˆ’1): (2:2:4)
We can simplify (2.2.4) a lot by getting rid of the summation sign. This next step may seem like a trick
at rst (and it is!), but itâ€™s a trick that is used in so many dierent ways that now we call it a â€˜method.â€™
What we do is rst to multiply (2.2.4) by n,t og e t
nF(n)=n(nâˆ’1) + 2nX
i=1F(iâˆ’1): (2:2:5)
Next, in (2.2.5) we replace nbynâˆ’1, yielding
(nâˆ’1)F(nâˆ’1) = (nâˆ’1)(nâˆ’2) + 2nâˆ’1X
i=1F(iâˆ’1): (2:2:6)
Finally we subtract (2.2.6) from (2.2.5), and the summation sign obligingly disappears, leaving behind just
nF(n)âˆ’(nâˆ’1)F(nâˆ’1) =n(nâˆ’1)âˆ’(nâˆ’1)(nâˆ’2) + 2F(nâˆ’1): (2:2:7)
36
2.2 Quicksort
After some tidying up, (2.2.7) becomes
F(n)=( 1+1
n)F(nâˆ’1) + (2 âˆ’2
n): (2:2:8)
which is exactly in the form of the general rst-order recurrence relation that we discussed in section 1.4.
In section 1.4 we saw that to solve (2.2.8) the winning tactic is to change to a new variable ynthat is
dened, in this case, by
F(n)=n+1
nn
nâˆ’1nâˆ’1
nâˆ’22
1yn
=(n+1 )yn:(2:2:9)
If we make the change of variable F(n)=(n+1 )ynin (2.2.8), then it takes the form
yn=ynâˆ’1+2 (nâˆ’1)=n(n+1 ) (n1) (2 :2:10)
as an equation for the ynâ€™s (y0=0 ) .
The solution of (2.2.10) is obviously
yn=2nX
j=1jâˆ’1
j(j+1 )
=2nX
j=1f2
j+1âˆ’1
jg
=2nX
j=11
jâˆ’4n=(n+1 ):
Hence from (2.2.9),
F(n)=2 (n+1 )fnX
j=11=jgâˆ’4n (2:2:11)
is the average number of pairwise comparisons that we do if we Quicksort an array of length n. Evidently
F(n)2nlogn(n!1) (see (1.1.7) with g(t)=1=t), and we have proved
Theorem 2.2.2. The average number of pairwise comparisons of array entries that Quicksort makes when
it sorts arrays of nelements is exactly as shown in (2.2.11), and is 2nlogn(n!1).
Quicksort is, on average, a very quick sorting method, even though its worst case requires a quadratic
amount of labor.
Exercises for section 2.2
1. Write out an array of 10 numbers that contains no splitter. Write out an array of 10 numbers that
contains 10 splitters.
2. Write a program that does the following. Given a positive integer n. Choose 100 random permutations
of [1;2;:::;n ],* and count how many of the 100 had at least one splitter. Execute your program for n=
5;6;:::;12 and tabulate the results.
3. Think of some method of sorting nnumbers that isnâ€™t in the text. In the worst case, how many comparisons
might your method do? How many swaps?
* For a fast and easy way to do this see A. Nijenhuis and H. S. Wilf, Combinatorial Algorithms , 2nd ed. (New
York: Academic Press, 1978), chap. 6.
37
Chapter 2: Recursive Algorithms
4. Consider the array
x=f2;4;1;10;5;3;9;7;8;6g
withleft=1a n dright = 10. Suppose that the procedure splitis called, and suppose the random integer
Lin step 1 happens to be 5. Carry out the complete splitalgorithm (not on a computer; use pencil and
paper). Particularly, record the condition of the array xafter each value of jis processed in the forj =:::
loop.
5. Suppose H(0) = 1 and H(n)1+1
nPn
i=1H(iâˆ’1) (n1). How big might H(n)b e ?
6. IfQ(0) = 0 and Q(n)n2+Pn
i=1Q(iâˆ’1) (n1), how big might Q(n)b e ?
7. (Research problem) Find the asymptotic behavior, for large n, of the probability that a randomly chosen
permutation of nletters has a splitter.
2.3Recursive graph algorithms
Algorithms on graphs are another rich area of applications of recursive thinking. Some of the problems
are quite dierent from the ones that we have so far been studying in that they seem to need exponential
amounts of computing time, rather than the polynomial times that were required for sorting problems.
We will illustrate the dramatically increased complexity with a recursive algorithm for the â€˜maximum
independent set problem,â€™ one which has received a great deal of attention in recent years.
Suppose a graph Gis given. By an independent set of vertices of Gwe mean a set of vertices no two of
which are connected by an edge of G. In the graph in Fig. 2.3.1 the set f1;2;6gis an independent set and so
is the set f1;3g. The largest independent set of vertices in the graph shown there is the set f1;2;3;6g.T h e
problem of nding the size of the largest independent set in a given graph is computationally very dicult.
All algorithms known to date require exponential amounts of time, in their worst cases, although no one hasproved the nonexistence of fast (polynomial time) algorithms.
If the problem itself seems unusual, and maybe not deserving of a lot of attention, be advised that in
Chapter 5 we will see that it is a member in good standing of a large family of very important computationalproblems (the â€˜NP-completeâ€™ problems) that are tightly bound together, in that if we can gure out better
ways to compute any one of them, then we will be able to do all of them faster.
Fig. 2.3.1
Here is an algorithm for the independent set problem that is easy to understand and to program,
although, of course, it may take a long time to run on a large graph G.
We are looking for the size of the largest independent set of vertices of G. Suppose we denote that number
bymaxset (G). Fix some vertex of the graph, say vertex v. Letâ€™s distinguish two kinds of independent sets
of vertices of G. There are those that contain vertex vand those that donâ€™t contain vertex v.
If an independent set Sof vertices contains vertex v, then what does the rest of the set Sconsist of?
The remaining vertices of Sare an independent set in a smaller graph, namely the graph that is obtained
fromGby deleting vertex vas well as all vertices that are connected to vertex vby an edge. This latter
set of vertices is called the neighborhood of vertexv, and is written Nbhd(v).
The setSconsists, therefore, of vertex vtogether with an independent set of vertices from the graph
Gâˆ’fvgâˆ’Nbhd(v).
Now consider an independent set Sthat doesnâ€™t contain vertex v. In that case the set Sis simply an
independent set in the smaller graph Gâˆ’fvg.
38
2.3 Recursive graph algorithms
We now have all of the ingredients of a recursive algorithm. Suppose we have found the two numbers
maxset (Gâˆ’fvg)a n dmaxset (Gâˆ’fvgâˆ’Nbhd(v)). Then, from the discussion above, we have the relation
maxset (G)=max
maxset (Gâˆ’fvg);1+maxset (Gâˆ’fvgâˆ’Nbhd(v))/bracerightbig
:
We obtain the following recursive algorithm.
functionmaxset 1(G);
freturns the size of the largest independent set of
vertices ofGg
ifGhas no edges
thenmaxset 1: =jV(G)j
else
choose some nonisolated vertex vofG;
n1:=maxset 1(Gâˆ’fvg);
n2:=maxset 1(Gâˆ’fvgâˆ’Nbhd(v));
maxset 1: =max(n1;1+n2)
end.fmaxset 1g
Example:
Here is an example of a graph Gand the result of applying the maxset 1 algorithm to it. Let the graph
Gbe a 5-cycle. That is, it has 5 vertices and its edges are (1 ;2);(2;3);(3;4);(4;5);(1;5). What are the two
graphs on which the algorithm calls itself recursively?
Suppose we select vertex number 1 as the chosen vertex vin the algorithm. Then Gâˆ’f1gandGâˆ’
f1gâˆ’Nbhd(1) are respectively the two graphs shown in Fig. 2.3.2.
23 45 34
Fig. 2.3.2: Gâˆ’f1g Gâˆ’f1gâˆ’Nbhd(1)
The reader should now check that the size of the largest independent set of Gis equal to the larger of
the two numbers maxset 1(Gâˆ’f1g), 1 +maxset 1(Gâˆ’f1gâˆ’Nbhd(1)) in this example.
Of course the creation of these two graphs from the original input graph is just the beginning of the
story, as far as the computation is concerned. Unbeknownst to the programmer, who innocently wrote the
recursive routine maxset 1 and then sat back to watch, the compiler will go ahead with the computation by
generating a tree-full of graphs. In Fig. 2.3.3 we show the collection of all of the graphs that the compiler
might generate while executing a single call to maxset 1 on the input graph of this example. In each case,
the graph that is below and to the left of a given one is the one obtained by deleting a single vertex, and theone below and to the right of each graph is obtained by deleting a single vertex and its entire neighborhood.
Now we are going to study the complexity of maxset 1. The results will be suciently depressing that
we will then think about how to speed up the algorithm, and we will succeed in doing that to some extent.
To open the discussion, letâ€™s recall that in Chapter 0 it was pointed out that the complexity of a
calculation is usefully expressed as a function of the number of bits of input data. In problems about graphs,
however, it is more natural to think of the amount of labor as a function of n, the number of vertices of the
graph. In problems about matrices it is more natural to use n, the size of the matrix, and so forth.
Do these distinctions alter the classication of problems into â€˜polynomial time do-ableâ€™ vs. â€˜hardâ€™? Take
the graph problems, for instance. How many bits of input data does it take to describe a graph? Well,
certainly we can march through the entire list of n(nâˆ’1)=2 pairs of vertices and check o the ones that are
actually edges in the input graph to the problem. Hence we can describe a graph to a computer by making
39
Chapter 2: Recursive Algorithms
Fig. 2.3.3: A tree-full of graphs is created
a list ofn(nâˆ’1)=2 0â€™s and 1â€™s. Each 1 represents a pair that is an edge, each 0 represents one that isnâ€™t an
edge.
Thus (n2) bits describe a graph. Since n2is a polynomial in n, any function of the number of input
data bits that can be bounded by a polynomial in same, can also be bounded by a polynomial in nitself.
Hence, in the case of graph algorithms, the â€˜easinessâ€™ vs. â€˜hardnessâ€™ judgment is not altered if we base the
distinction on polynomials in nitself, rather than on polynomials in the number of bits of input data.
Hence, with a clear conscience, we are going to estimate the running time or complexity of graph
algorithms in terms of the number of vertices of the graph that is input.
Now letâ€™s do this for algorithm maxset 1a b o v e .
The rst step is to nd out if Ghas any edges. To do this we simply have to look at the input data.
In the worst case we might look at all of the input data, all ( n2) bits of it. Then, if Gactually has some
edges, the additional labor needed to process Gconsists of two recursive calls on smaller graphs and one
computation of the larger of two numbers.
IfF(G) denotes the total amount of computational labor that we do in order to nd maxset 1(G), then
we see that
F(G)cn2+F(Gâˆ’fvg)+F(Gâˆ’fvgâˆ’Nbhd(v)): (2:3:1)
Next, letf(n)=m a x jV(G)j=nF(G), and take the maximum of (2.3.1) over all graphs Gofnvertices. The
result is that
f(n)cn2+f(nâˆ’1) +f(nâˆ’2) (2 :3:2)
because the graph Gâˆ’fvgâˆ’Nbhd(v) might have as many as nâˆ’2 vertices, and would have that many
ifvhad exactly one neighbor.
Now itâ€™s time to â€˜solveâ€™ the recurrent inequality (2.3.2). Fortunately the hard work has all been done,
and the answer is in theorem 1.4.1. That theorem was designed expressly for the analysis of recursive
algorithms, and in this case it tells us that f(n)=O((1:619n)). Indeed the number cin that theorem is
(1 +p
5)=2=1:61803:::.W ec h o s et h eâ€˜ â€™ that appears in the conclusion of the theorem simply by rounding
cupwards.
What have we learned? Algorithm maxset 1 will nd the answer in a time of no more than O(1:619n)
units if the input graph Ghasnvertices. This is a little improvement of the most simple-minded possible
40
2.3 Recursive graph algorithms
algorithm that one might think of for this problem, which is to examine every single subset of the vertices of
ofGand ask if it is an independent set or not. That algorithm would take (2n) time units because there
are 2nsubsets of vertices to look at. Hence we have traded in a 2nf o ra1:619nby being a little bit cagey
about the algorithm. Can we do still better?
There have in fact been a number of improvements of the basic maxset 1 algorithm worked out. Of
these the most successful is perhaps the one of Tarjan and Trojanowski that is cited in the bibliography at
the end of this chapter. We are not going to work out all of those ideas here, but instead we will show what
kind of improvements on the basic idea will help us to do better in the time estimate.
We can obviously do better if we choose vin such a way as to be certain that it has at least two
neighbors. If we were to do that then although we wouldnâ€™t aect the number of vertices of Gâˆ’fvg(always
nâˆ’1) we would at least reduce the number of vertices of Gâˆ’fvgâˆ’Nbhd(v) as much as possible.
So, as our next thought, we might replace the instruction â€˜choose some nonisolated vertex vofGâ€™i n
maxset 1 by an instruction â€˜choose some vertex vofGthat has at least two neighbors.â€™ Then we could be
quite certain that Gâˆ’fvgâˆ’Nbhd(v) would have at most nâˆ’3 vertices.
What if there isnâ€™t any such vertex in the graph G?T h e nGwould contain only vertices with 0 or 1
neighbors. Such a graph Gwould be a collection of Edisjoint edges together with a number mof isolated
vertices. The size of the largest independent set of vertices in such a graph is easy to nd. A maximumindependent set contains one vertex from each of the Eedges and it contains all mof the isolated vertices.
Hence in this case, maxset =E+m=jV(G)jâˆ’jE(G)j, and we obtain a second try at a good algorithm in
the following form.
proceduremaxset 2(G);
freturns the size of the largest independent set of
vertices ofGg
ifGhas no vertex of degree 2
thenmaxset 2: =jV(G)jâˆ’jE(G)j
else
choose a vertex v
of degree 2;
n1:=maxset 2(Gâˆ’fvg);
n2:=maxset 2(Gâˆ’fvgâˆ’Nbhd(v)) ;
maxset 2: =max(n1;1+n2)
end.fmaxset 2g
How much have we improved the complexity estimate? If we apply to maxset 2 the reasoning that led
to (2.3.2) we nd
f(n)cn2+f(nâˆ’1) +f(nâˆ’3) (f(0) = 0;n=2;3;:::); (2:3:3)
wheref(n) is once more the worst-case time bound for graphs of nvertices.
Just as before, (2.3.3) is a recurrent inequality of the form that was studied at the end of section 1.4,
in theorem 1.4.1. Using the conclusion of that theorem, we nd from (2.3.3) that f(n)=O((c+)n)w h e r e
c=1:46557::is the positive root of the equation c3=c2+1 .
The net result of our eort to improve maxset 1t omaxset 2 has been to reduce the running-time bound
fromO(1:619n)t oO(1:47n), which isnâ€™t a bad dayâ€™s work. In the exercises below we will develop maxset 3,
whose running time will be O(1:39n). The idea will be that since in maxset 2 we were able to insure that v
had at least two neighbors, why not try to insure that vhas at least 3 of them?
As long as we have been able to reduce the time bound more and more by insuring that the selected
vertex has lots of neighbors, why donâ€™t we keep it up, and insist that vshould have 4 or more neighbors?
Regrettably the method runs out of steam precisely at that moment. To see why, ask what the â€˜trivial caseâ€™
would then look like. We would be working on a graph Gin which no vertex has more than 3 neighbors.
Well, what â€˜trivialthingâ€™ shall we do, in this â€˜trivial caseâ€™?
The fact is that there isnâ€™t any way of nding the maximum independent set in a graph where all
vertices have 3 neighbors thatâ€™s any faster than the general methods that weâ€™ve already discussed. In fact,
if one could nd a fast method for that restricted problem it would have extremely important consequences,
because we would then be able to do all graphs rapidly, not just those special ones.
41
Chapter 2: Recursive Algorithms
We will learn more about this phenomenon in Chapter 5, but for the moment letâ€™s leave just the
observation that the general problem of maxset turns out to be no harder than the special case of maxset
in which no vertex has more than 3 neighbors.
Aside from the complexity issue, the algorithm maxset has shown how recursive ideas can be used to
transform questions about graphs to questions about smaller graphs.
Hereâ€™s another example of such a situation. Suppose Gis a graph, and that we have a certain supply
of colors available. To be exact, suppose we have Kcolors. We can then attempt to colorthe vertices of G
properly in Kcolors (see section 1.6).
If we donâ€™t have enough colors, and Ghas lots of edges, this will not be possible. For example, suppose
Gis the graph of Fig. 2.3.4, and suppose we have just 3 colors available. Then there is no way to color the
vertices without ever nding that both endpoints of some edge have the same color. On the other hand, if
we have four colors available then we can do the job.
Fig. 2.3.4
There are many interesting computational and theoretical problems in the area of coloring of graphs.
Just for its general interest, we are going to mention the four-color theorem, and then we will turn to a study
of some of the computational aspects of graph coloring.
First, just for general cultural reasons, letâ€™s slow down for a while and discuss the relationship between
graph colorings in general and the four-color problem, even though it isnâ€™t directly relevant to what weâ€™re
doing.
The original question was this. Suppose that a delegation of Earthlings were to visit a distant planet
and nd there a society of human beings. Since that race is well known for its squabbling habits, you can
be sure that the planet will have been carved up into millions of little countries, each with its own ruling
class, system of government, etc., and of course, all at war with each other. The delegation wants to escape
quickly, but before doing so it draws a careful map of the 5,000,000 countries into which the planet hasbeen divided. To make the map easier to read, the countries are then colored in such a way that whenever
two countries share a stretch of border they are of two dierent colors. Surprisingly, it was found that the
coloring could be done using only red, blue, yellow and green.
It was noticed over 100 years ago that no matter how complicated a map is drawn, and no matter how
many countries are involved, it seems to be possible to color the countries in such a way that
(a) every pair of countries that have a common stretch of border have dierent colors and
(b) no more than fourcolors are used in the entire map.
It was then conjectured that four colors are always sucient for the proper coloring of the countries
of any map at all. Settling this conjecture turned out to be a very hard problem. It was nally solved in1976 by K. Appel and W. Haken* by means of an extraordinary proof with two main ingredients. First they
showed how to reduce the general problem to only a nite number of cases, by a mathematical argument.
Then, since the â€˜nite numberâ€™ was over 1800, they settled all of those cases with quite a lengthy computercalculation. So now we have the â€˜Four Color Theorem,â€™ which asserts that no matter how we carve up the
plane or the sphere into countries, we will always be able to color those countries with at most four colors
so that countries with a common frontier are colored dierently.
We can change the mapcoloring problem into a graph coloring problem as follows. Given a map. From
the map we will construct a graph G. There will be a vertex of Gcorresponding to each country on the
map. Two of these vertices will be connected by an edge of the graph Gif the two countries that they
correspond to have a common stretch of border (we keep saying â€˜stretch of borderâ€™ to emphasize that if thetwo countries have just a single point in common they are allowed to have the same color). As an illustration
* Every planar map is four colorable, Bull. Amer. Math. Soc. ,82(1976), 711-712.
42
2.3 Recursive graph algorithms
Fig. 2.3.5(a) Fig. 2.3.5(b)
of this construction, we show in Fig. 2.3.5(a) a map of a distant planet, and in Fig. 2.3.5(b) the graph that
results from the construction that we have just described.
By a â€˜planar graphâ€™ we mean a graph Gthat can be drawn in the plane in such a way that two edges
never cross (except that two edges at the same vertex have that vertex in common). The graph that resultsfrom changing a map of countries into a graph as described above is always a planar graph. In Fig. 2.3.6(a)
we show a planar graph G. This graph doesnâ€™t look planar because two of its edges cross. However, that isnâ€™t
the graphâ€™s fault, because with a little more care we might have drawn the same graph as in Fig. 2.3.6(b), in
which its planarity is obvious. Donâ€™t blame the graph if it doesnâ€™t look planar. It might be planar anyway!
Fig. 2.3.6(a) Fig. 2.3.6(b)
The question of recognizing whether a given graph is planar is itself a formidable problem, although the
solution, due to J. Hopcroft and R. E. Tarjan,* is an algorithm that makes the decision in linear time ,i.e.
inO(V) time for a graph of Vvertices.
Although every planar graph can be properly colored in four colors, there are still all of those other
graphs that are not planar to deal with. For any one of those graphs we can ask, if a positive integer Kis
given, whether or not its vertices can be K-colored properly.
As if that question werenâ€™t hard enough, we might ask for even more detail, namely about the number
of ways of properly coloring the vertices of a graph. For instance, if we have Kcolors to work with, suppose
Gis the empty graphKn, that is, the graph of nvertices that has no edges at all. Then Ghas quite a large
number of proper colorings, Knof them, to be exact. Other graphs of nvertices have fewer proper colorings
than that, and an interesting computational question is to count the proper colorings of a given graph.
We will now nd a recursive algorithm that will answer this question. Again, the complexity of the
algorithm will be exponential, but as a small consolation we note that no polynomial time algorithm for this
problem is known.
Choose an edge eof the graph, and let its endpoints be vandw. Now delete the edge efrom the graph,
and let the resulting graph be called Gâˆ’feg. Then we will distinguish two kinds of proper colorings of
Gâˆ’feg: those in which vertices vandwhave the same color and those in which vandwhave dierent colors.
Obviously the number of proper colorings of Gâˆ’feginKcolors is the sum of the numbers of colorings of
each of these two kinds.
* Ecient planarity testing, J. Assoc. Comp. Mach. 21(1974), 549-568.
43
Chapter 2: Recursive Algorithms
Consider the proper colorings in which vertices vandwhave the same color. We claim that the number
of such colorings is equal to the number of allcolorings of a certain new graph G=feg, whose construction
we now describe:
The vertices of G=fegconsist of the vertices of Gother than vorwand one new vertex that we will
call â€˜vwâ€™( s oG=fegwill have one less vertex than Ghas).
Now we describe the edges ofG=feg.F i r s t ,i faandbare two vertices of G=fegneither of which is the
new vertex â€˜ vwâ€™, then (a;b)i sa ne d g eo f G=fegif and only if it is an edge of G. Second, (vw;b)i sa ne d g e
ofG=fegif and only if either ( v;b)o r(w;b)( o rb o t h )i sa ne d g eo f G.
We can think of this as â€˜collapsingâ€™ the graph Gby imagining that the edges of Gare elastic bands,
and that we squeeze vertices vandwtogether into a single vertex. The result is G=feg(anyway, it is if we
replace any resulting double bands by single ones!).
In Fig. 2.3.7(a) we show a graph Gof 7 vertices and a chosen edge e. The two endpoints of earev
andw. In Fig. 2.3.7(b) we show the graph G=fegthat is the result of the construction that we have just
described.
Fig. 2.3.7(a) Fig. 2.3.7(b)
The point of the construction is the following
Lemma 2.3.1. Letvandwbe two vertices of Gsuch thate=(v;w)2E(G). Then the number of proper
K-colorings of Gâˆ’fegin whichvandwhave the same color is equal to the number of all proper colorings
of the graph G=feg.
Proof: SupposeG=feghas a proper K-coloring. Color the vertices of Gâˆ’fegitself inKcolors as follows.
Every vertex of Gâˆ’fegother thanvorwkeeps the same color that it has in the coloring of G=feg.V e r t e xv
and vertex weach receive the color that vertex vwhas in the coloring of G=feg.N o ww eh a v ea K-coloring
of the vertices of Gâˆ’feg.
It is a proper coloring because if fis any edge of Gâˆ’fegthen the two endpoints of fhave dierent
colors. Indeed, this is obviously true if neither endpoint of fisvorwbecause the coloring of G=fegwas a
proper one. There remains only the case where one endpoint of fis, say,vand the other one is some vertex
xother than vorw. But then the colors of vandxmust be dierent because vwandxwere joined in
G=fegby an edge, and therefore must have gotten dierent colors there.
To get back to the main argument, we were trying to compute the number of proper K-colorings of
Gâˆ’feg. We observed that in any K-coloringvandwhave either the same or dierent colors. We have
shown that the number of colorings in which they receive the same color is equal to the number of all propercolorings of a certain smaller (one less vertex) graph G=feg. It remains to look at the case where vertices v
andwreceive dierent colors.
Lemma 2.3.2. Lete=(v;w)be an edge of G. Then the number of proper K-colorings of Gâˆ’fegin which
vandwhave dierent colors is equal to the number of all proper K-colorings of Gitself.
Proof: Obvious (isnâ€™t it?).
Now letâ€™s put together the results of the two lemmas above. Let P(K;G) denote the number of ways of
properly coloring the vertices of a given graph G. Then lemmas 2.3.1 and 2.3.2 assert that
P(K;Gâˆ’feg)=P(K;G=feg)+P(K;G)
44
2.3 Recursive graph algorithms
or if we solve for P(K;G), then we have
P(K;G)=P(K;Gâˆ’feg)âˆ’P(K;G=feg): (2:3:4)
The quantity P(K;G), the number of ways of properly coloring the vertices of a graph GinKcolors,
is called the chromatic polynomial of G.
We claim that it is, in fact, a polynomial in Kof degree jV(G)j. For instance, if Gis the complete
graph ofnvertices then obviously P(K;G)=K(Kâˆ’1)(Kâˆ’n+ 1), and that is indeed a polynomial in
Kof degreen.
Proof of claim: The claim is certainly true if Ghas just one vertex. Next suppose the assertion is true for
graphs of<Vvertices, then we claim it is true for graphs of Vvertices also. This is surely true if GhasV
vertices and no edges at all. Hence, suppose it is true for all graphs of Vvertices and fewer than Eedges,
and letGhaveVvertices and Eedges. Then (2.3.4) implies that P(K;G) is a polynomial of the required
degreeVbecauseGâˆ’feghas fewer edges than Gdoes, so its chromatic polynomial is a polynomial of degree
V.G=feghas fewer vertices than Ghas, and so P(K;G=feg) is a polynomial of lower degree. The claim is
proved, by induction.
Equation (2.3.4) gives a recursive algorithm for computing the chromatic polynomial of a graph G, since
the two graphs that appear on the right are both â€˜smallerâ€™ than G, one in the sense that it has fewer edges
thanGhas, and the other in that it has fewer vertices. The algorithm is the following.
functionchrompoly (G:graph): polynomial;
fcomputes the chromatic polynomial of a graph Gg
ifGhas no edges thenchrompoly :=KjV(G)j
else
choose an edge eofG;
chrompoly :=chrompoly (Gâˆ’feg)âˆ’chrompoly (G=feg)
end.fchrompoly g
Next we are going to look at the complexity of the algorithm chrompoly (we will also refer to it as â€˜the
delete-and-identifyâ€™ algorithm). The graph Gcan be input in any one of a number of ways. For example,
we might input the full list of edges of G, as a list of pairs of vertices.
The rst step of the computation is to choose the edge eand to create the edge list of the graph Gâˆ’feg.
The latter operation is trivial, since all we have to do is to ignore one edge in the list.
Next we call chrompoly on the graph Gâˆ’feg.
The third step is to create the edge list of the collapsed graph G=fegfrom the edge list of Gitself. That
involves some work, but it is rather routine, and its cost is linear in the number of edges of G,s a ycjE(G)j.
Finally we call chrompoly on the graph G=feg.
LetF(V;E) denote the maximum cost of calling chrompoly on any graph of at most Vvertices and at
mostEedges. Then we see at once that
F(V;E)F(V;Eâˆ’1) +cE+F(Vâˆ’1;Eâˆ’1) (2 :3:5)
together with F(V;0) = 0. If we put, successively, E=1;2;3, we nd that F(V;1)c,F(V;2)4c,a n d
F(V;3)11c. Hence we seek a solution of (2.3.5) in the form F(V;E)f(E)c, and we quickly nd that if
f(E)=2f(Eâˆ’1) +E (f(0) = 0) (2 :3:6)
then we will have such a solution.
Since (2.3.6) is a rst-order dierence equation of the form (1.4.5), we nd that
f(E)=2EEX
j=0j2âˆ’j
2E+1:(2:3:7)
45
Chapter 2: Recursive Algorithms
The last â€˜ â€™ follows from the evaluationPj2âˆ’j= 2 that we discussed in section 1.3.
To summarize the developments so far, then, we have found out that the chromatic polynomial of a graph
can be computed recursively by an algorithm whose cost is O(2E) for graphs of Eedges. This is exponential
cost, and such computations are prohibitively expensive except for graphs of very modest numbers of edges.
Of course the mere fact that our proved time estimate is O(2E) doesnâ€™t necessarily mean that the
algorithm can be that slow, because maybe our complexity analysis wasnâ€™t as sharp as it might have been.
However, consider the graph G(s;t) that consists of sdisjoint edges and tisolated vertices, for a total of
2s+tvertices altogether. If we choose an edge of G(s;t) and delete it, we get G(sâˆ’1;t+ 2), whereas the
graphG=fegisG(sâˆ’1;t+ 1). Each of these two new graphs has sâˆ’1e d g e s .
We might imagine arranging the computation so that the extra isolated vertices will be â€˜free,â€™ i.e., will
not cost any additional labor. Then the work that we
do onG(s;t) will depend only on s, and will be twice as much as the work we do on G(sâˆ’1;). Therefore
G(s;t) will cost at least 2soperations, and our complexity estimate wasnâ€™t a mirage, there really are graphs
that make the algorithm do an amount 2jE(G)jof work.
Considering the above remarks it may be surprising that there is a slightly dierent approach to the
complexity analysis that leads to a time bound (for the same algorithm) that is a bit sharper than O(2E)i n
many cases (the work of the complexity analyst is never nished!). Letâ€™s look at the algorithm chrompoly
in another way.
For a graph Gwe can dene a number Î³(G)=jV(G)j+jE(G)j, which is rather an odd kind of thing
to dene, but it has a nice property with respect to this algorithm, namely that whatever Gwe begin with,
we will nd that
Î³(Gâˆ’feg)=Î³(G)âˆ’1;Î³(G=feg)Î³(G)âˆ’2: (2:3:8)
Indeed, if we delete the edge ethenÎ³must drop by 1, and if we collapse the graph on the edge ethen we
will have lost one vertex and at least one edge, so Î³will drop by at least 2.
Hence, ifh(Î³) denotes the maximum amount of labor that chrompoly does on any graph Gfor which
jV(G)j+jE(G)jÎ³ (2:3:9)
then we claim that
h(Î³)h(Î³âˆ’1) +h(Î³âˆ’2) (Î³2): (2:3:10)
Indeed, ifGis a graph for which (2.3.9) holds, then if Ghas any edges at all we can do the delete-and-identify
step to prove that the labor involved in computing the chromatic polynomial of Gis at most the quantity
on the right side of (2.3.10). Else, if Ghas no edges then the labor is 1 unit, which is again at most equal
to the right side of (2.3.10), so the result (2.3.10) follows.
With the initial conditions h(0) =h(1) = 1 the solution of the recurrent inequality (2.3.10) is obviously
the relation h(Î³)FÎ³,w h e r eFÎ³is the Fibonacci number. We have thereby proved that the time complexity
of the algorithm chrompoly is
O(FjV(G)j+jE(G)j)=O
(1+p
5
2)jV(G)j+jE(G)j
=O(1:62jV(G)j+jE(G)j):(2:3:11)
This analysis does not, of course, contradict the earlier estimate, but complements it. What we have
shown is that the labor involved is always
O
min(2jE(G)j;1:62jV(G)j+jE(G)j)
: (2:3:12)
On a graph with â€˜fewâ€™ edges relative to its number of vertices (how few?) the rst quantity in the parentheses
in (2.3.12) will be the smaller one, whereas if Ghas more edges, then the second term is the smaller one. In
either case the overall judgment about the speed of the algorithm (itâ€™s slow!) remains.
46
2.4 Fast matrix multiplication
Exercises for section 2.3
1. LetGbe a cycle of nvertices. What is the size of the largest independent set of vertices in V(G)?
2. LetGbe a path of nvertices. What is the size of the largest independent set of vertices in V(G)?
3. LetGbe a connected graph in which every vertex has degree 2. What must such a graph consist of?
Prove.
4. LetGbe a connected graph in which every vertex has degree 2. What must such a graph look like?
5. LetGbe a not-necessarily-connected graph in which every vertex has degree 2. What must such a
graph look like? What is the size of the largest independent set of vertices in such a graph? How long would
it take you to calculate that number for such a graph G? How would you do it?
6. Write out algorithm maxset 3, which nds the size of the largest independent set of vertices in a graph.
Its trivial case will occur if Gh a sn ov e r t e xo fd e g r e e 3. Otherwise, it will choose a vertex vof degree
3 and proceed as in maxset 2.
7. Analyze the complexity of your algorithm maxset 3 from exercise 6 above.
8. Use (2.3.4) to prove by induction that P(K;G) is a polynomial in Kof degree jV(G)j. Then show that
ifGis a tree then P(K;G)=K(Kâˆ’1)jV(G)jâˆ’1.
9. Write out an algorithm that will change the vertex adjacency matrix of a graph Gto the vertex adjacency
matrix of the graph G=feg,w h e r eeis a given edge of G.
10. How many edges must Ghave before the second quantity inside the â€˜ Oâ€™ in (2.3.12) is the smaller of the
two?
11. Let(G) be the size of the largest independent set of vertices of a graph G,l e t(G) be its chromatic
number, and let n=jV(G)j. Show that, for every G,(G)n=(G).
2.4 Fast matrix multiplication
Everybody knows how to multiply two 2 2 matrices. If we want to calculate

c11c12
c21c22
=
a11a12
a21a22
b11b12
b21b22
(2:4:1)
then, â€˜of course,â€™
ci;j=2X
k=1ai;kbk;j (i;j=1;2): (2:4:2)
Now look at (2.4.2) a little more closely. In order to calculate each one of the 4 ci;jâ€™s we have to do 2
multiplications of numbers. The cost of multiplying two 2 2 matrices is therefore 8 multiplications of
numbers. If we measure the cost in units of additions of numbers, the cost is 4 such additions. Hence, thematrix multiplication method that is shown in (2.4.1) has a complexity of 8 multiplications of numbers and
4 additions of numbers.
This may seem rather unstartling, but the best ideas often have humble origins.Suppose we could nd another way of multiplying two 2 2 matrices in which the cost was only 7
multiplications of numbers, together with more than 4 additions of numbers. Would that be a cause for
dancing in the streets, or would it be just a curiosity, of little importance? In fact, it would be extremely
important, and the consequences of such a step were fully appreciated only in 1969 by V. Strassen, to whomthe ideas that we are now discussing are due.*
What weâ€™re going to do next in this section is the following:
(a) describe another way of multiplying two 2 2 matrices in which the cost will be only 7 multipli-
cations of numbers plus a bunch of additions of numbers, and
(b) convince you that it was worth the trouble.
The usefulness of the idea stems from the following amazing fact: if two 2 2 matrices can be multiplied
with only 7 multiplications of numbers, then two NNmatrices can be multiplied using only O(N
2:81:::)
* V. Strassen, Gaussian elimination is not optimal, Numerische Math. 13(1969), 354-6.
47
Chapter 2: Recursive Algorithms
multiplications of numbers instead of the N3such multiplications that the usual method involves (the number
â€˜2.81...â€™ is log27).
In other words, if we can reduce the number of multiplications of numbers that are needed to multiply two
22 matrices, then that improvement will show up in the exponent ofNwhen we measure the complexity of
multiplying two NNmatrices. The reason, as we will see, is that the little improvement will be pyramided
by numerous recursive calls to the 2 2 procedure{ but we get ahead of the story.
Now letâ€™s write out another way to do the 2 2 matrix multiplication that is shown in (2.4.1). Instead
of doing it al a(2.4.2), try the following 11-step approach.
First compute, from the input 2 2 matrices shown in (2.4.1), the following 7 quantities:
I=(a12âˆ’a22)(b21+b22)
II=(a11+a22)(b11+b22)
III=(a11âˆ’a21)(b11+b12)
IV=(a11+a12)b22
V=a11(b12âˆ’b22)
VI=a22(b21âˆ’b11)
VII=(a21+a22)b11(2:4:3)
and then calculate the 4 entries of the product matrix C=ABfrom the 4 formulas
c11=I+IIâˆ’IV+VI
c12=IV+V
c21=VI+VII
c22=IIâˆ’III+Vâˆ’VII:(2:4:4)
The rst thing to notice about this seemingly overelaborate method of multiplying 2 2 matrices is that
only 7 multiplications of numbers are used (count the â€˜ â€™ signs in (2.4.3)). â€˜Well yes,â€™ you might reply, â€˜but
18 additions are needed, so where is the gain?â€™
It will turn out that multiplications are more important than additions, not because computers can do
them faster, but because when the routine is called recursively each â€˜ â€™ operation will turn into a multipli-
cation of two big matrices whereas each â€˜ â€™ will turn into an addition or subtraction of two big matrices,
and thatâ€™s much cheaper.
Next weâ€™re going to describe how Strassenâ€™s method (equations (2.4.3), (2.4.4)) of multiplying 2 2
matrices can be used to speed up multiplications of NNmatrices. The basic idea is that we will partition
each of the large matrices into four smaller ones and multiply them together using (2.4.3), (2.4.4).
Suppose that Nis a power of 2, say N=2n, and let there be given two NNmatrices,AandB.
We imagine that AandBhave each been partitioned into four 2nâˆ’12nâˆ’1matrices, and that the product
matrixCis similarly partitioned. Hence we want to do the matrix multiplication that is indicated by

C11C12
C21C22
=
A11A12
A21A22
B11B12
B21B22
(2:4:5)
where now each of the capital letters represents a 2nâˆ’12nâˆ’1matrix.
To do the job in (2.4.5) we use exactly the 11 formulas that are shown in (2.4.3) and (2.4.4), except
that the lower-case letters are now all upper case. Suddenly we very much appreciate the reduction of the
number of â€˜ â€™ signs because it means one less multiplication of large matrices, and we donâ€™t so much mind
that it has been replaced by 10 more â€˜ â€™ signs, at least not if Nis very large.
This yields the following recursive procedure for multiplying large matrices.
48
2.4 Fast matrix multiplication
functionMatrProd (A,B: matrix;N:integer):matrix;
fMatrProd isAB,w h e r eAandBareNNg
fuses Strassen method g
ifNis not a power of 2 then
borderAandBby rows and columns of 0â€™s until
their size is the next power of 2 and change N;
ifN=1thenMatrProd :=AB
else
partitionAandBas shown in (2.4.5);
I:=MatrProd (A11âˆ’A22;B 21+B22;N=2);
II:=MatrProd (A11+A22;B 11+B22;N=2);
etc. etc., through all 11 of the formulas
shown in (2.4.3), (2.4.4), ending with ...
C22:=IIâˆ’III+Vâˆ’VII
end.fMatrProd g
Note that this procedure calls itself recursively 7 times. The plus and minus signs in the program each
represent an addition or subtraction of two matrices , and therefore each one of them involves a call to a
matrix addition or subtraction procedure (just the usual method of adding, nothing fancy!). Therefore the
functionMatrProd makes 25 calls, 7 of which are recursively to itself, and 18 of which are to a matrix
addition/subtraction routine.
We will now study the complexity of the routine in two ways. We will count the number of multiplications
of numbers that are needed to multiply two 2n2nmatrices using MatrProd (call that number f(n)), and
then we will count the number of additions of numbers (call it g(n)) thatMatrProd needs in order to
multiply two 2n2nmatrices.
The multiplications of numbers are easy to count. MatrProd calls itself 7 times, in each of which it
does exactly f(nâˆ’1) multiplications of numbers, hence f(n)=7f(nâˆ’1) andf(0) = 1 (why?). Therefore
we see that f(n)=7nfor alln0. HenceMatrProd does 7nmultiplications of numbers in order to do one
multiplication of 2n2nmatrices.
Letâ€™s take the last sentence in the above paragraph and replace â€˜2nâ€™b yNthroughout. It then tells
us thatMatrProd does 7logN= log 2multiplications of numbers in order to do one multiplication of NN
matrices. Since 7logN= log 2=Nlog 7=log 2=N2:81:::, we see that Strassenâ€™s method uses only O(N2:81)
multiplications of numbers, in place of the N3such multiplications that are required by the usual formulas.
It remains to count the additions/subtractions of numbers that are needed by MatrProd .
In each of its 7 recursive calls to itself MatrProd doesg(nâˆ’1) additions of numbers. In each of its
18 calls to the procedure that adds or subtracts matrices it does a number of additions of numbers that isequal to the square of the size of the matrices that are being added or subtracted. That size is 2
nâˆ’1,s oe a c h
of the 18 such calls does 22nâˆ’2additions of numbers. It follows that g(0) = 0 and for n1w eh a v e
g(n)=7g(nâˆ’1) + 18 4nâˆ’1
=7g(nâˆ’1) +9
24n:
We follow the method of section 1.4 on this rst-order linear dierence equation. Hence we make the
change of variable g(n)=7nyn(n0) and we nd that y0= 0 and for n1,
yn=ynâˆ’1+9
2(4=7)n:
If we sum over nwe obtain
yn=9
2nX
j=1(4=7)j
9
21X
j=0(4=7)n
=2 1=2:
49
Chapter 2: Recursive Algorithms
Finally,g(n)=7nyn(10:5)7n=O(7n), and this is O(N2:81) as before. This completes the proof of
Theorem 2.4.1. In Strassenâ€™s method of fast matrix multiplication the number of multiplications of num-
bers, of additions of numbers and of subtractions of numbers that are needed to multiply together two NN
matrices are each O(N2:81)(in contrast to the (N3)of the conventional method).
In the years that have elapsed since Strassenâ€™s original paper many researchers have been whittling
away at the exponent of Nin the complexity bounds. Several new, and more elaborate algorithms have
been developed, and the exponent, which was originally 3, has progressed downwards through 2.81 to values
below 2.5. It is widely believed that the true minimum exponent is 2 + ,i.e.,t h a tt w oNNmatrices can
be multiplied in time O(N2+), but there seems to be a good deal of work to be done before that result can
be achieved.
Exercises for section 2.4
1. Suppose we could multiply together two 3 3 matrices with only 22 multiplications of numbers. How
fast, recursively, would we then be able to multiply two NNmatrices?
2. (cont.) With what would the â€˜22â€™ in problem 1 above have to be replaced in order to achieve an
improvement over Strassenâ€™s algorithm given in the text?3. (cont.) Still more generally, with how few multiplications would we have to be able to multiply two
MMmatrices in order to insure that recursively we would then be able to multiply two NNmatrices
faster than the method given in this section?4. We showed in the text that if Ni sap o w e ro f2t h e nt w o NNmatrices can be multiplied in at most
timeCN
log27,w h e r eCis a suitable constant. Prove that if Nis not a power of 2 then two NNmatrices
can be multiplied in time at most 7 CNlog27.
2.5 The discrete Fourier transform
It is a lot easier to multiply two numbers than to multiply two polynomials.
If you should want to multiply two polynomials fandg, of degrees 77 and 94, respectively, you are
in for a lot of work. To calculate just one coecient of the product is already a lot of work. Think about
the calculation of the coecient of x50in the product, for instance, and you will see that about 50 numbers
must be multiplied together and added in order to calculate just that one coecient of fg, and there are
171 other coecients to calculate!
Instead of calculating the coecients of the product fgit would be much easier just to calculate the
values of the product at, say, 172 points. To do that we could just multiply the values of fand ofgat each
of those points, and after a total cost of 172 multiplications we would have the values of the product.
The values of the product polynomial at 172 distinct points determine that polynomial completely, so
that sequence of values isthe answer. Itâ€™s just that we humans prefer to see polynomials given by means of
their coecients instead of by their values.
The Fourier transform, that is the subject of this section, is a method of converting from one representa-
tion of a polynomial to another. More exactly, it converts from the sequence of coecients of the polynomial
to the sequence of values of that polynomial at a certain set of points. Ease of converting between these two
representations of a polynomial is vitally important for many reasons, including multiplication of polynomi-
als, high precision integer arithmetic in computers, creation of medical images in CAT scanners and NMR
scanners, etc.
Hence, in this section we will study the discrete Fourier transform of a nite sequence of numbers,
methods of calculating it, and some applications.
This is a computational problem which at rst glance seems very simple. What weâ€™re asked to do,
basically, is to evaluate a polynomial of degree nâˆ’1a tndierent points. So what could be so dicult
about that?
If we just calculate the nvalues by brute force, we certainly wonâ€™t need to do more than nmultiplications
of numbers to nd each of the nvalues of the polynomial that we want, so we surely donâ€™t need more than
O(n2) multiplications altogether.
50
2.5 The discrete Fourier transform
The interesting thing is that this particular problem is so important, and turns up in so many dierent
applications, that it really pays to be very ecient about how the calculation is done. We will see in this
section that if we use a fairly subtle method of doing this computation instead of the obvious method, then
the work can be cut down from O(n2)t oO(nlogn). In view of the huge arrays on which this program is
often run, the saving is very much worthwhile.
One can think of the Fourier transform as being a way of changing the description, or coding of a
polynomial, so we will introduce the subject by discussing it from that point of view.
Next we will discuss the obvious way of computing the transform.
Then we will describe the â€˜Fast Fourier Transformâ€™, which is a rather un-obvious, but very fast, method
of computing the same creature.
Finally we will discuss an important application of the subject, to the fast multiplication of polynomials.
There are many dierent ways that might choose to describe (â€˜encodeâ€™) a particular polynomial. Take
the polynomial f(t)=t(6âˆ’5t+t2), for instance. This can be uniquely described in any of the following
ways (and a lot more).
It is the polynomial whose
(i)coecients are 0, 6, âˆ’5 ,1o rw h o s e
(ii)rootsare 0, 2 and 3, and whose highest coecient is 1 or whose
(iii)values att= 0, 1, 2, 3 are 0, 2, 0, 0, respectively, or whose
(iv)values at the fourth-roots of unity 1 ;i;âˆ’1;âˆ’iare 2, 5 + 5 i,âˆ’12, 5âˆ’5i,o re t c .
We want to focus on two of these ways of representing a polynomial. The rst is by its coecient
sequence; the second is by its sequence of values at the nthroots of unity, where nis 1 more than the degree
of the polynomial. The process by which we pass from the coecient sequence to the sequence of values at
the roots of unity is called forming the Fourier transform of the coecient sequence. To use the example
above, we would say that the Fourier transform of the sequence
0;6;âˆ’5;1( 2 :5:1)
is the sequence
2;5+5i;âˆ’12;5âˆ’5i: (2:5:2)
In general, if we are given a sequence
x0;x1;:::;x nâˆ’1 (2:5:3)
then we think of the polynomial
f(t)=x0+x1t+x2t2++xnâˆ’1tnâˆ’1(2:5:4)
and we compute its values at the nthroots of unity. These roots of unity are the numbers
!j=e2ij=n(j=0;1;:::;n âˆ’1): (2:5:5)
Consequently, if we calculate the values of the polynomial (2.5.4) at the nnumbers (2.5.5), we nd the
Fourier transform of the given sequence (2.5.3) to be the sequence
f(!j)=nâˆ’1X
k=0xk!jk
=nâˆ’1X
k=0xke2ijk=n(j=0;1;:::n âˆ’1):(2:5:6)
Before proceeding, the reader should pause for a moment and make sure that the fact that (2.5.1)-(2.5.2)
is a special case of (2.5.3)-(2.5.6) is clearly understood. The Fourier transform of a sequence of nnumbers
is another sequence of nnumbers, namely the sequence of values at the nthroots of unity of the very same
polynomial whose coecients are the members of the original sequence.
51
Chapter 2: Recursive Algorithms
The Fourier transform moves us from coecients tovalues at roots of unity . Some good reasons for
wanting to make that trip will appear presently, but for the moment, letâ€™s consider the computational side
of the question, namely how to compute the Fourier transform eciently.
We are going to derive an elegant and very speedy algorithm for the evaluation of Fourier transforms.
The algorithm is called the Fast Fourier Transform (FFT) algorithm. In order to appreciate how fast it is,
letâ€™s see how long it would take us to calculate the transform without any very clever procedure.
What we have to do is to compute the values of a given polynomial at ngiven points. How much work
is required to calculate the value of a polynomial at onegiven point? If we want to calculate the value of
the polynomial x0+x1t+x2t2+:::+xnâˆ’1tnâˆ’1at exactly one value of t, then we can do (think how you
would do it, before looking)
functionvalue(x:coe array; n:integer;t:complex);
fcomputesvalue :=x0+x1t++xnâˆ’1tnâˆ’1g
value := 0;
forj:=nâˆ’1to0stepâˆ’1d o
value :=tvalue +xj
end.fvalueg
This well-known algorithm (= â€˜synthetic divisionâ€™) for computing the value of a polynomial at a single
pointtobviously runs in time O(n).
If we calculate the Fourier transform of a given sequence of npoints by calling the function value n
times, once for each point of evaluation, then obviously we are looking at a simple algorithm that requires
(n2)t i m e .
With the FFT we will see that the whole job can be done in time O(nlogn), and we will then look
at some implications of that fact. To put it another way, the cost of calculating all nof the values of a
polynomial fat thenthroots of unity is much less than ntimes the cost of one such calculation.
First we consider the important case where nis a power of 2, say n=2r. Then the values of f,a
polynomial of degree 2râˆ’1, at the (2r)throots of unity are, from (2.5.6),
f(!j)=nâˆ’1X
k=0xkexpf2ijk=2rg(j=0;1;:::;2râˆ’1): (2:5:7)
Letâ€™s break up the sum into two sums, containing respectively the terms where kis even and those where k
is odd. In the rst sum write k=2mand in the second put k=2m+ 1. Then, for each j=0;1;:::;2râˆ’1,
f(!j)=2râˆ’1âˆ’1X
m=0x2me2ijm= 2râˆ’1+2râˆ’1âˆ’1X
m=0x2m+1e2ij(2m+1)=2r
=2râˆ’1âˆ’1X
m=0x2me2ijm= 2râˆ’1+e2ij= 2r2râˆ’1âˆ’1X
m=0x2m+1e2ijm= 2râˆ’1:(2:5:8)
Something special just happened. Each of the two sums that appear in the last member of (2.5.8) is
itself a Fourier transform, of a shorter sequence. The rst sum is the transform of the array
x[0];x[2];x[4];:::;x [2râˆ’2] (2 :5:9)
and the second sum is the transform of
x[1];x[3];x[5];:::;x [2râˆ’1]: (2:5:10)
The stage is set (well, almost set) for a recursive program.
52
2.5 The discrete Fourier transform
There is one small problem, though. In (2.5.8) we want to compute f(!j)f o r2rvalues ofj, namely for
j=0;1;:::;2râˆ’1. However, the Fourier transform of the shorter sequence (2.5.9) is dened for only 2râˆ’1
values ofj, namely for j=0;1;:::;2râˆ’1âˆ’1. So if we calculate the rst sum by a recursive call, then we
will need its values for jâ€™s that are outside the range for which it was computed.
This problem is no sooner recognized than solved. Let Q(j) denote the rst sum in (2.5.8). Then we
claim thatQ(j)i saperiodic function of j,o fp e r i o d2râˆ’1, because
Q(j+2râˆ’1)=2râˆ’1âˆ’1X
m=0x2mexpf2im(j+2râˆ’1)=2râˆ’1g
=2râˆ’1âˆ’1X
m=0x2mexpf2imj= 2râˆ’1ge2im
=2râˆ’1âˆ’1X
m=0x2mexpf2imj= 2râˆ’1g
=Q(j)(2:5:11)
for all integers j.I fQ(j) has been computed only for 0 j2râˆ’1âˆ’1 and if we should want its value for
somej2râˆ’1then we can get that value by asking for Q(jmod 2râˆ’1).
Now we can state the recursive form of the Fast Fourier Transform algorithm in the (most important)
case where nis a power of 2. In the algorithm we will use the type complexarray to denote an array of
complex numbers.
functionFFT(n:integer; x:complexarray):complexarray;
fcomputes fast Fourier transform of n=2knumbers xg
ifn=1thenFFT[0] :=x[0]
else
evenarray :=fx[0];x[2];:::;x [nâˆ’2]g;
oddarray :=fx[1];x[3];:::;x [nâˆ’1]g;
fu[0];u[1];:::u[n
2âˆ’1]g:=FFT(n=2;evenarray );
fv[0];v[1];:::v[n
2âˆ’1]g:=FFT(n=2;oddarray );
forj:= 0tonâˆ’1d o
:=expf2ij=n g;
FFT[j]: =u[jmodn
2]+v[jmodn
2]
end.fFFTg
Lety(k) denote the number of multiplications of complex numbers that will be done if we call FFT on
an array whose length is n=2k. The call to FFT(n=2;evenarray )c o s t sy(kâˆ’1) multiplications as does
the call toFFT(n=2;oddarray ). The â€˜for j:= 0 tonâ€™ loop requires nmore multiplications. Hence
y(k)=2y(kâˆ’1) + 2k(k1;y(0) = 0): (2:5:12)
If we change variables by writing y(k)=2kzk, then we nd that zk=zkâˆ’1+ 1, which, together with z0=0 ,
implies that zk=kfor allk0, and therefore that y(k)=k2k. This proves
Theorem 2.5.1. The Fourier transform of a sequence of ncomplex numbers is computed using only
O(nlogn)multiplications of complex numbers by means of the procedure FFT,i fnis a power of 2.
Next* we will discuss the situation when nis not a power of 2.
The reader may observe that by â€˜padding outâ€™ the input array with additional 0â€™s we can extend the
length of the array until it becomes a power of 2, and then call the FFT procedure that we have already
* The remainder of this section can be omitted at a rst reading.
53
Chapter 2: Recursive Algorithms
discussed. In a particular application, that may or may not be acceptable. The problem is that the original
question asked for the values of the input polynomial at the nthroots of unity, but after the padding, we
will nd the values at the Nthroots of unity, where Nis the next power of 2. In some applications, such as
the multiplication of polynomials that we will discuss later in this section, that change is acceptable, but inothers the substitution of N
throots fornthroots may not be permitted.
We will suppose that the FFT of a sequence of nnumbers is wanted, where nis not a power of 2, and
where the padding operation is not acceptable. If nis a prime number we will have nothing more to say,
i.e.we will not discuss any improvements to the obvious method for calculating the transform, one root of
unity at a time.
Suppose that nis not prime ( nis â€˜compositeâ€™). Then we can factor the integer nin some nontrivial
way, sayn=r1r2where neither r1norr2is 1.
We claim, then, that the Fourier transform of a sequence of length ncan be computed by recursively
nding the Fourier transforms of r1dierent sequences, each of length r2. The method is a straightforward
generalization of the idea that we have already used in the case where nwas a power of 2.
In the following we will write n=e2i=n. The train of â€˜=â€™ signs in the equation below shows how the
question on an input array of length nis changed into r1questions about input arrays of length r2.W e
have, for the value of the input polynomial fat thejthone of thennthroots of unity, the relations
f(e2ij=n)=nâˆ’1X
s=0xsnjs
=r1âˆ’1X
k=0r2âˆ’1X
t=0fxtr1+knj(tr1+k)g
=r1âˆ’1X
k=0r2âˆ’1X
t=0fxtr1+kntjr1nkjg
=r1âˆ’1X
k=0r2âˆ’1X
t=0xtr1+kr2tj/bracerightbig
nkj
=r1âˆ’1X
k=0ak(j)nkj:(2:5:13)
We will discuss (2.5.13), line-by-line. The rst â€˜=â€™ sign is the denition of the jthentry of the Fourier
transform of the input array x. The second equality uses the fact that every integer ssuch that 0 snâˆ’1
can be uniquely written in the form s=tr1+k,w h e r e0 tr2âˆ’1a n d0 kr1âˆ’1. The next â€˜=â€™ is
just a rearrangement, but the next one uses the all-important fact that nr1=r2(why?), and in the last
equation we are simply dening a set of numbers
ak(j)=r2âˆ’1X
t=0xtr1+kr2tj(0kr1âˆ’1;0jnâˆ’1): (2:5:14)
The important thing to notice is that for a xed kthe numbers ak(j) areperiodic inn,o fp e r i o dr2,i.e.,
thatak(j+r2)=ak(j) for allj. Hence, even though the values of the ak(j) are needed for j=0;1;:::;n âˆ’1,
they must be computed only for j=0;1;:::;r 2âˆ’1.
Now the entire job can be done recursively, because for xed kthe set of values of ak(j)(j=
0;1;:::;r 2âˆ’1) that we must compute is itself a Fourier transform, namely of the sequence
fxtr1+kg(t=0;1;:::;r 2âˆ’1): (2:5:15)
Letg(n) denote the number of complex multiplications that are needed to compute the Fourier transform
of a sequence of nnumbers. Then, for kxed we can recursively compute the r2values ofak(j)t h a tw en e e d
withg(r2) multiplications of complex numbers. There are r1such xed values of kfor which we must do the
54
2.5 The discrete Fourier transform
computation, hence all of the necessary values of ak(j) can be found with r1g(r2) complex multiplications.
Once theak(j) are all in hand, then the computation of the one value of the transform from (2.5.13) will
require an additional r1âˆ’1 complex multiplications. Since n=r1r2values of the transform have to be
computed, we will need r1r2(r1âˆ’1) complex multiplications.
The complete computation needs r1g(r2)+r2
1r2âˆ’r1r2multiplications if we choose a particular factor-
izationn=r1r2. The factorization that should be chosen is the one that minimizes the labor, so we have
the recurrence
g(n)= m i n
n=r1r2fr1g(r2)+r2
1r2gâˆ’n: (2:5:16)
Ifn=pis a prime number then there are no factorizations to choose from and our algorithm is no help
at all. There is no recourse but to calculate the pvalues of the transform directly from the denition (2.5.6),
and that will require pâˆ’1 complex multiplications to be done in order to get each of those pvalues. Hence
we have, in addition to the recurrence formula (2.5.16), the special values
g(p)=p(pâˆ’1) (if p is prime) : (2:5:17)
The recurrence formula (2.5.16) together with the starting values that are shown in (2.5.17) completely
determine the function g(n). Before proceeding, the reader is invited to calculate g(12) andg(18).
We are going to work out the exact solution of the interesting recurrence (2.5.16), (2.5.17), and when
we are nished we will see which factorization of nis the best one to choose. If we leave that question in
abeyance for a while, though, we can summarize by stating the (otherwise) complete algorithm for the fast
Fourier transform.
functionFFT(x:complexarray; n:integer):complexarray;
fcomputes Fourier transform of a sequence xof lengthng
ifnis prime
then
forj:=0tonâˆ’1d o
FFT[j]: =Pnâˆ’1
k=0x[k]njk
else
letn=r1r2be some factorization of n;
fsee below for best choice of r1;r 2g
fork:=0tor1âˆ’1d o
fak[0];ak[1];:::;a k[r2âˆ’1]g
:=FFT(fx[k];x[k+r1];:::;x [k+(r2âˆ’1)r1]g;r2);
forj:=0tonâˆ’1d o
FFT[j]: =Pr1âˆ’1
k=0ak[jmodr2]kj
n
end.fFFTg
Our next task will be to solve the recurrence relations (2.5.16), (2.5.17), and thereby to learn the best
choice of the factorization of n.
Letg(n)=nh(n), wherehis a new unknown function. Then the recurrence that we have to solve takes
the form
h(n)=
mindfh(n=d)+dgâˆ’1;ifnis composite;
nâˆ’1; ifnis prime.(2:5:18)
In (2.5.18), the â€˜minâ€™ is taken over all dthat divide nother than d=1a n dd=n.
The above relation determines the value of hfor all positive integers n. For example,
h(15) = min
d(h(15=d)+d)âˆ’1
= min(h(5) + 3;h(3) + 5) âˆ’1
= min(7;7)âˆ’1=6
55
Chapter 2: Recursive Algorithms
and so forth.
To nd the solution in a pleasant form, let
n=pa1
1pa2
2pas
s (2:5:19)
be the canonical factorization of ninto primes. We claim that the function
h(n)=a1(p1âˆ’1) +a2(p2âˆ’1) ++as(psâˆ’1) (2 :5:20)
is the solution of (2.5.18) (this claim is obviously (?) correct if nis prime).
To prove the claim in general, suppose it to be true for 1 ;2;:::;n âˆ’1, and suppose that nis not prime.
Then every divisor dofnmust be of the form d=pb1
1pb2
2pbss, where the primes piare the same as those
that appear in (2.5.19) and each biisai. Hence from (2.5.18) we get
h(n)=m i n
bf(a1âˆ’b1)(p1âˆ’1) ++(asâˆ’bs)(psâˆ’1) +pb1
1pbs
sgâˆ’1( 2 :5:21)
where now the â€˜minâ€™ extends over all admissible choices of the bâ€™s, namely exponents b1;:::;b ssuch that
0biai(8i=1;s) and not all biare 0 and not all bi=ai.
One such admissible choice would be to take, say, bj= 1 and all other bi=0 . I fw el e t H(b1;:::;b s)
denote the quantity in braces in (2.5.21), then with this choice the value of Hwould bea1(p1âˆ’1) ++
as(psâˆ’1) + 1, exactly what we need to prove our claim (2.5.20). Hence what we have to show is that the
above choice of the biâ€™s is the best one. We will show that if one of the biis larger than 1 then we can reduce
it without increasing the value of H.
To prove this, observe that for each i=1;swe have
H(b1;:::;b i+1;:::;b s)âˆ’H(b1;:::;b s)=âˆ’pi+d(piâˆ’1)
=(dâˆ’1)(piâˆ’1):
Since the divisor d2a n dt h ep r i m e pi2, the last dierence is nonnegative. Hence Hdoesnâ€™t increase
if we decrease one of the bâ€™s by 1 unit, as long as not all bi= 0. It follows that the minimum of Hoccurs
among the prime divisorsdofn.F u r t h e r ,i f dis prime, then we can easily check from (2.5.21) that it doesnâ€™t
matter which prime divisor of nthat we choose to be d, the function h(n) is always given by (2.5.20). If we
recall the change of variable g(n)=nh(n) we nd that we have proved
Theorem 2.5.2. (Complexity of the Fast Fourier Transform) The best choice of the factorization n=r1r2
in algorithm FFT is to take r1to be a prime divisor of n. If that is done, then algorithm FFT requires
g(n)=n(a1(p1âˆ’1) +a2(p2âˆ’1) ++as(psâˆ’1))
complex multiplications in order to do its job, where n=pa1
1passis the canonical factorization of the
integern.
Table 2.5.1 shows the number g(n) of complex multiplications required by FFT as a function of n.T h e
saving over the straightforward algorithm that uses n(nâˆ’1) multiplications for each nis apparent.
Ifnis a power of 2, say n=2q, then the formula of theorem 2.5.2 reduces to g(n)=nlogn=log2, in
agreement with theorem 2.5.1. What does the formula say if nis a power of 3? if nis a product of distinct
primes?
2.6 Applications of the FFT
Finally, we will discuss some applications of the FFT. A family of such applications begins with the
observation that the FFT provides the fastest game in town for multiplying two polynomials together.Consider a multiplication like
(1 + 2x+7x
2âˆ’2x3âˆ’x4)(4âˆ’5xâˆ’x2âˆ’x3+1 1x4+x5):
56
2.6 Applications of the FFT
ng ( n ) ng ( n )
2 2 22 242
3 6 23 506
4 8 24 1205 20 25 200
6 18 26 338
7 42 27 1628 24 28 224
9 36 29 812
10 50 30 210
11 110 31 930
12 48 32 16013 156 33 396
14 98 34 578
15 90 35 35016 64 36 216
17 272 37 1332
18 90 38 722
19 342 39 546
20 120 40 28021 168 41 1640
Table 2.5.1: The complexity of the FFT
We will study the amount of labor that is needed to do this multiplication by the straightforward algorithm,
and then we will see how the FFT can help.
If we do this multiplication in the obvious way then there is quite a bit of work to do. The coecient of
x
4in the product, for instance, is 1 11+2 (âˆ’1)+7 (âˆ’1)+(âˆ’2)(âˆ’5)+(âˆ’1)4 = 8, and 5 multiplications
are needed to compute just that single coecient of the product polynomial.
In the general case, we want to multiply
fnX
i=0aixigfmX
j=0bjxjg: (2:6:1)
In the product polynomial, the coecient of xkis
min(k;n)X
r=max(0 ;kâˆ’m)arbkâˆ’r: (2:6:2)
Forkxed, the number of terms in the sum (2.6.2) is min( k;n)âˆ’max(0;kâˆ’m)+ 1. If we sum this amount
of labor over k=0;m+nwe nd that the total amount of labor for multiplication of two polynomials of
degreesmandnis (mn). In particular, if the polynomials are of the same degree nthen the labor is
(n2).
By using the FFT the amount of labor can be reduced from ( n2)t o (nlogn).
To understand how this works, letâ€™s recall the denition of the Fourier transform of a sequence. It is
the sequence of values of the polynomial whose coecients are the given numbers, at the nthroots of unity,
wherenis the length of the input sequence.
Imagine two universes, one in which the residents are used to describing polynomials by means of their
coecients, and another one in which the inhabitants are fond of describing polynomials by their values at
roots of unity. In the rst universe the locals have to work fairly hard to multiply two polynomials because
they have to carry out the operations (2.6.2) in order to nd each coecient of the product.
57
Chapter 2: Recursive Algorithms
In the second universe, multiplying two polynomials is a breeze. If we have in front of us the values
f(!) of the polynomial fat the roots of unity, and the values g(!) of the polynomial gat the same roots
of unity, then what are the values ( fg)(!) of the product polynomial fgat the roots of unity? To nd each
one requires only a single multiplication of two complex numbers, because the value of fgat!is simply
f(!)g(!).
Multiplying values is easier than nding the coecients of the product.
Since we live in a universe where people like to think about polynomials as being given by their coecient
arrays, we have to take a somewhat roundabout route in order to do an ecient multiplication.
Given: A polynomial f, of degreen, and a polynomial gof degreem; by their coecient arrays. Wanted:
The coecients of the product polynomial fg, of degreem+n.
Step 1: Let Nâˆ’1 be the smallest integer that is a power of 2 and is greater than m+n+1 .
Step 2. Think of fandgas polynomials each of whose degrees is Nâˆ’1. This means that we should
adjoinNâˆ’nmore coecients, all = 0, to the coecient array of fandNâˆ’mmore coecients, all = 0, to
the coecient array of g. Now both input coecient arrays are of length N.
Step 3. Compute the FFT of the array of coecients of f. Now we are looking at the values of fat the
Nthroots of unity. Likewise compute the FFT of the array of coecients of gto obtain the array of values
ofgat the same Nthroots of unity. The cost of this step is O(NlogN).
Step 4. For each of the Nthroots of unity !multiply the number f(!) by the number g(!). We now
have the numbers f(!)g(!), which are exactly the values of the unknown product polynomial fgat theNth
roots of unity. The cost of this step is Nmultiplications of numbers, one for each !.
Step 5. We now are looking at the values offgat theNthroots, and we want to get back to the
coecients offgbecause that was what we were asked for. To go backwards, from values at roots of unity
to coecients, calls for the inverse Fourier transform , which we will describe in a moment. Its cost is also
O(NlogN).
The answer to the original question has been obtained at a total cost of O(NlogN)=O((m+
n)log(m+n)) arithmetic operations. Itâ€™s true that we did have to take a walk from our universe to the next
one and back again, but the round trip was a lot cheaper than the O((m+n)2) cost of a direct multiplication.
It remains to discuss the inverse Fourier transform. Perhaps the neatest way to do that is to juxtapose
the formulas for the Fourier transform and for the inverse tranform, so as to facilitate comparison of the two,
so here they are. If we are given a sequence fx0;x1;:::;x nâˆ’1gthen the Fourier transform of the sequence is
the sequence (see (2.5.6))
f(!j)=nâˆ’1X
k=0xke2ijk=n(j=0;1;:::;n âˆ’1): (2:6:3)
Conversely, if we are given the numbers f(!j)(j=0;:::;n âˆ’1) then we can recover the coecient sequence
x0;:::;x nâˆ’1by the inverse formulas
xk=1
nnâˆ’1X
j=0f(!j)eâˆ’2ijk=n(k=0;1;:::;n âˆ’1): (2:6:4)
The dierences between the inverse formulas and the original transform formulas are rst the appearance of
â€˜1=nâ€™ in front of the summation and second the â€˜ âˆ’â€™ sign in the exponential. We leave it as an exercise for
the reader to verify that these formulas really do invert each other.
We observe that if we are already in possession of a computer program that will nd the FFT, then we
can use it to calculate the inverse Fourier transform as follows:
(i) Given a sequence ff(!)gof values of a polynomial at the nthroots of unity, form the complex
conjugate of each member of the sequence.
(ii) Input the conjugated sequence to your FFT program.
(iii) Form the complex conjugate of each entry of the output array, and divide by n. You now have the
inverse transform of the input sequence.
The cost is obviously equal to the cost of the FFT plus a linear number of conjugations and divisions
byn.
58
2.7 A review
An outgrowth of the rapidity with which we can now multiply polynomials is a rethinking of the methods
by which we do ultrahigh-precision arithmetic. How fast can we multiply two integers, each of which has
ten million bits? By using ideas that developed directly (though not at all trivially) from the ones that we
have been discussing, SchÂ¨ onhage and Strassen found the fastest known method for doing such large-scale
multiplications of integers. The method relies heavily on the FFT, which may not be too surprising since
an integernis given in terms of its bits b0;b1;:::;b mby the relation
n=X
i0bi2i: (2:6:5)
However the sum in (2.6.5) is seen at once to be the value of a certain polynomial at x= 2. Hence in asking
for the bits of the product of two such integers we are asking for something very similar to the coecients
of the product of two polynomials, and indeed the fastest known algorithms for this problem depend uponthe Fast Fourier Transform.
Exercises for section 2.6
1. Let!be ann
throot of unity, and let kbe a xed integer. Evaluate
1+!k+!2k++!k(nâˆ’1):
2. Verify that the relations (2.6.3) and (2.6.4) indeed are inverses of each other.
3. Letf=Pnâˆ’1
j=0ajxj:Show that
1
nX
!n=1jf(!)j2=ja0j2++janâˆ’1j2
4. The values of a certain cubic polynomial at 1 ;i;âˆ’1;âˆ’iare 1;2;3;4, respectively. Find its value at 2.
5. Write a program that will do the FFT in the case where the number of data points is a power of 2.
Organize your program so as to minimize additional array storage beyond the input and output arrays.
6. Prove that a polynomial of degree nis uniquely determined by its values at n+ 1 distinct points.
2.7 A review
Here is a quick review of the algorithms that we studied in this chapter.
Sorting is an easy computational problem. The most obvious way to sort narray elements takes time
(n2). We discussed a recursive algorithm that sorts in an average time of ( nlogn).
Finding a maximum independent set in a graph is a hard computational problem. The most obvious
way to nd one might take time (2n) if the graph Ghasnvertices. We discussed a recursive method that
runs in time (1 :39n). The best known methods run in time (2n=3).
Finding out if a graph is K-colorable is a hard computational problem. The most obvious way to do it
takes time ( Kn), ifGhasnvertices. We discussed a recursive method that runs in time O(1:62n+E)i fG
hasnvertices and Eedges. One recently developed method * runs in time O((1 +3p
3)n). We will see in
section 5.7 that this problem can be done in an average time that is O(1) for xed K.
Multiplying two matrices is an easy computational problem. The most obvious way to do it takes time
(n3) if the matrices are nn. We discussed a recursive method that runs in time O(n2:82). A recent
method ** runs in time O(nÎ³)f o rs o m eÎ³<2:5.
* E. Lawler, A note on the complexity of the chromatic number problem, Information Processing Letters
5(1976), 66-7.
** D. Coppersmith and S. Winograd, On the asymptotic complexity of matrix multiplication, SIAM J.
Comp. 11(1980), 472-492.
59
Chapter 2: Recursive Algorithms
Finding the discrete Fourier transform of an array of nelements is an easy computational problem. The
most obvious way to do it takes time ( n2). We discussed a recursive method that runs in time O(nlogn)
ifnis a power of 2.
When we write a program recursively we are making life easier for ourselves and harder for the compiler
and the computer. A single call to a recursive program can cause it to execute a tree-full of calls to itselfbefore it is able to respond to our original request.
For example, if we call Quicksort to sort the array
f5;8;13;9;15;29;44;71;67g
then the tree shown in Fig. 2.7.1 might be generated by the compiler.
Fig. 2.7.1: A tree of calls to Quicksort
Again, if we call maxset 1 on the 5-cycle, the tree in Fig. 2.3.3 of calls may be created.
A single invocation of chrompoly , where the input graph is a 4-cycle, for instance, might generate the
tree of recursive calls that appears in Fig. 2.7.2.
Fig. 2.7.2: A tree of calls to chrompoly
60
2.7 A review
Fig. 2.7.3: The recursive call tree for FFT
Finally, if we call the â€˜power of 2â€™ version of the FFT algorithm on the sequence f1;i;âˆ’i;1gthenFFT
will proceed to manufacture the tree shown in Fig. 2.7.3.
It must be emphasized that the creation of the tree of recursions is done by the compiler without any
further eort on the part of the programmer. As long as weâ€™re here, how does a compiler go about makingsuch a tree?
It does it by using an auxiliary stack. It adopts the philosophy that if it is asked to do two things at
once, well after all, it canâ€™t do that, so it does one of those two things and drops the other request on top ofa stack of unnished business. When it nishes executing the rst request it goes to the top of the stack to
nd out what to do next.
Example
Letâ€™s follow the compiler through its tribulations as it attempts to deal with our request for maximum
independent set size that appears in Fig. 2.3.3. We begin by asking for the maxset 1 of the 5-cycle. Our
program immediately makes two recursive calls to maxset 1, on each of the two graphs that appear on the
second level of the tree in Fig. 2.3.3. The stack is initially empty.
The compiler says to itself â€˜I canâ€™t do these both at onceâ€™, and it puts the right-hand graph (involving
vertices 3,4) on the stack, and proceeds to call itself on the left hand graph (vertices 2,3,4,5).
When it tries to do that one, of course, two more graphs are generated, of which the right-hand one
(4,5) is dropped onto the stack, on top of the graph that previously lived there, so now two graphs are on
the stack, awaiting processing, and the compiler is dealing with the graph (3,4,5).
This time the graph of just one vertex (5) is dropped onto the stack, which now holds three graphs, as
the compiler works on (4,5).
Next, that graph is broken up into (5), and an empty graph, which is dutifully dropped onto the stack,
so the compiler can work on (5).
Finally, something fruitful happens: the graph (5) has no edges, so the program maxset 1g i v e s ,i ni t s
trivial case, very specic instructions as to how to deal with this graph. We now know that the graph that
consists of just the single vertex (5) has a maxset 1 values of 1.
The compiler next reaches for the graph on top of the stack, nds that it is the empty graph, which has
no edges at all, and therefore its maxset size is 0.
It now knows the n
1=1a n dt h e n2= 0 values that appear in the algorithm maxset 1, and therefore it
can execute the instruction maxset 1: =max(n1;1+n2), from which it nds that the value of maxset 1f o r
the graph (4,5) is 1, and it continues from there, to dig itself out of the stack of unnished business.
In general, if it is trying to execute maxset 1 on a graph that has edges, it will drop the graph Gâˆ’
fvgâˆ’Nbhd(v) on the stack and try to do the graph Gâˆ’fvg.
The reader should try to write out, as a formal algorithm, the procedure that we have been describing,
whereby the compiler deals with a recursive computation that branches into two sub-computations until a
trivial case is reached.
61
Chapter 2: Recursive Algorithms
Exercise for section 2.7
1. In Fig. 2.7.3, add to the picture the output that each of the recursive calls gives back to the box above it
that made the call.
Bibliography
A denitive account of all aspects of sorting is in
D. E. Knuth, The art of computer programming ,V o l . 3 : Sorting and searching , Addison Wesley, Reading
MA, 1973.
All three volumes of the above reference are highly recommended for the study of algorithms and discrete
mathematics.
AO(2n=3) algorithm for the maximum independent set problem can be found in
R. E. Tarjan and A. Trojanowski, Finding a maximum independent set, SIAM J.Computing 6 (1977), 537-
546.
Recent developments in fast matrix multiplication are traced in
Victor Pan, How to multiply matrices faster, Lecture notes in computer science No. 179, Springer-Verlag,
1984.
The realization that the Fourier transform calculation can be speeded up has been traced back toC. Runge, Zeits. Math. Phys. ,48(1903) p. 443.
and also appears in
C. Runge and H. KÂ¨ onig,Die Grundlehren der math. Wissensch. , 11, Springer Verlag, Berlin 1924.
The introduction of the method in modern algorithmic terms is generally credited to
J. M. Cooley and J. W. Tukey, An algorithm for the machine calculation of complex Fourier series, Mathe-
matics of Computation ,19(1965), 297-301.
A number of statistical applications of the method are in
J. M. Cooley, P. A. W. Lewis and P. D. Welch, The Fast Fourier Transform and its application to time series
analysis, in Statistical Methods for Digital Computers , Enslein, Ralston and Wilf eds., John Wiley & Sons,
New York, 1977, 377-423.
The use of the FFT for high precision integer arithmetic is due toAS c h Â¨ onhage and V. Strassen, Schnelle Multiplikation grosser Zahlen, Computing ,7(1971), 281-292.
An excellent account of the above as well as of applications of the FFT to polynomial arithmetic is by
A. V. Aho, J. E. Hopcroft and J. D. Ullman, The design and analysis of computer algorithms, Addison
Wesley, Reading, MA, 1974 (chap. 7).
62
Chapter 3: The Network Flow Problem
3.1 Introduction
The network ï¬‚ow problem is an example of a beautiful theoretical subject that has many important
applications. It also has generated algorithmic questions that have been in a state of extremely rapid
development in the past 20 years. Altogether, the fastest algorithms that are now known for the problem
are much faster, and some are much simpler, than the ones that were in use a short time ago, but it is still
unclear how close to the â€˜ultimateâ€™ algorithm we are.
Denition. Anetwork is an edge-capacitated directed graph, with two distinguished vertices called the
source and the sink.
To repeat that, this time a little more slowly, suppose rst that we are given a directed graph ( digraph )
G. That is, we are given a set of vertices, and a set of ordered pairs of these vertices, these pairs being
theedges of the digraph. It is perfectly OK to have both an edge from utovand an edge from vtou,o r
both, or neither, for all u6=v.N oe d g e( u;u) is permitted. If an edge eis directed fromvertexvtovertex
w,t h e nvis the initial vertex ofeandwis the terminal vertex ofe. We may then write v=Init(e)a n d
w=Term (e).
Next, in a network there is associated with each directed edge eof the digraph a positive real number
called its capacity , and denoted by cap(e).
Finally, two of the vertices of the digraph are distinguished. One, s, is the source, and the other, t,i s
the sink of the network.
We will let Xdenote the resulting network. It consists of the digraph G, the given set of edge capacities,
the source, and the sink. A network is shown in Fig. 3.1.1.
Fig. 3.1.1: A network
Now roughly speaking, we can think of the edges of Gas conduits for a ï¬‚uid, the capacity of each edge
being the carrying-capacity of the edge for that ï¬‚uid. Imagine that the ï¬‚uid ï¬‚ows in the network from the
source to the sink, in such a way that the amount of ï¬‚uid in each edge does not exceed the capacity of that
edge.
We want to know the maximum net quantity of ï¬‚uid that could be ï¬‚owing from source to sink.That was a rough description of the problem; here it is more precisely.
Denition. A ï¬‚ow in a network Xis a function fthat assigns to each edge eof the network a real number
f(e), in such a way that
(1) For each edge ewe have 0f(e)cap(e)and
(2) For each vertex vother than the source and the sink, it is true that
X
Init (e)=vf(e)=X
Term (e)=vf(e): (3:1:1)
63
Chapter 3: The Network Flow Problem
The condition (3.1.1) is a ï¬‚ow conservation condition. It states that the outï¬‚ow from v(the left side
of (3.1.1)) is equal to the inï¬‚ow to v(the right side) for all vertices vother than sandt. In the theory of
electrical networks such conservation conditions are known as Kirchhoâ€™s laws. Flow cannot be manufactured
anywhere in the network except at sort. At other vertices, only redistribution or rerouting takes place.
Since the source and the sink are exempt from the conservation conditions there may, and usually will,
be a nonzero net ï¬‚ow out of the source, and a nonzero net ï¬‚ow into the sink. Intuitively it must already be
clear that these two are equal, and we will prove it below, in section 3.4. If we let Qbe the net outï¬‚ow from
the source, then Qis also the net inï¬‚ow to the sink.
The quantity Qis called the value of the ï¬‚ow .
In Fig. 3.1.2 there is shown a ï¬‚ow in the network of Fig. 3.1.1. The amounts of ï¬‚ow in each edge are
shown in the square boxes. The other number on each edge is its capacity. The letter inside the small circle
next to each vertex is the name of that vertex, for the purposes of the present discussion. The value of the
ï¬‚ow in Fig. 3.1.2 is Q= 32.
Fig. 3.1.2: A ï¬‚ow in a network
Thenetwork ï¬‚ow problem , the main subject of this chapter, is: given a network X, nd the maximum
possible value of a ï¬‚ow in X, and nd a ï¬‚ow of that value .
3.2 Algorithms for the network ï¬‚ow problem
The rst algorithm for the network ï¬‚ow problem was given by Ford and Fulkerson. They used that
algorithm not only to solve instances of the problem, but also to prove theorems about network ï¬‚ow, a
particularly happy combination. In particular, they used their algorithm to prove the â€˜max-ï¬‚ow-min-cutâ€™
theorem, which we state below as theorem 3.4.1, and which occupies a central position in the theory.
The speed of their algorithm, it turns out, depends on the edge capacities in the network as well as on
the numbers Vof vertices, and Eof edges, of the network. Indeed, for certain (irrational) values of edge
capacities they found that their algorithm might not converge at all (see section 3.5).
In 1969 Edmonds and Karp gave the rst algorithm for the problem whose speed is bounded by a
polynomial function of EandVonly. In fact that algorithm runs in time O(E2V). Since then there has
been a steady procession of improvements in the algorithms, culminating, at the time of this writing anyway,with anO(EVlogV) algorithm. The chronology is shown in Table 3.2.1.
The maximum number of edges that a network of Vvertices can have is ( V
2). A family of networks
might be called dense if there is a K> 0 such that jE(X)j>KjV(X)j2for all networks in the family.
The reader should check that for dense networks, all of the time complexities in Table 3.2.1, beginning withKarzanovâ€™s algorithm, are in the neighborhood of O(V
3). On the other hand, for sparse networks (networks
with relatively few edges), the later algorithms in the table will give signicantly better performances than
the earlier ones.
64
3.3 The algorithm of Ford and Fulkerson
Author (s)Year Complexity
Ford;Fulkerson 1956 âˆ’âˆ’âˆ’âˆ’âˆ’
Edmonds;Karp 1969 O(E2V)
Dinic 1970 O(EV2)
Karzanov 1973 O(V3)
Cherkassky 1976 O(p
EV2)
Malhotra;et al: 1978O(V3)
Galil 1978 O(V5=3E2=3)
Galil and Naamad 1979 O(EVlog2V)
Sleator and Tarjan 1980 O(EVlogV)
Goldberg and Tarjan 1985 O(EVlog (V2=E))
Table 3.2.1: Progress in network ï¬‚ow algorithms
Exercise 3.2.1. GivenK>0. Consider the family of all possible networks Xfor which jE(X)j=KjV(X)j.
In this family, evaluate all of the complexity bounds in Table 3.2.1 and nd the fastest algorithm for the
family.
Among the algorithms in Table 3.2.1 we will discuss just two in detail. The rst will be the original
algorithm of Ford and Fulkerson, because of its importance and its simplicity, if not for its speed.
The second will be the 1978 algorithm of Malhotra, Pramodh-Kumar and Maheshwari (MPM), for
three reasons. It uses the idea, introduced by Dinic in 1970 and common to all later algorithms, of layered
networks , it is fast, and it is extremely simple and elegant in its conception, and so it represents a good
choice for those who may wish to program one of these algorithms for themselves.
3.3 The algorithm of Ford and Fulkerson
The basic idea of the Ford-Fulkerson algorithm for the network ï¬‚ow problem is this: start with some
ï¬‚ow function (initially this might consist of zero ï¬‚ow on every edge). Then look for a ï¬‚ow augmenting path
in the network. A ï¬‚ow augmenting path is a path from the source to the sink along which we can push some
additional ï¬‚ow.
In Fig. 3.3.1 below we show a ï¬‚ow augmenting path for the network of Fig. 3.2.1. The capacities of the
edges are shown on each edge, and the values of the ï¬‚ow function are shown in the boxes on the edges.
Fig. 3.3.1: A ï¬‚ow augmenting path
Fig. 3.3.2: The path above, after augmentation.
An edge can get elected to a ï¬‚ow augmenting path for two possible reasons. Either
65
Chapter 3: The Network Flow Problem
(a) the direction of the edge is coherent with the direction of the path from source to sink and the present
value of the ï¬‚ow function on the edge is below the capacity of that edge, or
(b) the direction of the edge is opposed to that of the path from source to sink and the present value of the
ï¬‚ow function on the edge is strictly positive.Indeed, on all edges of a ï¬‚ow augmenting path that are coherently oriented with the path we can increase
the ï¬‚ow along the edge, and on all edges that are incoherently oriented with the path we can decrease the
ï¬‚ow on the edge, and in either case we will have increased the value of the ï¬‚ow (think about that one until
it makes sense).
It is, of course, necessary to maintain the conservation of ï¬‚ow, i.e., to respect Kirchhoâ€™s laws. To do
this we will augment the ï¬‚ow on every edge of an augmenting path by the same amount. If the conservation
conditions were satised before the augmentation then they will still be satised after such an augmentation.
It may be helpful to remark that an edge is coherently or incoherently oriented only with respect to a
given path from source to sink. That is, the coherence, or lack of it, is not only a property of the directed
edge, but depends on how the edge sits inside a chosen path.
Thus, in Fig. 3.3.1 the rst edge is directed towards the source, i.e., incoherently with the path. Hence
if we can decrease the ï¬‚ow in that edge we will have increased the value of the ï¬‚ow function, namely the net
ï¬‚ow out of the source. That particular edge can indeed have its ï¬‚ow decreased, by at most 8 units. Thenext edge carries 10 units of ï¬‚ow towards the source. Therefore if we decrease the ï¬‚ow on that edge, by up
to 10 units, we will also have increased the value of the ï¬‚ow function. Finally, the edge into the sink carries
12 units of ï¬‚ow and is oriented towards the sink. Hence if we increase the ï¬‚ow in this edge, by at most 3
units since its capacity is 15, we will have increased the value of the ï¬‚ow in the network.
Since every edge in the path that is shown in Fig. 3.3.1 can have its ï¬‚ow altered in one way or the
other so as to increase the ï¬‚ow in the network, the path is indeed a ï¬‚ow augmenting path. The most that
we might accomplish with this path would be to push 3 more units of ï¬‚ow through it from source to sink.
We couldnâ€™t push more than 3 units through because one of the edges (the edge into the sink) will toleratean augmentation of only 3 ï¬‚ow units before reaching its capacity.
To augment the ï¬‚ow by 3 units we would diminish the ï¬‚ow by 3 units on each of the rst two edges and
increase it by 3 units on the last edge. The resulting ï¬‚ow in this path is shown in Fig. 3.3.2. The ï¬‚ow inthe full network, after this augmentation, is shown in Fig. 3.3.3. Note carefully that if these augmentations
are made then ï¬‚ow conservation at each vertex of the network will still hold (check this!).
Fig. 3.3.3: The network, after augmentation of ï¬‚ow
After augmenting the ï¬‚ow by 3 units as we have just described, the resulting ï¬‚ow will be the one that
is shown in Fig. 3.3.3. The value of the ï¬‚ow in Fig. 3.1.2 was 32 units. After the augmentation, the ï¬‚owfunction in Fig. 3.3.3 has a value of 35 units.
We have just described the main idea of the Ford-Fulkerson algorithm. It rst nds a ï¬‚ow augmenting
path. Then it augments the ï¬‚ow along that path as much as it can. Then it nds another ï¬‚ow augmenting
66
3.3 The algorithm of Ford and Fulkerson
path, etc. etc. The algorithm terminates when no ï¬‚ow augmenting paths exist. We will prove that when
that happens, the ï¬‚ow will be at the maximum possible value, i.e., we will have found the solution of the
network ï¬‚ow problem.
We will now describe the steps of the algorithm in more detail.
Denition. Letfbe a ï¬‚ow function in a network X. We say that an edge eofXisusable from vtowif
eithereis directed from vtowand the ï¬‚ow in eis less than the capacity of the edge, or eis directed from
wtovand the ï¬‚ow in eis>0.
Now, given a network and a ï¬‚ow in that network, how do we nd a ï¬‚ow augmenting path from the
source to the sink? This is done by a process of labelling and scanning the vertices of the network, beginning
with the source and proceeding out to the sink. Initially all vertices are in the conditions â€˜unlabeledâ€™ and
â€˜unscanned.â€™ As the algorithm proceeds, various vertices will become labeled, and if a vertex is labeled, itmay become scanned . To scan a vertex vmeans, roughly, that we stand at vand look around at all neighbors
wofvthat havenâ€™t yet been labeled. If eis some edge that joins vwith a neighbor w, and if the edge e
is usable from vtowas dened above, then we will label w, because any ï¬‚ow augmenting path that has
already reached from the source to vcan be extended another step, to w.
The label that every vertex vgets is a triple ( u;;z), and here is what the three items mean.
The â€˜uâ€™ part of the label of vis the name of the vertex that was being scanned when vwas labeled.
The â€˜â€™ will be â€˜+â€™ if vwas labeled because the edge ( u;v) was usable from utov(i.e.,if the ï¬‚ow from
utovwas less than the capacity of ( u;v)) and it will be â€˜ âˆ’â€™i fvwas labeled because the edge ( v;u)w a s
usable from utov(i.e., if the ï¬‚ow from vtouwas>0).
Finally, the â€˜ zâ€™ component of the label represents the largest amount of ï¬‚ow that can be pushed from
the source to the present vertex valong any augmenting path that has so far been found. At each step the
algorithm will replace the current value of zby the amount of new ï¬‚ow that could be pushed through to z
along the edge that is now being examined, if that amount is smaller than z.
So much for the meanings of the various labels. As the algorithm proceeds, the labels that get attached
to the dierent vertices form a record of how much ï¬‚ow can be pushed through the network from the source
to the various vertices, and by exactly which routes.
To begin with, the algorithm labels the source with ( âˆ’1;+;1). The source now has the label-status
labeled and the scan-status unscanned .
Next we will scan the source. Here is the procedure for scanning any vertex u.
procedurescan(u:vertex; X:network;f:ï¬‚ow );
forevery â€˜unlabeledâ€™ vertex vthat is connected
touby an edge in either or both directions, do
ifthe ï¬‚ow in ( u;v)i sl e s st h a n cap(u;v)
then
labelvwith (u;+;minfz(u);cap(u;v)âˆ’flow(u;v)g)
else if the ï¬‚ow in ( v;u)i s>0
then
labelvwith (u;âˆ’;minfz(u);flow (v;u)g)a n d
change the label-status of vto â€˜labeledâ€™;
change the scan-status of uto â€˜scannedâ€™
end.fscang
We can use the above procedure to describe the complete scanning and labelling of the vertices of the
network, as follows.
67
Chapter 3: The Network Flow Problem
procedurelabelandscan (X:network;f:ï¬‚ow;whyhalt :reason);
give every vertex the scan-status â€˜ unscanned â€™
and the label-status â€˜ unlabeled â€™;
u:=source ;
labelsource with ( âˆ’1;+;1);
label-status of source := â€˜labeled â€™;
while fthere is a â€˜labeled â€™a n dâ€˜unscanned â€™v e r t e xv
andsinkis â€˜unlabeled â€™g
doscan(v;X;f);
ifsinkisunlabeled
then â€˜whyhalt â€™:=â€˜flow is maximum â€™
elseâ€˜whyhalt â€™:= â€˜itâ€™st im et oa ugm en t â€™
end.flabelandscan g
Obviously the labelling and scanning process will halt for one of two reasons: either the sink tacquires
a label, or the sink never gets labeled but no more labels can be given. In the rst case we will see that a
ï¬‚ow augmenting path from source to sink has been found, and in the second case we will prove that the ï¬‚ow
is at its maximum possible value, so the network ï¬‚ow problem has been solved.
Suppose the sink does get a label, for instance the label ( u;;z). Then we claim that the value of the
ï¬‚ow in the network can be augmented by zunits.
To prove this we will construct a ï¬‚ow augmenting path, using the labels on the vertices, and then we
will change the ï¬‚ow by zunits on every edge of that path in such a way as to increase the value of the ï¬‚ow
function by zunits. This is done as follows.
If the sign part of the label of tis â€˜+,â€™ then increase the ï¬‚ow function by zunits on the edge ( u;t), else
decrease the ï¬‚ow on edge ( t;u)b yzunits.
Then move back one step away from the sink, to vertex u, and look at its label, which might be ( w;;z1).
If the sign is â€˜+â€™ then increase the ï¬‚ow on edge ( w;u)b yzunits (not by z1units!), while if the sign is â€˜ âˆ’â€™
then decrease the ï¬‚ow on edge ( u;w)b yzunits. Next replace ubyw, etc., until the source shas been
reached.
A little more formally, the ï¬‚ow augmentation algorithm is the following.
procedureaugmentflow (X:network;f:ï¬‚ow ;amount :real);
fassumes that labelandscan has just been done g
v:=sink;
amount := the â€˜zâ€™ part of the label of sink;
repeat
(previous;sign;z ): =label(v);
ifsign=â€˜+â€™
then
increasef(previous;v )b yamount
else
decreasef(v;previous )b yamount ;
v:=previous
untilv=source
end.faugmentflow g
The value of the ï¬‚ow in the network has now been increased byzunits. The whole process of labelling
and scanning is now repeated, to search for another ï¬‚ow augmenting path. The algorithm halts only when
we are unable to label the sink. The complete Ford-Fulkerson algorithm is shown below.
68
3.4 The max-ï¬‚ow min-cut theorem
procedurefordfulkerson (X:network;f:ï¬‚ o w ;maxflowvalue :real);
fnds maximum ï¬‚ow in a given network Xg
setf:=0 on every edge of X;
maxflowvalue :=0;
repeat
labelandscan (X,f,whyhalt );
ifwhyhalt =â€˜itâ€™stimetoaugment â€™then
augmentflow (X,f,amount );
maxflowvalue :=maxflowvalue +amount
untilwhyhalt =â€˜flow is maximum â€™
end.ffordfulkerson g
Letâ€™s look at what happens if we apply the labelling and scanning algorithm to the network and ï¬‚ow
shown in Fig. 3.1.2. First vertex sgets the label ( âˆ’1;+;1). We then scan s.V e r t e xAgets the label
(s;âˆ’;8),Bcannot be labeled, and Cgets labeled with ( s;+;10), which completes the scan of s.
Next we scan vertex A, during which Dacquires the label ( A;+;8). ThenCis scanned, which results
inEgetting the label ( C;âˆ’;10). Finally, the scan of Dresults in the label ( D;+;3) for the sink t.
From the label of twe see that there is a ï¬‚ow augmenting path in the network along which we can push
3 more units of ï¬‚ow from stot. We nd the path as in procedure augmentflow above, following the labels
backwards from ttoD,Aands. The path in question will be seen to be exactly the one shown in Fig.
3.3.1, and further augmentation proceeds as we have discussed above.
3.4 The max-ï¬‚ow min-cut theorem
Now we are going to look at the state of aairs that holds when the ï¬‚ow augmentation procedure
terminates because it has not been able to label the sink. We want to show that then the ï¬‚ow will have a
maximum possible value.
LetWV(X), and suppose that Wcontains the source and Wdoes not contain the sink. Let W
denote
all other vertices of X,i.e.,W=V(X)âˆ’W.
Denition. By the cut(W;W)we mean the set of all edges of Xwhose initial vertex is in Wand whose
terminal vertex is in W.
For example, one cut in a network consists of all edges whose initial vertex is the source.
Now, every unit of ï¬‚ow that leaves the source and arrives at the sink must at some moment ï¬‚ow from
av e r t e xo f Wt oav e r t e xo f W,i.e., must ï¬‚ow along some edge of the cut ( W;W). If we dene the capacity
of a cut to be the sum of the capacities of all edges in the cut, then it seems clear that the value of a ï¬‚ow
can never exceed the capacity of any cut, and therefore that the maximum value of a ï¬‚ow cannot exceed the
minimum capacity of any cut.
The main result of this section is the â€˜max-ï¬‚ow min-cutâ€™ theorem of Ford and Fulkerson, which we state
as
Theorem 3.4.1. The maximum possible value of any ï¬‚ow in a network is equal to the minimum capacity
of any cut in that network.
Proof : We will rst do a little computation to show that the value of a ï¬‚ow can never exceed the capacity of
a cut. Second, we will show that when the Ford-Fulkerson algorithm terminates because it has been unableto label the sink, then at that moment there is a cut in the network whose edges are saturated with ï¬‚ow,
i.e., such that the ï¬‚ow in each edge of the cut is equal to the capacity of that edge.
LetUandVbe two (not necessarily disjoint) sets of vertices of the network X, and letfbe a ï¬‚ow
function for X.B yf(U;V) we mean the sum of the values of the ï¬‚ow function along all edges whose initial
vertex lies in Uand whose terminal vertex lies in V. Similarly, by cap(U;V) we mean the sum of the
capacities of all of those edges. Finally, by the net ï¬‚ow out of Uwe meanf(U;
U)âˆ’f(U;U).
69
Chapter 3: The Network Flow Problem
Lemma 3.4.1. Letfbe a ï¬‚ow of value Qin a network X, and let (W;W)be a cut in X.T h e n
Q=f(W;W)âˆ’f(W;W )cap(W;W): (3:4:1)
Proof of lemma: The net ï¬‚ow out of sisQ. The net ï¬‚ow out of any other vertex w2Wis 0. Hence, if
V(X) denotes the vertex set of the network X,w eo b t a i n
Q=X
w2Wff(w;V(X))âˆ’f(V(X);w)g
=f(W;V(X))âˆ’f(V(X);W)
=f(W;W [W)âˆ’f(W[W;W )
=f(W;W)+f(W;W)âˆ’f(W;W)âˆ’f(W;W )
=f(W;W)âˆ’f(W;W ):
This proves the â€˜=â€™ part of (3.4.1), and the â€˜ â€™ part is obvious, completing the proof of lemma 3.4.1.
We now know that the maximum value of the ï¬‚ow in a network cannot exceed the minimum of the
capacities of the cuts in the network.
To complete the proof of the theorem we will show that a ï¬‚ow of maximum value, which surely exists,
must saturate the edges of some cut.
Hence, letfbe a ï¬‚ow in Xof maximum value, and call procedure labelandscan (X,f,whyhalt ). Let
Wbe the set of vertices of Xthat have been labeled when the algorithm terminates. Clearly s2W.
Equally clearly, t=2W, for suppose the contrary. Then we would have termination with â€˜ whyhalt â€™ = â€˜itâ€™s
time to augment,â€™ and if we were then to call procedure augmentflow we would nd a ï¬‚ow of higher value,
contradicting the assumed maximality of f.
Sinces2Wandt=2W,t h es e tWdenes a cut ( W;W).
We claim that every edge of the cut ( W;W) is saturated. Indeed, if ( x;y)i si nt h ec u t , x2W,y=2W,
then edge ( x;y) is saturated, else ywould have been labeled when we were scanning xand we would have
y2W, a contradiction. Similarly, if ( y;x) is an edge where y2Wandx2W, then the ï¬‚ow f(y;x)=0 ,
else againywould have been labeled when we were scanning x, another contradiction.
Therefore, every edge from WtoWis carrying as much ï¬‚ow as its capacity permits, and every edge
fromWtoWis carrying no ï¬‚ow at all. Hence the sign of equality holds in (3.4.1), the value of the ï¬‚ow is
equal to the capacity of the cut ( W;W), and the proof of theorem 3.4.1 is nished.
3.5 The complexity of the Ford-Fulkerson algorithm
The algorithm of Ford and Fulkerson terminates if and when it arrives at a stage where the sink is not
labeled but no more vertices can be labeled. If at that time we let Wbe the set of vertices that have been
labeled, then we have seen that ( W;W) is a minimum cut of the network, and the present value of the ï¬‚ow
is the desired maximum for the network.
The question now is, how long does it take to arrive at that stage, and indeed, is it guaranteed that we
willeverget there? We are asking if the algorithm is nite, surely the most primitive complexity question
imaginable.
First consider the case where every edge of the given network Xhasinteger capacity. Then during the
labelling and ï¬‚ow augmentation algorithms, various additions and subtractions are done, but there is noway that any nonintegral ï¬‚ows can be produced.
It follows that the augmented ï¬‚ow is still integral. The valueof the ï¬‚ow therefore increases by an integer
amount during each augmentation. On the other hand if, say, C
denotes the combined capacity of all edges
that are outbound from the source, then it is eminently clear that the value of the ï¬‚ow can never exceed C.
Since the value of the ï¬‚ow increases by at least 1 unit per augmentation, we see that no more than Cï¬‚ow
augmentations will be needed before a maximum ï¬‚ow is reached. This yields
70
3.5 Complexity of the Ford-Fulkerson algorithm
Theorem 3.5.1. In a network with integer capacities on all edges, the Ford-Fulkerson algorithm terminates
after a nite number of steps with a ï¬‚ow of maximum value.
This is good news and bad news. The good news is that the algorithm is nite. The bad news is that
the complexity estimate that we have proved depends not only on the numbers of edges and vertices in X,
but on the edge capacities. If the bound Crepresents the true behavior of the algorithm, rather than some
weakness in our analysis of the algorithm, then even on very small networks it will be possible to assign edgecapacities so that the algorithm takes a very long time to run.
And it ispossible to do that.
We will show below an example due to Ford and Fulkerson in which the situation is even worse than
the one envisaged above: not only will the algorithm take a very long time to run; it wonâ€™t converge at all!
Consider the network Xthat is shown in Fig. 3.5.1. It has 10 vertices s,t,x
1;:::;x 4,y1;:::;y 4.T h e r e
are directed edges ( xi;xj)8i6=j,(xi;yj)8i;j,(yi;yj)8i6=j,(yi;xj)8i;j,(s;xi)8i,a n d(yj;t)8j.
Fig. 3.5.1: How to give the algorithm a hard time
In this network, the four edges Ai=(xi;yi)(i=1;4) will be called the special edges .
Next we will give the capacities of the edges of X.W r i t er=(âˆ’1+p
5)=2, and let
S=( 3+p
5)=2=1X
n=0rn:
Then to every edge of Xexcept the four special edges we assign the capacity S. The special edges
A1;A 2;A 3;A 4are given capacities 1 ;r;r2;r2, respectively (you can see that this is going to be interest-
ing).
Suppose, for our rst augmentation step, we nd the ï¬‚ow augmenting path s!x1!y1!t,a n dt h a t
we augment the ï¬‚ow by 1 unit along that path. The four special edges will then have residual capacities
(excesses of capacity over ï¬‚ow) of 0 ;r;r2;r2, respectively.
Inductively, suppose we have arrived at a stage of the algorithm where the four special edges, taken in
some rearrangement A0
1;A02;A03;A04, have residual capacities 0 ;rn;rn+1;rn+1. We will now show that the
algorithm might next do two ï¬‚ow augmentation steps the net result of which would be that the inductivestate of aairs would again hold, with nreplaced by n+1 .
Indeed, choose the ï¬‚ow augmenting path
s!x
0
2!y0
2!x0
3!y0
3!t:
71
Chapter 3: The Network Flow Problem
The only special edges that are on this path are A0
2andA0
3. Augment the ï¬‚ow along this path by rn+1units
(the maximum possible amount).
Next, choose the ï¬‚ow augmenting path
s!x0
2!y0
2!y0
1!x0
1!y0
3!x0
3!y0
4!t:
Notice that with respect to this path the special edges A0
1andA0
3are incoherently directed. Augment the
ï¬‚ow along this path by rn+2units, once more the largest possible amount.
The reader may now verify that the residual capacities of the four special edges are rn+2,0 ,rn+2,rn+1.
In the course of doing this verication it will be handy to use the fact that
rn+2=rnâˆ’rn+1(8n0):
These two augmentation steps together have increased the ï¬‚ow value by rn+1+rn+2=rnunits. Hence
the ï¬‚ow in an edge will never exceed Sunits.
The algorithm converges to a ï¬‚ow of value S. Now comes the bad news: the maximum ï¬‚ow in this
network has the value 4 S(nd it!).
Hence, for this network
(a) the algorithm does not halt after nitely many steps even though the edge capacities are nite and
(b) the sequence of ï¬‚ow values converges to a number that is not the maximum ï¬‚ow in the network.
The irrational capacities on the edges may at rst seem to make this example seem â€˜cooked up.â€™ But
the implication is that even with a network whose edge capacities are all integers, the algorithm might take
a very long time to run.
Motivated by the importance and beauty of the theory of network ï¬‚ows, and by the unsatisfactory time
complexity of the original algorithm, many researchers have attacked the question of nding an algorithm
whose success is guaranteed within a time bound that is independent of the edge capacities, and dependsonly on the size of the network.
We turn now to the consideration of one of the main ideas on which further progress has depended, that
oflayering a network with respect to a ï¬‚ow function. This idea has triggered a whole series of improved
algorithms. Following the discussion of layering we will give a description of one of the algorithms, the MPM
algorithm, that uses layered networks and guarantees fast operation.
3.6 Layered networks
Layering a network is a technique that has the eect of replacing a single max-ï¬‚ow problem by several
problems, each a good deal easier than the original. More precisely, in a network with Vvertices we will nd
that we can solve a max-ï¬‚ow problem by solving at most Vslightly dierent problems, each on a layered
network. We will then discuss an O(V
2) method for solving each such problem on a layered network, and
the result will be an O(V3) algorithm for the original network ï¬‚ow problem.
Now we will discuss how to layer a network with respect to a given ï¬‚ow function . The purpose of the
italics is to emphasize the fact that one does not just â€˜layer a network.â€™ Instead, there is given a network X
and a ï¬‚ow function ffor that network, and together they induce a layered network Y=Y(X;f), as follows.
First let us say that an edge eofXishelpful from utovif eithereis directed from utovandf(e)i s
below capacity or eis directed from vtouand the ï¬‚ow f(e) is positive.
Next we will describe the layered network Y. Recall that in order to describe a network one must
describe the vertices of the network, the directed edges, give the capacities of those edges, and designate thesource and the sink. The network Ywill be constructed one layer at a time from the vertices of X, using
the ï¬‚owfas a guide. For each layer, we will say which vertices of Xgo into that layer, then we will say
which vertices of the previous layer are connected to each vertex of the new layer. All of these edges will be
directed from the earlier layer to the later one. Finally we will give the capacities of each of these new edges.
The 0
thlayer of Yconsists only of the source s. The vertices that comprise layer 1 of Ywill be every
vertexvofXsuch that in Xthere is a helpful edge from stov. We then draw an edge in Ydirected from
stovf o re a c hs u c hv e r t e x v. We assign to that edge in Ya capacitycap(s;v)âˆ’f(s;v)+f(v;s):
72
3.6 Layered networks
The set of all such vwill be called layer 1 of Y. Next we construct layer 2 of Y. The vertex set of layer
2 consists of all vertices wthat do not yet belong to any layer, and such that there is a helpful edge in X
from some vertex vof layer 1 to w.
Next we draw the edges from layer 1 to layer 2: for each vertex vin layer 1 we draw a single edge in Y
directed from vto every vertex win layer 2 for which there is a helpful edge in Xfromvtow.
Note that the edge always goes fromvtowregardless of the direction of the helpful edge in X.N o t e
also that in contrast to the Ford-Fulkerson algorithm, even after an edge has been drawn from vtowinY,
additional edges may be drawn to the same wfrom other vertices v0;v00in layer 1.
Assign capacities to the edges from layer 1 to layer 2 in the same way as described above, that is, the
capacity in Yof the edge from vtowiscap(v;w)âˆ’f(v;w)+f(w;v). This latter quantity is, of course, the
total residual (unused) ï¬‚ow-carrying capacity of the edges in both directions between vandw.
The layering continues until we reach a layer Lsuch that there is a helpful edge from some vertex of
layerLto the sink t, or else until no additional layers can be created (to say that no more layers can be
created is to say that among the vertices that havenâ€™t yet been included in the layered network that we are
building, there arenâ€™t any that are adjacent to a vertex that is in the layered network, by a helpful edge).
In the former case, we then create a layer L+ 1 that consists solely of the sink t, we connect tby edges
directed from the appropriate vertices of layer L, assign capacities to those edges, and the layering process
is complete. Observe that not all vertices of Xneed appear in Y.
In the latter case, where no additional layers can be created but the sink hasnâ€™t been reached, the
present ï¬‚ow function fin fX is maximum, and the network ï¬‚ow problem in Xhas been solved.
Here is a formal statement of the procedure for layering a given network Xwith respect to a given ï¬‚ow
functionfinX. Input are the network Xand the present ï¬‚ow function fin that network. Output are the
layered network Y, and a logical variable maxflow that will be True, on output, if the ï¬‚ow is at a maximum
value,False otherwise.
procedurelayer (X,f,Y,maxflow );
fforms the layered network Ywith respect to the ï¬‚ow finXg
fmaxflow will be â€˜Trueâ€™ if the input ï¬‚ow falready has the
maximum possible value for the network, else it will be â€˜ False â€™g
L:= 0;layer(L): =fsource g;maxflow :=false;
repeat
layer(L+1 ): = ;;
foreach vertex uinlayer(L)d o
foreach vertex vsuch that flayer(v)=L+1o rvis
not in any layer gdo
q:=cap(u;v)âˆ’f(u;v)+f(v;u);
ifq>0then do
draw edgeu!vinY;
assign capacity qto that edge;
assign vertex vtolayer(L+1 ) ;
L:=L+1
iflayer(L) is empty then exit with maxflow :=true;
untilsinkis inlayer(L);
delete from layer(L)o fYall vertices other than sink,
and remove their incident edges from Y
end.flayerg
In Fig. 3.6.1 we show the typical appearance of a layered network. In contrast to a general network, in
a layered network every path from the source to some xed vertex vhas the same number of edges in it (the
number of the layer of v), and all edges on such a path are directed the same way, from the source towards
73
Chapter 3: The Network Flow Problem
Fig. 3.6.1: A general layered network
v. These properties of layered networks are very friendly indeed, and make them much easier to deal with
than general networks.
In Fig. 3.6.2 we show specically the layered network that results from the network of Fig. 3.1.2 with
the ï¬‚ow shown therein.
Fig. 3.6.2: A layering of the network in Fig. 3.1.2
The next question is this: exactly what problem would we like to solve on the layered network Y,a n d
what is the relationship of that problem to the original network ï¬‚ow problem in the original network X?
The answer is that in the layered network Ywe are looking for a blocking ï¬‚ow g. By a blocking ï¬‚ow we
mean a ï¬‚ow function ginYsuch that every path from source to sink in Yhas at least one saturated edge .
This immediately raises two questions: (a) what can we do if we nd a blocking ï¬‚ow in Y? (b) how can
we nd a blocking ï¬‚ow in Y? The remainder of this section will be devoted to answering (a). In the next
section we will give an elegant answer to (b).
Suppose that we have somehow found a blocking ï¬‚ow function, g,i nY. What we do with it is that we
use it to augment the ï¬‚ow function finX, as follows.
procedureaugment (f,X;g,Y);
faugment ï¬‚ow finXby using a blocking ï¬‚ow g
in the corresponding layered network Yg
forevery edge e:u!vof the layered network Y,
do
increase the ï¬‚ow fin the edge u!vof the
network Xby the amount
minfg(e);cap(u!v)âˆ’f(u!v)g;
ifnot all ofg(e) has been used
then decrease the ï¬‚ow in edge v!uby
the unused portion of g(e)
end.faugment g
74
3.6 Layered networks
After augmenting the ï¬‚ow in the original network X, what then? We construct a new layered network,
fromXand the newly augmented ï¬‚ow function fonX.
The various activities that are now being described may sound like some kind of thinly disguised repack-
aging of the Ford-Fulkerson algorithm, but they arenâ€™t just that, because here is what can be proved to
happen:
First, if we start with zero ï¬‚ow in X, make the layered network Y, nd a blocking ï¬‚ow in Y,a u g m e n t
the ï¬‚ow in X, make a new layered network Y, nd a blocking ï¬‚ow, etc. etc., then after at most Vphases
(â€˜phaseâ€™ = layer + block + augment) we will have found the maximum ï¬‚ow in Xand the process will halt.
Second, each phase can be done very rapidly. The MPM algorithm, to be discussed in section 3.7, nds
a blocking ï¬‚ow in a layered network in time O(V2).
By the height of a layered network Ywe will mean the number of edges in any path from source to sink.
The network of Fig. 3.6.1 has height 3. Letâ€™s now show
Theorem 3.6.1. The heights of the layered networks that occur in the consecutive phases of the solution
of a network ï¬‚ow problem form a strictly increasing sequence of positive integers. Hence, for a network X
withVvertices, there can be at most Vphases before a maximum ï¬‚ow is found.
LetY(p) denote the layered network that is constructed at the pthphase of the computation and let
H(p) denote the height of Y(p). We will rst prove
Lemma 3.6.1. If
v0!v1!v2!!vm(v0=source )
is a path in Y(p+1 ), and if every vertex vi(i=1;m)of that path also appears in Y(p), then for every
a=0;mit is true that if vertex vawas in layer bofY(p)thenab.
P r o o fo fl e m m a : The result is clearly true for a= 0. Suppose it is true for v0;v1;:::;v a, and suppose
va+1was in layer cof network Y(p). We will show that a+1c. Indeed, if not then c>a + 1. Since va,
by induction, was in a layer a, it follows that the edge
e:va!va+1
was not present in network Y(p) since its two endpoints were not in two consecutive layers. Hence the ï¬‚ow
inYbetweenvaandva+1could not have been aected by the augmentation procedure of p hase p.B u te d g e
eis inY(p+1). Therefore it represented an edge of Ythat was helpful from vatova+1at the beginning of
phasep+ 1, was unaected by phase p, but was not helpful at the beginning of phase p. This contradiction
establishes the lemma.
Now we will prove the theorem. Let
s!v1!v2!!vH(p+1)âˆ’1!t
be a path from source to sink in Y(p+1 ) .
Consider rst the case where every vertex of the path also lies in Y(p), and apply the lemma to
vm=t(m=H(p+1 ) );a=m. We conclude at once that H(p+1 )H(p). Now we want to exclude the
â€˜=â€™ sign. If H(p+1 )=H(p) then the entire path is in Y(p) and in Y(p+ 1), and so all of the edges in Y
that the edges of the path represent were helpful both before and after the augmentation step of phase p,
contradicting the fact that the blocking ï¬‚ow that was used for the augmentation saturated some edge of the
chosen path. The theorem is now proved for the case where the path had all of its vertices in Y(p)a l s o .
Now suppose that this was not the case. Let e:va!va+1be the rst edge of the path whose terminal
vertexva+1was not in Y(p). Then the corresponding edge(s) of Ywas unaected by the augmentation in
phasep. It was helpful from vatova+1at the beginning of phase p+ 1 because e2Y(p+ 1) and it was
unaected by phase p,y e te=2Y(p). The only possibility is that vertex va+1would have entered into Y(p)
in the layer H(p) that contains the sink, but that layer is special, and contains only t. Hence, ifvawas
in layerbofY(p), thenb+1=H(p). By the lemma once more, ab,s oa+1b+1=H(p), and therefore
H(p+1 )>H(p), completing the proof of theorem 3.6.1.
75
Chapter 3: The Network Flow Problem
To summarize, if we want to nd a maximum ï¬‚ow in a given network Yby the method of layered
networks, we carry out
proceduremaxflow (X,Y,f);
set the ï¬‚ow function fto zero on all edges of Y;
repeat
(i) construct the layered network Y=Y(X;f)
if possible, else exit with ï¬‚ow at maximumvalue;
(ii) nd a blocking ï¬‚ow ginY;
(iii) augment the ï¬‚ow finYwith the blocking
ï¬‚owg, by calling procedure augment above
until exit occurs in (i) above;
end.fmaxflow g
According to theorem 3.6.1, the procedure will repeat steps (i), (ii), (iii) at most Vtimes because the
height of the layered network increases each time around, and it certainly can never exceed V. The labor
involved in step (i) is certainly O(E), and so is the labor in step (iii). Hence if BFL denotes the labor involved
in some method for nding a blocking ï¬‚ow in a layered network, then the whole network ï¬‚ow problem canbe done in time O(V(E+ BFL)).
The idea of layering networks is due to Dinic. Since his work was done, all eorts have been directed at
the problem of reducing BFL as much as possible.
3.7 The MPM algorithm
Now we suppose that we are given a layered network Yand we want to nd a blocking ï¬‚ow in Y.T h e
following ingenious suggestion is due to Malhotra, Pramodh-Kumar and Maheshwari.
LetVbe some vertex of Y.T h e in-potential ofvis the sum of the capacities of all edges directed into
v,a n dt h e outpotential ofvis the total capacity of all edges directed out from v.T h e potential ofvis the
smaller of these two.
(A) Find a vertex vof smallest potential, say P
. Now we will push Pmore units of ï¬‚ow from source
to sink, as follows.
(B) (Pushout) Take the edges that are outbound from vin some order, and saturate each one with ï¬‚ow,
unless and until saturating one more would lift the total ï¬‚ow used over P. Then assign all remaining ï¬‚ow
to the next outbound edge (not necessarily saturating it), so the total outï¬‚ow from vbecomes exactly P.
(C) Follow the ï¬‚ow to the next higher layer of Y.T h a ti s ,f o re a c hv e r t e x v0of the next layer, let h(v0)
be the ï¬‚ow into v0. Now saturate all except possibly one outbound edge of v0, to pass through v0theh(v0)
units of ï¬‚ow. When all vertices v0in that layer have been done, repeat for the next layer, etc. We never nd
a vertex with insucient capacity, in or out, to handle the ï¬‚ow that is thrust upon it, because we began bychoosing a vertex of minimum potential.
(D) (Pullback) When all layers â€˜aboveâ€™ vhave been done, then follow the ï¬‚ow to the next layer â€˜belowâ€™
v. For each vertex v
0of that layer, let h(v0) be the ï¬‚ow out of v0tov. Then saturate all except possibly one
incoming edge ofv0, to pass through v0theh(v0) units of ï¬‚ow. When all v0in that layer have been done,
proceed to the next layer below v,e t c .
(E) (Update capacities) The ï¬‚ow function that has just been created in the layered network must be
stored somewhere. A convenient way to keep it is to carry out the augmentation procedure back in the
network Xat this time, thereby, in eect â€˜storingâ€™ the contributions to the blocking ï¬‚ow in Yin the ï¬‚ow
array for X. This can be done concurrently with the MPM algorithm as follows: Every time we increase the
ï¬‚ow in some edge u!vofYwe do it by augmenting the ï¬‚ow from utovinX, and then decreasing the
capacity of edge u!vinYby the same amount. In that way the capacities of the edges in Ywill always
be the updated residual capacities, and the ï¬‚ow function finXwill always reï¬‚ect the latest augmentation
of the ï¬‚ow in Y.
(F) (Prune) We have now pushed the original h(v) units of ï¬‚ow through the whole layered network. We
intend to repeat the operation on some other vertex vof minimum potential, but rst we can prune o of
the network some vertices and edges that are guaranteed never to be needed again.
76
3.6 Layered networks
The vertex vitself has either all incoming edges or all outgoing edges, or both, at zero residual capacities.
Hence no more ï¬‚ow will ever be pushed throug v. Therefore we can delete vfrom the network Ytogether
with all of its incident edges, incoming or outgoing. Further, we can delete from Yall of the edges that were
saturated by the ï¬‚ow pushing process just completed, i.e., all edges that now have zero residual capacity.
Next, we may now nd that some vertex whas had all of its incoming or all of its outgoing edges deleted.
That vertex will never be used again, so delete it and any other incident edges it may still have. Continue
the pruning process until only vertices remain that have nonzero potential. If the source and the sink are
still connected by some path, then repeat from (A) above.
Else the algorithm halts. The blocking ï¬‚ow function gthat we have just found is the following: if eis
an edge of the input layered network Y,t h e ng(e) is the sum of all of the ï¬‚ows that were pushed through
edgeeat all stages of the above algorithm.
It is obviously a blocking ï¬‚ow: since no path between sandtremains, every path must have had at
least one of its edges saturated at some step of the algorithm. What is the complexity of this algorithm?Certainly we delete at least one vertex from the network at every pruning stage, because the vertex vthat
had minimum potential will surely have had either all of its incoming or all of its outgoing edges (or both)
saturated.
It follows that steps (A){(E) can be executed at most Vtimes before we halt with a blocking ï¬‚ow.
The cost of saturating all edges that get saturated , since every edge has but one saturation to give to its
network, is O(E). The number of partial edge-saturation operations is at most two per vertex visited. For
each minimal-potential vertex vw ev i s i ta tm o s t Vother vertices, so we use at most Vminimal-potential
vertices altogether. So the partial edge saturation operations cost O(V
2) and the total edge saturations cost
O(E).
The operation of nding a vertex of minium potential is â€˜free,â€™ in the following sense. Initially we
compute and store the in- and out- potentials of every vertex. Thereafter, each time the ï¬‚ow in some edge
is increased, the outpotential of its initial vertex and the inpotential of its terminal vertex are reduced by
the same amount. It follows that the cost of maintaining these arrays is linear in the number of vertices, V.
Hence it aects only the constants implied by the â€˜big ohâ€™ symbols above, but not the orders of magnitude.
The total cost is therefore O(V2) for the complete MPM algorithm that nds a blocking ï¬‚ow in a layered
network. Hence a maximum ï¬‚ow in a netwrok can be found in O(V3) time, since at most Vlayered networks
need to be looked at in order to nd a maximum ï¬‚ow in the original network.
In contrast to the nasty example network of section 3.5, with its irrational edge capacities, that made
the Ford-Fulkerson algorithm into an innite process that converged to the wrong answer, the time bound
O(V3) that we have just proved for the layered-network MPM algorithm is totally independent of the edge
capacities.
3.8 Applications of network ï¬‚ow
We conclude this chapter by mentioning some applications of the network ï¬‚ow problem and algorithm.Certainly, among these, one most often mentions rst the problem of maximum matching in a bipartite
graph. Consider a set of Ppeople and a set of Jjobs, such that not all of the people are capable of doing
all of the jobs.
We construct a graph of P+Jvertices to represent this situation, as follows. Take Pvertices to
represent the people, Jvertices to represent the jobs, and connect vertex pto vertexjby an undirected edge
if personpcan do jobj. Such a graph is called bipartite . In general a graph Gis bipartite if its vertices can
be partitioned into two classes in such a way that no edge runs between two vertices of the same class (seesection 1.6).
In Fig. 3.8.1 below we show a graph that might result from a certain group of 8 people and 9 jobs.The maximum matching problem is just this: assuming that each person can handle at most one of
the jobs, and that each job needs only one person, assign people to the jobs in such a way that the largest
possible number of people are employed. In terms of the bipartite graph G, we want to nd a maximum
number of edges, no two incident with the same vertex .
To solve this problem by the method of network ï¬‚ows we construct a network Y. First we adjoin two
new vertices s;tto the bipartite graph G.I f w e l e tP;Jdenote the two classes of vertices in the graph G,
then we draw an edge from sto eachp2Pand an edge from each j2Jtot. Each edge in the network is
77
Chapter 3: The Network Flow Problem
Fig. 3.8.1: Matching people to jobs
Fig. 3.8.2: The network for the matching problem
given capacity 1. The result for the graph of Fig. 3.8.1 is shown in Fig. 3.8.2.
Consider a maximum integer-valued ï¬‚ow in this network, of value Q. Since each edge has capacity 1, Q
edges of the type ( s;p) each contain a unit of ï¬‚ow. Out of each vertex pthat receives some of this ï¬‚ow there
will come one unit of ï¬‚ow (since inï¬‚ow equals outï¬‚ow at such vertices), which will then cross to a vertex jof
J.N os u c hjwill receive more than one unit because at most one unit can leave it for the sink t. Hence the
ï¬‚ow denes a matching of Qedges of the graph G. Conversely, any matching in Gdenes a ï¬‚ow, hence a
maximum ï¬‚ow corresponds to a maximum matching. In Fig. 3.8.3 we show a maximum ï¬‚ow in the networkof Fig. 3.8.2 and therefore a maximum matching in the graph of Fig. 3.8.1.
Fig. 3.8.3: A maximum ï¬‚ow
For a second application of network ï¬‚ow methods, consdier an undirected graph G.T h eedge-connectivity
ofGis dened as the smallest number of edges whose removal would disconnect G. Certainly, for instance,
78
3.6 Layered networks
if we remove all of the edges incident to a single vertex v, we will disconnect the graph. Hence the edge
connectivity cannot exceed the minimum degree of vertices in the graph. However the edge connectivity
could be a lot smaller than the minimum degree as the graph of Fig. 3.8.4 shows, in which the minimum is
large, but the removal of just one edge will disconnect the graph.
Fig. 3.8.4: Big degree, low connectivity
Finding the edge connectivity is quite an important combinatorial problem, and it is by no means
obvious that network ï¬‚ow methods can be used on it, but they can, and here is how.
GivenG, a graph of Vvertices. We solve not just one, but Vâˆ’1 network ï¬‚ow problems, one for each
vertexj=2;:::;V .
Fix such a vertex j. Then consider vertex 1 of Gto be the source and vertex jto be the sink of a
network Xj. Replace each edge of Gby two edges of Xj, one in each direction, each with capacity 1. Now
solve the network ï¬‚ow problem in Xjobtaining a maximum ï¬‚ow Q(j). Then the smallest of the numbers
Q(j), forj=2;:::;V is the edge connectivity of G. We will not prove this here.
As a nal application of network ï¬‚ow we discuss the beautiful question of determining whether or not
there is a matrix of 0â€™s and 1â€™s that has given row and column sums. For instance, is there a 6 8 matrix
whose row sums are respectively (5, 5, 4, 3, 5, 6) and whose column sums are (3, 4, 4, 4, 3, 3, 4, 3)? Of
course the phrase â€˜row sumsâ€™ means the same thing as â€˜number of 1â€™s in each rowâ€™ since we have said that
the entries are only 0 or 1.
Hence in general, let there be given a row sum vector ( r1;:::;r m) and a column sum vector ( s1;:::;s n).
Wed ask if there exists an mnmatrixAof 0â€™s and 1â€™s that has exactly ri1â€™s in theith row and exactly
sj1â€™s in thejth column, for each i=1;:::;m ,j=1;:::;n . The reader will no doubt have noticed that for
such a matrix to exist it must surely be true that
r1++rm=s1++sn (3:8:1)
since each side counts the total number of 1â€™s in the matrix. Hence we will suppose that (3.8.1) is true.
Now we will construct a network Yofm+n+ 2 vertices named s,x1;:::;x m,y1;:::;y n,a n dt.T h e r e
is an edge of capacity ridrawn from the source sto vertexxi,f o re a c hi=1;:::;m , and an edge of capacity
sjdrawn from vertex yjto the sink t,f o re a c hj=1;:::;n . Finally, there are mnedges of capacity 1 drawn
from each edge xito each vertex yj.
Next nd a maximum ï¬‚ow in this netwrok. Then there is a 0-1 matrix with the given row and column
sum vectors if and only if a maximum ï¬‚ow saturates every edge outbound from the source, that is, if and
only if a maximum ï¬‚ow has value equal to the right or left side of equation (3.8.1). If such a ï¬‚ow exists then
a matrixAof the desired kind is constructed by putting ai;jequal to the ï¬‚ow in the edge from xitoyj.
S. Even and R. E. Tarjan, Network ï¬‚ow and testing graph connectivity, SIAM J. Computing 4(1975),
507-518.
79
Chapter 3: The Network Flow Problem
Exercises for section 3.8
1. Apply the max-ï¬‚ow min-cut theorem to the network that is constructed in order to solve the bipartite
matching problem. Precisely what does a cut correspond to in this network? What does the theorem tell
you about the matching problem?2. Same as question 1 above, but applied to the question of discovering whether or not there is a 0-1 matrix
with a certain given set of row and column sums.
Bibliography
The standard reference for the network ï¬‚ow problem and its variants is
L. R. Ford and D. R. Fulkerson, Flows in Networks, Princeton University Press, Princeton, NJ, 1974.
The algorithm, the example of irrational capacities and lack of convergence to maximum ï¬‚ow, and many
applications are discussed there. The chronology of accelerated algorithms is based on the following papers.
The rst algorithms with a time bound independent of the edge capacities are in
J. Edmonds and R. M. Karp, Theoretical improvements in algorithmic eciency for network ï¬‚ow prob-
lems, JACM 19, 2 (1972), 248-264.
E. A. Dinic, Algorithm for solution of a problem of maximal ï¬‚ow in a network with power estimation,
Soviet Math. Dokl., 11(1970), 1277-1280.
The paper of Dinic, above, also originated the idea of a layered network. Further accelerations of the
netowrk ï¬‚ow algorithms are found in the following.
A. V. Karzanov, Determining the maximal ï¬‚ow in a network by the method of preï¬‚ows, Soviet Math.
Dokl.15(1974), 434-437.
B. V. Cherkassky, Algorithm of construction of maximal ï¬‚ow in networks with complexity of O(V
2E)
operations, Akad. Nauk. USSR, Mathematical methods for the solution of economical problems 7(1977),
117-126.
The MPM algorithm, discussed in the tex, is due to
V. M. Malhotra, M. Pramodh-Kumar and S. N. Maheshwari, An O(V3) algorithm for nding maximum
ï¬‚ows in networks, Information processing Letters 7, (1978), 277-278.
Later algorithms depend on rened data structures that save fragments of partially construted aug-
menting paths. These developments were initiated in
Z. Galil, A new algorithm for the maximal ï¬‚ow problem, Proc. 19th IEEE Symposium on the Founda-
tions of Computer Science, Ann Arbor, October 1978, 231-245.
Andrew V. Goldberg and Robert E. Tarjan, A new approach to the maximum ï¬‚ow problem, 1985.A number of examples that show that the theoretical complexity estimates for the various algorithms
cannot be improved are contained in
Z. Galil, On the theoretical eciency of various network ï¬‚ow algorithms, IBM report RC7320, September
1978.
The proof given in the text, of theorem 3.6.1, leans heavily on the one in
Shimon Even, Graph Algorithms, Computer Science Press, Potomac, MD, 1979.
If edge capacities are all 0â€™s and 1â€™s, as in matching problems, then still faster algorithms can be given,
as in
S. Even and R. E. Tarjan, Network ï¬‚ow and testing graph connectivity, SIAM J. Computing 4, (1975),
507-518.
If every pair of vertices is to act, in turn, as source and sink, then considerable economies can be realized,
as in
R. E. Gomory and T. C. Hu, Multiterminal netwrok ï¬‚ows, SIAM Journal, 9(1961), 551-570.
Matching in general graphs is much harder than in bipartite graphs. The pioneering work is due to
J. Edmonds, Path, trees, and ï¬‚owers, Canadian J. Math. 17(1965), 449-467.
80
4.1 Preliminaries
Chapter 4: Algorithms in the Theory of Numbers
Number theory is the study of the properties of the positive integers. It is one of the oldest branches of
mathematics, and one of the purest, so to speak. It has immense vitality, however, and we will see in this
chapter and the next that parts of number theory are extremely relevant to current research in algorithms.
Part of the reason for this is that number theory enters into the analysis of algorithms, but that isnâ€™t
the whole story.
Part of the reason is that many famous problems of number theory, when viewed from an algorithmic
viewpoint (like, how do you decide whether or not a positive integer nis prime?) present extremely deep
and attractive unsolved algorithmic problems. At least, they are unsolved if we regard the question as not
just how to do these problems computationally, but how to do them as rapidly as possible.
But thatâ€™s not the whole story either.
There are close connections between algorithmic problems in the theory of numbers, and problems
in other elds, seemingly far removed from number theory. There is a unity between these seeminglydiverse problems that enhances the already considerable beauty of any one of them. At least some of these
connections will be apparent by the end of study of Chapter 5.
4.1Preliminaries
We collect in this section a number of facts about the theory of numbers, for later reference.
Ifnandmare positive integers then to dividenbymis to nd an integer q0 (the quotient )a n da n
integerr(t h eremainder ) such that 0 r<m andn=qm+r.
Ifr=0 ,w es a yt h a tâ€˜ mdividesn,â€™ or â€˜mis a divisor of n,â€™ and we write mjn. In any case the remainder
ris also called â€˜ nmodulom,â€™ and we write r=nmodm. Thus 4 = 11 mod 7, for instance.
Ifnhas no divisors other than m=nandm=1 ,t h e nnisprime ,e l s eniscomposite . Every positive
integerncan be factored into primes, uniquely apart from the order of the factors. Thus 120 = 2
335, and
in general we will write
n=pa1
1pa2
2pal
l=lY
i=1pai
i: (4:1:1)
We will refer to (4.1.1) as the canonical factorization ofn.
Many interesting and important properties of an integer ncan be calculated from its canonical factor-
ization. For instance, let d(n) be the number of divisors of the integer n. The divisors of 6 are 1, 2, 3, 6, so
d(6) = 4.
Can we nd a formula for d(n)? A small example may help to clarify the method. Since 120 = 2335,
a divisor of 120 must be of the form m=2a3b5c, in whichacan have the values 0,1,2,3, bcan be 0 or 1, and
ccan be 0 or 1. Thus there are 4 choices for a,2f o rband 2 forc, so there are 16 divisors of 120.
In general, the integer nin (4.1.1) has exactly
d(n)=( 1+a1)(1 +a2)(1 +al)( 4 :1:2)
divisors.
Ifmandnare nonnegative integers then their greatest common divisor , writtengcd(n;m), is the integer
gthat
(a) divides both mandnand
(b) is divisible by every other common divisor of mandn.
Thusgcd(12;8) = 4,gcd(42;33) = 3, etc. If gcd(n;m)=1t h e nw es a yt h a t nandmarerelatively
prime . Thus 27 and 125 are relatively prime (even though neither of them is prime).
Ifn>0 is given, then (n) will denote the number of positive integers msuch thatmnand
gcd(n;m)=1 . T h u s (6) = 2, because there are only two positive integers 6 that are relatively prime to
6 (namely 1 and 5). (n) is called the Euler -function, or the Euler totient function.
Letâ€™s nd a formula that expresses (n) in terms of the canonical factorization (4.1.1) of n.
81
Chapter 4: Algorithms in the Theory of Numbers
We want to count the positive integers mfor whichmn,a n dmis not divisible by any of the primes
pithat appear in (4.1.1). There are npossibilities for such an integer m. Of these we throw away n=p 1of
them because they are divisible by p1. Then we discard n=p 2multiples of p2, etc. This leaves us with
nâˆ’n=p 1âˆ’n=p 2âˆ’âˆ’n=pl (4:1:3)
possiblemâ€™s.
But we have thrown away too much. An integer mthat is a multiple of both p1andp2has been
discarded at least twice. So letâ€™s correct these errors by adding
n=(p1p2)+n=(p1p3)++n=(p1pl)++n=(plâˆ’1pl)
to (4.1.3).
The reader will have noticed that we added back too much, because an integer that is divisible by
p1p2p3, for instance, would have been re-entered at least twice. The â€˜bottom lineâ€™ of counting too much,
then too little, then too much, etc. is the messy formula
(n)=nâˆ’n=p 1âˆ’n=p 2âˆ’âˆ’n=pl+n=(p1p2)++n=(plâˆ’1pl)
âˆ’n=(p1p2p3)âˆ’âˆ’n=(plâˆ’2plâˆ’1pl)
++(âˆ’1)ln=(p1p2pl):(4:1:4)
Fortunately (4.1.4) is identical with the much simpler expression
(n)=n(1âˆ’1=p1)(1âˆ’1=p2)(1âˆ’1=pl)( 4 :1:5)
which the reader can check by beginning with (4.1.5) and expanding the product.
To calculate (120), for example, we rst nd the canonical factorization 120 = 2335. Then we apply
(4.1.5) to get
(120) = 120(1 âˆ’1=2)(1âˆ’1=3)(1âˆ’1=5)
=3 2:
Thus, among the integers 1 ;2;:::;120, there are exactly 32 that are relatively prime to 120.
Exercises for section 4.1
1. Find a formula for the sum of the divisors of an integer n, expressed in terms of its prime divisors and
their multiplicities.
2. How many positive integers are 1010and have an odd number of divisors? Find a simple formula for
the number of such integers that are n.
3. If(n) = 2 then what do you know about n?
4. For which nis(n) odd?
4.2 The greatest common divisor
Letmandnbe two positive integers. Suppose we divide nbym, to obtain a quotient qand a remainder
r, with, of course, 0 r<m .T h e nw eh a v e
n=qm+r: (4:2:1)
Ifgis some integer that divides both nandmthen obviously gdividesralso. Thus every common divisor
ofnandmis a common divisor of mandr. Conversely, if gis a common divisor of mandrthen (4.2.1)
shows that gdividesntoo.
It follows that gcd(n;m)=gcd(m;r). Ifr=0t h e nn=qm, and clearly, gcd(n;m)=m.
82
4.2 The greatest common divisor
If we use the customary abbreviation â€˜ nmodmâ€™f o rr, the remainder in the division of nbym,t h e n
what we have shown is that
gcd(n;m)=gcd(m;nmodm):
This leads to the following recursive procedure for computing the g.c.d.
functiongcd(n;m);
fndsgcdof given nonnegative integers nandmg
ifm=0thengcd:=nelsegcd:=gcd(m;nmodm)
end.
The above is the famous â€˜Euclidean algorithmâ€™ for the g.c.d. It is one of the oldest algorithms known.
The reader is invited to write the Euclidean algorithm as a recursive program, and get it working on
some computer. Use a recursive language, write the program more or less as above, and try it out with some
large, healthy integers nandm.
Thegcdprogram exhibits all of the symptoms of recursion. It calls itself with smaller values of its
variable list. It begins with â€˜if trivialcase then do trivialthingâ€™ ( m= 0), and this case is all-important
because itâ€™s the only way the procedure can stop itself.
If, for example, we want the g.c.d. of 13 and 21, we call the program with n=1 3a n dm= 21, and it
then recursively calls itself with the following arguments:
(21;13);(13;8);(8;5);(5;3);(3;2);(2;1);(1;0) (4 :2:2)
When it arrives at a call in which the â€˜ mâ€™ is 0, then the â€˜ n,â€™ namely 1 in this case, is the desired g.c.d.
What is the input to the problem? The two integers n,mwhose g.c.d. we want are the input, and the
number of bits that are needed to input those two integers is (log n)+(logm), namely (log mn). Hence
clogmnis the length of the input bit string. Now letâ€™s see how long the algorithm might run with an input
string of that length.
To measure the running time of the algorithm we need rst to choose a unit of cost or work. Letâ€™s
agree that one unit of labor is the execution of a single â€˜ amodbâ€™ operation. In this problem, an equivalent
measure of cost would be the number of times the algorithm calls itself recursively. In the example (4.2.2)the cost was 7 units.
Lemma 4.2.1. If1bathenamodb(aâˆ’1)=2.
Proof: Clearlyamodbbâˆ’1. Further,
amodb=aâˆ’ja
bk
b
aâˆ’b:
Thusamodbmin(aâˆ’b;bâˆ’1). Now we distinguish two cases.
First suppose b(a+1 )=2. Thenbâˆ’1aâˆ’band so
amodbbâˆ’1
a+1
2âˆ’1
=aâˆ’1
2
in this case.
Next, suppose b>(a+1 )=2. Thenaâˆ’bbâˆ’1a n d
amodbaâˆ’b<a âˆ’a+1
2=aâˆ’1
2
so the result holds in either case.
InHistoria Mathematica 21(1994), 401-419, Jerey Shallit traces this analysis back to Pierre-Joseph-
Etienne Finck, in 1841.
83
Chapter 4: Algorithms in the Theory of Numbers
Theorem 4.2.1. (A worst-case complexity bound for the Euclidean algorithm) Given two positive integers
a;b. The Euclidean algorithm will nd their greatest common divisor after a cost of at most b2l o g2Mc+1
integer divisions, where M=m a x(a;b).
Before we prove the theorem, letâ€™s return to the example ( a;b)=( 1 3;21) of the display (4.2.2). In that
caseM=2 1a n d2l o g2M+1=9:78:::. The theorem asserts that the g.c.d. will be found after at most 9
operations. In fact it was found after 7 operations in that case.
Proof of theorem: Suppose rst that ab. The algorithm generates a sequence a0;a1;:::wherea0=
a;a 1=b,a n d
aj+1=ajâˆ’1modaj(j1):
By lemma 4.2.1,
aj+1ajâˆ’1âˆ’1
2
ajâˆ’1
2:
Then, by induction on jit follows that
a2ja0
2j(j0)
a2j+1a1
2j(j0)
and so,
ar2âˆ’br=2cM (r=0;1;2;:::):
Obviously the algorithm has terminated if ar<1, and this will have happened when ris large enough so
that 2âˆ’br=2cM<1,i.e.,i fr>2l o g2M.I fa<b then after 1 operation we will be in the case â€˜ abâ€™t h a t
we have just discussed, and the proof is complete.
The upper bound in the statement of theorem 4.2.1 can be visualized as follows. The number log2M
is almost exactly the number of bits in the binary representation of M(what is â€˜exactlyâ€™ that number of
bits?). Theorem 4.2.1 therefore asserts that we can nd the g.c.d. of two integers in a number of operations
that is at most a linear function of the number of bits that it takes to represent the two numbers. In brief,we might say that â€˜Time = O(bits),â€™ in the case of Euclidâ€™s algorithm.
Exercises for section 4.2
1. Write a nonrecursive program, in Basic or Fortran, for the g.c.d. Write a recursive program, in Pascal or
a recursive language of your choice, for the g.c.d.
2. Choose 1000 pairs of integers ( n;m), at random between 1 and 1000. For each pair, compute the g.c.d.
using a recursive program and a nonrecursive program.
(a) Compare the execution times of the two programs.
(b) There is a theorem to the eect that the probability that two random integers have g.c.d. = 1 is
6=
2. What, precisely, do you think that this theorem means by â€˜the probability that ...â€™? What
percentage of the 1000 pairs that you chose had g.c.d. = 1? Compare your observed percentagewith 100 (6=
2).
3. Find out when Euclid lived, and with exactly what words he described his algorithm.4. Write a program that will light up a pixel in row mand column nof your CRT display if and only if
gcd(m;n) = 1. Run the program with enough values of mandnto ll your screen. If you see any interesting
visual patterns, try to explain them mathematically.
5. Show that if mandnhave a total of Bbits, then Euclidâ€™s algorithm will not need more than 2 B+3
operations before reaching termination.
84
4.3 The extended Euclidean algorithm
6. Suppose we have two positive integers m,n, and we have factored them completely into primes, in the
form
m=Y
pai
i;n=Y
qbi
i:
How would you calculate gcd(m;n) from the above information? How would you calculate the least common
multiple (lcm)o fmandnfrom the above information? Prove that gcd(m;n)=mn=lcm (m;n).
7. Calculate gcd(102131;56129) in two ways: use the method of exercise 6 above, then use the Euclidean
algorithm. In each case count the total number of arithmetic operations that you had to do to get the
answer.
8. LetFnbe thenthFibonacci number. How many operations will be needed to compute gcd(Fn;Fnâˆ’1)b y
the Euclidean algorithm? What is gcd(Fn;Fnâˆ’1)?
4.3 The extended Euclidean algorithm
Again suppose n;mare two positive integers whose g.c.d. is g. Then we can always write gin the form
g=tn+um (4:3:1)
wheretanduare integers. For instance, gcd(14;11) = 1, so we can write 1 = 14 t+1 1ufor integers t;u.
Can you spot integers t;uthat will work? One pair that does the job is (4 ;âˆ’5), and there are others (can
you nd all of them?).
The extended Euclidean algorithm nds not only the g.c.d. of nandm, it also nds a pair of integers t,
uthat satisfy (4.3.1). One â€˜applicationâ€™ of the extended algorithm is that we will obtain an inductive proof
of the existence oft;u, that is not immediately obvious from (4.3.1) (see exercise 1 below). While this hardly
rates as a â€˜practicalâ€™ application, it represents a very important feature of recursive algorithms. We might
say, rather generally, that the following items go hand-in-hand:
Recursive algorithms
Inductive proofs
Complexity analyses by recurrence formulas
If we have a recursive algorithm, then it is natural to prove the validity of the algorithm by mathematical
induction. Conversely, inductive proofs of theorems often (not always, alas!) yield recursive algorithms forthe construction of the objects that are being studied. The complexity analysis of a recursive algorithm will
use recurrence formulas, in a natural way. We saw that already in the analysis that proved theorem 4.2.1.
Now letâ€™s discuss the extended algorithm. Input to it will be two integers nandm. Output from it will
beg=gcd(n;m)andtwo integers tandufor which (4.3.1) is true.
A single step of the original Euclidean algorithm took us from the problem of nding gcd(n;m)t o
gcd(m;nmodm). Suppose, inductively, that we not only know g=gcd(m;nmodm) but we also know the
coecients t
0;u0for the equation
g=t0m+u0(nmodm): (4:3:2)
Can we get out, at the next step, the corresponding coecients t;ufor (4.3.1)? Indeed we can, by substituting
in (4.3.2) the fact that
nmodm=nâˆ’jn
mk
m (4:3:3)
we nd that
g=t0m+u0(nâˆ’jn
mk
m)
=u0n+(t0âˆ’u0jn
mk
)m:(4:3:4)
Hence the rule by which t0;u0for equation (4.3.2) transform into t; ufor equation (4.3.1) is that
t=u0
u=t0âˆ’jn
mk
u0:(4:3:5)
85
Chapter 4: Algorithms in the Theory of Numbers
We can now formulate recursively the extended Euclidean algorithm.
proceduregcdext (n;m;g;t;u );
fcomputes g.c.d. of nandm, and nds
integerst,uthat satisfy (4.3.1) g
ifm=0then
g:=n;t:= 1;u:= 0
else
gcdext (m;nmodm;g;t;u );
s:=u;
u:=tâˆ’bn=mcu;
t:=s
end.fgcdext g
It is quite easy to use the algorithm above to make a proof of the main mathematical result of this
section (see exercise 1), which is
Theorem 4.3.1. Letmandnbe given integers, and let gbe their greatest common divisor. Then there
exist integers t,usuch thatg=tm+un.
An immediate consequence of the algorithm and the theorem is the fact that nding inverses modulo a
given integer is an easy computational problem. We will need to refer to that fact in the sequel, so we stateit as
Corollary 4.3.1. Letmandnbe given positive integers, and let gbe their g.c.d. Then mhas a multi-
plicative inverse modulo nif and only if g=1. In that case, the inverse can be computed in polynomial
time.
Proof: By the extended Euclidean algorithm we can nd, in linear time, integers tandusuch thatg=
tm+un. But this last equation says that tmg(modn). Ifg= 1 then it is obvious that tis the inverse
modnofm.I fg>1 then there exists no tsuch thattm1( m o dn) sincetm=1+rnimplies that the
g.c.d. ofmandnis 1.
We will now trace the execution of gcdext if it is called with ( n;m)=( 1 4;11). The routine rst replaces
(14,11) by (11,3) and calls itself. Then it calls itself successively with (3,2), (2,1) and (1,0). When it executeswith (n;m)=( 1;0) it encounters the â€˜if m= 0â€™ statement, so it sets g:= 1;t:= 1;u:= 0.
Now it can complete the execution of the call with ( n;m)=( 2;1), which has so far been pending. To
do this it sets
u:=tâˆ’bn=mcu=1
t:= 0:
The call with ( n;m)=( 2;1) is now complete. The call to the routine with ( n;m)=( 3;2) has been in
limbo until just this moment. Now that the (2,1) call is nished, the (3,2) call executes and nds
u:= 0âˆ’b3=2c1=1
t:= 1:
The call to the routine with ( n;m)=( 1 1;3) has so far been languishing, but its turn has come. It
computes
u:= 1âˆ’b11=3c(âˆ’1) = 4
t:=âˆ’1:
Finally, the original call to gcdext from the user, with ( n;m)=( 1 4;11), can be processed. We nd
u:= (âˆ’1)âˆ’b14=11c4=âˆ’5
t:= 4:
86
4.4 Primality testing
Therefore, to the user, gcdext returns the values g=1;u=âˆ’5;t= 4, and we see that the procedure has
found the representation (4.3.1) in this case. The importance of the â€˜trivial caseâ€™ where m= 0 is apparent.
Exercises for section 4.3
1. Give a complete formal proof of theorem 4.3.1. Your proof should be by induction (on what?) and should
use the extended Euclidean algorithm.
2. Find integers t,usuch that
(a) 1 = 4t+7u
(b) 1 = 24t+3 5u
(c) 5 = 65t+ 100u
3. Leta1;:::;a nbe positive integers.
(a) How would you compute gcd(a1;:::;a n)?
(b) Prove that there exist integers t1;:::;t nsuch that
gcd(a1;:::;a n)=t1a1+t2a2++tnan:
(c) Give a recursive algorithm for the computation of t1;:::;t nin part (b) above.
4. Ifr=ta+ub,w h e r er;a;b;u;v are all integers, must r=gcd(a;b)? What, if anything, can be said about
the relationship of rtogcd(a;b)?
5. Let (t0;u0) be one pair of integers t;ufor whichgcd(a;b)=ta+ub. Find allsuch pairs of integers, aand
bbeing given.
6. Find allsolutions to exercises 2(a)-(c) above.
7. Find the multiplicative inverse of 49 modulo 73, using the extended Euclidean algorithm.8. Ifgcdext is called with ( n;m)=( 9 8;30), draw a picture of the complete tree of calls that will occur
during the recursive execution of the program. In your picture show, for each recursive call in the tree, the
values of the input parameters to that call and the values of the output variables that were returned by that
call.
4.4Primality testing
In Chapter 1 we discussed the important distinction between algorithms that run in polynomial time
vs.those that may require exponential time. Since then we have seen some fast algorithms and some slow
ones. In the network ï¬‚ow problem the complexity of the MPM algorithm was O(V
3) ,al o wp o w e ro ft h e
size of the input data string, and the same holds true for the various matching and connectivity problems
that are special cases of the network ï¬‚ow algorithm.
Likewise, the Fast Fourier Transform is really Fast. It needs only O(nlogn) time to nd the transform
of a sequence of length nifnis a power of two, and only O(n2) time in the worst case, where nis prime.
In both of those problems we were dealing with computational situations near the low end of the
complexity scale. It is feasible to do a Fast Fourier Transform on, say, 1000 data points. It is feasible to
calculate maximum ï¬‚ows in networks with 1000 vertices or so.
On the other hand, the recursive computation of the chromatic polynomial in section 2.3 of Chapter 2
was an example of an algorithm that might use exponential amounts of time.
In this chapter we will meet another computational question for which, to date, no one has ever been
able to provide a polynomial-time algorithm, nor has anyone been able to prove that such an algorithm does
not exist.
The problem is just this: Given a positive integer n.I snprime?
87
Chapter 4: Algorithms in the Theory of Numbers
The reader should now review the discussion in Example 3 of section 0.2. In that example we showed
that the obvious methods of testing for primality are slow in the sense of complexity theory. That is, we
do an amount of work that is an exponentially growing function of the length of the input bit string if we
use one of those methods. So this problem, which seems like a â€˜pushoverâ€™ at rst glance, turns out to beextremely dicult.
Although it is not known if a polynomial-time primality testing algorithm exists, remarkable progress
on the problem has been made in recent years.
One of the most important of these advances was made independently and almost simultaneously by
Solovay and Strassen, and by Rabin, in 1976-7. These authors took the imaginative step of replacingâ€˜certainlyâ€™ by â€˜probably,â€™ and they devised what should be called a probabilistic compositeness (an integer
iscomposite if it is notprime) test for integers, that runs in polynomial time.
Here is how the test works. First choose a number buniformly at random, 1 bnâˆ’1. Next,
subject the pair ( b;n) to a certain test, called a pseudoprimality test , to be described below. The test has
two possible outcomes: either the number nis correctly declared to be composite or the test is inconclusive.
If that were the whole story it would be scarcely have been worth the telling. Indeed the test â€˜Does b
dividen?â€™ already would perform the function stated above. However, it has a low probability of success
even ifnis composite, and if the answer is â€˜No,â€™ we would have learned virtually nothing.
The additional property that the test described below has, not shared by the more naive test â€˜Does b
dividen?,â€™ is that ifnis composite, the chance that the test will declare that result is at least 1/2 .
In practice, for a given nwe would apply the test 100 times using 100 numbers b
ithat are independently
chosen at random in [1 ;nâˆ’1]. Ifnis composite, the probability that it will be declared composite at least
once is at least 1 âˆ’2âˆ’100, and these are rather good odds. Each test would be done in quick polynomial time.
Ifnis not found to be composite after 100 trials, and if certainty is important, then it would be worthwhile
to subjectnto one of the nonprobabilistic primality tests in order to dispel all doubt.
It remains to describe the test to which the pair ( b;n) is subjected, and to prove that it detects com-
positeness with probability 1=2.
Before doing this we mention another important development. A more recent primality test, due to
Adleman, Pomerance and Rumely in 1983, is completely deterministic. That is, given nit will surely decide
w h e t h e ro rn o t nis prime. The test is more elaborate than the one that we are about to describe, and it
runs in tantalizingly close to polynomial time. In fact it was shown to run in time
O((logn)clog log log n)
for a certain constant c. Since the number of bits of nis a constant multiple of log n, this latter estimate is
of the form
O((Bits)clog log Bits):
The exponent of â€˜ Bits,â€™ which would be constant in a polynomial time algorithm, in fact grows extremely
slowly asngrows. This is what was referred to as â€˜tantalizingly closeâ€™ to polynomial time, earlier.
It is important to notice that in order to prove that a number is not prime, it is certainly sucient to
nd a nontrivial divisor of that number. It is not necessary to do that, however. All we are asking for is a
â€˜yesâ€™ or â€˜noâ€™ answer to the question â€˜is nprime?.â€™ If you should nd it discouraging to get only the answer
â€˜noâ€™ to the question â€˜Is 7122643698294074179 prime?,â€™ without getting any of the factors of that number,
then what you want is a fast algorithm for the factorization problem.
In the test that follows, the decision about the compositeness of nwill be reached without a knowledge
of any of the factors of n. This is true of the Adleman, Pomerance, Rumely test also. The question of
nding a factor of n, or all of them, is another interesting computational problem that is under active
investigation. Of course the factorization problem is at least as hard as nding out if an integer is prime,
and so no polynomial-time algorithm is known for it either. Again, there are probabilistic algorithms for the
factorization problem just as there are for primality testing, but in the case of the factorization problem,
even they donâ€™t run in polynomial-time.
In section 4.9 we will discuss a probabilistic algorithm for factoring large integers, after some motivation
in section 4.8, where we remark on the connection between computationally intractable problems and cryp-
tography. Specically, we will describe one of the â€˜Public Keyâ€™ data encryption systems whose usefulness
stems directly from the diculty of factoring large integers.
88
4.5 Interlude: the ring of integers modulo n
Isnâ€™t it amazing that in this technologically enlightened age we still donâ€™t know how to nd a divisor of
a whole number quickly?
4.5Interlude: the ring of integers modulo n
In this section we will look at the arithmetic structure of the integers modulo some xed integer n.
These results will be needed in the sequel, but they are also of interest in themselves and have numerous
applications.
Consider the ring whose elements are 0 ;1;2;:::;n âˆ’1 and in which we do addition, subtraction, and
multiplication modulo n. This ring is called Zn. For example, in Table 4.5.1 we show the addition and
multiplication tables of Z6.
+012345 012345
0012345 0000000
1123450 1012345
2234501 2024024
3345012 3030303
4450123 4042042
5501234 5054321
Table 4.5.1: Arithmetic in the ring Z 6
Notice that while Znis a ring, it certainly need not be a eld, because there will usually be some
noninvertible elements. Reference to Table 4.5.1 shows that 2, 3, 4 have no multiplicative inverses in Z6,
while 1, 5 do have such inverses. The dierence, of course, stems from the fact that 1 and 5 are relatively
prime to the modulus 6 while 2, 3, 4 are not. We learned, in corollary 4.3.1, that an element mofZnis
invertible if and only if mandnare relatively prime.
The invertible elements of Znform a multiplicative group. We will call that group the group of units of
Znand will denote it by Un. It has exactly (n) elements, by lemma 4.5.1, where is the Euler function of
(4.1.5).
The multiplication table of the group U18is shown in Table 4.5.2.
 1 5 7 11 13 17
1 1 5 7 11 13 17
5 5 7 17 1 11 137 7 17 13 5 1 11
11 11 1 5 13 17 7
13 13 11 1 17 7 5
1 7 1 7 1 3 1 1751
Table 4.5.2: Multiplication modulo 18
Notice that U
18contains(18) = 6 elements, that each of them has an inverse and that each row
(column) of the multiplication table contains a permutation of all of the group elements.
Letâ€™s look at the table a little more closely, with a view to nding out if the group U18iscyclic.I n a
cyclic group there is an element awhose powers 1 ;a;a2;a3;:::run through all of the elements of the group.
If we refer to the table again, we see that in U18the powers of 5 are 1 ;5;7;17;13;11;1;:::.T h u s t h e
order of the group element 5 is equal to the order of the group, and the powers of 5 exhaust all group
elements. The group U18is indeed cyclic, and 5 is a generator of U18.
89
Chapter 4: Algorithms in the Theory of Numbers
A number (like 5 in the example) whose powers run through all elements of Unis called a primitive root
modulon. Thus 5 is a primitive root modulo 18. The reader should now nd, from Table 4.5.2, allof the
primitive roots modulo 18.
Alternatively, since the order of a group element must always divide the order of the group, every
element ofUnhas an order that divides (n). The primitive roots are exactly the elements, if they exist, of
maximum possible order (n).
We pause to note two corollaries of these remarks, namely
Theorem 4.5.1 (â€˜Fermatâ€™s theoremâ€™). For every integer bthat is relatively prime to nwe have
b(n)1( m o dn): (4:5:1)
In particular, if nis a prime number then (n)=nâˆ’1, and we have
Theorem 4.5.2 (â€˜Fermatâ€™s little theoremâ€™). Ifnis prime, then for all b60( m o dn)we havebnâˆ’11
(modn).
It is important to know which groups Unare cyclic, i.e., which integers nhave primitive roots. The
answer is given by
Theorem 4.5.3. An integernhas a primitive root if and only if n=2orn=4orn=pa(pan odd prime)
orn=2pa(pan odd prime). Hence, the groups Unare cyclic for precisely such values of n.
The proof of theorem 4.5.3 is a little lengthy and is omitted. It can be found, for example, in the book
of LeVeque that is cited at the end of this chapter.
According to theorem 4.5.3, for example, U18is cyclic, which we have already seen, and U12is not cyclic,
which the reader should check.
Further, we state as an immediate consequence of theorem 4.5.3,
Corollary 4.5.3. Ifnis an odd prime, then Unis cyclic, and in particular the equation x2=1,i nUn,h a s
only the solutions x=1.
Next we will discuss the fact that if the integer ncan be factored in the form n=pa1
1pa2
2parrthen
the full ring Zncan also be factored, in a certain sense, as a â€˜productâ€™ of Zpai
i.
Letâ€™s take Z6as an example. Since 6 = 2 3, we expect that somehow Z6=Z2NZ3. What this means
is that we consider ordered pairs x1;x2,w h e r ex12Z2andx22Z3.
Here is how we do the arithmetic with the ordered pairs.First, (x
1;x2)+(y1;y2)=(x1+y1;x2+y2), in which the two â€˜+â€™ signs on the right are dierent: the
rst â€˜x1+y1â€™i sd o n ei n Z2while the â€˜x2+y2â€™i sd o n ei n Z3.
Second, (x1;x2)(y1;y2)=(x1y1;x2y2), in which the two multiplications on the right side are dierent:
the â€˜x1y1â€™i sd o n ei n Z2and the â€˜x2y2â€™i nZ3.
Therefore the 6 elements of Z6are
(0;0);(0;1);(0;2);(1;0);(1;1);(1;2):
A sample of the addition process is
(0;2) + (1;1) = (0 + 1;2+1 )
=( 1;0)
where the addition of the rst components was done modulo 2 and of the second components was done
modulo 3.
A sample of the multiplication process is
(1;2)(1;2) = (1 1;22)
=( 1;1)
in which multiplication of the rst components was done modulo 2 and of the second components was done
modulo 3.
In full generality we can state the factorization of Znas
90
4.5 Interlude: the ring of integers modulo n
Theorem 4.5.4. Letn=pa1
1pa2
2parr. The mapping which associates with each x2Znther-tuple
(x1;x2;:::;x r),w h e r exi=xmodpai
i(i=1;r), is a ring isomorphism of Znwith the ring of r-tuples
(x1;x2;:::;x r)in which
(a)xi2Zpai
i(i=1;r)and
(b)(x1;:::;x r)+(y1;:::;y r)=(x1+y1;:::;x r+yr)and
(c)(x1;:::;x r)(y1;:::;y r)=(x1y1;:::;x ryr)
(d) In (b), the ithâ€˜+â€™ sign on the right side is the addition operation of Zpai
iand in (c) the ithâ€˜â€™s i g ni s
the multiplication operation of Zpai
i,f o re a c hi=1;2;:::;r .
The proof of theorem 4.5.4 follows at once from the famous
Theorem 4.5.5 (â€˜The Chinese Remainder Theoremâ€™). Letmi(i=1;r)be pairwise relatively prime
positive integers, and let
M=m1m2mr:
Then the mapping that associates with each integer x(0xMâˆ’1)ther-tuple (b1;b2;:::;b r),w h e r e
bi=xmodmi(i=1;r), is a bijection between ZMandZm1 Zmr.
A good theorem deserves a good proof. An outstanding theorem deserves two proofs, at least, one
existential, and one constructive. So here are one of each for the Chinese Remainder Theorem.
Proof 1: We must show that each r-tuple (b1;:::;b r) such that 0 bi<m i(i=1;r) occurs exactly once.
There are obviously Msuch vectors, and so it will be sucient to show that each of them occurs at most
once as the image of some x.
In the contrary case we would have xandx0both corresponding to ( b1;b2;:::;b r), say. But then
xâˆ’x00 modulo each of the mi. Hencexâˆ’x0is divisible by M=m1m2mr.B u t jxâˆ’x0j<M, hence
x=x0.
Proof 2: Hereâ€™s how to compute a number xthat satises the simultaneous congruences xbimod
mi(i=1;r). First, by the extended Euclidean algorithm we can quickly nd t1;:::;t r,u1;:::;u r,s u c ht h a t
tj(M=m j)+ujmj=1f o rj=1;:::;r . Then we claim that the number x=P
jbjtj(M=m j) satises all of
the given congruences. Indeed, for each k=1;2;:::;r we have
x=rX
j=1bjtj(M=m j)
bktk(M=m k)( m o dmk)
bk(modmk)
where the rst congruence holds because each M=m j(j6=k) is divisible by mk, and the second congruence
follows since
tk(M=m k)=1âˆ’ukmk1m o dmk;
completing the second proof of the Chinese Remainder Theorem.
Now the proof of theorem 4.5.4 follows easily, and is left as an exercise for the reader.
The factorization that is described in detail in theorem 4.5.4 will be written symbolically as
Zn=rO
i=1Zpai
i: (4:5:2)
The factorization (4.5.2) of the ring Zninduces a factorization
Un=rO
i=1Upiai (4:5:3)
91
Chapter 4: Algorithms in the Theory of Numbers
of the group of units. Since Unis a group, (4.5.3) is an isomorphism of the multiplicative structure only. In
Z12, for example, we nd
U12=U4U3
whereU4=f1;3g,U3=f1;2g.S oU12can be thought of as the set f(1;1;);(1;2);(3;1);(3;2)g, together
with the componentwise multiplication operation described above.
Exercises for section 4.5
1. Give a complete proof of theorem 4.5.4.
2. Find all primitive roots modulo 18.
3. Find all primitive roots modulo 27.
4. Write out the multiplication table of the group U27.
5. Which elements of Z11are squares?
6. Which elements of Z13are squares?
7. Find all x2U27such thatx2= 1. Find all x2U15such thatx2=1 .
8. Prove that if there is a primitive root modulo nthen the equation x2= 1 in the group Unhas only the
solutionsx=1.
9. Find a number xthat is congruent to 1, 7 and 11 to the respective moduli 5, 11 and 17. Use the method
in the second proof of the remainder theorem 4.5.5.
10. Write out the complete proof of the â€˜immediateâ€™ corollary 4.5.3.
4.6Pseudoprimality tests
In this section we will discuss various tests that might be used for testing the compositeness of integers
probabilistically.
By apseudoprimality test we mean a test that is applied to a pair ( b;n) of integers, and that has the
following characteristics:
(a) The possible outcomes of the test are â€˜ nis compositeâ€™ or â€˜inconclusive.â€™
(b) If the test reports â€˜ nis compositeâ€™ then nis composite.
(c) The test runs in a time that is polynomial in log n.
If the test result is â€˜inconclusiveâ€™ then we say that nis pseudoprime to the base b(which means that n
is so far acting like a prime number, as far as we can tell).
The outcome of the test of the primality of ndepends on the base bthat is chosen. In a good pseu-
doprimality test there will be many bases bthat will give the correct answer. More precisely, a good
pseudoprimality test will, with high probability ( i.e., for a large number of choices of the base b) declare
that a composite nis composite. In more detail, we will say that a pseudoprimality test is â€˜goodâ€™ if there
is a xed positive number tsuch that every composite integer nis declared to be composite for at least tn
choices of the base b, in the interval 1 bn.
Of course, given an integer n, it is silly to say that â€˜there is a high probability that nis prime.â€™ Either
nis prime or it isnâ€™t, and we should not blame our ignorance on nitself. Nonetheless, the abuse of language
is suciently appealing that we will dene the problem away: we will say that a given integer nisvery
probably prime if we have subjected it to a good pseudoprimality test, with a large number of dierent bases
b, and have found that it is pseudoprime to all of those bases.
Here are four examples of pseudoprimality tests, only one of which is â€˜good.â€™
Test 1. Givenb,n. Output â€˜nis compositeâ€™ if bdividesn, else â€˜inconclusive.â€™
This isnâ€™t the good one. If nis composite, the probability that it will be so declared is the probability
that we happen to have found a bthat divides n,w h e r ebis not 1 orn. The probability of this event, if bis
chosen uniformly at random from [1 ;n], is
p1=(d(n)âˆ’2)=n
whered(n) is the number of divisors of n. Certainly p1is not bounded from below by a positive constant t,
ifnis composite.
92
4.6 Pseudoprimality tests
Test 2. Givenb,n. Output â€˜nis compositeâ€™ if gcd(b;n)6=1, else output â€˜inconclusive.â€™
This one is a little better, but not yet good. If nis composite, the number of bases bnfor which
Test 2 will produce the result â€˜compositeâ€™ is nâˆ’(n), whereis the Euler totient function, of (4.1.5). This
number of useful bases will be large if nhas some small prime factors, but in that case itâ€™s easy to nd
out thatnis composite by other methods. If nhas only a few large prime factors, say if n=p2, then the
proportion of useful bases is very small, and we have the same kind of ineciency as in Test 1 above.
Now we can state the third pseudoprimality test.
Test 3. Givenb,n.( I fbandnare not relatively prime or) if bnâˆ’161( m o dn)then output â€˜ nis
composite,â€™ else output â€˜inconclusive.â€™
Regrettably, the test is still not â€˜good,â€™ but itâ€™s a lot better than its predecessors. To cite an extreme
case of its un-goodness, there exist composite numbers n, called Carmichael numbers , with the property that
the pair (b;n) produces the output â€˜inconclusiveâ€™ for every integerbin [1;nâˆ’1] that is relatively prime to
n. An example of such a number is n= 1729, which is composite (1729 = 7 1319), but for which Test
3 gives the result â€˜inconclusiveâ€™ on every integer b<1729 that is relatively prime to 1729 ( i.e.,t h a ti sn o t
divisible by 7 or 13 or 19).
Despite such misbehavior, the test usually seems to perform quite well. When n= 169 (a dicult
integer for tests 1 and 2) it turns out that there are 158 dierent bâ€™s in [1,168] that produce the â€˜compositeâ€™
outcome from Test 3, namely every such bexcept for 19, 22, 23, 70, 80, 89, 99, 146, 147, 150, 168.
Finally, we will describe a good pseudoprimality test. The familial resemblance to Test 3 will be
apparent.
Test 4. (the strong pseudoprimality test): Given (b;n). Letnâˆ’1=2qm,w h e r emis an odd integer. If
either
(a)bm1( m o dn)or
(b) there is an integer iin[0;qâˆ’1]such that
bm2iâˆ’1( m o dn)
then return â€˜inconclusiveâ€™ else return â€˜ nis composite.â€™
First we validate the test by proving the
Proposition. If the test returns the message â€˜ nis composite,â€™ then nis composite.
Proof: Suppose not. Then nis an odd prime. We claim that
bm2i1( m o dn)
for alli=q;qâˆ’1;:::;0. If so then the case i= 0 will contradict the outcome of the test, and thereby
complete the proof. To establish the claim, it is clearly true when i=q, by Fermatâ€™s theorem. If true for i,
then it is true for iâˆ’1 also, because
(bm2iâˆ’1)2=bm2i
1( m o dn)
implies that the quantity being squared is +1 or âˆ’1. Sincenis an odd prime, by corollary 4.5.3 Unis cyclic,
and so the equation x2=1i nUnhas only the solutions x=1. But âˆ’1 is ruled out by the outcome of the
test, and the proof of the claim is complete.
What is the computational complexity of the test? Consider rst the computational problem of raising
a number to a power. We can calculate, for example, bmmodnwithO(logm) integer multiplications,
by successive squaring. More precisely, we compute b,b2,b4,b8;:::by squaring, and reducing modulo n
immediately after each squaring operation, rather than waiting until the nal exponent is reached. Then we
use the binary expansion of the exponent mto tell us which of these powers of bwe should multiply together
in order to compute bm. For instance,
b337=b256b64b16b:
93
Chapter 4: Algorithms in the Theory of Numbers
The complete power algorithm is recursive and looks like this:
functionpower (b,m,n);
freturnsbmmodng
ifm=0
then
power := 1
else
t:=sqr(power (b;bm=2c;n));
ifmis odd thent:=tb;
power :=tmodn
end.fpower g
Hence part (a) of the strong pseudoprimality test can be done in O(logm)=O(logn) multiplications
of integers of at most O(logn) bits each. Similarly, in part (b) of the test there are O(logn) possible values
ofito check, and for each of them we do a single multiplication of two integers each of which has O(logn)
bits (this argument, of course, applies to Test 3 above also).
The entire test requires, therefore, some low power of log nbit operations. For instance, if we were to
use the most obvious way to multiply two Bbit numbers we would do O(B2) bit operations, and then the
above test would take O((logn)3) time. This is a polynomial in the number of bits of input.
In the next section we are going to prove that Test 4 is a good pseudoprimality test in that if nis
composite then at least half of the integers b,1bnâˆ’1 will give the result â€˜ nis composite.â€™
For example, if n= 169, then it turns out that for 157 of the possible 168 bases bin [1,168], Test 4 will
reply â€˜169 is composite.â€™ The only bases bthat 169 can fool are 19, 22, 23, 70, 80, 89, 99, 146, 147, 150,
168. For this case of n= 169 the performances of Test 4 and of Test 3 are identical. However, there are no
analogues of the Carmichael numbers for Test 4.
Exercises for section 4.6
1 . G i v e na no d di n t e g e r n. LetT(n)b et h es e to fa l l b2[1;n] such that gcd(b;n)=1a n d bnâˆ’11
(modn). Show that jT(n)jdivides(n).
2. LetHbe a cyclic group of order n. How many elements of each order rare there in H(rdividesn)?
3. Ifn=pa,w h e r epi sa no d dp r i m e ,t h e nt h en u m b e ro f x2Unsuch thatxhas exact order r,i s(r), for
all divisors rof(n). In particular, the number of primitive roots modulo nis((n)).
4. Ifn=pa1
1pamm, and ifrdivides(n), then the number of x2Unsuch thatxr1(modn )i s
mY
i=1gcd((pai
i);r):
5. In a group Gsupposefmandgmare, respectively, the number of elements of order mand the number
of solutions of the equation xm=1 ,f o re a c h m=1;2;:::. What is the relationship between these two
sequences? That is, how would you compute the gâ€™s from the fâ€™s? thefâ€™s from the gâ€™s? If you have never
seen a question of this kind, look in any book on the theory of numbers, nd â€˜MÂ¨ obius inversion,â€™ and apply
it to this problem.
4.7Proof of goodness of the strong pseudoprimality test
In this section we will show that if nis composite, then at least half of the integers bin [1;nâˆ’1] will
yield the result â€˜ nis compositeâ€™ in the strong pseudoprimality test. The basic idea of the proof is that a
subgroup of a group that is not the entire group can consist of at most half of the elements of that group.
Supposenhas the factorization
n=pa1
1pass
and letni=piai(i=1;s).
94
4.7 Goodness of pseudoprimality test
Lemma 4.7.1. The order of each element of Unis a divisor of e=lcmf(ni);i=1;sg.
Proof: From the product representation (4.5.3) of Unwe nd that an element xofUncan be regarded as
ans-tuple of elements from the cyclic groups Uni(i=1;s). The order of xis equal to the lcm of the orders
of the elements of the s-tuple. But for each i=1;:::;s the order of the ithof those elements is a divisor of
(ni), and therefore the order of xdivides the lcmshown above.
Lemma 4.7.2. Letn>1be odd. For each element uofUnletC(u)=f1;u;u2;:::;ueâˆ’1gdenote the cyclic
group that ugenerates. Let Bbe the set of all elements uofUnfor whichC(u)either contains âˆ’1or has
odd order ( eodd). IfBgenerates the full group Unthennis a prime power.
Proof: Lete=2tm,w h e r emis odd andeis as shown in lemma 4.7.1. Then there is a jsuch that(nj)
is divisible by 2t.
Now ifnis a prime power, we are nished. So we can suppose that nis divisible by more than one
prime number. Since (n)i sa ne v e nn u m b e rf o ra l l n>2 (proof?), the number eis even. Hence t>0a n d
we can dene a mapping  of the group Unto itself by
 (x)=x2tâˆ’1m(x2Un)
(note that (x) is its own inverse).
This is in fact a group homomorphism:
8x;y2Un: (xy)= (x) (y):
LetBbe as in the statement of lemma 4.7.2. For each x2B, (x)i si nC(x)a n d
 (x)2= (x2)=1:
Since (x)i sa ne l e m e n to f C(x) whose square is 1,  (x) has order 1 or 2. Hence if  (x)6= 1, it is of order
2. If the cyclic group C(x) is of odd order then it contains no element of even order. Hence C(x)i so fe v e n
order and contains âˆ’1. Then it can contain no other element of order 2, so  (x)=âˆ’1 in this case.
Hence for everyx2B, (x)=1.
SupposeBgenerates the full group Un. Then not only for every x2Bbutfor everyx2Unit is true
that (x)=1.
Supposenis not a prime power. Then s>1 in the factorization (4.5.2) of Un. Consider the element v
ofUnwhich, when written out as an s-tuple according to that factorization, is of the form
v=( 1;1;1;:::;1;y;1;:::;1)
where the â€˜yâ€™i si nt h ejthcomponent, y2Unj(recall that jis as described above, in the second sentence of
this proof). We can suppose yto be an element of order exactly 2tinUnjsinceUnjis cyclic.
Consider (v). Clearly (v) is not 1, for otherwise the order of y,n a m e l y2t, would divide 2tâˆ’1m, which
is impossible because mis odd.
Also, (v)i sn o t âˆ’1, because the element âˆ’1o fUnis represented uniquely by the s-tuple all of whose
entries are âˆ’1. Thus (v) is neither 1 nor âˆ’1i nUn, which contradicts the italicized assertion above. Hence
s=1a n dnis a prime power, completing the proof.
Now we can prove the main result of Solovay, Strassen and Rabin, which asserts that Test 4 is good.
Theorem 4.7.1. LetB0be the set of integers bmodnsuch that (b;n)returns â€˜inconclusiveâ€™ in Test 4.
(a) IfB0generatesUnthennis prime.
(b) Ifnis composite then B0consists of at most half of the integers in [1;nâˆ’1].
Proof: Supposeb2B0and letmbe the odd part of nâˆ’1. Then either bm1o rbm2iâˆ’1f o rs o m e
i2[0;qâˆ’1]. In the former case the cyclic subgroup C(b) has odd order, since mis odd, and in the latter
caseC(b) contains âˆ’1.
95
Chapter 4: Algorithms in the Theory of Numbers
Hence in either case B0B,w h e r eBis the set dened in the statement of lemma 4.7.2 above. If B0
generates the full group UnthenBdoes too, and by lemma 4.7.2, nis a prime power, say n=pk.
Also, in either of the above cases we have bnâˆ’11, so the same holds for all b2B0, and so for all
x2Unwe havexnâˆ’11, sinceB0generatesUn.
NowUnis cyclic of order
(n)=(pk)=pkâˆ’1(pâˆ’1):
By theorem 4.5.3 there are primitive roots modulo n=pk. Letgbe one of these. The order of gis, on the
one hand,pkâˆ’1(pâˆ’1) since the set of all of its powers is identical with Un, and on the other hand is a divisor
ofnâˆ’1=pkâˆ’1 sincexnâˆ’11 for allx, and in particular for x=g.
Hencepkâˆ’1(pâˆ’1) (which, if k>1, is a multiple of p) dividespkâˆ’1 (which is one less than a multiple
ofp), and sok= 1, which completes the proof of part (a) of the theorem.
In part (b), nis composite and so B0cannot generate all of Un, by part (a). Hence B0generates a
proper subgroup of Un, and so can contain at most half as many elements as Uncontains, and the proof is
complete.
Another application of the same circle of ideas to computer science occurs in the generation of random
numbers on a computer. A good way to do this is to choose a primitive root modulo the word size of your
computer, and then, each time the user asks for a random number, output the next higher power of theprimitive root. The fact that you started with a primitive root insures that the number of â€˜random numbersâ€™
generated before repetition sets in will be as large as possible.
Now weâ€™ll summarize the way in which the primality test is used. Suppose there is given a large integer
n, and we would like to determine if it is prime.
We would do
functiontestn(n;outcome );
times := 0;
repeat
choose an integer buniformly at random in [2 ;nâˆ’1];
apply the strong pseudoprimality test (Test 4) to the
pair (b;n);
times :=times +1
until fresult is â€˜n is compositeâ€™ or times = 100 g;
iftimes = 100 thenoutcome :=â€˜nprobably primeâ€™
elseoutcome :=â€˜nis compositeâ€™
endftestng
If the procedure exits with â€˜ nis composite,â€™ then we can be certain that nis not prime. If we want to
see the factors of nthen it will be necessary to use some factorization algorithm, such as the one described
below in section 4.9.
On the other hand, if the procedure halts because it has been through 100 trials without a conclusive
result, then the integer nis very probably prime. More precisely, the chance that a composite integer n
would have behaved like that is less than 2
âˆ’100. If we want certainty, however, it will be necessary to apply a
test whose outcome will prove primality, such as the algorithm of Adleman, Rumely and Pomerance, referred
to earlier.
In section 4.9 we will discuss a probabilistic factoring algorithm. Before doing so, in the next section
we will present a remarkable application of the complexity of the factoring problem, to cryptography. Such
applications remind us that primality and factorization algorithms have important applications beyond pure
mathematics, in areas of vital public concern.
Exercises for section 4.7
1. Forn= 9 and for n= 15 nd all of the cyclic groups C(u), of lemma 4.7.2, and nd the set B.
2. Forn=9a n dn= 15 nd the set B0, of theorem 4.7.1.
96
4.8 Factoring and cryptography
4.8Factoring and cryptography
A computationally intractable problem can be used to create secure codes for the transmission of infor-
mation over public channels of communication. The idea is that those who send the messages to each other
will have extra pieces of information that will allow the m to solve the intractable problem rapidly, whereasan aspiring eavesdropper would be faced with an exponential amount of computation.
Even if we donâ€™t have a provably computationally intractable problem, we can still take a chance that
those who might intercept our messages wonâ€™t know any polynomial-time algorithms if we donâ€™t know any.
Since there are precious few provably hard problems, and hordes of apparently hard problems, it is scarcely
surprising that a number of sophisticated coding schemes rest on the latter rather than the former. Oneshould remember, though, that an adversary might discover fast algorithms for doing these problems and
keep that fact secret while deciphering all of our messages.
A remarkable feature of a family of recently developed coding schemes, called â€˜Public Key Encryption
Systems,â€™ is that the â€˜keyâ€™ to the code lies in the public domain, so it can be easily available to sender and
receiver (and eavesdropper), and can be readily changed if need be. On the negative side, the most widely
used Public Key Systems lean on computational problems that are only presumed to be intractable, like
factoring large integers, rather than having been proved so.
We are going to discuss a Public Key System called the RSA scheme, after its inventors: Rivest, Shamir
and Adleman. This particular method depends for its success on the seeming intractability of the problem
of nding the factors of large integers. If that problem could be done in polynomial time, then the RSA
system could be â€˜cracked.â€™
In this system there are three centers of information: the sender of the message, the receiver of the
message, and the Public Domain (for instance, the â€˜Personalsâ€™ ads of the N e wY o r kT i m e s ). Here is how the
system works.
(A)Who knows what and when
Here are the items of information that are involved, and who knows each item:
p,q: two large prime numbers, chosen by the receiver, and told to nobody else (not even to the sender!).
n: the product pqisn, and this is placed in the Public Domain.
E: a random integer, placed in the Public Domain by the receiver, who has rst made sure that Eis
relatively prime to ( pâˆ’1)(qâˆ’1) by computing the g.c.d., and choosing a new Eat random until the g.c.d.
is 1. This is easy for the receiver to do because pandqare known to him, and the g.c.d. calculation is fast.
P: a message that the sender would like to send, thought of as a string of bits whose value, when
regarded as a binary number, lies in the range [0 ;nâˆ’1].
In addition to the above, one more item of information is computed by the receiver, and that is the
integerDthat is the multiplicative inverse mod ( pâˆ’1)(qâˆ’1) ofE, i.e.,
DE1( m o d (pâˆ’1)(qâˆ’1)):
Again, since pandqare known, this is a fast calculation for the receiver, as we shall see.
To summarize,
The receiver knows p,q,D
The sender knows P
Everybody knows nandE
In Fig. 4.8.1 we show the interiors of the heads of the sender and receiver, as well as the contents of the
Public Domain.
97
Chapter 4: Algorithms in the Theory of Numbers
Fig. 4.8.1: Who knows what
(B)How to send a message
The sender takes the message P, looks at the public keys Eandn, computes CPE(modn), and
transmitsCover the public airwaves.
Note that the sender has no private codebook or anything secret other than the message itself.
(C)How to decode a message
The receiver receives C, and computes CDmodn. Observe, however, that ( pâˆ’1)(qâˆ’1) is(n), and
so we have
CDPDE
=P(1+t(n))(tis some integer)
P(modn)
where the last equality is by Fermatâ€™s theorem (4.5.1). The receiver has now recovered the original message
P.
If the receiver suspects that the code has been broken, i.e., that the adversaries have discovered the
primespandq, then the sender can change them without having to send any secret messages to anyone else.
Only the public numbers nandEwould change. The sender would not need to be informed of any other
changes.
Before proceeding, the reader is urged to contruct a little scenario. Make up a short (very short!) mes-
sage. Choose values for the other parameters that are needed to complete the picture. Send the message as
the sender would, and decode it as the receiver would. Then try to intercept the message, as an eavesdropper
would, and see what the diculties are.
(D)How to intercept the message
An eavesdropper who receives the message Cwould be unable to decode it without (inventing some
entirely new decoding scheme or) knowing the inverse DofE(mod (pâˆ’1)(qâˆ’1)). The eavesdropper,
however, does not even know the modulus ( pâˆ’1)(qâˆ’1) because pandqare unknown (only the receiver
knows them), and knowing the product pq=nalone is insucient. The eavesdropper is thereby compelled
to derive a polynomial-time factoring algorithm for large integers. May success attend those eorts!
The reader might well remark here that the receiver has a substantial computational problem in creating
two large primes pandq. To a certain extent this is so, but two factors make the task a good deal easier.
First,pandqwill need to have only half as many bits as nhas, so the job is of smaller size. Second, there
98
4.9 Factoring large integers
are methods that will produce large prime numbers very rapidly as long as one is not too particular about
which primes they are, as long as they are large enough. We will not discuss those methods here.
The elegance of the RSA cryptosystem prompts a few more remarks that are intended to reinforce the
distinction between exponential- and polynomial-time complexities.
How hard is it to factor a large integer? At this writing, integers of up to perhaps a couple of hundred
digits can be approached with some condence that factorization will be accomplished within a few hours of
the computing time of a very fast machine. If we think in terms of a message that is about the length of one
typewritten page, then that message would contain about 8000 bits, equivalent to about 2400 decimal digits.
This is in contrast to the largest feasible length that can be handled by contemporary factoring algorithms ofabout 200 decimal digits. A one-page message is therefore well into the zone of computational intractability.
How hard is it to nd the multiplicative inverse, mod( pâˆ’1)(qâˆ’1)? Ifpandqareknown then itâ€™s easy
to nd the inverse, as we saw in corollary 4.3.1. Finding an inverse mod nis no harder than carrying out
the extended Euclidean algorithm, i.e., itâ€™s a linear time job.
4.9 Factoring large integers
The problem of nding divisors of large integers is in a much more primitive condition than is primality
testing. For example, we donâ€™t even know a probabilistic algorithm that will return a factor of a large
composite integer, with probability >1=2, in polynomial time.
In this section we will discuss a probabilistic factoring algorithm that nds factors in an average time
that is only moderately exponential, and thatâ€™s about the state of the art at present.
Letnbe an integer whose factorization is desired.
Denition. By afactor base B we will mean a set of distinct nonzero integers fb
0;b1;:::;b hg.
Denition. Let B be a factor base. An integer awill be called a B-number if the integer cthat is dened
by the conditions
(a)ca2(modn)and
(b)âˆ’n=2c<n= 2
can be written as a product of factors from the factor base B.
If we lete(a;i) denote the exponent of biin that product, then we have
a2hY
i=0be(a;i)
i (modn):
Hence, for each B-number we get an ( h+ 1)-vector of exponents e(a).
Suppose we can nd enough B-numbers so that the resulting collection of exponent vectors is a linearly
dependent set, mod2. For instance, a set of h+2B-numbers would certainly have that property.
Then we could nontrivially represent the zero vector as a sum of a certain set Aof exponent vectors,
sayX
a2Ae(a)(0;0;:::;0) (mod 2) :
Now dene the integers
ri=( 1=2)X
a2Ae(a;i)(i=0;1;:::h)
u=Y
Aa(modn)
v=Y
ibri
i:
It then would follow, after an easy calculation, that u2v2(modn). Hence either uâˆ’voru+vhas
af a c t o ri nc o m m o nw i t h n. It may be, of course, that uv(modn), in which case we would have
99
Chapter 4: Algorithms in the Theory of Numbers
learned nothing. However if neither uv(modn)n o ruâˆ’v(modn) is true then we will have found
a nontrivial factor of n,n a m e l ygcd(uâˆ’v;n)o rgcd(u+v;n).
Example :
Take as a factor base B=fâˆ’2;5g, and let it be required to nd a factor of n= 1729. Then we claim
that 186 and 267 are B-numbers. To see that 186 is a B-number, note that 1862=2 01729 + ( âˆ’2)4,a n d
similarly, since 2672=4 11729 + ( âˆ’2)452, we see that 267 is a B-number, for this factor base B.
The exponent vectors of 186 and 167 are (4 ;0) and (4;2) respectively, and these sum to (0 ;0) (mod 2),
hence we nd that
u= 186 2671250 (mod 1729)
r1=4 ;r2=1
v=(âˆ’2)4(5)1=8 0
gcd(uâˆ’v;n)=gcd(1170;1729) = 13
and we have found the factor 13 of 1729.
There might have seemed to be some legerdemain involved in plucking the B-numbers 186 and 267 out
of the air, in the example above. In fact, as the algorithm has been implemented by its author, J. D. Dixon,
one simply chooses integers uniformly at random from [1 ;nâˆ’1] until enough B-numbers have been found
so their exponent vectors are linearly dependent modulo 2. In Dixonâ€™s implementation the factor base thatis used consists of âˆ’1 together with the rst hprime numbers.
It can then be proved that if nis not a prime power then with a correct choice of hrelative ton,i fw e
repeat the random choices until a factor of nis found, the average running time will be
expf(2 +o(1))(log log log n)
:5g:
This is not polynomial time, but it is moderately exponential only. Nevertheless, it is close to being about
the best that we know how to do on the elusive problem of factoring a large integer.
4.10 Proving primality
In this section we will consider a problem that sounds a lot like primality testing, but is really a little
dierent because the rules of the game are dierent. Basically the problem is to convince a skeptical audiencethat a certain integer is prime, requiring them to do only a small amount of computation in order to be so
persuaded.
First, though, suppose you were writing a 100-decimal-digit integer non the blackboard in front of a
large audience and you wanted to prove to them that nwasnotap r i m e .
If you simply wrote down two smaller integers whose product was n, the job would be done. Anyone
who wished to be certain could spend a few minutes multiplying the factors together and verifying that their
product was indeed n, and all doubts would be dispelled.
Indeed*, a spea ker at a mathematical convention in 1903 announced the result that 2
67âˆ’1i sn o ta
prime number, and to be utterly convincing all he had to do was to write
267âˆ’1 = 193707721 761838257287 :
We note that the speaker probably had to work very hard to ndthose factors, but having found them
it became quite easy to convince others of the truth of the claimed result.
A pair of integers r;sfor whichr6=1 ,s6=1 ,a n dn=rsconstitute a certicate attesting to the
compositeness of n. With this certicate C(n) and an auxiliary checking algorithm, viz.
(1) Verify that r6=1 ,a n dt h a t s6=1
(2) Verify that rs=n
we can prove, in polynomial time, that nis not a prime number.
* We follow the account given in V. Pratt, Every prime has a succinct certicate, SIAM J. Computing ,4
(1975), 214-220.
100
4.10 Proving primality
Now comes the hard part. How might we convince an audience that a certain integer nisap r i m e
number? The rules are that we are allowed to do any immense amount of calculation beforehand, and the
results of that calculation can be written on a certicate C(n) that accompanies the integer n. The audience,
however, will need to do only a polynomial amount of further computation in order to convince themselvesthatnis prime.
We will describe a primality-checking algorithm Awith the following properties:
(1) Inputs to Aare the integer nand a certain certicate C(n).
(2) Ifnis prime then the action of Aon the inputs ( n;C(n)) results in the output â€˜ nis prime.â€™
(3) Ifnis not prime then for every possible certicate C(n) the action of Aon the inputs ( n;C(n)) results
in the output â€˜primality of nis not veried.â€™
(4) Algorithm Aruns in polynomial time.
Now the question is, does such a procedure exist for primality verication? The answer is armative,
and we will now describe one. The fact that primality can be quickly veried, if not quickly discovered, is
of great importance for the developments of Chapter 5. In the language of section 5.1, what we are aboutto do is to show that the problem â€˜Is nprime?â€™ belongs to the class NP.
The next lemma is a kind of converse to â€˜Fermatâ€™s little theoremâ€™ (theorem 4.5.2 ).
Lemma 4.10.1. Letpbe a positive integer. Suppose there is an integer xsuch thatx
pâˆ’11( m o dp)
and such that for all divisors dofpâˆ’1,d<p âˆ’1,w eh a v exd61( m o dp).T h e npis prime.
Proof: First we claim that gcd(x;p)=1 ,f o rl e t g=gcd(x;p). Thenx=gg0;p=gg00. Sincexpâˆ’11
(modp)w eh a v expâˆ’1=1+tpandxpâˆ’1âˆ’tp=(gg0)pâˆ’1âˆ’tgg00= 1. The left side is a multiple of g.T h e
right side is not, unless g=1 .
It follows that x2Up, the group of units of Zp.T h u sxis an element of order pâˆ’1 in a group of order
(p). Hence (pâˆ’1)j(p). But always (p)pâˆ’1. Hence(p)=pâˆ’1a n dpis prime.
Lemma 4.10.1 is the basis for V. Prattâ€™s method of constructing certicates of primality. The construc-
tion of the certicate is actually recursive since step 30below calls for certicates of smaller primes. We
suppose that the certicate of the prime 2 is the trivial case, and that it can be veried at no cost.
Here is a complete list of the information that is on the certicate C(p) that accompanies an integer p
whose primality is to be attested to:
10: a list of the primes piand the exponents aifor the canonical factorization pâˆ’1=Qr
i=1pai
i
20: the certicates C(pi) of each of the primes p1;:::;p r
30: a positive integer x.
To verify that pis prime we could execute the following algorithm B:
(B1) Check that pâˆ’1=Qpai
i.
(B2) Check that each piis prime, using the certicates C(pi)(i=1;r).
(B3) For each divisor dofpâˆ’1,d<p âˆ’1, check that xd61( m o dp).
(B4) Check that xpâˆ’11( m o dp).
This algorithm Bis correct, but it might not operate in polynomial time. In step B3 we are looking at
every divisor of pâˆ’1, and there may be a lot of them.
Fortunately, it isnâ€™t necessary to check every divisor ofpâˆ’1. The reader will have no trouble proving
thatthere is a divisor dofpâˆ’1(d<p âˆ’1)for whichxd1( m o dp)if and only if there is such a divisor
that has the special form d=(pâˆ’1)=pi.
The primality checking algorithm Anow reads as follows.
(A1) Check that pâˆ’1=Qpai
i.
(A2) Check that each piis prime, using the certicates C(pi)(i=1;r).
(A3) For each i:= 1 tor,c h e c kt h a t
x(pâˆ’1)=p i61( m o dp):
101
Chapter 4: Algorithms in the Theory of Numbers
(A4) Check that xpâˆ’11( m o dp).
Now letâ€™s look at the complexity of algorithm A.
We will measure its complexity by the number of times that we have to do a computation of either of
the types (a) â€˜is m=Qqbj
j?â€™ or (b) â€˜is ys1( m o dp)?â€™
Letf(p) be that number. Then we have (remembering that the algorithm calls itself rtimes)
f(p)=1+rX
i=2f(pi)+r+1 ( 4 :10:1)
in which the four terms, as written, correspond to the four steps in the checking algorithm. The sum begins
with â€˜i= 2â€™ because the prime 2, which is always a divisor of pâˆ’1, is â€˜free.â€™
Now (4.10.1) can be written as
g(p)=rX
i=2g(pi)+4 ( 4 :10:2)
whereg(p)=1+f(p). We claim that g(p)4l o g2pfor allp.
This is surely true if p= 2. If true for primes less than pthen from (4.10.2),
g(p)rX
i=2f4l o g2pig+4
=4l o g2frY
i=2pig+4
4l o g2f(pâˆ’1)=2g+4
=4l o g2(pâˆ’1)
4l o g2p:
Hencef(p)4l o g2pâˆ’1 for allp2.
Since the number of bits in pis (logp), the number f(p) is a number of executions of steps that is
a polynomial in the length of the input bit string. We leave to the exercises the verication that each of
the steps that f(p) counts is also executed in polynomial time, so the entire primality-verication procedure
operates in polynomial time. This yields
Theorem 4.10.1. (V. Pratt, 1975) There exist a checking algorithm and a certicate such that primality
can be veried in polynomial time.
Exercises for section 4.10
1. Show that two positive integers of bbits each can be multiplied with at most O(b2) bit operations
(multiplications and carries).2. Prove that step A1 of algorithm Acan be executed in polynomial time, where time is now measured by
the number of bit operations that are implied by the integer multiplications.
3. Same as exercise 2 above, for steps A3 and A4.
4. Write out the complete certicate that attests to the primality of 19.
5. Find an upper bound for the total number of bits that are in the certicate of the integer p.
6. Carry out the complete checking algorithm on the certicate that you prepared in exercise 4 above.
7. Letp= 15. Show that there is no integer xas described in the hypotheses of lemma 4.10.1.
8. Letp= 17. Find all integers xthat satisfy the hypotheses of lemma 4.10.1.
102
4.10 Proving primality
Bibliography
The material in this chapter has made extensive use of the excellent review article
John D. Dixon, Factorization and primality tests, The American Mathematical Monthly ,91(1984), 333-352.
A basic reference for number theory, Fermatâ€™s theorem, etc. is
G. H. Hardy and E. M. Wright, An Introduction to the Theory of Numbers , Oxford University Press, Oxford,
1954
Another is
W. J. LeVeque, Fundamentals of Number Theory , Addison-Wesley, Re ading, MA, 1977
The probabilistic algorithm for compositeness testing was found by
M. O. Rabin, Probabilistic algorithms, in Algorithms and Complexity, New Directions and Recent Results ,
J. Traub ed., Academic Press, New York, 1976
and at about the same time byR. Solovay and V. Strassen, A fast Monte Carlo test for primality, SIAM Journal of Computing ,6(1977),
pp. 84-85; erratum ibid. ,7(1978), 118.
Some empirical properties of that algorithm are in
C. Pomerance, J. L. Selfridge and S. Wagsta Jr., The pseudoprimes to 25 10
9,Mathematics of Computation ,
35(1980 ), 1003-1026.
The fastest nonprobabilistic primality test appeared rst in
L. M. Adleman, On distinguishing prime numbers from composite numbers, IEEE Abstracts , May 1980,
387-406.
A more complete account, together with the complexity analysis, is in
L. M. Adleman, C. Pomerance and R. S. Rumely, On distinguishing prime numbers from composite numbers,
Annals of Mathematics 117(1983), 173-206.
A streamlined version of the above algorithm was given by
H. Cohen and H. W. Lenstra Jr., Primality testing and Jacobi sums, Report 82-18, Math. Inst. U. of
Amsterdam, Amsterdam, 1982.
The idea of public key data encryption is due to
W. Die and M. E. Hellman, New directions in cryptography, IEEE Transactions on Information Theory ,
IT-22 , 6 (1976), 644-654.
An account of the subject is contained in
M. E. Hellman, The mathematics of public key cryptography, Scientic American ,241, 2 (August 1979),
146-157.
T h eu s eo ff a c t o r i n ga st h ek e yt ot h ec o d ei sd u et o
R. L. Rivest, A. Shamir and L. M. Adleman, A method for obtaining digital signatures and public key
cryptosystems, Communications of the A.C.M. ,21, 2 (February 1978), 120-126
The probabilistic factoring algorithm in the text is that of
John D. Dixon, Asymptotically fast factorization of integers, Mathematics of Computation ,36(1981), 255-
260.
103
Chapter 5: NP-completeness
Chapter 5: NP-completeness
5.1 Introduction
In the previous chapter we met two computational problems for which fast algorithms have never been
found, but neither have such algorithms been proved to be unattainable. Those were the primality-testing
problem, for which the best-known algorithm is delicately poised on the brink of polynomial time, and the
integer-factoring problem, for which the known algorithms are in a more primitive condition.
In this chapter we will meet a large family of such problems (hundreds of them now!). This family is not
just a list of seemingly dicult computational problems. It is in fact bound together by strong structural
ties. The collection of problems, called the NP-complete problems, includes many well known and important
questions in discrete mathematics, such as the following.The travelling salesman problem (â€˜TSPâ€™): Givennpoints in the plane (â€˜citiesâ€™), and a distance D.I s
there a tour that visits all nof the cities, returns to its starting point, and has total length D?
Graph coloring: Given a graph Gand an integer K. Can the vertices of Gbe properly colored in Kor
fewer colors?
Independent set: Given a graph Ga n da ni n t e g e r K.D o e sV(G) contain an independent set of Kvertices?
Bin packing: Given a nite set Sof positive integers, and an integer N(the number of bins). Does there
exist a partition of SintoNor fewer subsets such that the sum of the integers in each subset is K?I n
other words, can we â€˜packâ€™ the integers of Sinto at most Nâ€˜bins,â€™ where the â€˜capacityâ€™ of each bin is K?
These are very dicult computational problems. Take the graph coloring problem, for instance. We
could try every possible way of coloring the vertices of GinKcolors to see if any of them work. There are
K
nsuch possibilities, if Ghasnvertices. Hence a very large amount of computation will be done, enough
so that ifGhas 50 vertices and we have 10 colors at our disposal, the problem would lie far beyond the
capabilities of the fastest computers that are now available.
Hard problems can have easy instances. If the graph Ghappens to have no edges at all, or very few of
them, then it will be very easy to nd out if a coloring is possible, or if an independent set of Kvertices is
present.
The real question is this (letâ€™s use â€˜Independent Setâ€™ as an illustration). Is it possible to design an
algorithm that will come packaged with a performance guarantee of the following kind:
The seller warrants that if a graph G,o fnvertices,
and a positive integer Kare input to this program, then
it will correctly determine if there is an independent set
ofKor more vertices in V(G), and it will do so in an
amount of time that is at most 1000n8minutes.
Hence there is no contradiction between the facts that the problem is hard and that there are easy
cases. The hardness of the problem stems from the seeming impossibility of producing such an algorithm
accompanied by such a manufacturerâ€™s warranty card. Of course the â€˜1000 n8â€™ didnâ€™t have to be exactly that.
But some quite specic polynomial in the length of the input bit string must appear in the performance
guarantee. Hence â€˜357 n9â€™ might have appeared in the guarantee, and so might â€˜23 n3,â€™ but â€˜nKâ€™ would not
be allowed.
Letâ€™s look carefully at why nKwould not be an acceptable worst-case polynomial time performance
bound. In the â€˜Independent Setâ€™ problem the input must describe the graph Gand the integer K.H o w
many bits are needed to do that? The graph can be specied, for example, by its vertex adjacency matrix
A. This is an nnmatrix in which the entry in row iand column jis 1 if (i;j)2E(G) and is 0 else.
Evidentlyn2bits of input will describe the matrix A. The integers Kandncan be entered with just
O(logn) bits, so the entire input bit string for the â€˜Independent Setâ€™ problem is n2bits long. Let Bdenote
the number of bits in the input string. Suppose that on the warranty card the program was guaranteed torun in a time that is <n
K.
Is this a guarantee of polynomial time performance? That question means â€˜Is there a polynomial Psuch
that for every instance of â€˜Independent Setâ€™ the running time Twill be at most P(B)?â€™ Well, is Tbounded
104
What is a language?
by a polynomial in BifT=nKandBn2? It would seem so; in fact obviously T=O(BK=2), and thatâ€™s
a polynomial, isnâ€™t it?
The key point resides in the order of the qualiers. We must give the polynomial that works for
every instance of the problem rst. Then that one single polynomial must work on every instance. If
the â€˜polynomialâ€™ that we give is BK=2, well thatâ€™s a dierent polynomial in Bfor dierent instances of
the problem, because Kis dierent for dierent instances. Therefore if we say that a certain program for
â€˜Independent Setâ€™ will always get an answer before BK=2minutes, where Bis the length of the input bit
string, then we would not have provided a polynomial-time guarantee in the form of a single polynomial in
Bthat applies uniformly to all problem instances.
The distinction is a little thorny, but is worthy of careful study because itâ€™s of fundamental importance.
What we are discussing is usually called a worst-case time bound, meaning a bound on the running time that
applies to every instance of the problem. Worst-case time bounds arenâ€™t the only possible interesting ones.
Sometimes we might not care if an algorithm is occasionally very slow as long as it is almost always fast .I n
other situations we might be satised with an algorithm that is fast on average . For the present, however,
we will stick to the worst-case time bounds and study some of the theory that applies to that situation. In
sections 5.6 and 5.7 we will study some average time bounds.
Now letâ€™s return to the properties of the NP-complete family of problems. Here are some of them.
10: The problems all seem to be computationally very dicult, and no polynomial time algorithms have
been found for any of them.
20: It has not been proved that polynomial time algorithms for these problems do not exist.
20: But this is not just a random list of hard problems. If a fast algorithm could be found for one
NP-complete problem then here would be fast algorithms for all of them.
20: Conversely, if it could be proved that no fast algorithm exists for one of the NP-complete problems,
then there could not be a fast algorithm for any other of those problems.
The above properties are not intended to be a denition of the concept of NP-completeness. Weâ€™ll get to
that later on in this section. They are intended as a list of some of the interesting features of these problems,
which, when coupled with their theoretical and practical importance, accounts for the intense worldwide
research eort that has gone into understanding them in recent years.
The question of the existence or nonexistence of polynomial-time algorithms for the NP-complete prob-
lems probably rates as the principal unsolved problem that faces theoretical computer science today.
Our next task will be to develop the formal machinery that will permit us to give precise denitions
of all of the concepts that are needed. In the remainder of this section we will discuss the additional ideas
informally, and then in section 5.2 weâ€™ll state them quite precisely.
What is a decision problem? First, the idea of a decision problem . A decision problem is one that asks
only for a yes-or-no answer: Can this graph be 5-colored? Is there a tour of length 15 miles? Is there a
set of 67 independent vertices?
Many of them problems that we are studying can be phrased as decision problems or as optimization
problems : What is the smallest number of colors with which Gcan be colored? What is the length of the
shortest tour of these cities? What is the size of the largest independent set of vertices in G?
Usually if we nd a fast algorithm for a decision problem then with just a little more work we will
be able to solve the corresponding optimization problem. For instance, suppose we have an algorithm that
solves the decision problem for graph coloring, and what we want is the solution of the optimization problem(the chromatic number).
Let a graph Gbe given, say of 100 vertices. Ask: can the graph be 50-colored? If so, then the chromatic
number lies between 1 and 50. Then ask if it can be colored in 25 colors. If not, then the chromatic
number lies between 26 and 50. Continue in this way, using bisection of the interval that is known to
contain the chromatic number. After O(logn) steps we will have found the chromatic number of a graph of
nvertices. The extra multiplicative factor of log nwill not alter the polynomial vs. nonpolynomial running
time distinction. Hence if there is a fast way to do the decision problem then there is a fast way to do theoptimization problem. The converse is obvious.
Hence we will restrict our discussion to decision problems.
105
Chapter 5: NP-completeness
What is a language?
Since every decision problem can have only the two answers â€˜Y/N,â€™ we can think of a decision problem
as asking if a given word (the input string) does or does not belong to a certain language . The language is
the totality of words for which the answer is â€˜Y.â€™
The graph 3-coloring language, for instance, is the set of all symetric, square matrices of 0,1 entries,
with zeroes on the main diagonal (these are the vertex adjacency matrices of graphs) such that the graph
that the matrix represents is 3-colorable. We can image that somewhere there is a vast dictionary of all of
the words in this language. A 3-colorability computation is therefore nothing but an attempt to discover
whether a given word belongs to the dictionary.
What is the class P?
We say that a decision problem belongs to the class P if there is an algorithm Aand a number csuch
that for every instance Iof the problem the algorithm Awill produce a solution in time O(Bc), whereBis
the number of bits in the input string that represents I.
To put it more brieï¬‚y, P is the set of easy decision problems.Examples of problems in P are most of the ones that we have already met in this book: Are these two
integers relatively prime? Is this integer divisible by that one? Is this graph 2-colorable? Is there a ï¬‚ow ofvalue greater than Kin this network? Can this graph be disconnected by the removal of Kor fewer edges?
Is there a matching of more than Kedges in this bipartite graph? For each of these problems there is a fast
(polynomial time) algorithm.
What is the class NP?
The class NP is a little more subtle. A decision problem Qbelongs to NP if there is an algorithm A
that does the following:
(a) Associated with each word of the language Q(i.e.,with each instance I for which the answer is â€˜Yesâ€™)
there is a certicateC(I) such that when the pair ( I;C(I)) are input to algorithm Ait recognizes
thatIbelongs to the language Q.
(b) IfIis some word that does not belong to the language Qthen there is no choice of certicate C(I)
that will cause Ato recognize Ia sam e m b e ro f Q.
(c) Algorithm Aoperates in polynomial time.
To put this one more brieï¬‚y, NP is the class of decision problems for which it is easy to check the
correctness of a claimed answer, with the aid of a little extra information. So we arenâ€™t asking for a way to
nda solution, but only to verify that an alleged solution really is correct.
Here is an analogy that may help to clarify the distinction between the classes P and NP. We have all
had the experience of reading through a truly ingenious and dicult proof of some mathematical theorem,
and wondering how the person who found the proof in the rst place ever did it. Our task, as a reader, was
only to verify the proof, and that is a much easier job than the mathematician who invented the proof had.
To pursue the analogy a bit farther, some proofs are extremely time consuming even to check (see the proof
of the four-color theorem!), and similarly, some computational problems are not even known to belong to
NP, let alone to P.
In P are the problems where itâ€™s easy to nda solution, and in NP are the problems where itâ€™s easy to
check a solution that may have been very tedious to nd.
Hereâ€™s another example. Consider the graph coloring problem to be the decision problem Q. Certainly
this problem is not known to be in P. It is, however, in NP, and here is an algorithm, and a method of
constructing certicates that proves it.
SupposeGis some graph that isK-colorable. The certicate of Gmight be a list of the colors that get
assigned to each vertex in some proper K-coloring of the vertices of G. Where did we get that list, you ask?
Well, we never said it was easy to construct a certicate. If you actually want to nd one then you will have
to solve a hard problem. But weâ€™re really only talking about checking the correctness of an alleged answer.
Tocheck that a certain graph Greally isK-colorable we can be convinced if you will show us the color of
each vertex in a proper K-coloring.
If you do provide that certicate, then our checking algorithm Ais very simple. It checks rst that every
vertex has a color and only one color. It then checks that no more than Kcolors have been used altogether.
106
What is NP-completeness?
It nally checks that for each edge eofGit is true that the two endpoints of ehave dierent colors.
Hence the graph coloring problem belongs to NP.
For the travelling salesman problem we would provide a certicate that contains a tour, whose total
length is K, of all of the cities. The checking algorithm Awould then verify that the tour really does visit
all of the cities and really does have total length K.
The travelling salesman probelm, therefore, also belongs to NP.
â€˜Well,â€™ you might reply, â€˜if weâ€™re allowed to look at the answers, how could a problem fail to belong to
NP?â€™
Try this decision problem: an instance I of the problem consists of a set of ncities in the plane and a
positive number K. The question is â€˜Is it true that there is nota tour of all of these cities whose total length
is less than K?â€™ Clearly this is a kind of a negation of the travelling salesman problem. Does it belong to
NP? If so, there must be an algorithm Aand a way of making a certicate C(I) for each instance I such
that we can quickly verify that no such tour exists of the given cities. Any suggestions for the certicate?
The algorithm? No one else knows how to do this either.
It is not known if this negation of the travelling salesman problem belongs to NP.
Are there problems that dobelong to NP but for which it isnâ€™t immediately obvious that this is so?
Yes. In fact thatâ€™s one of the main reasons that we studied the algorithm of Pratt, in section 4.10. Prattâ€™s
algorithm is exactly a method of producing a certicate with the aid of which we can quickly check that a
given integer is prime. The decision problem â€˜Given n, is it prime?â€™ is thereby revealed to belong to NP,
although that fact wasnâ€™t obvious at a glance.
It is very clear that P NP. Indeed if Q2Pis some decision problem then we can verify membership in
the language Qwith the empty certicate. That is, we donâ€™t even need a certicate in order to do a quick
calculation that checks membership in the language because the problem itself can be quickly solved.
It seems natural to suppose that NP is larger than P. That is, one might presume that there are problems
whose solutions can be quickly checked with the aid of a certicate even though they canâ€™t be quickly found
in the rst place.
No example of such a problem has ever been produced (and proved), nor has it been proved that no
such problem exists. The question of whether or not P=NP is the one that we cited earlier as being perhaps
the most important open question in the subject area today.
It is fairly obvious that the class P is called â€˜the class Pâ€™ because â€˜Pâ€™ is the rst letter of â€˜Polynomial
Time.â€™ But what does â€˜NPâ€™ stand for? Stay tuned. The answer will appear in section 5.2.
What is reducibility?
Suppose that we want to solve a system of 100 simultaneous linear equations in 100 unknowns, of the
formAx=b. We run down to the local software emporium and quickly purchase a program for $49.95 that
solves such systems. When we get home and read the ne print on the label we discover, to our chagrin,
that the system works only on systems where the matrix Ais symmetric, and the coecient matrix in the
system that we want to solve is, of course, not symmetric.
One possible response to this predicament would be to look for the solution to the system ATAx=ATb,
in which the coecient matrix ATAis now symmetric.
What we would have done would be to have reduced the problem that we really are interested in to an
instance of a problem for which we have an algorithm.
More generally, let QandQ0be two decision problems. We will say that Q0isquickly reducible to Qif
whenever we are given an instance I0of the problem Q0we can convert it, with only a polynomial amount
of labor, into an instance IofQ,i ns u c haw a yt h a t I0andIboth have the same answer (â€˜Yesâ€™ or â€˜Noâ€™).
Thus if we buy a program to solve Q,t h e nw ec a nu s ei tt os o l v e Q0, with just a small amount of extra
work.
What is NP-completeness?
How would you like to buy one program, for $49.95, that can solve 500 dierent kinds of problems?
Thatâ€™s what NP-completeness is about.
To state it a little more carefully, a decision problem is NP-complete if it belongs to NP and every
problem in NP is quickly reducible to it.
107
Chapter 5: NP-completeness
The implications of NP-completeness are numerous. Suppose we could prove that a certain decision
problemQis NP-complete. Then we could concentrate our eorts to nd polynomial-time algorithms on
just that one problem Q. Indeed if we were to succeed in nding a polynomial time algorithm to do instances
ofQthen we would automatically have found a fast algorithm for doing every problem in NP. How does
that work?
Take an instance I0of some problem Q0in NP. Since Q0is quickly reducible to Qwe could transform
the instance I0into an instance IofQ. Then use the super algorithm that we found for problems in Qto
decideI. Altogether only a polynomial amount of time will have been used from start to nish.
Letâ€™s be more specic. Suppose that tomorow morning we prove that the graph coloring problem is
NP-complete, and that on the next morning you nd a fast algorithm for solving it. Then consider some
instance of the bin packing problem. Since graph coloring is NP-complete, the instance of bin packing can
be quickly converted into an instance of graph coloring for which the â€˜Yes/Noâ€™ answer is the same. Now use
the fast graph coloring algorithm that you found (congratulations, by the way!) on the converted problem.
The answer you get is the correct answer for the original bin packing problem.
So, a fast algorithm for some NP-complete problem implies a fast algorithm for every problem in NP.
Conversely suppose we can prove that it is impossible to nd a fast algorithm for some particular problem
Qin NP. Then we canâ€™t nd a fast algorithm for any NP-complete problem Q0either. For if we could then
we would be able to solve instances of Qby quickly reducing them to instances of Q0and solving them.
If we could prove that there is no fast way to test the primality of a given integer then we would have
proved that there is no fast way to decide of graphs are K-colorable , because, as we will see, the graph coloring
problem is NP-complete and primality testing is in NP. Think about that one for a few moments, and the
extraordinary beauty and structural unity of these computational problems will begin to reveal itself.
To summarize: quick for one NP-complete problem implies quick for all of NP; provably slow for one
problem in NP implies provably slow for all NP-complete problems.
Thereâ€™s just one small detail to attend to. Weâ€™ve been discussing the economic advantages of keeping
ï¬‚ocks of unicorns instead of sheep. If there arenâ€™t any unicorns then the discussion is a little silly.
NP-complete problems have all sorts of marvellous properties. Itâ€™s lovely that every problem in NP can
be quickly reduced to just that one NP-complete problem. But are there any NP-complete problems? Why,
after all, should there be a single computational problem with the property that every one of the diverse
creatures that inhabit NP should be quickly reducible to it?
Well, there areNP-complete problems, hordes of them, and proving that will occupy our attention for
the next two sections. Hereâ€™s the plan.
In section 5.2 we are going to talk about a simple computer, called a Turing machine. It is an idealized
computer, and its purpose is to standardize ideas of computability and time of computation by referring all
problems to the one standard machine.
A Turing machine is an extremely simple nite-state computer, and when it performs a computation,
a unit of computational labor will be very clearly and unambiguously describable. It turns out that the
important aspects of polynomial time computability do not depend on the particular computer that is
chosen as the model. The beauty of the Turing machine is that it is at once a strong enough concept that it
can in principle perform any calculation that any other nite state machine can do, while at the same timeit is logically clean and simple enough to be useful for proving theorems about complexity.
The microcomputer on your desktop might have been chosen as the standard against which polynomial
time computability is measured. If that had been done then the class P of quickly solvable problems would
scarcely have changed at all (the polynomials would be dierent but they would still be polynomials), buttheproofs that we humans would have to give in order to establish the relevant theorems would have gotten
much more complicated because of the variety of dierent kinds of states that modern computers have.
Next, in section 5.3 we will prove that there isan NP-complete problem. It is called the satisability
problem . Its status as an NP-complete problem was established by S. Cook in 1971, and from that work all
later progress in the eld has ï¬‚owed. The proof uses the theory of Turing machines.
The rst NP-complete problem was the hardest one to nd. We will nd, in section 5.4, a few more
NP-complete problems, so the reader will get some idea of the methods that are used in identifying them.
Since nobody knows a fast way to solve these problems, various methods have been developed that give
approximate solutions quickly, or that give exact solutions in fast average time, and so forth. The beautiful
108
5.2 Turing Machines
book of Garey and Johnson (see references at the end of the chapter) calls this â€˜coping with NP-completeness,â€™
and we will spend the rest of this chapter discussing some of these ideas.
Exercises for section 5.1
1. Prove that the following decision problem belongs to P: Given integers Kanda1;:::;a n. Is the median
of theaâ€™s smaller than K?
2. Prove that the following decision problem is in NP: given an nnmatrixAof integer entries. Is
detA=0 ?
3. For which of the following problems can you prove membership in P?
(a) Given a graph G.D o e sGcontain a circuit of length 4?
(b) Given a graph G.I sGbipartite?
(c) Givennintegers. Is there a subset of them whose sum is an even number?
(d) Givennintegers. Is there a subset of them whose sum is divisible by 3?
(e) Given a graph G.D o e sGcontain an Euler circuit?
4. For which of the following problems can you prove membership in NP?
(a) Given a set of integers and another integer K. Is there a subset of the given integers whose sum is
K?
(b) Given a graph Gand an integer K.D o e sGcontain a path of length K?
(c) Given a set of Kintegers. Is it true that not all of them are prime?
(d) Gievn a set of Kintegers. Is it true that all of them are prime?
5.2 Turing Machines
A Turing machine consists of
(a) a doubly innite tape, that is marked o into squares that are numbered as shown in Fig. 5.2.1 below.
Each square can contain a single character from the character set that the machine recognizes. For
simplicity we can assume that the character set contains just three symbols: â€˜0,â€™ â€˜1,â€™ and â€˜â€™ (blank).
(b) a tape head that is capable of either reading a single character from a square on the tape or writing a
single character on a square, or moving its position relative to the tape by an increment of one square in
either direction.
(c) a nite list of states such that at every instant the machine is in exactly one of those states. The possible
states of the machine are, rst of all, the regular states q1;:::;q s, and second, three special states
q0: the initial state
qY, the nal state in a problem to which the answer is â€˜Yesâ€™
qN: the nal state in a problem to which the answer is â€˜Noâ€™
(d) a program (orprogram module , if we think of it as a pluggable component) that directs the machine
through the steps of a particular task.
Fig. 5.2.1: A Turing machine tape
Letâ€™s describe the program module in more detail. Suppose that at a certain instant the machine is in
stateq(other than qYorqN) and that the symbol that has just been read from the tape is â€˜ symbol .â€™ Then
from the pair ( q;symbol ) the program module will decide
(i) to what state q0the machine shall next go, and
(ii) what single character the machine will now write on the tape in the square over which the head is now
positioned, and
(iii) whether the tape head will next move one square to the right or one square to the left.
109
Chapter 5: NP-completeness
One step of the program, therefore, goes from
(state;symbol )t o(newstate;newsymbol;increment ): (5:2:1)
If and when the state reaches qYorqNthe computation is over and the machine halts.
The machine should be thought of as part hardware and part software. The programmerâ€™s job is, as
usual, to write the software. To write a program for a Turing machine, what we have to do is to tell it how
to make each and every one of the transitions (5.2.1). A Turing machine program looks like a table in which,
for every possible pair ( state, symbol ) that the machine might nd itself in, the programmer has specied
what the newstate ,t h enewsymbol and the increment shall be.
To begin a computation with a Turing machine we take the input string x,o fl e n g t hB, say, that describes
the problem that we want to solve, and we write xin squares 1 ;2;:::;B of the tape. The tape head is then
positioned over square 1, the machine is put into state q0, the program module that the programmer prepared
is plugged into its slot, and the computation begins.
The machine reads the symbol in square 1. It now is in state q0and has read symbol , so it can consult
the program module to nd out what to do. The program instructs it to write at square 1 a newsymbol ,t o
move the head either to square 0 or to square 2, and to enter a certain newstate ,s a yq0. The whole process
is then repeated, possibly forever, but hopefully after nitely many steps the machine will enter the state
qYor stateqN, at which moment the computation will halt with the decision having been made.
If we want to watch a Turing machine in operation, we donâ€™t have to build it. We can simulate one.
Here is a pidgin-Pascal simulation of a Turing machine that can easily be turned into a functioning program.
It is in two principal parts.
The procedure turmach has for input a string xof lengthB, and for output it sets the Boolean variable
accept toTrueorFalse , depending on whether the outcome of the computation is that the machine halted
in stateqYorqNrespectively. This procedure is the â€˜hardwareâ€™ part of the Turing machine. It doesnâ€™t vary
from one job to the next.
Procedure gonextto is the program module of the machine, and it will be dierent for each task. Its
inputs are the present state of the machine and the symbol that was just read from the tape. Its outputs
are the newstate into which the machine goes next, the newsymbol that the tape head now writes on the
current square, and the increment (1) by which the tape head will now move.
procedure turmach (B:integer; x:array[1..B]; accept :Boolean);
fsimulates Turing machine action on input string xof lengthBg
fwrite input string on tape in rst Bsquares g
forsquare := 1 toBdo
tape[square] :=x[square];
frecord boundaries of written-on part of tape g
leftmost :=1; rightmost :=B;
finitialize tape head and state g
state :=0; square :=1;
while state6=â€˜ Y â€™a n d state 6=â€˜ N â€™d o
fread symbol at current tape square g
ifsquare<leftmost orsquare>rightmost
then symbol :=â€˜â€™else symbol :=tape[square]
fask program module for state transition g
gonnextto(state,symbol,newstate,newsybol,increment) ;
state:=newstate ;
fupdate boundaries and write new symbol g;
ifsquare>rightmost then leftmost :=square ;
tape[square]:=newsymbol ;
fmove tape head g
square := square+increment
end;fwhileg
accept :=fstate =â€˜Yâ€™g
end.fturmach g
110
5.2 Turing Machines
Now letâ€™s try to write a particular program module gonextto . Consider the following problem: given
an input string x, consisting of 0â€™s and 1â€™s, of length B. FInd out if it is true that the string contains an
odd number of 1â€™s.
We will write a program that will scan the input string from left to right, and at each moment the
machine will be in state 0 if it has so far scanned an even number of 1â€™s, in state 1 otherwise. In Fig. 5.2.2
we show a program that will get the job done.
state symbol newstate newsymbol increment
00 0 0 + 1
01 1 1 + 1
0 blank qN blank âˆ’1
10 1 0 + 1
11 0 1 + 1
1 blank qY blank âˆ’1
Fig. 5.2.2: A Turing machine program for bit parity
Exercise. Program the above as procedure gonextto , run it for some input string, and print out the state of
the machine, the contents of the tape, and the position of the tape head after each step of the computation.
In the next section we are going to use the Turing machine concept to prove Cookâ€™s theorem, which
is the assertion that a certain problem is NP-complete. Right now letâ€™s review some of the ideas that have
already been introduced from the point of view of Turing machines.
We might immediately notice that some terms that were just a little bit fuzzy before are now much more
sharply in focus. Take the notion of polynomial time, for example. To make that idea precise one needs a
careful denition of what â€˜the length of the input bit stringâ€™ means, and what one means by the number ofâ€˜stepsâ€™ in a computation.
But on a Turing machine both of these ideas come through with crystal clarity. The input bit string x
is what we write on the tape to get things started, and its length is the number of tape squares it occupies.A â€˜stepâ€™ in a Turing machine calculation is obviously a single call to the program module. A Turing machine
caluclation was done â€˜in time P(B)â€™ if the input string occupied Btape squares and the calculation took
P(B)s t e p s .
Another word that we have been using without ever nailing down precisely is â€˜algorithm.â€™ We all
understand informally what an algorithm is. But now we understand formally too. An algorithm for a
problem is a program module for a Turing machine that will cause the machine to halt after nitely many
steps in state â€˜Yâ€™ for every instance whose answer is â€˜Yes,â€™ and after nitely many steps in state â€˜Nâ€™ for every
instance whose answer is â€˜No.â€™
A Turing machine and an algorithm dene a language . The language is the set of all input strings x
that lead to termination in state â€˜Y,â€™ i.e.,t oa n accepting calculation.
Now letâ€™s see how the idea of a Turing machine can clarify the description of the class NP. This is
the class of problems for which the decisions can be made quickly if the input strings are accompanied by
suitable certicates.
By acerticate we mean a nite strip of Turing machine tape, consisting of 0 or more squares, each
of which contains a symbol from the character set of the machine. A certicate can be loaded into a
Turing machine as follows. If the certicate contains m>0 tape squares, then replace the segment from
square number âˆ’mto square number âˆ’1, inclusive, of the Turing machine tape with the certicate. The
information on the certicate is then available to the program module just as any other information on the
tape is available.
To use a Turing machine as a checking orverifying computer, we place the input string xthat describes
the problem instance in squares 1 ;2;:::;B of the tape, and we place the certicate C(x)o fxin squares
âˆ’m;âˆ’m+1;:::;âˆ’1 of the tape. We then write a verifying program for the program module in which the
program veries that the string xis indeed a word in the language of the machine, and in the course of the
verication the program is quite free to examine the certicate as well as the problem instance.
A Turing machine that is being used as a verifying computer is called a nondeterministic machine. The
hardware is the same, but the manner of input and the question that is being asked are dierent from the
111
Chapter 5: NP-completeness
situation with a deterministic Turing machine, in which we decide whether or not the input string is in the
language, without using any certicates.
The class NP (â€˜Nondeterministic Polynomialâ€™) consists of those decision problems for which there exists
a fast (polynomial time) algorithm that will verify, given a problem instance string xand a suitable certicate
C(x), thatxbelongs to the language recognized by the machine, and for which, if xdoes not belong to the
language, nocerticate would cause an accepting computation to ensue.
5.3 Cookâ€™s Theorem
The NP-complete problems are the hardest problems in NP, in the sense that if Q0is any decision
problem in NP and Qis an NP-complete problem, then every instance of Q0is polynomially reducible to an
instance of Q. As we have already remarked, the surprising thing is that there is an NP-complete problem
at all, since it is not immediately clear why any single problem should hold the key to the polynomial time
solvability of every problem in the class NP. But there is one. As soon as we see why there is one, then weâ€™ll
be able to see more easily why there are hundreds of them, including many computational questions aboutdiscrete structures such as graphs, networks and games and about optimization problems, about algebraic
structures, formal logic, and so forth.
Here is the satisability problem , the rst problem that was proved to be NP-complete, by Stephen Cook
in 1971.
We begin with a list of (Boolean) variables x
1;:::;x n.Aliteral is either one of the variables xior the
negation of one of the variables, as  xi. There are 2 npossible literals.
Aclause is a set of literals.
The rules of the game are these. We assign the value â€˜Trueâ€™ (T) or â€˜Falseâ€™ (F), to each one of the
variables . Having done that, each one of the literals inherits a truth value, namely a literal xihas the same
truth or falsity as the corresponding variable xi, and a literal  xihas the opposite truth value from that of
the variable xi.
Finally each of the clauses also inherits a truth value from this process, and it is determined as follows.
A clause has the value â€˜Tâ€™ if and only if at least one of the literals in that clause has the value â€˜T,â€™ and
otherwise it has the value â€˜F.â€™
Hence starting with an assignment of truth values to the variables, some true and some false, we end
up with a determination of the truth values of each of the clauses, some true and some false.
Denition. A set of clauses is satisable if there exists an assignment of truth values to the variables that
makes all of the clauses true.
Think of the word â€˜orâ€™ as being between each of the literals in a clause, and the word â€˜andâ€™ as being
between the clauses.
The satisability problem (SAT). Given a set of clauses. Does there exist a set of truth values (=T or
F), one for each variable, such that every clause contains at least one literal whose value is T (i.e., such that
every clause is satised)?
Example: Consider the set x1;x2;x3of variables. From these we might manufacture the following list of
four clauses:
fx1;x2g;fx1;x3g;fx2;x3g;fx1;x3g:
If we choose the truth values ( T;T;F ) for the variables, respectively, then the four clauses would
acquire the truth values ( T;T;T;F ), and so this would not be a satisfying truth assignment for the set
of clauses. There are only eight possible ways to assign truth values to three variables, and after a little
more experimentation we might nd out that these clauses would in fact be satised if we were to make the
assignments ( T;T;T ) (how can we recognize a set of clauses that is satised by assigning to every variable
the value â€˜Tâ€™?).
The example already leaves one with the feeling that SAT might be a tough computational problem,
because there are 2npossible sets of truth values that we might have to explore if we were to do an exhaustive
search.
It is quite clear, however, that this problem belongs to NP. Indeed, it is a decision problem. Furthermore
we can easily assign a certicate to every set of clauses for which the answer to SAT is â€˜Yes, the clauses
112
5.3 Cookâ€™s Theorem
are satisable.â€™ The certicate contains a set of truth values, one for each variable, that satisfy all of the
clauses. A Turing machine that receives the set of clauses, suitably encoded, as input, along with the above
certicate, would have to verify only that if the truth values are assigned to the variables as shown on the
certicate then indeed every clause does contain at least one literal of value â€˜T.â€™ That verication is certainlya polynomial time computation.
Now comes the hard part. We want to show
Theorem 5.3.1. (S. Cook, 1971): SAT is NP-complete.
Before we carry out the proof, it may be helpful to give a small example of the reducibility ideas that
we are going to use.
Fig. 5.3.1: A 3-coloring problem
Example. Reducing graph-coloring to SAT
Consider the graph Gof four vertices that is shown in Fig. 5.3.1, and the decision problem â€˜Can the
vertices ofGbe properly colored in 3 colors?â€™
Letâ€™s see how that decision problem can be reduced to an instance of SAT. We will use 12 Boolean
variables: the variable xi;jcorresponds to the assertion that â€˜vertex ihas been colored in color jâ€™(i=
1;2;3;4;j=1;2;3).
The instance of SAT that we construct has 31 clauses. The rst 16 of these are
C(i): =fxi;1;xi;2;xi;3g (i=1;2;3;4)
T(i): =fxi;1;xi;2g (i=1;2;3;4)
U(i): =fxi;1;xi;3g (i=1;2;3;4)
V(i): =fxi;2;xi;3g (i=1;2;3;4):(5:3:1)
In the above, the four clauses C(i) assert that each vertex has been colored in at least one color. The
clausesT(i) say that no vertex has both color 1 and color 2. Similarly the clauses U(i)(resp.V(i)) guarantee
that no vertex has been colored 1 and 3 ( resp. 2a n d3 ) .
All 16 of the clauses in (5.3.1) together amount to the statement that â€˜each vertex has been colored in
one and only one of the three available colors.â€™
Next we have to construct the clauses that will assure us that the two endpoints of an edge of the graph
are never the same color. For this purpose we dene, for each edge eof the graph Gand colorj(=1,2,3),
a clauseD(e;j) as follows. Let uandvbe the two endpoints of e;t h eD(e;j): =fxu;j;xv;jg, which asserts
that not both endpoints of the edge ehave the same color j.
The original instance of the graph coloring problem has now been reduced to an instance of SAT. In
more detail, there exists an assignment of values T, F to the 12 Boolean variables x1;1;:::;x 4;3such that
each of the 31 clauses contains at least one literal whose value is T if and only if the vertices of the graph G
can be properly colored in three colors. The graph is 3-colorable if and only if the clauses are satisable.
It is clear that if we have an algorithm that will solve SAT, then we can also solve graph coloring
problems. A few moments of thought will convince the reader that the transformation of one problem to the
other that was carried out above involves only a polynomial amount of computation, despite the seeminglylarge number of variables and clauses. Hence graph coloring is quickly reducible to SAT.
113
Chapter 5: NP-completeness
Proof of Cookâ€™s theorem
We want to prove that SAT is NP-complete, i.e., that every problem in NP is polynomially reducible
to an instance of SAT. Hence let Qbe some problem in NP and let Ibe an instance of problem Q. SinceQ
is in NP there exists a Turing machine that recognizes encoded instances of problem Q, if accompanied by
a suitable certicate, in polynomial time.
Let TMQ be such a Turing machine, and let P(n) be a polynomial in its argument nwith the property
that TMQ recognizes every pair ( x;C(x)), wherexis a word in the language QandC(x) is its certicate,
in time P(n), wherenis the length of x.
We intend to construct, corresponding to each word Iin the language Q, and instance f(I)o fS A Tf o r
which the answer is â€˜Yes, the clauses are all simultaneously satisable.â€™ Conversely, if the word Iis not in
the language Q, the clauses will not be satisable.
The idea can be summarized like this: the instance of SAT that will be constructed will be a collection
of clauses that together express the fact that there exists a certicate that causes Turing machine TMQ to do
an accepting calculation. Therefore, in order to test whether or not the word Qbelongs to the language, it
suces to check that the collection of clauses is satisable.
To construct an instance of SAT means that we are going to dene a number of variables, of literals,
and of clauses, in such a way that the clauses are satisable if and only if xis in the language Q,i.e.,t h e
machine TMQ accepts xand its certicate.
What we must do, then, is to express the accepting computation of the Turing machine as the simul-
taneous satisfaction of a number of logical propositions. It is precisely here that the relative simplicity of a
Turing machine allows us to enumerate all of the possible paths to an accepting computation in a way that
would be quite unthinkable with a â€˜realâ€™ computer.
Now we will describe the Boolean variables that will be used in the clauses under construction.VariableQ
i;kis true if after step iof the checking calculation it is true that the Turing machine TMQ
is in stateqk, false otherwise.
VariableSi;j;a=fafter stepi,s y m b o lais in tape square jg.
VariableTi;j=fafter stepi, the tape head is positioned over square jg.
Letâ€™s count the variables that weâ€™ve just introduced. Since the Turing machine TMQ does its accepting
calculation in time P(n) it follows that the tape head will never venture more than P(n) squares away
from its starting position. Therefore the subscript j, which runs through the various tape squares that are
scanned during the computation, can assume only O(P(n)) dierent values.
Indexaruns over the letters in the alphabet that the machine can read, so it can assume at most some
xed number Aof values.
The indexiruns over the steps of the accepting computation, and so it takes at most O(P(n)) dierent
values.
Finally,kindexes the states of the Turing machine, and there is only some xed nite number, K,s a y ,
of states that TMQ might be in. Hence there are altogether O(P(n)2) variables, a polynomial number of
them.
Is it true that every random assignment of true or false values to each of these variables corresponds to
an accepting computation on ( x;C(x))? Certainly not. For example, if we arenâ€™t careful we might assign
true values to T9;4and toT10;33, thereby burning out the bearings on the tape transport mechanism! (why?)
Our remaining task, then, will be to describe precisely the conditions under which a set of values assigned
to the variables listed above actually denes a possible accepting calculation for ( x;C(x)). Then we will be
sure that whatever set of satisfying values of the variables might be found by solving the SAT problem, they
will determine a real accepting calculation of the machine TMQ.
This will be done by requiring that a number of clauses be all true (â€˜satisedâ€™) at once, where each
clause will exprss one necessary condition. In the following, the bold face type will describe, in words, the
condition that we want to express, and it will be followed by the formal set of clauses that actually expressesthe condition on input to SAT.
At each step, the machine is in at least one state.
H e n c ea tl e a s to n eo ft h e Kavailable state variables must be true. This leads to the rst set of clauses,
114
At stepP(n)the machine is in state qY.
one for each step iof the computation:
fQi;1;Qi;2;:::;Q i;Kg
SinceiassumesO(P(n)) values, these are O(P(n)) clauses.
At each step, the machine is not in more than one state
Therefore, for each step i, and each pair j0,j00of distinct states, the clause
fQi;j0;Qi;j00g
must be true. These are O(P(n)) additonal clauses to add to the list, but still more are needed.
At each step, each tape square contains exactly one symbol from the alphabet of the machine.
This leads to two lists of clauses which require, rst, that there is at least one symbol in each square at
each step, and second, that there are not two symbols in each square at each step. The clauses that do this
are
fSi;j;1;Si;j;2;:::;S i;j;Ag
whereAis the number of letters in the machineâ€™s alphabet, and
fSi;j;k0;Si;j;k00g
for each step i, squarej, and pairk0;k00of distinct symbols in the alphabet of the machine.
The reader will by now have gotten the idea of how to construct the clauses, so for the next three
categories we will simply list the functions that must be performed by the corresponding lists of clauses, andleave the construction of the clauses as an exercise.
At each step, the tape head is positioned over a single square.Initially the machine is in state 0, the head is over square 1, the input string xis in squares 1
ton,a n dC(x)(the input certicate of x) is in squares 0, -1, ..., âˆ’P(n).
At stepP(n)the machine is in state q
Y.
The last set of restrictions is a little trickier:
At each step the machine moves to its next conguration (state, symbol, head position) in
accordance with the application of its program module to its previous (state, symbol).
To nd the clauses that will do this job, consider rst the following condition: the symbol in square j
of the tape cannot change during step iof the computation if the tape head isnâ€™t positioned there at that
moment. This translates into the collection
fTi;j;Si;j;k;Si+1;j;kg
of clauses, one for each triple ( i;j;k) = (state, square, symbol). These clauses express the condition in the
following way: either (at time i) the tape head is positioned over square j(Ti;jis true) or else the head is
not positioned there, in which case either symbol kis not in the jth square before the step or else symbol k
is (still) in the jth square after the step is executed.
It remains to express the fact that the transitions from one conguration of the machine to the next are
the direct results of the operation of the program module. The three sets of clauses that do this are
fTi;j;Qi;k;Si;j;l;Ti+1;j+INCg
fTi;j;Qi;k;Si;j;l;Qi+1;k0g
fTi;j;Qi;k;Si;j;l;Si+1;j;l0g:
115
Chapter 5: NP-completeness
In each case the format of the clause is this: â€˜either the tape head is not positioned at square j,o rt h e
present state is not qkor the symbol just read is not l, but if they are then ...â€™ There is a clause as above for
each stepi=0;:::;P (n) of the computation, for each square j=âˆ’P(n);P(n) of the tape, for each symbol
lin the alphabet, and for each possible state qkof the machine, a polynomial number of clauses in all. The
new conguration triple (INC ;k0;l0) is, of course, as computed by the program module.
Now we have constructed a set of clauses with the following property. If we execute a recognizing
computation on a string xand its certicate, in time at most P(n), then this computation determines a
set of (True, False) values for all of the variables listed above, in such a way that all of the clauses just
constructed are simultaneously satised.
Conversely if we have a set of values of the SAT variables that satisfy all of the clauses at once, then
that set of values of the variables describes a certicate that would cause TMQ to do a computation that
would recognize the string xand it also describes, in minute detail, the ensuing accepting computation that
TMQ would do if it were given xand that certicate.
Hence every language in NP can be reduced to SAT. It is not dicult to check through the above
construction and prove that the reduction is accomplishable in polynomial time. It follows that SAT isNP-complete.
5.4 Some other NP-complete problems
Cookâ€™s theorem opened the way to the identication of a large number of NP-complete problems. The
proof that Satisability is NP-complete required a demonstration that every problem in NP is polynomially
reducible to SAT. To prove that some other problem Xis NP-complete it will be sucient to prove that
SAT reduces to problem X. For if that is so then every problem in NP can be reduced to problem Xby
rst reducing to an instance of SAT and then to an instance of X.
In other words, life after Cookâ€™s theorem is a lot easier. To prove that some problem is NP-complete
we need show only that SAT reduces to it. We donâ€™t have to go all the way back to the Turing machine
computations any more. Just prove that if you can solve your problem then you can solve SAT. By Cookâ€™stheorem you will then know that by solving your problem you will have solved every problem in NP.
For the honor of being â€˜the second NP-complete problem,â€™ consider the following special case of SAT,
called 3-satisability , or 3SAT. An instance of 3SAT consists of a number of clauses, just as in SAT, except
that the clauses are permitted to contain no more than three literals each. The question, as in SAT, is â€˜Are
the clauses simultaneously satisable by some assignment of T, F values to the variables?â€™
Interestingly, though, the general problem SAT is reducible to the apparently more special problem
3SAT, which will show us
Theorem 5.4.1. 3-satisability is NP-complete.
Proof. Let an instance of SAT be given. We will show how to transform it quickly to an instance of 3SAT
that is satisable if and only if the original SAT problem was satisable.
More precisely, we are going to replace clauses that contain more than three literals with collections
of clauses that contain exactly three literals and that have the same satisability as the original. In fact,
suppose our instance of SAT contains a clause
fx
1;x2;:::;x kg(k4): (5:4:1)
Then this clause will be replaced by kâˆ’2 new clauses, utilizing kâˆ’3 new variables zi(i=1;:::;k âˆ’3) that
are introduced just for this purpose. The kâˆ’2 new clauses are
fx1;x2;z1g;fx3;z1;z2g;fx4;z2;z3g;:::;fxkâˆ’1;xk;zkâˆ’3g: (5:4:2)
We now make the following
Claim. Ifx
1;:::;x
kis an assignment of truth values to the xâ€™s for which the clause (5.4.1) is true, then there
exist assignments z
1;:::;z
kâˆ’3of truth values to the zâ€™s such that all of the clauses (5.4.2) are simultaneously
satised by ( x;z). Conversely, if ( x;z) is some assignment that satises all of (5.4.2), then xalone
satises (5.4.1).
116
5.4 Some other NP-complete problems
To prove the claim, rst suppose that (5.4.1) is satised by some assignment x. Then one, at least, of
thekliteralsx1;:::;x k,s a yxr, has the value â€˜T.â€™ Then we can satisfy all kâˆ’2 of the transformed clauses
(5.4.2) by assigning z
s:= â€˜T0forsrâˆ’2a n dz
s=â€˜F0fors>r âˆ’2. It is easy to check that each one of
thekâˆ’2 new clauses is satised.
Conversely, suppose that all of the new clauses are satised by some assignment of truth values to the
xâ€™s and thezâ€™s. We will show that at least one of the xâ€™s must be â€˜True,â€™ so that the original clause will be
satised.
Suppose, to the contrary, that all of the xâ€™s are false. Since, in the new clauses none of the xâ€™s are
negated, the fact that the new clauses are satised tells us that they would remain satised without any of
thexâ€™s. Hence the clauses
fz1g;fz1;z2g;fz2;z3g;:::;fzkâˆ’4;zkâˆ’3g;fzkâˆ’3g
are satised by the values of the zâ€™s. If we scan the list from left to right we discover, in turn, that z1is true,
z2is true,:::, and nally, much to our surprise, that zkâˆ’3is true, and zkâˆ’3is also false, a contradiction
which establishes the truth of the claim made above.
The observation that the transformations just discussed can be carried out in polynomial time completes
the proof of theorem 5.4.1.
We remark, in passing, that the problem â€˜2SATâ€™ is in P.
Our collection of NP-complete problems is growing. Now we have two, and a third is on the way. We
will show next how to reduce 3SAT to a graph coloring problem, thereby proving
Theorem 5.4.2. The graph vertex coloring problem is NP-complete.
Proof: Given an instance of 3SAT, that is to say, given a collection of kclauses, involving nvariables and
having at most three literals per clause, we will construct, in polynomial time, a graph Gwith the property
that its vertices can be properly colored in n+ 1 colors if and only if the given clauses are satisable. We
will assume that n>4, the contrary case being trivial.
The graph Gwill have 3n+kvertices:
fx1;:::;x ng;fx1;:::;xng;fy1;:::;y ng;fC1;:::;C kg
Now we will describe the set of edges of G. First each vertex xiis joined to  xi(i=1;:::;n ). Next, every
vertexyiis joined to every other vertex yj(j6=i), to every other vertex xj(j6=i), and to every vertex
xj(j6=i).
Vertexxiis connected to Cjifxiisnotone of the literals in clause Cj. Finally, xiis connected to Cj
if xiis not one of the literals in Cj.
May we interrupt the proceedings to say again why weâ€™re doing all of this? You have just read the
description of a certain graph G. The graph is one that can be drawn as soon as someone hands us a 3SAT
problem. We described the graph by listing its vertices and then listing its edges. What does the graph dofor us?
Well suppose that we have just bought a computer program that can decide if graphs are colorable in
a given number of colors. We paid $ 49.95 for it, and weâ€™d like to use it. But the rst problem that needs
solving happens to be a 3SAT problem, not a graph coloring problem. We arenâ€™t so easily discouraged,though. We convert the 3SAT problem into a graph that is ( n+1)-colorable if and only if the original 3SAT
problem was satisable. Now we can get our moneyâ€™s worth by running the graph coloring program even
though what we really wanted to do was to solve a 3SAT problem.
117
Chapter 5: NP-completeness
In Fig. 5.4.1 we show the graph Gof 11 vertices that correesponds to the following instance of 3SAT:
Fig. 5.4.1: The graph for a 3SAT problem
Now we claim that this graph is n+ 1 colorable if and only if the clauses are satisable.
ClearlyGcannot be colored in fewer than ncolors, because the nverticesy1;:::;y nare all connected
to each other and therefore they alone already require ndierent colors for a proper coloration. Suppose
thatyiis assigned color i(i=1;:::;n ).
Do we need new colors in order to color the xivertices? Since vertex yiis connected to every xvertex
and every xvertex except xi,xi, if coloriis going to be used on the xâ€™s or the xâ€™s, it will have to be assigned
to one ofxi,xi, but not to both, since they are connected to each other. Hence a new color, color n+1, will
have to be introduced in order to color the xâ€™s and xâ€™s.
Further, if we are going to color the vertices of Gin onlyn+ 1 colors, the only way to do it will be
to assign color n+ 1 to exactly one member of each pair ( xi;xi), and color ito the other one, for each
i=1;:::;n . That one of the pair that gets color n+ 1 will be called the False vertex, the other one is the
Truevertex of the pair ( xi;xi), for eachi=1;:::;n .
It remains to color the vertices C1;:::;C k. The graph will be n+1 colorable if and only if we can do this
without using any new colors. Since each clause contains at most three literals, and n>4, every variable Ci
must be adjacent to both xjand xjfor at least one value of j. Therefore no vertex Cican be colored in the
colorn+ 1 in a proper coloring of G, and therefore every Cimust be colored in one of the colors 1 ;:::;n .
SinceCiis connected by an edge to every vertex xjor xjthat is not in the clause Ci, it follows that Ci
cannot be colored in the same color as any xjor xjthat is not in the clause Ci.
Hence the color that we assign to Cimust be the same as the color of some â€˜ Trueâ€™v e r t e xXjor xjthat
corresponds to a literal that is in clause Ci. Therefore the graph is n+ 1 colorable if and only if there is a
â€˜Trueâ€™ vertex for each Ci, and this means exactly that the clauses are satisable.
It is easy to verify that the transformation from the 3SAT problem to the graph coloring problem can
be carried out in polynomial time, and the proof is nished.
By means of many, often quite ingenious, transformations of the kind that we have just seen, the list of
NP-complete problems has grown rapidly since the rst example, and the 21 additional problems found by
R. Karp. Hundreds of such problems are now known. Here are a few of the more important ones.
118
5.5 Half a loaf ...
Maximum clique: We are given a graph Gand an integer K. The question is to determine whether or
not there is a set of Kvertices inG, each of which is joined, by an edge of G, to all of the others.
Edge coloring: Given a graph Gand an integer K. Can we color the edges ofGinKcolors, so that
whenever two edges meet at a vertex, they will have dierent colors?
Let us refer to an edge coloring of this kind as a proper coloring of the edges of G.
A beautiful theorem of Vizingdeals with this question. If  denotes the largest degree of any vertex
in the given graph, the Vizingâ€™s theorem asserts that the edges of Gcan be properly colored in either  or
 + 1 colors. Since it is obvious that at least  colors will be needed, this means that the edge chromatic
number is in doubt by only one unit, for every graph G! Nevertheless the decision as to whether the correct
answer is  or  + 1 is NP-complete.
Hamilton path: In a given graph G, is there a path that visits every vertex of Gexactly once?
Target sum: Given a nite set of positive integers whose sum is S. Is there a subset whose sum is S=2?
The above list, together with SAT, 3SAT, Travelling Salesman and Graph Coloring, constitutes a modest
sampling of the class of these seemingly intractable problems. Of course it must not be assumed that everyproblem that â€˜sounds likeâ€™ an NP-complete problem is necessarily so hard. If for example we ask for an Euler
path instead of a Hamilton path ( i.e., if we want to traverse edges rather than vertices ) the problem would
no longer be NP-complete, and in fact it would be in P, thanks to theorem 1.6.1.
As another example, the fact that one can nd the edge connectivity of a given graph in polynomial
time (see section 3.8) is rather amazing considering the quite dicult appearance of the problem. One
of our motivations for including the network ï¬‚ow algorithms in this book was, indeed, to show how very
sophisticated algorithms can sometimes prove that seemingly hard problems are in fact computationally
tractable.
Exercises for section 5.4
1. Is the claim that we made and proved above (just after (5.4.2)) identical with the statement that the
clause (5.4.1) is satisable if and only if the clauses (5.4.2) are simultaneously satisable? Discuss.
2. Is the claim that we made and proved above (just after (5.4.2)) identical with the statement that the
Boolean expression (5.4.1) is equal to the product of the Boolean expressions (5.4.2) in the sense that their
truth values are identical on every set of inputs? Discuss.
3. Let it be desired to nd out if a given graph G,o fVvertices, can be vertex colored in Kcolors. If we
transform the problem into an instance of 3SAT, exactly how many clauses will there be?
5.5 Half a loaf ...
If we simply haveto solve an NP-complete problem, then we are faced with a very long computation. Is
there anything that can be done to lighten the load? In a number of cases various kinds of probabilistic and
approximate algorithms have been developed, some very ingenious, and these may often be quite serviceable,
as we have already seen in the case of primality testing. Here are some of the strategies of â€˜nearâ€™ solutions
that have been developed.
Type I: â€˜Almost surely ...â€™
Suppose we have an NP-complete problem that asks if there is a certain kind of substructure embedded
inside a given structure. Then we may be able to develop an algorithm with the following properties:
(a) It always runs in polynomial time
(b) When it nds a solution then that solution is always a correct one
(c) It doesnâ€™t always nd a solution, but it â€˜almost alwaysâ€™ does, in the sense that the ratio of successes to
total cases approaches unity as the size of the input string grows large.
An example of such an algorithm is one that will nd a Hamilton path in almost all graphs, failing to
do so sometimes, but not often, and running always in polynomial time. We will describe such an algorithm
below.
V. G. Vizing, On an estimate of the chromatic class of a p-graph (Russian), Diskret. Analiz. 3(1964),
25-30.
119
Chapter 5: NP-completeness
Type II: â€˜Usually fast ...â€™
In this category of quasi-solution are algorithms in which the uncertainty lies not in whether a solution
will be found, but in how long it will take to nd one. An algorithm of this kind will
(a) always nd a solution and the solution will always be correct, and
(b) operate in an average of subexponential time, although occasionally it may require exponential time.
The averaging is over all input strings of a given size.
An example of this sort is an algorithm that will surely nd a maximum independent set in a graph,
will on the average require â€˜onlyâ€™ O(nclogn) time to do so, but will occasionally, i.e., for some graphs, require
nearly 2ntime to get an answer. We will outline such an algorithm below, in section 5.6. Note that O(nclogn)
is not a polynomial time estimate, but itâ€™s an improvement over 2n.
Type II: â€˜Usually fast ...â€™
In this kind of an algorithm we donâ€™t even get the right answer, but itâ€™s close. Since this means giving
up quite a bit, people like these algorithms to be very fast. Of course we are going to drop our insistence
that the questions be posed as decision problems, and instead they will be asked as optimization problems:nd the shortest tour through these cities, or, nd the size of the maximum clique in this graph, or, nd a
coloring of this graph in the fewest possible colors, etc.
In response these algorithms will
(a) run in polynomial time
(b) always produce some output
(c) provide a guarantee that the output will not deviate from the optimal solution by more than such-and-
such.
An example of this type is the approximate algorithm for the travelling salesman problem that is given
below, in section 5.8. It quickly yields a tour of the cities that is guaranteed to be at most twice as long as
the shortest possible tour.
Now letâ€™s look at examples of each of these kinds of approximation algorithms.
An example of an algorithm of Type I is due to Angluin and Valiant. It tries to nd a Hamilton path
(or circuit) in a graph G. It doesnâ€™t always nd such a path, but in theorem 5.5.1 below we will see that it
usually does, at least if the graph is from a class of graphs that are likely to have Hamilton paths at all.
Input to the algorithm are the graph Gand two distinguished vertices s;t. It looks for a Hamilton path
between the vertices s;t(ifs=ton input then we are looking for a Hamilton circuit in G).
The procedure maintains a partially constructed Hamilton path P, fromsto some vertex ndp, and it
attempts to extend Pby adjoining an edge to a new, previously unvisited vertex. In the process of doing
so it will delete from the graph G, from time to time, an edge, so we will also maintain a variable graph G
0,
that is initially set to G, but which is acted upon by the program.
To do its job, the algorithm chooses at random an edge ( ndp;v ) that is incident with the current endpoint
of the partial path P, and it deletes the edge ( ndp;v ) from the graph G0, so it will never be chosen again. If
vis a vertex that is not on the path Pthen the path is extended by adjoining the new edge ( ndp;v ).
So much is fairly clear. However if the new vertex vis already on the path P, then we short circuit the
path by deleting an edge from it and drawing in a new edge, as is shown below in the formal statement of
the algorithm, and in Fig. 5.5.1. In that case the path does not get longer, but it changes so that it now has
120
5.5 Half a loaf ...
enhanced chances of ultimate completion.
Fig. 5.5.1: The short circuit
Here is a formal statement of the algorithm of Angluin and Valiant for nding a Hamilton path or circuit
in an undirected graph G.
procedureuhc(G:graph;s;t:v e r t e x ) ;
fnds a Hamilton path (if s6=t) or a Hamilton
circuit (ifs=t)Pin an undirected graph G
and returns â€˜success â€™, or fails, and returns â€˜failure â€™g
G0:=G;ndp:=s;P:= empty path;
repeat
ifndpis an isolated point of G0
then return â€˜failure â€™
else
choose uniformly at random an edge ( ndp;v ) from
among the edges of G0that are incident with ndp
and delete that edge from G0;
ifv6=tandv=2P
then adjoin the edge ( ndp;v )t oP;ndp:=v
else
ifv6=tandv2P
then
fThis is the short-circuit of Fig. 5.5.1 g
u:= neighbor of vinPthat is closer to ndp;
delete edge ( u;v) fromP;
adjoin edge ( ndp;v )t oP;
ndp:=u
end;ftheng
endfelseg
untilPcontains every vertex of G(exceptT,i f
s6=t)a n de d g e( ndp;t)i si nGbut not inG0;
adjoin edge ( ndp;t)t oPand return â€˜successâ€™
end.fuhcg
As stated above, the algorithm makes only a very modest claim: either it succeeds or it fails! Of course
what makes it valuable is the accompanying theorem, which asserts that in fact the procedure almost always
succeeds, provided the graph Ghas a good chance of having a Hamilton path or circuit.
121
Chapter 5: NP-completeness
What kind of graph has such a â€˜good chanceâ€™? A great deal of research has gone into the study of how
many edges a graph has to have before almost surely it must contain certain given structures. For instance,
how many edges must a graph of nvertices have before we can be almost certain that it will contain a
complete graph of 4 vertices?
To say that graphs have a property â€˜almost certainlyâ€™ is to say that the ratio of the number of graphs
onnvertices that have the property to the number of graphs on nvertices approaches 1 as ngrows without
bound.
For the Hamilton path problem, an important dividing line, or threshold, turns out to be at the level
ofclognedges. That is to say, a graph of nvertices that has o(nlogn) edges has relatively little chance
of being even connected, whereas a graph with >c nlognedges is almost certainly connected, and almost
certainly has a Hamilton path.
We now state the theorem of Angluin and Valiant, which asserts that the algorithm above will almost
surely succeed if the graph Ghas enough edges.
Theorem 5.5.1. Fix a positive real number a. There exist numbers Mandcsuch that if we choose a graph
Gat random from among those of nvertices and at least cnlognedges, and we choose arbitrary vertices s;t
inG, then the probability that algorithm UHC returns â€˜successâ€™ before making a total of Mnlognattempts
to extend partially constructed paths is 1âˆ’O(nâˆ’a).
5.6 Backtracking (I): independent sets
In this section we are going to describe an algorithm that is capable of solving some NP-complete
problems fast, on the average , while at the same time guaranteeing that a solution will always be found, be
it quickly or slowly.
The method is called backtracking , and it has long been a standard method in computer search problems
when all else fails. It has been common to think of backtracking as a very long process, and indeed it can be.
But recently it has been shown that the method can be very fast on average, and that in the graph coloring
problem, for instance, it functions in an average of constant time,i.e.,the time is independent of the number
of vertices, although to be sure, the worst-case behavior is very exponential.
We rst illustrate the backtrack method in the context of a search for the largest independent set of
vertices (a set of vertices no two of which are joined by an edge) in a given graph G, an NP-complete
problem. In this case the average time behavior of the method is not constant, or even polynomial, but is
subexponential. The method is also easy to analyze and to describe in this case.
Hence consider a graph Gofnvertices, in which the vertices have been numbered 1 ;2;:::;n .W ew a n t
to nd, in G, the size of the largest independent set of vertices. In Fig. 5.6.1 below, the graph Ghas 6
vertices.
Fig. 5.6.1: Find the largest independent set
Begin by searching for an independent set Sthat contains vertex 1, so let S:=f1g. Now attempt to
enlargeS. We cannot enlarge Sby adjoining vertex 2 to it, but we can add vertex 3. Our set Sis now
f1;3g.
Now we cannot adjoin vertex 4 (joined to 1) or vertex 5 (joined to 1) or vertex 6 (joined to 3), so we are
stuck. Therefore we backtrack, by replacing the most recently added member of Sby the next choice that
we might have made for it. In this case, we delete vertex 3 from S, and the next choice would be vertex 6.
The setSisf1;6g. Again we have a dead end.
If we backtrack again, there are no further choices with which to replace vertex 6, so we backtrack even
further, and not only delete 6 from Sbut also replace vertex 1 by the next possible choice for it, namely
vertex 2.
122
5.6 Backtracking (I): independent sets
To speed up the discussion, we will now show the list of all sets Sthat turn up from start to nish of
the algorithm:
f1g;f13g;f16g;f2g;f24g;f245g;f25g;f3g;
f34g;f345g;f35g;f4g;f45g;f5g;f6g
A convenient way to represent the search process is by means of the backtrack search tree T. This is
a tree whose vertices are arranged on levels L:= 0;1;2;:::;n for a graph of nvertices. Each vertex of T
corresponds to an independent set of vertices in G. Two vertices of T, corresponding to independent sets
S0;S00of vertices of G, are joined by an edge in TifS0S00,a n dS00âˆ’S0consists of a single element: the
highest-numbered vertex in S00. On levelLwe nd a vertex SofTfor every independent set of exactly L
vertices ofG. Level 0 consists of a single root vertex, corresponding to the empty set of vertices of G.
The complete backtrack search tree for the problem of nding a maximum independent set in the graph
Gof Fig. 5.6.1 is shown in Fig. 5.6.2 below.
Fig. 5.6.2: The backtrack search tree
The backtrack algorithm amounts just to visiting every vertex of the search tree T, without actually
having to write down the tree explicitly, in advance.
Observe that the list of sets Sabove, or equivalently, the list of nodes of the tree T, consists of exactly
every independent set in the graph G. A reasonable measure of the complexity of the searching job, therefore,
is the number of independent sets that Ghas. In the example above, the graph Ghad 19 independent sets
of vertices, including the empty set.
The question of the complexity of backtrack search is therefore the same as the question of determining
the number of independent sets of the graph G.
Some graphs have an enormous number of independent sets. The graph Knofnvertices and no edges
whatever has 2nindependent sets of vertices. The backtrack tree will have 2nnodes, and the search will be
a long one indeed.
The complete graph Knofnvertices and every possible edge, n(nâˆ’1)=2 in all, has just n+1 independent
sets of vertices.
Any other graph Gofnvertices will have a number of independent sets that lies between these two
extremes of n+1a n d2n. Sometimes backtracking will take an exponentially long time, and sometimes it
will be fairly quick. Now the question is, on the average how fast is the backtrack method for this problem?
What we are asking for is the average number of independent sets that a graph of nvertices has. But
that is the sum, over all vertex subsets Sf1;:::;n g, of the probability that Sis an independent set. If
Shaskvertices, then the probability that Sis independent is the probability that, among the k(kâˆ’1)=2
possible edges that might join a pair of vertices in S, exactly zero of these edges actually live in the random
graphG. Since each of these/parenleftbigk
2
edges has a probability 1 =2 of appearing in G, the probability that none of
them appear is 2âˆ’k(kâˆ’1)=2. Hence the average number of independent sets in a graph of nvertices is
In=nX
k=0n
k
2âˆ’k(kâˆ’1)=2: (5:6:1)
123
Chapter 5: NP-completeness
Hence in (5.6.1) we have an exact formula for the average number of independent sets in a graph of n
vertices. A short table of values of Inis shown below, in Table 5.6.1, along with values of 2n, for comparison.
Clearly the average number of independent sets in a graph is a lot smaller than the maximum number that
graphs of that size might have.
nI n 2n
23:54
35:68
48:51 6
51 2:33 2
10 52 1024
15 149:8 32768
20 350:6 1048576
30 1342:5 1073741824
40 3862:9 1099511627776
Table 5.6.1: Independent sets and all sets
In the exercises it will be seen that the rate of growth of Inasngrows large is O(nlogn). Hence the
average amount of labor in a backtrack search for the largest independent set in a graph grows subexponen-
tially, although faster than polynomially. It is some indication of how hard this problem is that even on theaverage the amount of labor needed is not of polynomial growth.
Exercises for section 5.6
1. What is the average number of independent sets of size kthat are in graphs of Vvertices and Eedges?
2. Lett
kdenote the kth term in the sum (5.6.1).
(a) Show that tk=tkâˆ’1=(nâˆ’k+1 )=(k2k+1).
(b) Show that tk=tkâˆ’1is>1w h e nkis small, then is <1a f t e rkpasses a certain critical value k0. Hence
show that the terms in the sum (5.6.1) increase in size until k=k0and then decrease.
3. Now we will estimate the size of k0in the previous problem.
(a) Show that tk<1w h e nk=blog2ncandtk>1w h e nk=blog2nâˆ’log2log2nc. Hence the index k0of
the largest term in (5.6.1) satises
blog2nâˆ’log2log2nck0blog2nc
(b) The entire sum in (5.6.1) is at most n+1 times as large as its largest single term. Use Stirlingâ€™s formula
(1.1.10) and 3(a) above to show that the k0th term isO((n+)logn) and therefore the same is true of
the whole sum, i.e.,o fIn.
5.7 Backtracking (II): graph coloring
In another NP-complete problem, that of graph-coloring, the average amount of labor in a backtrack
search isO(1) (bounded) as n, the number of vertices in the graph, grows without bound. More precisely,
for xedK, if we ask â€˜Is the graph G,o fVvertices, properly vertex-colorable in Kcolors?,â€™ then the average
labor in a backtrack search for the answer is bounded. Hence not only is the average of polynomial growth,
but the polynomial is of degree 0 (in V).
To be even more specic, consider the case of 3 colors. It is already NP-complete to ask if the vertices of
a given graph can be colored in 3 colors. Nevertheless, the average number of nodes in the backtrack search
tree for this problem is about 197 , averaged over all graphs of all sizes. This means that if we input a random
graph of 1,000,000 vertices, and ask if it is 3-colorable, then we can expect an answer (probably â€˜Noâ€™) after
only about 197 steps of computation.
To prove this we will need some preliminary lemmas.
124
5.7 Backtracking (II): graph coloring
Lemma 5.7.1. Lets1;:::;s Kbe nonnegative numbers whose sum is L. Then the sum of their squares is
at leastL2=K.
Proof: We have
0KX
i=1(siâˆ’L
K)2
=KX
i=1(s2
iâˆ’2Lsi
K+L2
K2)
=KX
i=1s2
iâˆ’2L2
K+L2
K
=KX
i=1s2
iâˆ’L2
K:
The next lemma deals with a kind of inside-out chromatic polynomial question. Instead of asking â€˜How
many proper colorings can a given graph have?,â€™ we ask â€˜How many graphs can have a given proper coloring?â€™
Lemma 5.7.2. LetCbe one of the KLpossible ways to color in Kcolors a set of Labstract vertices
1;2;:::;L . Then the number of graphs Gwhose vertex set is that set of Lcolored vertices and for which C
is a proper coloring of Gis at most 2L2(1âˆ’1=K)=2.
Proof: In the coloring C, supposes1vertices get color 1, :::;s Kget colorK, where, of course, s1++sK=
L. If a graph Gis to admit Cas a proper vertex coloring then its edges can be drawn only between vertices
of dierent colors. The number of edges that Gmight have is therefore
s1s2+s1s3++s1sK+s2s3++s2sK++sKâˆ’1sK
for which we have the following estimate:
X
1i<jKsisj=1
2X
i6=jsisj
=1
2KX
i;j=1sisjâˆ’KX
i=1s2
i
=1
2(X
si)2âˆ’1
2X
s2
i
L2
2âˆ’1
2L2
K(by lemma 5 :7:1)
=L2
2(1âˆ’1
K):(5:7:1)
The number of possible graphs Gis therefore at most 2L2(1âˆ’1=K)=2.
Lemma 5.7.3. The total number of proper colorings in Kcolors of all graphs of Lvertices is at most
KL2L2(1âˆ’1=K)=2:
Proof: We are counting the pairs ( G;C), where the graph GhasLvertices and Cis a proper coloring of
G. If we keep Cxed and sum on G, then by lemma 5.7.2 the sum is at most 2L2(1âˆ’1=K)=2. Since there are
KLsuchCâ€™s, the proof is nished.
Now letâ€™s think about a backtrack search for a K-coloring of a graph. Begin by using color 1 on vertex
1. Then use color 1 on vertex 2 unless (1 ;2) is an edge, in which case use color 2. As the coloring progresses
through vertices 1 ;2;:::;L we color each new vertex with the lowest available color number that does not
cause a conï¬‚ict with some vertex that has previously been colored.
125
Chapter 5: NP-completeness
At some stage we may reach a dead end: out of colors, but not out of vertices to color. In the graph of
Fig. 5.7.1 if we try to 2-color the vertices we can color vertex 1 in color 1, vertex 2 in color 2, vertex 3 in
color 1 and then weâ€™d be stuck because neither color would work on vertex 4.
Fig. 5.7.1: Color this graph
When a dead end is reached, back up to the most recently colored vertex for which other color choices
are available, replace its color with the next available choice, and try again to push forward to the nextvertex.
The (futile) attempt to color the graph in Fig. 5.7.1 with 2 colors by the backtrack method can be
portrayed by the backtrack search tree in Fig. 5.7.2.
The search is thought of as beginning at â€˜Root.â€™ The label at each node of the tree describes the
colors of the vertices that have so far been colored. Thus â€˜212â€™ means that vertices 1,2,3 have been colored,respectively, in colors 2,1,2.
Fig. 5.7.2: A frustrated search tree
Fig. 5.7.3: A happy search tree
126
5.7 Backtracking (II): graph coloring
If instead we use 3 colors on the graph of Fig. 5.7.1 then we get a successful coloring; in fact we get 12
of them, as is shown in Fig. 5.7.3.
Letâ€™s concentrate on a particular levelof the search tree. Level 2, for instance, consists of the nodes of
the search tree that are at a distance 2 from â€˜Root.â€™ In Fig. 5.7.3, level 2 contains 6 nodes, correspoondingto the partial colorings 12, 13, 21, 23, 31, 32 of the graph. When the coloring reaches vertex 2 it has seen
only the portion of the graph Gthat is induced by vertices 1 and 2.
Generally, a node at level Lof the backtrack search tree corresponds to a proper coloring in Kcolors
of the subgraph of Gthat is induced by vertices 1 ;2;:::;L .
LetH
L(G) denote that subgraph. Then we see the truth of
Lemma 5.7.4. The number of nodes at level Lof the backtrack search tree for coloring a graph GinK
colors is equal to the number of proper colorings of HL(G)inKcolors, i.e.,t oP(K;H L(G)),w h e r ePis the
chromatic polynomial.
We are now ready for the main question of this section: what is the average number of nodes in a
backtrack search tree for K-coloring graphs of nvertices? This is
A(n;K)=1
no:of graphsX
graphs Gnfno:of nodes in tree for Gg
=2âˆ’(n
2)X
GnfnX
L=0fno:of nodes at level Lgg
=2âˆ’(n
2)X
GnnX
L=0P(K;H L(G)) (by lemma 5 :7:4)
=2âˆ’(n
2)nX
L=0fX
GnP(K;H L(G))g:
Fix some value of Land consider the inner sum. As Gruns over all graphs of Nvertices,HL(G) selects
the subgraph of Gthat is induced by vertices 1 ;2;:::;L . Now lots of graphs Gofnvertices have the same
HL(G) sitting at vertices 1 ;2;:::;L .I n f a c t e x a c t l y 2(n
2)âˆ’(L
2)dierent graphs Gofnvertices all have the
same graph HofLvertices in residence at vertices 1 ;2;:::;L (see exercise 15 of section 1.6). Hence (5.7.2)
gives
A(n;K)=2âˆ’(n
2)nX
L=02(n
2)âˆ’(L
2)X
HLP(K;H)/bracerightbig
=nX
L=02âˆ’(L
2)X
HLP(K;H)/bracerightbig
:
The inner sum is exactly the number that is counted by lemma 5.7.3, and so
A(n;K)nX
L=02âˆ’(L
2)KL2L2(1âˆ’1=K)=2
1X
L=0KL2L=22âˆ’L2=2K:
The innite series actually converges! Hence A(n;k) is bounded, for all n. This proves
Theorem 5.7.1. LetA(n;k)denote the average number of nodes in the backtrack search trees for K-
coloring the vertices of all graphs of nvertices. Then there is a constant h=h(K), that depends on the
number of colors, K, but not on n, such that A(n;k)h(K)for alln.
127
Chapter 5: NP-completeness
5.8 Approximate algorithms for hard problems
Finally we come to Type III of the three kinds of â€˜half-a-loaf-is-better-than-noneâ€™ algorithms that were
described in section 5.5. In these algorithms we donâ€™t nd the exact solution of the problem, only anapproximate one. As consolation we have an algorithm that runs in polynomial time as well as a performance
guarantee to the eect that while the answer is approximate, it can certainly deviate by no more than such-
and-such from the exact answer.
An elegant example of such a situation is in the Travelling Salesman Problem, which we will now express
as an optimization problem rather than as a decision problem.
We are given npoints (â€˜citiesâ€™) in the plane, as well as the distances between every pair of them, and we
are asked to nd a round-trip tour of all of these cities that has minimum length. We will assume throughoutthe following discussion that the distances satisfy the triangle inequality. This restriction of the TSP is often
called the â€˜Euclideanâ€™ Travelling Salesman Problem.
The algorithm that we will discuss for this problem has the properties
(a) it runs in polynomial time and
(b) the round-trip tour that it nds will never be more than twice as long as the shortest possible tour.
The rst step in carrying out the algorithm is to nd a minimum spanning tree (MST) for the ngiven
cities. A MST is a tree whose nodes are the cities in question, and which, among all possible trees on that
vertex set, has minimum possible length.
It may seem that nding a MST is just as hard as solving the TSP, but NIN (No, Itâ€™s Not). The MST
problem is one of those all-too-rare computational situations in which it pays to be greedy.
Generally speaking, in a greedy algorithm,
(i) we are trying to construct some optimal structure by adding one piece at a time, and
(ii) at each step we make the decision about which piece will be added next by choosing, among all
available pieces, the single one that will carry us as far as possible in the desirable direction (be
greedy!).
The reason that greedy algorithms are not usually the best possible ones is that it may be better not
to take the single best piece at each step, but to take some other piece, in the hope that at a later step we
will be able to improve things even more. In other words, the global problem of nding the best structure
might not be solveable by the localprocedure of being as greedy as possible at each step.
In the MST problem, though, the greedy strategy works, as we see in the following algorithm.
procedure mst(x:array ofnpoints in the plane);
fconstructs a spanning tree Tof minimum length, on the
vertices fx
1;:::;x ngin the plane g
letTconsist of a single vertex x1;
whileThas fewer than nvertices do
foreach vertex vthat is not yet in T, nd the
distanced(v) fromvto the nearest vertex of T;
letvbe a vertex of smallest d(v);
adjoinvto the vertex set of T;
adjoin toTthe edge from vto the nearest
vertexw6=vofT;
endfwhileg
end.fmstg
Proof of correctness of mst: LetTbe the tree that is produced by running mst, and lete1;:::;e nâˆ’1be
its edges, listed in the same order in which the alfgorithm mstproduced them.
LetT0be a minimum spanning tree for x. Leterbe the rst edge of Tthat does not appear in T0.I n
the minimum tree T0,e d g e se1;:::;e râˆ’1all appear, and we let Sbe the union of their vertex sets. In T0let
fbe the edge that joins the subtree on Sto the subtree on the remaining vertices of x.
Supposefis shorter than er.T h e nfwas one of the edges that was available to the algorithm mst
at the instant that it chose er, and since erwas the shortest edge available at that moment, we have a
contradiction.
128
5.7 Backtracking (II): graph coloring
Supposefis longer than er.T h e nT0would not be minimal because the tree that we would obtain by
exchanging fforerinT0( why is it still a tree if we do that exchange?) would be shorter, contradicting the
minimality of T0.
Hencefanderhave the same length. In T0exchangefforer.T h e nT0is still a tree, and is still a
minimum spanning tree.
The index of the rst edge of Tthat does not appear in T0is now at least r+ 1, one unit larger than
before. The process of replacing edges of Tthat do not appear in T0without aecting the minimality of T
can be repeated until every edge of Tappears in T0,i.e.,u n t i lT=T0. HenceTwas a minimum spanning
tree.
That nishes one step of the process that leads to a polynomial time travelling salesman algorithm that
nds a tour of at most twice the minimum length.
The next step involves nding an Euler circuit. Way back in theorem 1.6.1 we learned that a connected
graph has an Euler circuit if and only if every vertex has even degree. Recall that the proof was recursive
in nature, and immediately implies a linear time algorithm for nding Euler circuits recursively. We also
noted that the proof remains valid even if we are dealing with a multigraph , that is, with a graph in which
several edges are permitted between single pairs of vertices. We will in fact need that extra ï¬‚exibility for
the purpose at hand.
Now we have the ingredients for a quick near-optimal travelling salesman tour.
Theorem 5.8.1. There is an algorithm that operates in polynomial time and which will return a travelling
salesman tour whose length is at most twice the length of a minimum tour.
Here is the algorithm. Given the ncities in the plane:
(1) Find a minimum spanning tree Tfor the cities.
(2) Double each edge of the tree, thereby obtaining a â€˜multitreeâ€™ T(2)in which between each pair of
vertices there are 0 or 2 edges.
(3) Since every vertex of the doubled tree has even degree, there is an Eulerian tour Wof the edges of
T(2); nd one, as in the proof of theorem 1.6.1.
(4) Now we construct the output tour of the cities. Begin at some city and follow the walk W. However,
having arrived at some vertex v, go fromvdirectly (via a straight line) to the next vertex of the walk
Wthat you havenâ€™t visited yet. This means that you will often short-circuit portions of the walk W
by going directly from some vertex to another one that is several edges â€˜down the road.â€™
The tourZ0that results from (4) above is indeed a tour of all of the cities in which each city is visited
once and only once. We claim that its length is at most twice optimal.
LetZbe an optimum tour, and let ebe some edge of Z.T h e nZâˆ’eis a path that visits all of the
cities. Since a path is a tree, Zâˆ’eis a spanning tree of the cities, hence Zâˆ’eis at least as long as Tis,
and soZis surely at least as long as Tis.
Next consider the length of the tour Z0.As t e po f Z0that walks along an edge of the walk Whas length
equal to the length of that edge of W.As t e po f Z0that short circuits several edges of Whas length at most
equal to the sum of the lengths of the edges of Wthat were short-circuited. If we sum these inequalities
over all steps of Z0we nd that the length of Z0is at most equal to the length of W,w h i c hi si nt u r nt w i c e
the length of the tree T.
If we put all of this together we nd that
length(Z)>length(Zâˆ’e)length(T)=1
2length(W)1
2length(Z0)
as claimed (!)
More recently it has been proved (Cristodes, 1976) that in polynomial time we can nd a TSP tour
whose total length is at most 3/2 as long as the minimum tour. The algorithm makes use of Edmondsâ€™s
algorithm for maximum matching in a general graph (see the reference at the end of Chapter 3). It will be
interesting to see if the factor 3/2 can be further rened.
Polynomial time algorithms are known for other NP-complete problems that guarantee that the answer
obtained will not exceed, by more than a constant factor, the optimum answer. In some cases the guarantees
apply to the dierence between the answer that the algorithm gives and the best one. See the references
below for more information.
129
Chapter 5: NP-completeness
Exercises for section 5.8
1. Consider the following algorithm:
procedure mst2( x:array ofnpoints in the plane);
fallegedly nds a tree of minimum total length that
visits every one of the given points g
ifn=1
thenT:=fx1g
else
T:=mst2(nâˆ’1;xâˆ’xn);
letube the vertex of Tthat is nearest to xn;
mst2:=Tplus vertex xnplus edge (xn;u)
end.fmst2g
Is this algorithm a correct recursive formulation of the minimum spanning tree greedy algorithm? If so then
prove it, and if not then give an example of a set of points where mst2 gets the wrong answer.
Bibliography
Before we list some books and journal articles it should be mentioned that research in the area of
NP-completeness is moving rapidly, and the state of the art is changing all the time. Readers who would
like updates on the subject are referred to a series of articles that have appeared in issues of the Journalof Algorithms in recent years. These are called â€˜NP-completeness: An ongoing guide.â€™ They are written
by David S. Johnson, and each of them is a thorough survey of recent progress in one particular area of
NP-completeness research. They are written as updates of the rst reference below.
Journals that contain a good deal of research on the areas of this chapter include the Journal of Algo-
rithms, the Journal of the Association for Computing Machinery, the SIAM Journal of Computing, Infor-mation Processing Letters, and SIAM Journal of Discrete Mathematics.
The most complete reference on NP-completeness is
M. Garey and D. S. Johnson, Computers and Intractability; A guide to the theory of NP-completeness ,W .
H. Freeman and Co., San Francisco, 1979.
The above is highly recommended. It is readable, careful and complete.
The earliest ideas on the computational intractability of certain problems go back to
Alan Turing, On computable numbers, with an application to the Entscheidungsproblem , Proc. London
Math. Soc., Ser. 2, 42(1936), 230-265.
Cookâ€™s theorem, which originated the subject of NP-completeness, is in
S. A. Cook, The complexity of theorem proving procedures, Proc., Third Annual ACM Symposium on the
Theory of Computing, ACM, New York, 1971, 151-158.
After Cookâ€™s work was done, a large number of NP-complete problems were found by
Richard M. Karp, Reducibility among combinatorial problems, in R. E. Miller and J. W. Thatcher, eds.,
Complexity of Computer Computations , Plenum, New York, 1972, 85-103.
The above paper is recommended both for its content and its clarity of presentation.The approximate algorithm for the travelling salesman problem is in
D. J. Rosencrantz, R. E. Stearns and P. M. Lewis, An analysis of several heuristics for the travelling salesman
problem, SIAM J. Comp. 6, 1977, 563-581.
Another approximate algorithm for the Euclidean TSP which guarantees that the solution found is no more
than 3/2 as long as the optimum tour, was found by
N. Cristodes, Worst case analysis of a new heuristic for the travelling salesman problem, Technical Report,
Graduate School of Industrial Administration, Carnegie-Mellon University, Pittsburgh, 1976.
The minimum spanning tree algorithm is due toR. C. Prim, Shortest connection netwroks and some generalizations, Bell System Tech. J. 36(1957), 1389-
1401.
The probabilistic algorithm for the Hamilton path problem can be found in
130
5.7 Backtracking (II): graph coloring
D. Angluin and L. G. Valiant, Fast probabilistic algorithms for Hamilton circuits and matchings, Proc. Ninth
Annual ACM Symposium on the Theory of Computing, ACM, New York, 1977.
The result that the graph coloring problem can be done in constant average time is due to
H. Wilf, Backtrack: An O(1) average time algorithm for the graph coloring problem, Information Processing
Letters 18(1984), 119-122.
Further renements of the above result can be found in
E. Bender and H. S. Wilf, A theoretical analysis of backtracking in the graph coloring problem, Journal of
Algorithms 6(1985), 275-282.
If you enjoyed the average numbers of independent sets and average complexity of backtrack, you mightenjoy the subject of random graphs. An excellent introduction to the subject is
Edgar M. Palmer, Graphical Evolution, An introduction to the theory of random graphs, Wiley-Interscience,
New York, 1985.
131
Index
Index
adjacent 40
Adleman, L. 149, 164, 165, 176
Aho, A. V. 103Angluin, D. 208-211, 227
Appel, K. 69
average complexity 57, 211 .
backtracking 211 .
Bender, E. 227
Bentley, J. 54
Berger, R. 3big oh 9
binary system 19
bin-packing 178binomial theorem 37bipartite graph 44, 182
binomial coecients 35
|, growth of 38
blocking ï¬‚ow 124
Burnsideâ€™s lemma 46
cardinality 35
canonical factorization 138
capacity of a cut 115
Carmichael numbers 158certicate 171, 182, 193Cherkassky, B. V. 135
Chinese remainder theorem 154
chromatic number 44chromatic polynomial 73
Cohen, H. 176
coloring graphs 43complement of a graph 44
complexity 1
|, worst-case 4
connected 41Cook, S. 187, 194-201, 226
Cookâ€™s theorem 195 .
Cooley, J. M. 103Coppersmith, D. 99
cryptography 165
Cristodes, N. 224, 227cut in a network 115
|, capacity of 115
cycle 41cyclic group 152
decimal system 19
decision problem 181
degree of a vertex 40deterministic 193
Die, W. 176
digraph 105Dinic, E. 108, 134divide 137
Dixon, J. D. 170, 175, 177
domino problem 3
â€˜easyâ€™ computation 1
edge coloring 206
edge connectivity 132
132
Index
Edmonds, J. 107, 134, 224
Enslein, K. 103
Euclidean algorithm 140, 168
|, complexity 142
|, extended 144 .
Euler totient function 138, 157
Eulerian circuit 41
Even, S. 135
exponential growth 13
factor base 169
Fermatâ€™s theorem 152, 159
FFT, complexity of 93
|, applications of 95 .
Fibonacci numbers 30, 76, 144
ï¬‚ow 106
|, value of 106
|, augmentation 109
|, blocking 124
ï¬‚ow augmenting path 109
Ford-Fulkerson algorithm 108 .
Ford, L. 107 .
four-color theorem 68
Fourier transform 83 .
|, discrete 83
|, inverse 96
Fulkerson, D. E. 107 .
Galil, Z. 135
Gardner, M. 2
Garey, M. 188
geometric series 23
Gomory, R. E. 136
graphs 40 .
|, coloring of 43, 183, 216 .
|, connected 41
|, complement of 44
|, complete 44
|, empty 44
|, bipartite 44
|, planar 70
greatest common divisor 138
group of units 151
Haken, W. 69
Hamiltonian circuit 41, 206, 208 .
Hardy, G. H. 175
height of network 125
Hellman, M. E. 176
hexadecimal system 21
hierarchy of growth 11
Hoare, C. A. R. 51
Hopcroft, J. 70, 103
H u ,T .C . 1 3 6
independent set 61, 179, 211 .
intractable 5
Johnson, D. S. 188, 225, 226
Karp, R. 107, 134, 205, 226
Karzanov, A. 134
Knuth, D. E. 102
KÂ¨onig, H. 103
133
Index
k-subset 35
language 182
Lawler, E. 99
layered network 120 .
Lenstra, H. W., Jr. 176
LeVeque, W. J. 175
L e w i s ,P .A .W . 1 0 3
Lewis, P. M. 227
Lâ€™Hospitalâ€™s rule 12
little oh 8
Lomuto, N. 54
Maheshwari, S. N. 108 ., 135
Malhotra, V. M. 108 ., 135
matrix multiplication 77 .
max-ï¬‚ow-min-cut 115
maximum matching 130
minimum spanning tree 221
moderately exponential growth 12
MPM algorithm 108, 128 .
MST 221
multigraph 42
network 105
| ï¬‚ow 105 .
|, dense 107
|, layered 108, 120 .
|, height of 125
Nijenhuis, A. 60
nondeterministic 193
NP 182NP-complete 61, 180
NP-completeness 178 .
octal system 21
optimization problem 181
orders of magnitude 6 .
P 182
Palmer, E. M. 228
Pan, V. 103
Pascalâ€™s triangle 36
path 41
periodic function 87
polynomial time 2, 179, 185
polynomials, multiplication of 96
Pomerance, C. 149, 164, 176positional number systems 19 .
Pramodh-Kumar, M. 108 ., 135
Pratt, V. 171, 172
Prim, R. C. 227
primality, testing 6, 148 ., 186
|, proving 170
prime number 5
primitive root 152
pseudoprimality test 149, 156 .
|, strong 158
public key encryption 150, 165
Quicksort 50 .
Rabin, M. O. 149, 162, 175
Ralston, A. 103
134
Index
recurrence relations 26 .
recurrent inequality 31recursive algorithms 48 .
reducibility 185relatively prime 138ringZ
n151.
Rivest, R. 165, 176roots of unity 86Rosenkrantz, D. 227RSA system 165, 168Rumely, R. 149, 164, 176Runge, C. 103
SAT 195
satisability 187, 195scanned vertex 111SchÂ¨onhage, A. 103
Selfridge, J. 176Shamir, A. 165, 176slowsort 50Solovay, R. 149, 162, 176splitter 52
Stearns, R. E. 227
Stirlingâ€™s formula 16, 216Strassen, V. 78, 103, 149, 162, 176synthetic division 86
3SAT 201target sum 206
Tarjan, R. E. 66, 70, 103, 135 (â€˜Theta ofâ€™) 10tiling 2tractable 5travelling salesman problem 178, 184, 221tree 45Trojanowski, A. 66, 103â€˜TSPâ€™ 178, 221Tukey, J. W. 103Turing, A. 226Turing machine 187 .
Ullman, J. D. 103
usable edge 111Valiant, L. 208-11, 227vertices 40Vizing, V. 206
Wagsta, S. 176
Welch, P. D. 103Wilf, H. 60, 103, 227, 228Winograd, S. 99worst-case 4, 180Wright, E. M. 175
135
